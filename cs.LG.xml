<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.08133</link><description>&lt;p&gt;
MeshDiffusion&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MeshDiffusion: Score-based Generative 3D Mesh Modeling. (arXiv:2303.08133v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#24335;3D&#32593;&#26684;&#24314;&#27169;&#26041;&#27861;&#65292;&#20381;&#36182;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#21518;&#22788;&#29702;&#30340;&#21069;&#25552;&#19979;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#33410;&#20016;&#23500;&#30340;3D&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;3D&#29289;&#20307;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#22330;&#26223;&#29983;&#25104;&#21644;&#29289;&#29702;&#20223;&#30495;&#31561;&#22810;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#30456;&#27604;&#20110;&#20307;&#32032;&#21644;&#28857;&#20113;&#31561;&#20854;&#20182;3D&#34920;&#31034;&#65292;&#32593;&#26684;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#20248;&#36234;&#65292;&#22240;&#20026;(1)&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#20219;&#24847;&#22320;&#25805;&#32437;&#24418;&#29366;&#20197;&#20379;&#37325;&#26032;&#29031;&#26126;&#21644;&#20223;&#30495;&#65292;(2)&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;&#29616;&#20195;&#22270;&#24418;&#27969;&#27700;&#32447;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#27969;&#27700;&#32447;&#22823;&#22810;&#25968;&#38024;&#23545;&#32593;&#26684;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#20197;&#24448;&#21487;&#25193;&#23637;&#30340;3D&#32593;&#26684;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#27425;&#20248;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#19988;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#36807;&#20110;&#24179;&#28369;&#25110;&#22024;&#26434;&#30340;&#34920;&#38754;&#65292;&#32570;&#20047;&#31934;&#32454;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#32593;&#26684;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#20351;&#29992;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;3D&#32593;&#26684;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#21464;&#24418;&#22235;&#38754;&#20307;&#32593;&#26684;&#26469;&#34920;&#31034;&#32593;&#26684;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#32593;&#26684;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parametrization. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08117</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#22312;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#26102;&#26159;&#21542;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#31867;&#20284;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36825;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23545;&#35821;&#35328;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20381;&#36182;&#20851;&#31995;&#21644;&#32452;&#25104;&#25104;&#20998;&#20998;&#26512;&#26641;&#12290;&#20294;&#26159;&#20154;&#20204;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#25110;&#20165;&#36827;&#34892;&#19982;&#35299;&#26512;&#24369;&#30456;&#20851;&#30340;&#19968;&#20123;&#35745;&#31639;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#19968;&#27493;&#27493;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;(a)&#26159;&#21542;&#26377;&#21487;&#33021;&#26126;&#30830;&#25551;&#36848;&#20855;&#26377;&#29616;&#23454;&#23884;&#20837;&#32500;&#24230;&#65292;&#22836;&#25968;&#31561;&#30340;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#36827;&#34892;&#35299;&#26512;&#29978;&#33267;&#36817;&#20284;&#35299;&#26512;&#65307;(b)&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#20160;&#20040;&#33021;&#22815;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#20013;&#31561;&#22823;&#23567;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#65288;Marcus&#31561;&#65292;1993&#65289;&#30340;Inside-Outside&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#22312;PCFG&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#19978;&#65292;Inside-Outside&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#20943;&#23569;&#26412;&#22320;&#37327;&#23376;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#27604;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#24555;&#22320;&#23454;&#29616;&#35757;&#32451;&#25910;&#25947;&#21644;&#25552;&#39640;&#27979;&#35797;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08116</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimizing Quantum Federated Learning Based on Federated Quantum Natural Gradient Descent. (arXiv:2303.08116v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#20943;&#23569;&#26412;&#22320;&#37327;&#23376;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#27604;&#20256;&#32479;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26356;&#24555;&#22320;&#23454;&#29616;&#35757;&#32451;&#25910;&#25947;&#21644;&#25552;&#39640;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#26159;&#22312;&#22810;&#20010;&#26412;&#22320;&#37327;&#23376;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#30340;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#37327;&#23376;&#25193;&#23637;&#12290;&#19968;&#20010;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#26368;&#23567;&#21270;&#19981;&#21516;&#37327;&#23376;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#32852;&#37030;&#37327;&#23376;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#65288;FQNGD&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30001;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#32452;&#25104;&#30340;QFL&#26694;&#26550;&#20013;&#12290;&#19982;Adam&#21644;Adagrad&#31561;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#27604;&#65292;FQNGD&#31639;&#27861;&#25152;&#38656;&#30340;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#35201;&#23569;&#24471;&#22810;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26412;&#22320;&#37327;&#23376;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;FQNGD&#23545;&#20110;QFL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#36739;&#39640;&#30340;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum federated learning (QFL) is a quantum extension of the classical federated learning model across multiple local quantum devices. An efficient optimization algorithm is always expected to minimize the communication overhead among different quantum participants. In this work, we propose an efficient optimization algorithm, namely federated quantum natural gradient descent (FQNGD), and further, apply it to a QFL framework that is composed of a variational quantum circuit (VQC)-based quantum neural networks (QNN). Compared with stochastic gradient descent methods like Adam and Adagrad, the FQNGD algorithm admits much fewer training iterations for the QFL to get converged. Moreover, it can significantly reduce the total communication overhead among local quantum devices. Our experiments on a handwritten digit classification dataset justify the effectiveness of the FQNGD for the QFL framework in terms of a faster convergence rate on the training set and higher accuracy on the test se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;Simfluence&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#26679;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#20363;&#65292;&#20174;&#32780;&#30740;&#31350;&#35757;&#32451;&#26679;&#20363;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.08114</link><description>&lt;p&gt;
Simfluence&#65306;&#36890;&#36807;&#27169;&#25311;&#35757;&#32451;&#36807;&#31243;&#24314;&#27169;&#21333;&#20010;&#35757;&#32451;&#26679;&#20363;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. (arXiv:2303.08114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;Simfluence&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#26679;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#20363;&#65292;&#20174;&#32780;&#30740;&#31350;&#35757;&#32451;&#26679;&#20363;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;TDA&#65289;&#26041;&#27861;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#20998;&#37197;&#19968;&#20010;&#26631;&#37327;&#24433;&#21709;&#20998;&#25968;&#26469;&#23454;&#29616;&#65292;&#20551;&#35774;&#24433;&#21709;&#26159;&#21487;&#21152;&#30340;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#35832;&#22914;&#31034;&#20363;&#20043;&#38388;&#30340;&#20887;&#20313;&#12289;&#35757;&#32451;&#39034;&#24207;&#21644;&#35838;&#31243;&#23398;&#20064;&#25928;&#24212;&#31561;&#22240;&#32032;&#65292;&#35757;&#32451;&#31034;&#20363;&#20197;&#39640;&#24230;&#38750;&#21487;&#21152;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Simfluence&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;TDA&#33539;&#24335;&#65292;&#30446;&#26631;&#19981;&#26159;&#20026;&#27599;&#20010;&#31034;&#20363;&#29983;&#25104;&#21333;&#20010;&#30340;&#24433;&#21709;&#20998;&#25968;&#65292;&#32780;&#26159;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65306;&#29992;&#25143;&#21487;&#20197;&#35810;&#38382;&#8220;&#22914;&#26524;&#25105;&#30340;&#27169;&#22411;&#35757;&#32451;&#20102;&#31034;&#20363; z1&#12289;z2&#12289;&#8230;&#8230;&#12289;zn&#65292;&#23427;&#22312;&#31034;&#20363;ztest&#19978;&#30340;&#34920;&#29616;&#20250;&#22914;&#20309;&#65311;&#8221;&#65307;&#28982;&#21518;&#27169;&#25311;&#22120;&#24212;&#35813;&#36755;&#20986;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#27169;&#25311;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#39044;&#27979;&#20102;&#22312;ztest&#19978;&#30340;&#25439;&#22833;&#12290;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.  To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.08112</link><description>&lt;p&gt;
&#29992;&#35843;&#35856;&#36879;&#38236;&#20174;Transformer&#20013;&#33719;&#21462;&#28508;&#22312;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36845;&#20195;&#25512;&#29702;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;transformers&#27169;&#22411;&#65292;&#26088;&#22312;&#20102;&#35299;&#27169;&#22411;&#39044;&#27979;&#26159;&#22914;&#20309;&#36880;&#23618;&#36827;&#34892;&#31934;&#21270;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#22359;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#26159;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21069;&#32773;&#32473;&#20986;&#20102;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20294;&#24120;&#24120;&#26131;&#30862;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20855;&#26377;&#22810;&#36798;20B&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#34920;&#26126;&#20854;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#26174;&#31034;&#65292;&#35843;&#35856;&#36879;&#38236;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/AlignmentResearch/tuned-lens &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;FlyHash&#27169;&#22411;&#21644;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;FlyHash&#27169;&#22411;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.08109</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#12289;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#32447;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Vision-based route following by an embodied insect-inspired sparse neural network. (arXiv:2303.08109v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;FlyHash&#27169;&#22411;&#21644;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#21457;&#29616;FlyHash&#27169;&#22411;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31181;&#21483;&#20570;FlyHash&#27169;&#22411;&#65288;Dasgupta et al., 2017&#65289;&#30340;&#20223;&#29983;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#38750;&#31232;&#30095;&#27169;&#22411;&#22312;&#36335;&#32447;&#36319;&#38543;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#20219;&#21153;&#38656;&#35201;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#30340;&#35270;&#35273;&#36755;&#20837;&#21644;&#27839;&#36884;&#23384;&#20648;&#30340;&#35760;&#24518;&#26469;&#25511;&#21046;&#36716;&#21521;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;FlyHash&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#32534;&#30721;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compared the efficiency of the FlyHash model, an insect-inspired sparse neural network (Dasgupta et al., 2017), to similar but non-sparse models in an embodied navigation task. This requires a model to control steering by comparing current visual inputs to memories stored along a training route. We concluded the FlyHash model is more efficient than others, especially in terms of data encoding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#20986;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#24456;&#22823;&#24433;&#21709;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#28304;&#22495;&#20013;&#23398;&#20064;&#27169;&#22411;&#24182;&#36866;&#24212;&#26032;&#30340;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08106</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;&#65306;&#27010;&#24565;&#65292;&#29616;&#29366;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization in Machine Learning Models for Wireless Communications: Concepts, State-of-the-Art, and Open Issues. (arXiv:2303.08106v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#24212;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#20986;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#24456;&#22823;&#24433;&#21709;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#28304;&#22495;&#20013;&#23398;&#20064;&#27169;&#22411;&#24182;&#36866;&#24212;&#26032;&#30340;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34987;&#35748;&#20026;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#19968;&#31181;&#28508;&#22312;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#65292;&#24212;&#29992;ML&#25216;&#26415;&#35299;&#20915;&#26080;&#32447;&#20256;&#36755;&#38142;&#30340;&#19981;&#21516;&#23618;&#27425;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#24212;&#29992;&#37117;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#65292;&#20854;&#20551;&#23450;&#28304;&#65288;&#35757;&#32451;&#65289;&#21644;&#30446;&#26631;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#29420;&#31435;&#19988;&#38543;&#26426;&#20998;&#24067;&#30456;&#21516;&#65288;i.i.d.&#65289;&#12290;&#36825;&#20010;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#22240;&#20026;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#25110;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39046;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#36890;&#36807;&#22312;&#19981;&#21516;&#21644;&#29420;&#29305;&#30340;&#28304;&#22495;/&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;OOD&#30456;&#20851;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;&#21463;&#21040;&#23545;&#26080;&#32447;&#24212;&#29992;&#20013;DG&#38656;&#27714;&#30340;&#37325;&#35270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36890;&#29992;&#24615;&#30340;&#27010;&#24565;&#21644;&#20854;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#21516;&#26102;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven machine learning (ML) is promoted as one potential technology to be used in next-generations wireless systems. This led to a large body of research work that applies ML techniques to solve problems in different layers of the wireless transmission link. However, most of these applications rely on supervised learning which assumes that the source (training) and target (test) data are independent and identically distributed (i.i.d). This assumption is often violated in the real world due to domain or distribution shifts between the source and the target data. Thus, it is important to ensure that these algorithms generalize to out-of-distribution (OOD) data. In this context, domain generalization (DG) tackles the OOD-related issues by learning models on different and distinct source domains/datasets with generalization capabilities to unseen new domains without additional finetuning. Motivated by the importance of DG requirements for wireless applications, we present a comprehe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08103</link><description>&lt;p&gt;
&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24212;&#29992;&#65288;&#22914;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65289;&#36890;&#24120;&#38754;&#20020;&#26631;&#35760;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20803;&#23545;&#27604;&#26631;&#31614;&#26657;&#27491;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23558;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#34701;&#20837;&#20803;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;Gramian angular field&#21644;&#20195;&#34920;&#23398;&#20064;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21487;&#20197;&#20351;&#24471;&#26576;&#20123;&#31639;&#27861;&#30340;&#36951;&#25022;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30028;&#38480;&#65292;&#24182;&#32473;&#20986;&#20102;&#19979;&#38480;&#35777;&#26126;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08102</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Regret Bounds for Bandits with Fixed Expert Advice. (arXiv:2303.08102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22266;&#23450;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21487;&#20197;&#20351;&#24471;&#26576;&#20123;&#31639;&#27861;&#30340;&#36951;&#25022;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30028;&#38480;&#65292;&#24182;&#32473;&#20986;&#20102;&#19979;&#38480;&#35777;&#26126;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19987;&#23478;&#26159;&#22266;&#23450;&#21644;&#24050;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#36172;&#21338;&#26426;&#19982;&#19987;&#23478;&#24314;&#35758;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#19987;&#23478;&#26159;&#34892;&#21160;&#22266;&#23450;&#21644;&#24050;&#30693;&#20998;&#24067;&#12290;&#30456;&#27604;&#20197;&#21069;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#36951;&#25022;&#26159;&#30001;&#34913;&#37327;&#19987;&#23478;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#20449;&#24687;&#35770;&#37327;&#25152;&#25511;&#21046;&#30340;&#12290;&#22312;&#19968;&#20123;&#33258;&#28982;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;EXP4&#30340;&#31532;&#19968;&#20010;&#36951;&#25022;&#30028;&#38480;&#65292;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#30456;&#20284;&#65292;&#21017;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#20110;&#38646;&#12290;&#20026;&#21478;&#19968;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;KL&#25955;&#24230;&#26469;&#25551;&#36848;&#19987;&#23478;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#21478;&#19968;&#31181;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#30028;&#38480;&#21487;&#20197;&#27604;EXP4&#26356;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#26576;&#20123;&#19987;&#23478;&#31867;&#21035;&#25552;&#20379;&#20102;&#19979;&#38480;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#20998;&#26512;&#30340;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20960;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of bandits with expert advice when the experts are fixed and known distributions over the actions. Improving on previous analyses, we show that the regret in this setting is controlled by information-theoretic quantities that measure the similarity between experts. In some natural special cases, this allows us to obtain the first regret bound for EXP4 that can get arbitrarily close to zero if the experts are similar enough. While for a different algorithm, we provide another bound that describes the similarity between the experts in terms of the KL-divergence, and we show that this bound can be smaller than the one of EXP4 in some cases. Additionally, we provide lower bounds for certain classes of experts showing that the algorithms we analyzed are nearly optimal in some cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#26469;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#21542;&#36234;&#30028;&#65292;&#22312;&#27604;&#36739;&#20013;&#21457;&#29616;&#20854;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08081</link><description>&lt;p&gt;
&#35299;&#37322;&#20301;&#31227;&#65306;&#30740;&#31350;&#27169;&#22411;&#19982;&#36716;&#31227;&#25968;&#25454;&#20998;&#24067;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation Shift: Investigating Interactions between Models and Shifting Data Distributions. (arXiv:2303.08081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#26469;&#26816;&#27979;&#20998;&#24067;&#36716;&#31227;&#19979;&#23398;&#20064;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#21542;&#36234;&#30028;&#65292;&#22312;&#27604;&#36739;&#20013;&#21457;&#29616;&#20854;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#20026;&#20248;&#31168;&#65292;&#25552;&#20379;&#20102;&#31639;&#27861;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#24448;&#24448;&#20250;&#19979;&#38477;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#26032;&#30340;&#36755;&#20837;&#25968;&#25454;&#24448;&#24448;&#27809;&#26377;&#30446;&#26631;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#27169;&#22411;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#25110;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#35797;&#22270;&#29702;&#35299;&#23398;&#20064;&#27169;&#22411;&#21644;&#36716;&#31227;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#27169;&#22411;&#22914;&#20309;&#35299;&#37322;&#29305;&#24449;&#30340;&#36716;&#31227;&#24615;&#36136;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35299;&#37322;&#20301;&#31227;&#30340;&#24314;&#27169;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#22909;&#22320;&#25351;&#31034;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#21512;&#25104;&#31034;&#20363;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#26041;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;Python&#21253;&#20013;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
As input data distributions evolve, the predictive performance of machine learning models tends to deteriorate. In practice, new input data tend to come without target labels. Then, state-of-the-art techniques model input data distributions or model prediction distributions and try to understand issues regarding the interactions between learned models and shifting distributions. We suggest a novel approach that models how explanation characteristics shift when affected by distribution shifts. We find that the modeling of explanation shifts can be a better indicator for detecting out-of-distribution model behaviour than state-of-the-art techniques. We analyze different types of distribution shifts using synthetic examples and real-world data sets. We provide an algorithmic method that allows us to inspect the interaction between data set features and learned models and compare them to the state-of-the-art. We release our methods in an open-source Python package, as well as the code used
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.08068</link><description>&lt;p&gt;
&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39118;&#26684;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#65288;&#22914;&#39118;&#26684;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26080;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#21487;&#20197;&#25552;&#21462;&#39118;&#26684;&#65292;&#20294;&#25552;&#21462;&#30340;&#39118;&#26684;&#36890;&#24120;&#19982;&#20854;&#20182;&#29305;&#24449;&#28151;&#21512;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20998;&#31867;&#26631;&#31614;&#26469;&#25351;&#23548;VAEs&#25552;&#21462;&#39118;&#26684;&#65292;&#21363;&#26465;&#20214;VAEs&#65288;CVAEs&#65289;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20165;&#25552;&#21462;&#39118;&#26684;&#30340;&#26041;&#27861;&#23578;&#26410;&#24314;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;CVAE&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20165;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22823;&#33268;&#30001;&#20004;&#20010;&#24182;&#34892;&#37096;&#20998;&#32452;&#25104;; &#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#37096;&#20998;&#65292;&#20197;&#21450;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#12290;CL&#27169;&#22411;&#36890;&#24120;&#20197;&#26080;&#38656;&#25968;&#25454;&#25193;&#20805;&#30340;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#19982;&#26679;&#24335;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#35270;&#20026;&#26679;&#24335;&#20013;&#30340;&#25200;&#21160;&#12290;&#20197;&#25552;&#21462;&#30340;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#20026;&#26465;&#20214;&#65292;CVAE&#23398;&#20064;&#20165;&#25552;&#21462;&#39118;&#26684;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20808;&#35757;&#32451;CL&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;CL&#27169;&#22411;&#25351;&#23548;CVAE&#30340;&#35757;&#32451;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to extract fine-grained features such as styles from unlabeled data in data analysis. Unsupervised methods, such as variational autoencoders (VAEs), can extract styles, but the extracted styles are usually mixed with other features. We can isolate the styles using VAEs conditioned by class labels, known as conditional VAEs (CVAEs). However, methods to extract only styles using unlabeled data are not established. In this paper, we construct a CVAE-based method that extracts style features using only unlabeled data. The proposed model roughly consists of two parallel parts; a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. CL models generally learn representations independent of data augmentation, which can be seen as a perturbation in styles, in a self-supervised way. Taking the style-independent features as a condition, the CVAE learns to extract only styles. In the training procedure, a CL model is tra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.08040</link><description>&lt;p&gt;
&#12298;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#26816;&#26597;&#21592;&#65306;&#36890;&#36807;&#35299;&#37322;&#31354;&#38388;&#36827;&#34892;&#20844;&#24179;&#23457;&#26680;&#12299;
&lt;/p&gt;
&lt;p&gt;
Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08040
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#65292;&#25552;&#39640;&#20102;&#23457;&#35745;&#20844;&#24179;&#24615;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#20855;&#26377;&#26368;&#22909;&#30340;&#24847;&#22270;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20063;&#21487;&#33021;&#24310;&#32493;&#12289;&#25918;&#22823;&#29978;&#33267;&#21019;&#36896;&#31038;&#20250;&#20559;&#35265;&#12290;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27495;&#35270;&#24615;&#65288;&#38750;&#27495;&#35270;&#24615;&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#27495;&#35270;&#25928;&#26524;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20195;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;&#20998;&#32452;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#36829;&#35268;&#24773;&#20917;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#26816;&#26597;&#32452;&#38388;&#27495;&#35270;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#24819;&#65292;&#21363;&#22522;&#20110;&#35299;&#37322;&#31354;&#38388;&#23545;&#27169;&#22411;&#23545;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#20381;&#36182;&#24230;&#36827;&#34892;&#27979;&#37327;&#65292;&#35299;&#37322;&#31354;&#38388;&#26159;&#19968;&#31181;&#25552;&#20379;&#27604;&#36755;&#20837;&#25968;&#25454;&#25110;&#39044;&#27979;&#20998;&#24067;&#30340;&#21407;&#22987;&#31354;&#38388;&#26356;&#25935;&#24863;&#23457;&#35745;&#30340;&#20449;&#24687;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#26029;&#35328;&#29702;&#35770;&#19978;&#30340;&#20154;&#21475;&#32479;&#35745;&#23457;&#26680;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12289;&#21512;&#25104;&#26679;&#20363;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;Pytorch&#23454;&#29616;&#21644;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even if deployed with the best intentions, machine learning methods can perpetuate, amplify or even create social biases. Measures of (un-)fairness have been proposed as a way to gauge the (non-)discriminatory nature of machine learning models. However, proxies of protected attributes causing discriminatory effects remain challenging to address. In this work, we propose a new algorithmic approach that measures group-wise demographic parity violations and allows us to inspect the causes of inter-group discrimination. Our method relies on the novel idea of measuring the dependence of a model on the protected attribute based on the explanation space, an informative space that allows for more sensitive audits than the primary space of input data or prediction distributions, and allowing for the assertion of theoretical demographic parity auditing guarantees. We provide a mathematical analysis, synthetic examples, and experimental evaluation of real-world data. We release an open-source Pyt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.08035</link><description>&lt;p&gt;
ISimDL: &#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#39537;&#21160;&#30340;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24378;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ISimDL&#65292;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#65292;&#26377;&#25928;&#35780;&#20272;&#20102;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#31995;&#32479;&#24050;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#38656;&#35201;&#19987;&#29992;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#33455;&#29255;&#12290;&#22312;&#32435;&#31859;&#26102;&#20195;&#65292;&#35774;&#22791;&#36234;&#26469;&#36234;&#23481;&#26131;&#21463;&#21040;&#27704;&#20037;&#24615;&#21644;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#27492;&#31867;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#24182;&#20102;&#35299;&#31070;&#32463;&#21152;&#36895;&#22120;&#33455;&#29255;&#20013;&#30340;&#25925;&#38556;&#22914;&#20309;&#22312;DL&#24212;&#29992;&#32423;&#21035;&#19978;&#34920;&#29616;&#20026;&#38169;&#35823;&#65292;&#20854;&#20013;&#25925;&#38556;&#21487;&#33021;&#23548;&#33268;&#26080;&#27861;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25925;&#38556;&#27880;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#36719;&#20214;&#32423;&#21035;&#20462;&#25913;&#31070;&#32463;&#20803;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#25191;&#34892;DL&#31995;&#32479;&#30340;&#38887;&#24615;&#30740;&#31350;&#65292;&#23601;&#22909;&#20687;&#30828;&#20214;&#21463;&#21040;&#30636;&#21464;&#25925;&#38556;&#30340;&#24433;&#21709;&#19968;&#26679;&#12290;&#29616;&#26377;&#30340;&#25925;&#38556;&#27169;&#22411;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#20998;&#26512;&#26356;&#24555;&#65292;&#20294;&#38656;&#35201;&#35813;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#20801;&#35768;&#36827;&#19968;&#27493;&#20998;&#26512;&#31579;&#36873;&#20986;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ISimDL&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#31070;&#32463;&#20803;&#28789;&#25935;&#24230;&#29983;&#25104;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#21152;&#36895;&#25925;&#38556;&#27880;&#20837;&#27169;&#25311;&#12290;ISimDL&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;&#20808;&#36827;&#30340;DL&#31995;&#32479;&#23545;&#30828;&#20214;&#25925;&#38556;&#30340;&#38887;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20943;&#23569;&#20102;&#25925;&#38556;&#27880;&#20837;&#20998;&#26512;&#25152;&#38656;&#30340;&#27169;&#25311;&#25968;&#37327;&#65292;&#21516;&#26102;&#20173;&#30830;&#20445;&#36275;&#22815;&#35206;&#30422;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;ISimDL&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#23427;&#25552;&#20379;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate impo
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item><item><title>EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08028</link><description>&lt;p&gt;
EdgeServe:&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#25191;&#34892;&#23618;
&lt;/p&gt;
&lt;p&gt;
EdgeServe: An Execution Layer for Decentralized Prediction. (arXiv:2303.08028v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08028
&lt;/p&gt;
&lt;p&gt;
EdgeServe &#26159;&#19968;&#31181;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#32780;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#23558;&#25968;&#25454;&#36335;&#30001;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;&#23427;&#20855;&#26377;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#22312;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#31561;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;EdgeServe &#23637;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#21487;&#33021;&#26469;&#33258;&#20110;&#32593;&#32476;&#20013;&#19981;&#21516;&#33410;&#28857;&#25910;&#38598;&#30340;&#25968;&#25454;&#28304;&#12290;&#36825;&#31181;&#38382;&#39064;&#34987;&#31216;&#20043;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#65292;&#24182;&#22312;&#25968;&#25454;&#36335;&#30001;&#12289;&#35745;&#31639;&#24067;&#23616;&#21644;&#26102;&#38388;&#21516;&#27493;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#26377;&#36259;&#30340;&#31995;&#32479;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EdgeServe&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#20197;&#20026;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#25552;&#20379;&#26381;&#21153;&#12290; EdgeServe &#20381;&#36182;&#20110;&#19968;&#20010;&#20302;&#24310;&#36831;&#30340;&#28040;&#24687;&#20195;&#29702;&#31243;&#24207;&#65292;&#36890;&#36807;&#32593;&#32476;&#36335;&#30001;&#25968;&#25454;&#21040;&#21487;&#20197;&#25552;&#20379;&#39044;&#27979;&#30340;&#33410;&#28857;&#12290;EdgeServe &#20381;&#36182;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#22312;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#34935;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21435;&#20013;&#24515;&#21270;&#39044;&#27979;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;EdgeServe&#65306;&#65288;1&#65289;&#22810;&#25668;&#20687;&#26426;&#29289;&#20307;&#36319;&#36394;&#65292;&#65288;2&#65289;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21644;&#65288;3&#65289;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relevant features for a machine learning task may be aggregated from data sources collected on different nodes in a network. This problem, which we call decentralized prediction, creates a number of interesting systems challenges in managing data routing, placing computation, and time-synchronization. This paper presents EdgeServe, a machine learning system that can serve decentralized predictions. EdgeServe relies on a low-latency message broker to route data through a network to nodes that can serve predictions. EdgeServe relies on a series of novel optimizations that can tradeoff computation, communication, and accuracy. We evaluate EdgeServe on three decentralized prediction tasks: (1) multi-camera object tracking, (2) network intrusion detection, and (3) human activity recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38142;&#24335;&#22238;&#24402;&#27169;&#22411;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#29190;&#21457;&#20013;&#35782;&#21035;&#24773;&#24863;&#12290;&#35813;&#26694;&#26550;&#26126;&#30830;&#32771;&#34385;&#20102;&#25991;&#21270;&#12289;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#24863;&#31354;&#38388;&#20197;&#21450;&#19981;&#21516;&#24773;&#24863;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08027</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35821;&#38899;&#29190;&#21457;&#35782;&#21035;&#30340;&#20998;&#23618;&#22238;&#24402;&#38142;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Regression Chain Framework for Affective Vocal Burst Recognition. (arXiv:2303.08027v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38142;&#24335;&#22238;&#24402;&#27169;&#22411;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#29190;&#21457;&#20013;&#35782;&#21035;&#24773;&#24863;&#12290;&#35813;&#26694;&#26550;&#26126;&#30830;&#32771;&#34385;&#20102;&#25991;&#21270;&#12289;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#24863;&#31354;&#38388;&#20197;&#21450;&#19981;&#21516;&#24773;&#24863;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24773;&#24863;&#20449;&#21495;&#30340;&#19968;&#31181;&#24120;&#35265;&#24418;&#24335;&#65292;&#38750;&#35821;&#35328;&#35821;&#38899;&#29190;&#21457;&#22312;&#26085;&#24120;&#31038;&#20132;&#20114;&#21160;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#29702;&#35299;&#21644;&#24314;&#27169;&#20154;&#31867;&#30340;&#35821;&#38899;&#29190;&#21457;&#23545;&#20110;&#24320;&#21457;&#24378;&#22823;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25506;&#32034;&#29702;&#35299;&#35821;&#38899;&#29190;&#21457;&#30340;&#35745;&#31639;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38142;&#24335;&#22238;&#24402;&#27169;&#22411;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#38899;&#29190;&#21457;&#20013;&#35782;&#21035;&#24773;&#24863;&#65292;&#26126;&#30830;&#32771;&#34385;&#20102;&#22810;&#20010;&#20851;&#31995;&#65306;&#65288;i&#65289;&#24773;&#24863;&#29366;&#24577;&#21644;&#25991;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#20302;&#32500;&#65288;&#21796;&#37266;&#21644;&#20215;&#20540;&#65289;&#19982;&#39640;&#32500;&#65288;10&#31181;&#24773;&#24863;&#31867;&#21035;&#65289;&#24773;&#24863;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#24773;&#24863;&#31867;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34920;&#31034;&#19982;&#23618;&#27425;&#21644;&#26102;&#38388;&#32858;&#21512;&#27169;&#22359;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21442;&#21152;&#20102;ACII&#24773;&#24863;&#35821;&#38899;&#29190;&#21457;&#27604;&#36187;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a common way of emotion signaling via non-linguistic vocalizations, vocal burst (VB) plays an important role in daily social interaction. Understanding and modeling human vocal bursts are indispensable for developing robust and general artificial intelligence. Exploring computational approaches for understanding vocal bursts is attracting increasing research attention. In this work, we propose a hierarchical framework, based on chain regression models, for affective recognition from VBs, that explicitly considers multiple relationships: (i) between emotional states and diverse cultures; (ii) between low-dimensional (arousal &amp; valence) and high-dimensional (10 emotion classes) emotion spaces; and (iii) between various emotion classes within the high-dimensional space. To address the challenge of data sparsity, we also use self-supervised learning (SSL) representations with layer-wise and temporal aggregation modules. The proposed systems participated in the ACII Affective Vocal Burst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#23618;&#27425;&#34920;&#31034;&#21644;&#20219;&#21153;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08019</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#21644;&#20219;&#21153;&#30456;&#20851;&#20851;&#38190;&#35789;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pretrained Representations with Task-related Keywords for Alzheimer's Disease Detection. (arXiv:2303.08019v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#23618;&#27425;&#34920;&#31034;&#21644;&#20219;&#21153;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#20154;&#21475;&#36805;&#36895;&#32769;&#40836;&#21270;&#65292;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#22312;&#32769;&#24180;&#20154;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#20854;&#20855;&#26377;&#38544;&#21311;&#24615;&#36136;&#65292;&#23548;&#33268;&#35748;&#30693;&#39046;&#22495;&#65288;&#35760;&#24518;&#12289;&#20132;&#27969;&#31561;&#65289;&#36880;&#28176;&#19981;&#21487;&#36870;&#36716;&#30340;&#24694;&#21270;&#12290;&#22522;&#20110;&#35821;&#38899;&#30340;AD&#26816;&#27979;&#20026;&#24191;&#27867;&#31579;&#26597;&#21644;&#21450;&#26102;&#24178;&#39044;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#25512;&#21160;AD&#26816;&#27979;&#24314;&#27169;&#20174;&#20302;&#23618;&#29305;&#24449;&#36716;&#21521;&#39640;&#23618;&#27425;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20174;&#39640;&#23618;&#27425;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#20013;&#25552;&#21462;&#26356;&#22909;&#30340;AD&#30456;&#20851;&#32447;&#32034;&#12290;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#23548;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#25551;&#36848;&#21644;&#35748;&#30693;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#22312;ADReSS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#24182;&#22312;&#26410;&#35265;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#12290;&#32467;&#26524;&#21450;&#19982;&#26368;&#36817;&#25991;&#29486;&#30340;&#27604;&#36739;&#34920;&#26126;&#20102;&#25928;&#29575;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the global population aging rapidly, Alzheimer's disease (AD) is particularly prominent in older adults, which has an insidious onset and leads to a gradual, irreversible deterioration in cognitive domains (memory, communication, etc.). Speech-based AD detection opens up the possibility of widespread screening and timely disease intervention. Recent advances in pre-trained models motivate AD detection modeling to shift from low-level features to high-level representations. This paper presents several efficient methods to extract better AD-related cues from high-level acoustic and linguistic features. Based on these features, the paper also proposes a novel task-oriented approach by modeling the relationship between the participants' description and the cognitive task. Experiments are carried out on the ADReSS dataset in a binary classification setup, and models are evaluated on the unseen test set. Results and comparison with recent literature demonstrate the efficiency and superi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#12289;&#35821;&#20041;&#24863;&#30693;&#30340;&#27874;&#26463;&#36171;&#24418;&#26041;&#26696;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#22826;&#36203;&#20857;&#39057;&#27573;&#20013;&#25351;&#23548;&#27874;&#26463;&#36171;&#24418;&#65292;&#30456;&#36739;&#20110;&#32463;&#20856;&#30340; MIMO &#27874;&#26463;&#36171;&#24418;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.08017</link><description>&lt;p&gt;
&#27874;&#26463;&#36171;&#24418;&#25216;&#26415;&#22312;&#22826;&#36203;&#20857;&#39057;&#27573;&#30340;&#21487;&#38752;&#24615;&#25506;&#35752;&#65306;&#22240;&#26524;&#34920;&#36798;&#24335;&#26159;&#21069;&#36827;&#30340;&#36947;&#36335;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reliable Beamforming at Terahertz Bands: Are Causal Representations the Way Forward?. (arXiv:2303.08017v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21160;&#24577;&#12289;&#35821;&#20041;&#24863;&#30693;&#30340;&#27874;&#26463;&#36171;&#24418;&#26041;&#26696;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#22826;&#36203;&#20857;&#39057;&#27573;&#20013;&#25351;&#23548;&#27874;&#26463;&#36171;&#24418;&#65292;&#30456;&#36739;&#20110;&#32463;&#20856;&#30340; MIMO &#27874;&#26463;&#36171;&#24418;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#26080;&#32447;&#26381;&#21153;&#65292;&#20363;&#22914;&#20803;&#23431;&#23449;&#65292;&#38656;&#35201;&#39640;&#20449;&#24687;&#36895;&#29575;&#12289;&#21487;&#38752;&#24615;&#21644;&#20302;&#24310;&#36831;&#12290;&#21033;&#29992;&#20016;&#23500;&#30340;&#22826;&#36203;&#20857;&#24102;&#23485;&#21644;&#22823;&#37327;&#22825;&#32447;&#21019;&#24314;&#29421;&#31364;&#30340;&#27874;&#26463;&#36171;&#24418;&#26041;&#26696;&#30340;&#22810;&#29992;&#25143;&#26080;&#32447;&#31995;&#32479;&#21487;&#20197;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#32570;&#20047;&#23545;&#20449;&#36947;&#21160;&#24577;&#30340;&#24688;&#24403;&#24314;&#27169;&#65292;&#23548;&#33268;&#22312;&#39640;&#31227;&#21160;&#24615;&#22330;&#26223;&#20013;&#20986;&#29616;&#19981;&#20934;&#30830;&#30340;&#27874;&#26463;&#36171;&#24418;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#12289;&#35821;&#20041;&#24863;&#30693;&#30340;&#27874;&#26463;&#36171;&#24418;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#21464;&#20998;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#26032;&#39062;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#35745;&#31639;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#27874;&#26463;&#36171;&#24418;&#30340;&#22240;&#26524;&#34920;&#36798;&#24335;&#30340;&#26102;&#21464;&#21160;&#24577;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#22240;&#26524;&#25512;&#23548;&#26041;&#27861;&#22312;&#22826;&#36203;&#20857;&#27874;&#26463;&#36171;&#24418;&#25216;&#26415;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#30340;MIMO&#27874;&#26463;&#36171;&#24418;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future wireless services, such as the metaverse require high information rate, reliability, and low latency. Multi-user wireless systems can meet such requirements by utilizing the abundant terahertz bandwidth with a massive number of antennas, creating narrow beamforming solutions. However, existing solutions lack proper modeling of channel dynamics, resulting in inaccurate beamforming solutions in high-mobility scenarios. Herein, a dynamic, semantically aware beamforming solution is proposed for the first time, utilizing novel artificial intelligence algorithms in variational causal inference to compute the time-varying dynamics of the causal representation of multi-modal data and the beamforming. Simulations show that the proposed causality-guided approach for Terahertz (THz) beamforming outperforms classical MIMO beamforming techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38134;&#34892;&#26381;&#21153;&#20013;&#30340;&#25216;&#26415;&#36741;&#21161;&#34384;&#24453;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#35780;&#20998;&#20132;&#26131;&#20197;&#35782;&#21035;&#34384;&#24453;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.08016</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#25551;&#36848;&#20013;&#30340;&#34384;&#24453;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detection of Abuse in Financial Transaction Descriptions Using Machine Learning. (arXiv:2303.08016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38134;&#34892;&#26381;&#21153;&#20013;&#30340;&#25216;&#26415;&#36741;&#21161;&#34384;&#24453;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#35780;&#20998;&#20132;&#26131;&#20197;&#35782;&#21035;&#34384;&#24453;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26032;&#25903;&#20184;&#24179;&#21488;&#65288;NPP&#65289;&#24341;&#20837;&#20102;&#23558;&#28040;&#24687;&#20316;&#20026;&#20184;&#27454;&#25551;&#36848;&#30340;&#36739;&#38271;&#26684;&#24335;&#21518;&#65292;&#20154;&#20204;&#29616;&#22312;&#21457;&#29616;&#23427;&#34987;&#29992;&#20110;&#27807;&#36890;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#34987;&#29992;&#20316;&#23450;&#21521;&#30340;&#23478;&#24237;&#26292;&#21147;&#24418;&#24335;&#12290;&#36825;&#31181;&#21033;&#29992;&#25216;&#26415;&#23454;&#29616;&#30340;&#34384;&#24453;&#34892;&#20026;&#22312;&#35782;&#21035;&#12289;&#37319;&#21462;&#25514;&#26045;&#21644;&#32416;&#27491;&#36825;&#31181;&#34892;&#20026;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#28595;&#22823;&#21033;&#20122;&#32852;&#37030;&#38134;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#65288;CBA AI Labs&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#23450;&#26399;&#35780;&#20998;&#25152;&#26377;&#20132;&#26131;&#65292;&#24182;&#22312;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#20013;&#35782;&#21035;&#39640;&#39118;&#38505;&#34384;&#24453;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since introducing changes to the New Payments Platform (NPP) to include longer messages as payment descriptions, it has been identified that people are now using it for communication, and in some cases, the system was being used as a targeted form of domestic and family violence. This type of tech-assisted abuse poses new challenges in terms of identification, actions and approaches to rectify this behaviour. Commonwealth Bank of Australia's Artificial Intelligence Labs team (CBA AI Labs) has developed a new system using advances in deep learning models for natural language processing (NLP) to create a powerful abuse detector that periodically scores all the transactions, and identifies cases of high-risk abuse in millions of records. In this paper, we describe the problem of tech-assisted abuse in the context of banking services, outline the developed model and its performance, and the operating framework more broadly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.08011</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22320;&#39044;&#27979;&#21508;&#31181;&#28151;&#27788;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Large statistical learning models effectively forecast diverse chaotic systems. (arXiv:2303.08011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08011
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#27788;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#21457;&#29616;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#12289;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#28151;&#27788;&#21644;&#19981;&#21487;&#39044;&#27979;&#26159;&#21516;&#20041;&#35789;&#65292;&#20294;&#26368;&#36817;&#32479;&#35745;&#39044;&#27979;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#22797;&#26434;&#31995;&#32479;&#30340;&#38271;&#26102;&#38388;&#35266;&#27979;&#20013;&#33719;&#24471;&#24847;&#24819;&#19981;&#21040;&#30340;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#35268;&#27169;&#19978;&#30340;&#28151;&#27788;&#39044;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545; 135 &#31181;&#19981;&#21516;&#20302;&#32500;&#28151;&#27788;&#31995;&#32479;&#30340;&#20247;&#21253;&#25968;&#25454;&#24211;&#36827;&#34892; 24 &#31181;&#20195;&#34920;&#24615;&#26368;&#39640;&#30340;&#22810;&#20803;&#39044;&#27979;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22987;&#32456;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20135;&#29983;&#25345;&#32493;&#25968;&#21313;&#20010;&#26446;&#38597;&#26222;&#35834;&#22827;&#26102;&#38388;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#26368;&#20339;&#30340;&#28151;&#27788;&#39044;&#27979;&#32467;&#26524;&#30001;&#26368;&#36817;&#24341;&#20837;&#30340;&#20998;&#23618;&#31070;&#32463;&#22522;&#30784;&#20989;&#25968;&#27169;&#22411;&#23454;&#29616;&#65292;&#20294;&#21363;&#20351;&#26159;&#36890;&#29992;&#30340;&#21464;&#21387;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#21551;&#21457;&#24335;&#28151;&#21512;&#26041;&#27861;&#22914;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#20648;&#23618;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chaos and unpredictability are traditionally synonymous, yet recent advances in statistical forecasting suggest that large machine learning models can derive unexpected insight from extended observation of complex systems. Here, we study the forecasting of chaos at scale, by performing a large-scale comparison of 24 representative state-of-the-art multivariate forecasting methods on a crowdsourced database of 135 distinct low-dimensional chaotic systems. We find that large, domain-agnostic time series forecasting methods based on artificial neural networks consistently exhibit strong forecasting performance, in some cases producing accurate predictions lasting for dozens of Lyapunov times. Best-in-class results for forecasting chaos are achieved by recently-introduced hierarchical neural basis function models, though even generic transformers and recurrent neural networks perform strongly. However, physics-inspired hybrid methods like neural ordinary equations and reservoir computers c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08010</link><description>&lt;p&gt;
&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#32423;&#32852;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65306;&#24403;&#28145;&#24230;&#38598;&#25104;&#27604;&#21333;&#19968;&#27169;&#22411;&#26356;&#26377;&#25928;&#26102;
&lt;/p&gt;
&lt;p&gt;
Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models. (arXiv:2303.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31616;&#21333;&#12289;&#21487;&#38752;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37096;&#32626;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#65292;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#35745;&#31639;&#24320;&#38144;&#22823;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#36825;&#31181;&#35266;&#28857;&#65292;&#34920;&#26126;&#23545;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#38598;&#25104;&#21487;&#20197;&#27604;&#22312;&#21516;&#19968;&#26550;&#26500;&#26063;&#20013;&#32553;&#25918;&#21333;&#19968;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#36890;&#36807;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#32423;&#32852;&#38598;&#25104;&#25104;&#21592;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#25928;&#29575;&#25552;&#39640;&#25193;&#23637;&#21040;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35768;&#22810;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#37117;&#26159;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#26032;&#39062;&#35265;&#35299;&#26159;&#20165;&#23558;&#25509;&#36817;&#20108;&#20998;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#20256;&#36882;&#21040;&#21518;&#32493;&#32423;&#32852;&#38454;&#27573;&#12290;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#26089;&#26399;&#36864;&#20986;&#38598;&#25104;&#22312;&#20351;&#29992;&#27604;&#22522;&#32447;&#26356;&#23569;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#19982;&#23436;&#25972;&#38598;&#25104;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles are a simple, reliable, and effective method of improving both the predictive performance and uncertainty estimates of deep learning approaches. However, they are widely criticised as being computationally expensive, due to the need to deploy multiple independent models. Recent work has challenged this view, showing that for predictive accuracy, ensembles can be more computationally efficient (at inference) than scaling single models within an architecture family. This is achieved by cascading ensemble members via an early-exit approach. In this work, we investigate extending these efficiency gains to tasks related to uncertainty estimation. As many such tasks, e.g. selective classification, are binary classification, our key novel insight is to only pass samples within a window close to the binary decision boundary to later cascade stages. Experiments on ImageNet-scale data across a number of network architectures and uncertainty tasks show that the proposed window-base
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;Longformer&#39537;&#21160;&#30340;&#22522;&#32447;&#26126;&#26174;&#26356;&#22909;&#30340;RoBERTa&#21477;&#23376;&#32423;&#32452;&#21512;&#36719;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.07991</link><description>&lt;p&gt;
&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#26080;&#30417;&#30563;&#22320;&#25552;&#21462;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;Longformer&#39537;&#21160;&#30340;&#22522;&#32447;&#26126;&#26174;&#26356;&#22909;&#30340;RoBERTa&#21477;&#23376;&#32423;&#32452;&#21512;&#36719;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24207;&#21015;&#36716;&#25442;&#22120;&#26088;&#22312;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#36739;&#38271;&#25991;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#23427;&#20204;&#22312;&#19979;&#28216;&#25991;&#26723;&#32423;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#38271;&#26684;&#24335;&#27169;&#22411;&#20013;&#20196;&#29260;&#32423;&#21035;&#39044;&#27979;&#30340;&#36136;&#37327;&#23578;&#19981;&#22826;&#20102;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26550;&#26500;&#22312;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#19982;Longformer&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#26631;&#20934;&#36719;&#37327;&#27880;&#24847;&#26041;&#27861;&#34920;&#29616;&#26174;&#30528;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36719;&#37327;&#20851;&#27880;&#26550;&#26500;&#65292;&#23427;&#23558;RoBERTa&#24212;&#29992;&#20110;&#21477;&#23376;&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#25552;&#21462;&#21487;&#20449;&#29702;&#30001;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;Longformer&#34893;&#29983;&#22522;&#32447;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;&#26126;&#26174;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-sequence transformers are designed to improve the representation of longer texts by language models and their performance on downstream document-level tasks. However, not much is understood about the quality of token-level predictions in long-form models. We investigate the performance of such architectures in the context of document classification with unsupervised rationale extraction. We find standard soft attention methods to perform significantly worse when combined with the Longformer language model. We propose a compositional soft attention architecture that applies RoBERTa sentence-wise to extract plausible rationales at the token-level. We find this method to significantly outperform Longformer-driven baselines on sentiment classification datasets, while also exhibiting significantly lower runtimes.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.07988</link><description>&lt;p&gt;
&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Partial Neural Optimal Transport. (arXiv:2303.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07988
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26144;&#23556;&#65292;&#21363;&#25351;&#23450;&#36136;&#37327;&#30340;&#24230;&#37327;&#37096;&#20998;&#20043;&#38388;&#30340;OT&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#20855;&#26377;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#22312;&#39640;&#22122;&#22768;&#21644;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#35299;&#20915;&#23398;&#20064;&#26377;&#22122;&#22768;&#20559;&#24046;&#38382;&#39064;&#65288;LPN&#65289;&#65292;&#32780;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24555;&#12290;&#22312;26&#32500;&#65292;0.498&#30340;&#22122;&#22768;&#29575;&#19979;&#65292;&#29992;&#8220;&#29468;&#27979;&#28982;&#21518;&#39640;&#26031;&#28040;&#20803;&#8221;&#31639;&#27861;&#38656;&#35201;3.12&#22825;&#65292;&#22312;8&#20010;GPU&#19978;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21482;&#38656;&#35201;66&#20998;&#38047;&#12290;</title><link>http://arxiv.org/abs/2303.07987</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#20013;&#26356;&#24555;&#22320;&#35299;&#20915;&#23398;&#20064;&#26377;&#22122;&#22768;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Practically Solving LPN in High Noise Regimes Faster Using Neural Networks. (arXiv:2303.07987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#20855;&#26377;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#21487;&#22312;&#39640;&#22122;&#22768;&#21644;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#35299;&#20915;&#23398;&#20064;&#26377;&#22122;&#22768;&#20559;&#24046;&#38382;&#39064;&#65288;LPN&#65289;&#65292;&#32780;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24555;&#12290;&#22312;26&#32500;&#65292;0.498&#30340;&#22122;&#22768;&#29575;&#19979;&#65292;&#29992;&#8220;&#29468;&#27979;&#28982;&#21518;&#39640;&#26031;&#28040;&#20803;&#8221;&#31639;&#27861;&#38656;&#35201;3.12&#22825;&#65292;&#22312;8&#20010;GPU&#19978;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21482;&#38656;&#35201;66&#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#23398;&#20064;&#26377;&#22122;&#22768;&#20559;&#24046;&#38382;&#39064;&#65288;LPN&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#26063;&#65292;&#23427;&#20204;&#22312;&#39640;&#22122;&#22768;&#65292;&#20302;&#32500;&#24230;&#21306;&#22495;&#23454;&#38469;&#19978;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24773;&#20917;&#65292;&#21363;LPN&#26679;&#26412;&#30340;&#25968;&#37327;&#20805;&#36275;&#12289;&#38750;&#24120;&#26377;&#38480;&#21644;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35299;&#20915;LPN&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#23613;&#21487;&#33021;&#24555;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#23545;&#20110;&#26576;&#20123;&#24773;&#20917;&#65292;&#25105;&#20204;&#36824;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#25105;&#20204;&#27169;&#22411;&#35774;&#35745;&#29702;&#35770;&#30340;&#29702;&#35770;&#12290;&#22312;&#32500;&#24230;$n=26$&#12289;&#22122;&#22768;&#29575;$\tau=0.498$&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;Esser&#12289;Kubler&#21644;May&#65288;CRYPTO 2017&#65289;&#20043;&#21069;&#30340;&#23454;&#39564;&#30456;&#27604;&#65292;"&#29468;&#27979;&#28982;&#21518;&#39640;&#26031;&#28040;&#20803;"&#31639;&#27861;&#38656;&#35201;64&#20010;CPU&#26680;&#24515;3.12&#22825;&#30340;&#26102;&#38388;&#65292;&#32780;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21482;&#38656;&#35201;8&#20010;GPU&#30340;66&#20998;&#38047;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20063;&#21487;&#20197;&#25554;&#20837;&#28151;&#21512;&#31639;&#27861;&#20197;&#35299;&#20915;&#20013;&#31561;&#25110;&#22823;&#22411;&#32500;&#24230;&#30340;LPN&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of solving the learning parity with noise problem (LPN) using neural networks. Our main contribution is designing families of two-layer neural networks that practically outperform classical algorithms in high-noise, low-dimension regimes. We consider three settings where the numbers of LPN samples are abundant, very limited, and in between. In each setting we provide neural network models that solve LPN as fast as possible. For some settings we are also able to provide theories that explain the rationale of the design of our models. Comparing with the previous experiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$, noise rate $\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm takes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66 minutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms for solving middle or large dimension LPN instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.07971</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#38544;&#21547;&#32467;&#26500;&#24402;&#32435;&#30340;&#19978;&#19979;&#25991;&#20013;&#28044;&#29616;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Emergent In-Context Learning as Implicit Structure Induction. (arXiv:2303.07971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24341;&#21457;&#20102;&#28044;&#29616;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#22522;&#20110;&#31034;&#20363;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#32452;&#21512;&#24615;&#25805;&#20316;&#30340;&#37325;&#26032;&#32452;&#21512;&#12290;&#22312;&#22522;&#20110;&#35821;&#35328;&#23398;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#24403;&#39044;&#35757;&#32451;&#20998;&#24067;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#26102;&#65292;&#22914;&#20309;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#20108;&#20010;&#30028;&#38480;&#20026;&#25552;&#31034;LLM&#36755;&#20986;&#26397;&#30528;&#31572;&#26696;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#23454;&#35777;&#25104;&#21151;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#32452;&#21512;&#26412;&#36136;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#19982;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340; Transformer &#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07940</link><description>&lt;p&gt;
&#20851;&#20110;&#20135;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Connection between Concept Drift and Uncertainty in Industrial Artificial Intelligence. (arXiv:2303.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#23383;&#23402;&#29983;&#26159;&#24037;&#19994;4.0&#38761;&#21629;&#30340;&#21069;&#27839;&#65292;&#20854;&#25216;&#26415;&#30001;&#29289;&#32852;&#32593;&#21644;&#23454;&#26102;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#25903;&#25345;&#12290;&#20174;&#24037;&#19994;&#36164;&#20135;&#20013;&#25910;&#38598;&#30340;&#20449;&#24687;&#20197;&#25345;&#32493;&#30340;&#26041;&#24335;&#20135;&#29983;&#65292;&#20135;&#29983;&#30340;&#25968;&#25454;&#27969;&#24517;&#39035;&#22312;&#20005;&#26684;&#30340;&#26102;&#38388;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#27969;&#36890;&#24120;&#20250;&#21463;&#21040;&#38750;&#24179;&#31283;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#25968;&#25454;&#27969;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#27169;&#22411;&#25429;&#33719;&#30340;&#30693;&#35782;&#21487;&#33021;&#20250;&#21464;&#24471;&#36807;&#26102;&#65288;&#23548;&#33268;&#25152;&#35859;&#30340;&#27010;&#24565;&#28418;&#31227;&#25928;&#24212;&#65289;&#12290;&#21450;&#26089;&#21457;&#29616;&#65288;&#28418;&#31227;&#65289;&#30340;&#21464;&#21270;&#23545;&#20110;&#26356;&#26032;&#27169;&#22411;&#30340;&#30693;&#35782;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#22330;&#26223;&#20013;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#19982;&#27969;&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#22522;&#26412;&#30495;&#30456;&#19981;&#23481;&#26131;&#25214;&#21040;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#20135;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#27010;&#24565;&#28418;&#31227;&#21644;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#26469;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-based digital twins are at the leading edge of the Industry 4.0 revolution, which are technologically empowered by the Internet of Things and real-time data analysis. Information collected from industrial assets is produced in a continuous fashion, yielding data streams that must be processed under stringent timing constraints. Such data streams are usually subject to non-stationary phenomena, causing that the data distribution of the streams may change, and thus the knowledge captured by models used for data analysis may become obsolete (leading to the so-called concept drift effect). The early detection of the change (drift) is crucial for updating the model's knowledge, which is challenging especially in scenarios where the ground truth associated to the stream data is not readily available. Among many other techniques, the estimation of the model's confidence has been timidly suggested in a few studies as a criterion for detecting drifts in unsupervised settings. The goal of thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#23545;&#21475;&#38899;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#12290;</title><link>http://arxiv.org/abs/2303.07924</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#35757;&#32451;&#22312;&#25552;&#39640;&#21475;&#38899;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#23545;&#21475;&#38899;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22686;&#38271;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20855;&#22791;&#23545;&#21475;&#38899;&#21464;&#21270;&#31561;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#30340;&#35821;&#38899;&#38899;&#39057;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21475;&#38899;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25913;&#36827;&#12290;&#25968;&#23383;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19982;&#21333;&#39046;&#22495;&#35757;&#32451;&#30456;&#27604;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#65288;&#30456;&#23545;&#20540;&#65289;&#65292;&#21516;&#26102;&#22312;&#26631;&#20934;&#27861;&#35821;&#19978;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to the rise of self-supervised learning, automatic speech recognition (ASR) systems now achieve near-human performance on a wide variety of datasets. However, they still lack generalization capability and are not robust to domain shifts like accent variations. In this work, we use speech audio representing four different French accents to create fine-tuning datasets that improve the robustness of pre-trained ASR models. By incorporating various accents in the training set, we obtain both in-domain and out-of-domain improvements. Our numerical experiments show that we can reduce error rates by up to 25% (relative) on African and Belgian accents compared to single-domain training while keeping a good performance on standard French.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07900</link><description>&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23427;&#20204;&#29983;&#25104;&#20174;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#26679;&#26412;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#21463;&#28418;&#31227;-&#25193;&#25955;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20294;&#22312;&#36817;&#26399;&#30340;&#23454;&#36341;&#23548;&#21521;&#30340;&#20986;&#29256;&#29289;&#20013;&#65292;&#36825;&#20123;&#36215;&#28304;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#28418;&#31227;-&#25193;&#25955;&#29289;&#29702;&#26680;&#24515;&#27010;&#24565;&#35299;&#37322;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#27010;&#29575;&#25193;&#25955;&#19982;&#28183;&#36879;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#20989;&#25968;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#25903;&#37197;PAC Bayes&#19968;&#26679;&#30340;&#27010;&#21270;&#30028;&#38480;&#65292;&#20851;&#32852;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#20989;&#25968;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#33021;&#22815;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26377;&#22810;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#23548;&#20986;&#19978;&#30028;&#65292;&#22312;&#26356;&#39640;&#23618;&#20197;&#21450;&#25317;&#26377;&#32467;&#26500;&#21270;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2303.07874</link><description>&lt;p&gt;
&#23398;&#20064;&#32773;&#30340;&#36125;&#21494;&#26031;&#22797;&#26434;&#24230;&#19982;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayes Complexity of Learners vs Overfitting. (arXiv:2303.07874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#20989;&#25968;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#23427;&#33021;&#22815;&#25903;&#37197;PAC Bayes&#19968;&#26679;&#30340;&#27010;&#21270;&#30028;&#38480;&#65292;&#20851;&#32852;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#20989;&#25968;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#33021;&#22815;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26377;&#22810;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#21487;&#23548;&#20986;&#19978;&#30028;&#65292;&#22312;&#26356;&#39640;&#23618;&#20197;&#21450;&#25317;&#26377;&#32467;&#26500;&#21270;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#22797;&#26434;&#24230;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#65306;&#65288;i&#65289;&#23427;&#25903;&#37197;&#30528;PAC Bayes&#19968;&#26679;&#30340;&#27010;&#21270;&#30028;&#38480;&#65292;&#65288;ii&#65289;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20851;&#32852;&#20102;&#20989;&#25968;&#22797;&#26434;&#24230;&#30340;&#33258;&#28982;&#27010;&#24565;&#65288;&#22914;&#21464;&#37327;&#65289;&#65292;&#65288;iii&#65289;&#23427;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#32447;&#24615;&#27169;&#22411;&#20043;&#38388;&#30340;&#27010;&#21270;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#26377;&#35768;&#22810;&#38024;&#23545;&#36825;&#20123;&#29305;&#24615;&#30340;&#35770;&#25991;&#21644;&#30028;&#38480;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#36825;&#19977;&#20010;&#26465;&#20214;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26377;&#22810;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#34429;&#28982;&#19968;&#33324;&#24773;&#20917;&#19979;&#35745;&#31639;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#20294;&#19968;&#33324;&#21487;&#23548;&#20986;&#19968;&#20010;&#19978;&#30028;&#65292;&#21363;&#20351;&#22312;&#26356;&#39640;&#23618;&#20197;&#21450;&#25317;&#26377;&#32467;&#26500;&#21270;&#20989;&#25968;&#65288;&#22914;&#21608;&#26399;&#20989;&#25968;&#65289;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25512;&#23548;&#30340;&#19978;&#30028;&#20801;&#35768;&#23637;&#31034;&#26679;&#26412;&#25968;&#37327;&#22312;2&#21644;&#24635;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#33391;&#22909;&#27867;&#21270;&#20998;&#31163;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new notion of complexity of functions and we show that it has the following properties: (i) it governs a PAC Bayes-like generalization bound, (ii) for neural networks it relates to natural notions of complexity of functions (such as the variation), and (iii) it explains the generalization gap between neural networks and linear schemes. While there is a large set of papers which describes bounds that have each such property in isolation, and even some that have two, as far as we know, this is a first notion that satisfies all three of them. Moreover, in contrast to previous works, our notion naturally generalizes to neural networks with several layers.  Even though the computation of our complexity is nontrivial in general, an upper-bound is often easy to derive, even for higher number of layers and functions with structure, such as period functions. An upper-bound we derive allows to show a separation in the number of samples needed for good generalization between 2 and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#31867;&#22686;&#37327;&#23398;&#20064;(OCL)&#20013;&#22914;&#20309;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;(DA)&#26469;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;(CF)&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#22686;&#24378;&#30340;Mixup(EnMix)&#26041;&#27861;&#65292;&#21516;&#26102;&#28151;&#21512;&#22686;&#24378;&#26679;&#26412;&#21644;&#20854;&#26631;&#31614;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;OCI&#24615;&#33021;&#24182;&#38450;&#27490;CF&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07864</link><description>&lt;p&gt;
DualMix: &#37322;&#25918;&#25968;&#25454;&#22686;&#24378;&#23545;&#22312;&#32447;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
DualMix: Unleashing the Potential of Data Augmentation for Online Class-Incremental Learning. (arXiv:2303.07864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#31867;&#22686;&#37327;&#23398;&#20064;(OCL)&#20013;&#22914;&#20309;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;(DA)&#26469;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;(CF)&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25552;&#20986;&#22686;&#24378;&#30340;Mixup(EnMix)&#26041;&#27861;&#65292;&#21516;&#26102;&#28151;&#21512;&#22686;&#24378;&#26679;&#26412;&#21644;&#20854;&#26631;&#31614;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;OCI&#24615;&#33021;&#24182;&#38450;&#27490;CF&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31867;&#22686;&#37327;&#23398;&#20064;(OCL)&#24050;&#32463;&#24341;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#21521;&#20808;&#21069;&#35757;&#32451;&#30340;&#27169;&#22411;&#30693;&#35782;&#20013;&#28155;&#21152;&#26032;&#31867;&#21035;&#30340;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#27969;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;OCL&#23398;&#20064;&#21487;&#33021;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;(CF)&#65292;&#22240;&#20026;&#26087;&#31867;&#21035;&#30340;&#20915;&#31574;&#36793;&#30028;&#22312;&#34987;&#26032;&#31867;&#21035;&#25200;&#21160;&#26102;&#21487;&#33021;&#21464;&#24471;&#19981;&#20934;&#30830;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#24050;&#32463;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;(DA)&#26469;&#20943;&#36731;&#27169;&#22411;&#36951;&#24536;&#65292;&#28982;&#32780;DA&#22312;OCI&#20013;&#30340;&#20316;&#29992;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#20851;&#24615;&#36739;&#20302;&#30340;&#22686;&#24378;&#26679;&#26412;&#22312;&#38450;&#27490;&#36951;&#24536;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#30340;&#22686;&#24378;&#20063;&#21487;&#33021;&#38477;&#20302;&#25968;&#25454;&#21644;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#21033;&#29992;&#36866;&#24403;&#30340;DA&#25552;&#39640;OCI&#24615;&#33021;&#24182;&#38450;&#27490;CF&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;Mixup(EnMix)&#26041;&#27861;&#65292;&#21516;&#26102;&#28151;&#21512;&#22686;&#24378;&#26679;&#26412;&#21644;&#20854;&#26631;&#31614;&#65292;&#26174;&#31034;&#20102;&#22686;&#24378;&#26679;&#26412;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online Class-Incremental (OCI) learning has sparked new approaches to expand the previously trained model knowledge from sequentially arriving data streams with new classes. Unfortunately, OCI learning can suffer from catastrophic forgetting (CF) as the decision boundaries for old classes can become inaccurate when perturbated by new ones. Existing literature have applied the data augmentation (DA) to alleviate the model forgetting, while the role of DA in OCI has not been well understood so far. In this paper, we theoretically show that augmented samples with lower correlation to the original data are more effective in preventing forgetting. However, aggressive augmentation may also reduce the consistency between data and corresponding labels, which motivates us to exploit proper DA to boost the OCI performance and prevent the CF problem. We propose the Enhanced Mixup (EnMix) method that mixes the augmented samples and their labels simultaneously, which is shown to enhance the sample 
&lt;/p&gt;</description></item><item><title>BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.07853</link><description>&lt;p&gt;
BoundaryCAM&#65306;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07853
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26159;&#35299;&#20915;&#20998;&#21106;&#32593;&#32476;&#38656;&#27714;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22823;&#37327;&#20687;&#32032;&#32423;&#25513;&#27169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32423;WSSS&#25216;&#26415;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#29305;&#24449;&#30340;&#29702;&#35299;&#65292;&#22240;&#20026;&#32593;&#32476;&#26080;&#27861;&#20174;&#20165;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#20013;&#23548;&#20986;&#20219;&#20309;&#23545;&#35937;&#36793;&#30028;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;BoundaryCAM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#31867;&#28608;&#27963;&#22270;&#32467;&#21512;&#21508;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#29992;&#20110;&#26500;&#24314;&#36793;&#30028;&#22270;&#65292;&#20197;&#20351;BoundaryCAM&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#29992;&#20110;&#30830;&#23450;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FPUS23&#21487;&#20197;&#20026;&#20020;&#24202;&#36229;&#22768;&#30417;&#27979;&#24037;&#20316;&#27969;&#31243;&#24102;&#26469;&#25913;&#21892;&#65292;&#20197;&#21450;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2303.07852</link><description>&lt;p&gt;
&#19968;&#20221;&#24102;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#30340;&#32650;&#27700;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features. (arXiv:2303.07852v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#29992;&#20110;&#30830;&#23450;&#32974;&#20799;&#26041;&#21521;&#65292;&#32974;&#20301;&#21644;&#35299;&#21078;&#23398;&#29305;&#24449;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FPUS23&#21487;&#20197;&#20026;&#20020;&#24202;&#36229;&#22768;&#30417;&#27979;&#24037;&#20316;&#27969;&#31243;&#24102;&#26469;&#25913;&#21892;&#65292;&#20197;&#21450;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#26159;&#35780;&#20272;&#32974;&#20799;&#22312;&#22922;&#23072;&#26399;&#38388;&#30340;&#29983;&#38271;&#65292;&#21457;&#23637;&#21644;&#25972;&#20307;&#20581;&#24247;&#29366;&#20917;&#26368;&#31361;&#20986;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#20026;&#20102;&#25913;&#21892;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#24182;&#21487;&#33021;&#24320;&#21457;&#19968;&#20010;&#23478;&#24237;&#20351;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#30340;&#32974;&#20799;&#30417;&#25252;&#24179;&#21488;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32974;&#20799;&#27169;&#22411;&#36229;&#22768;&#25968;&#25454;&#38598;FPUS23&#65292;&#23427;&#21487;&#29992;&#20110;&#30830;&#23450;&#65288;1&#65289;&#29992;&#20110;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#35745;&#37327;&#20540;&#30340;&#27491;&#30830;&#35786;&#26029;&#24179;&#38754;&#65292;&#65288;2&#65289;&#32974;&#20799;&#26041;&#21521;&#65292;&#65288;3&#65289;&#23427;&#20204;&#30340;&#35299;&#21078;&#32467;&#26500;&#21644;&#65288;4&#65289;23&#21608;&#23381;&#40836;&#26102;&#32974;&#20799;&#27169;&#22411;&#35299;&#21078;&#23398;&#30340;&#36793;&#30028;&#26694;&#12290;&#25972;&#20010;&#25968;&#25454;&#38598;&#30001;15,728&#24352;&#22270;&#20687;&#32452;&#25104;&#65292;&#29992;&#20110;&#35757;&#32451;&#24314;&#31435;&#22312;ResNet34&#39592;&#24178;&#32593;&#32476;&#19978;&#30340;&#22235;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19978;&#36848;&#32974;&#20799;&#29305;&#24449;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;FPUS23&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound imaging is one of the most prominent technologies to evaluate the growth, progression, and overall health of a fetus during its gestation. However, the interpretation of the data obtained from such studies is best left to expert physicians and technicians who are trained and well-versed in analyzing such images. To improve the clinical workflow and potentially develop an at-home ultrasound-based fetal monitoring platform, we present a novel fetus phantom ultrasound dataset, FPUS23, which can be used to identify (1) the correct diagnostic planes for estimating fetal biometric values, (2) fetus orientation, (3) their anatomical features, and (4) bounding boxes of the fetus phantom anatomies at 23 weeks gestation. The entire dataset is composed of 15,728 images, which are used to train four different Deep Neural Network models, built upon a ResNet34 backbone, for detecting aforementioned fetus features and use-cases. We have also evaluated the models trained using our FPUS23 da
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27425;&#35201;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#26102;&#37096;&#32626;&#22522;&#20110;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#25233;&#37057;&#30151;&#31579;&#26597;&#24037;&#20855;&#26102;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#19979;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07847</link><description>&lt;p&gt;
&#23454;&#26102;&#37096;&#32626;&#25233;&#37057;&#30151;&#31579;&#26597;&#24037;&#20855;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#21152;&#36895;&#35745;&#25968;&#25454;(arXiv&#65306;2303.07847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Real-time Deployment of a Screening Tool for Depression Detection Using Actigraphy. (arXiv:2303.07847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07847
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#27425;&#35201;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#26102;&#37096;&#32626;&#22522;&#20110;&#21152;&#36895;&#35745;&#25968;&#25454;&#30340;&#25233;&#37057;&#30151;&#31579;&#26597;&#24037;&#20855;&#26102;&#20351;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#19979;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25233;&#37057;&#30151;&#31579;&#26597;&#21644;&#35786;&#26029;&#26159;&#20170;&#22825;&#38750;&#24120;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#35768;&#22810;&#23616;&#38480;&#24615;&#65292;&#20027;&#35201;&#26159;&#39640;&#24230;&#20381;&#36182;&#20020;&#24202;&#21307;&#29983;&#21644;&#26377;&#20559;&#24046;&#30340;&#33258;&#25105;&#25253;&#21578;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#34920;&#26126;&#21033;&#29992;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#29992;&#25143;&#30340;&#34987;&#21160;&#25968;&#25454;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#22312;&#27425;&#35201;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#22987;&#65292;&#23454;&#26102;&#37096;&#32626;&#22522;&#20110;&#29992;&#25143;&#30340;&#34892;&#20026;&#25968;&#25454;&#30340;&#25233;&#37057;&#30151;&#31579;&#26597;&#24037;&#20855;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#19979;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#20027;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#30340;&#30041;&#19968;&#27861;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#20462;&#25913;&#29256;&#26412;&#23548;&#33268;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;0.96&#65292;&#20854;&#20013;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20174;&#20027;&#25968;&#25454;&#38598;&#20013;&#35774;&#32622;&#19968;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated depression screening and diagnosis is a highly relevant problem today. There are a number of limitations of the traditional depression detection methods, namely, high dependence on clinicians and biased self-reporting. In recent years, research has suggested strong potential in machine learning (ML) based methods that make use of the user's passive data collected via wearable devices. However, ML is data hungry. Especially in the healthcare domain primary data collection is challenging. In this work, we present an approach based on transfer learning, from a model trained on a secondary dataset, for the real time deployment of the depression screening tool based on the actigraphy data of users. This approach enables machine learning modelling even with limited primary data samples. A modified version of leave one out cross validation approach performed on the primary set resulted in mean accuracy of 0.96, where in each iteration one subject's data from the primary set was set 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.07846</link><description>&lt;p&gt;
&#39640;&#25928;&#29575;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#26469;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#23398;&#20064;&#19981;&#21463;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#20197;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#21363;&#36890;&#36807;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#65292;&#24050;&#32463;&#34987;&#30740;&#31350;&#24182;&#24212;&#29992;&#20110;&#24207;&#36143;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#24182;&#19981;&#26159;&#39044;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#23478;&#28436;&#31034;&#26679;&#26412;&#25165;&#33021;&#25104;&#21151;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#12290;&#20026;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#22823;&#37327;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#19981;&#21463;&#21508;&#31181;&#25197;&#26354;&#24433;&#21709;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#65292;&#24182;&#24314;&#31435;&#38750;&#22270;&#20687;&#25511;&#21046;&#20219;&#21153;&#30340;&#39044;&#27979;&#34920;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#30340;&#19981;&#21516;&#25439;&#22351;&#26041;&#27861;&#65292;&#20197;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#21508;&#31181;&#25197;&#26354;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#20351;&#19968;&#20010;&#20449;&#24687;&#37327;&#22823;&#30340;&#29305;&#24449;&#27969;&#24418;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#29983;&#25104;&#22120;&#19982;&#19968;&#20010;&#22797;&#26434;&#30340;&#20998;&#31867;&#22120;&#21327;&#21516;&#24037;&#20316;&#33021;&#22815;&#25552;&#39640;&#29366;&#24577;&#34920;&#24449;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, in which learning is performed by demonstration, has been studied and advanced for sequential decision-making tasks in which a reward function is not predefined. However, imitation learning methods still require numerous expert demonstration samples to successfully imitate an expert's behavior. To improve sample efficiency, we utilize self-supervised representation learning, which can generate vast training signals from the given data. In this study, we propose a self-supervised representation-based adversarial imitation learning method to learn state and action representations that are robust to diverse distortions and temporally predictive, on non-image control tasks. In particular, in comparison with existing self-supervised learning methods for tabular data, we propose a different corruption method for state and action representations that is robust to diverse distortions. We theoretically and empirically observe that making an informative feature manifold with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.07814</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#25163;&#26415;&#34892;&#20026;&#20999;&#20998;
&lt;/p&gt;
&lt;p&gt;
Kinematic Data-Based Action Segmentation for Surgical Applications. (arXiv:2303.07814v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#20999;&#20998;&#26159;&#39640;&#32423;&#27969;&#31243;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#36890;&#24120;&#22312;&#35270;&#39057;&#25110;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#36816;&#21160;&#23398;&#25968;&#25454;&#19978;&#25191;&#34892;&#12290;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#65292;&#34892;&#21160;&#20999;&#20998;&#23545;&#20110;&#24037;&#20316;&#27969;&#20998;&#26512;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#19982;&#36816;&#21160;&#23398;&#25968;&#25454;&#30456;&#20851;&#30340;&#34892;&#21160;&#20998;&#21106;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#65292;MS-TCN-BiLSTM&#21644;MS-TCN-BiGRU&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#12290; &#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30001;&#20855;&#26377;&#38454;&#20869;&#35268;&#21017;&#21270;&#21644;&#21452;&#21521;LSTM&#25110;GRU&#30340;&#32454;&#21270;&#38454;&#27573;&#30340;&#39044;&#27979;&#29983;&#25104;&#22120;&#32452;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;World Frame Rotation&#21644;Horizontal-Flip&#65292;&#21033;&#29992;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#24378;&#20960;&#20309;&#32467;&#26500;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;&#21487;&#21464;&#32452;&#32455;&#27169;&#25311;&#65288;VTS&#65289;&#25968;&#25454;&#38598;&#21644;&#26032;&#25512;&#20986;&#30340;&#32928;&#36947;&#20462;&#22797;&#27169;&#25311;&#65288;BRS&#65289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. In the context of surgical procedures, action segmentation is critical for workflow analysis algorithms. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two multi-stage architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Horizontal-Flip, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset,
&lt;/p&gt;</description></item><item><title>ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07811</link><description>&lt;p&gt;
ICICLE: &#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07811
&lt;/p&gt;
&lt;p&gt;
ICICLE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21487;&#35299;&#37322;&#30340;&#31867;&#22686;&#37327;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#33021;&#22815;&#22686;&#37327;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#20419;&#36827;&#26032;&#26087;&#20219;&#21153;&#20043;&#38388;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#23398;&#20064;&#23545;&#35299;&#37322;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#27169;&#22411;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#32780;&#25913;&#21464;&#65292;&#23548;&#33268;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#26679;&#26412;&#30340; Interpretable Class-InCremental LEarning (ICICLE) &#26041;&#27861;&#65292;&#37319;&#29992;&#21407;&#22411;&#37096;&#20998;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#28857;&#65306;&#35299;&#37322;&#24615;&#27491;&#21017;&#21270;&#12289;&#20197;&#24494;&#31890;&#31890;&#24230;&#20026;&#22522;&#30784;&#30340;&#21407;&#22411;&#21021;&#22987;&#21270;&#31574;&#30053;&#20197;&#21450;&#38024;&#23545;&#21407;&#22411;&#37096;&#20998;&#30340;&#20219;&#21153;&#26102;&#25928;&#20559;&#24046;&#34917;&#20607;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ICICLE&#20943;&#23569;&#20102;&#35299;&#37322;&#24615;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#32500;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#26041;&#27861;&#22312;&#29616;&#26377;&#25216;&#26415;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#25193;&#23637;&#20102;&#21040;&#38750;&#32447;&#24615;&#36857;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23725;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#30340;&#21464;&#20307;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#35777;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.07774</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#22240;&#26524;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Testing Causality for High Dimensional Data. (arXiv:2303.07774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#39640;&#32500;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#26041;&#27861;&#22312;&#29616;&#26377;&#25216;&#26415;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#25193;&#23637;&#20102;&#21040;&#38750;&#32447;&#24615;&#36857;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23725;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#30340;&#21464;&#20307;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#35777;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#39640;&#32500;&#35266;&#27979;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#26159;&#31185;&#23398;&#21457;&#29616;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#32447;&#24615;&#36857;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#25512;&#26029;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#26041;&#21521;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#25913;&#36827;&#30340;&#23614;&#37096;&#20998;&#26512;&#24182;&#22312;&#26576;&#20123;&#20998;&#24067;&#20551;&#35774;&#19979;&#25193;&#23637;&#38750;&#32447;&#24615;&#36857;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22240;&#26524;&#20272;&#35745;&#22120;&#35299;&#37322;&#20026;&#38543;&#26426;&#27491;&#20132;&#30697;&#38453;&#19978;&#30340;&#20989;&#25968;&#26469;&#33719;&#24471;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#23558;&#36825;&#26679;&#30340;&#31354;&#38388;&#19978;&#30340;Lipschitz&#20989;&#25968;&#27987;&#24230;&#24212;&#29992;&#20110;&#20854;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23725;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#30340;&#21464;&#20307;&#65292;&#24182;&#32473;&#20986;&#20102;&#23725;&#20272;&#35745;&#39033;&#19982;&#20854;&#22522;&#26412;&#20107;&#23454;&#23545;&#24212;&#39033;&#20043;&#38388;&#30340;&#21487;&#35777;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining causal relationship between high dimensional observations are among the most important tasks in scientific discoveries. In this paper, we revisited the \emph{linear trace method}, a technique proposed in~\citep{janzing2009telling,zscheischler2011testing} to infer the causal direction between two random variables of high dimensions. We strengthen the existing results significantly by providing an improved tail analysis in addition to extending the results to nonlinear trace functionals with sharper confidence bounds under certain distributional assumptions. We obtain our results by interpreting the trace estimator in the causal regime as a function over random orthogonal matrices, where the concentration of Lipschitz functions over such space could be applied. We additionally propose a novel ridge-regularized variant of the estimator in \cite{zscheischler2011testing}, and give provable bounds relating the ridge-estimated terms to their ground-truth counterparts. We support o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.07768</link><description>&lt;p&gt;
&#22810;&#32500;&#25968;&#32452;&#30340;&#22810;&#20999;&#29255;&#32858;&#31867;&#20013;&#30340;DBSCAN&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DBSCAN of Multi-Slice Clustering for three-order Tensor. (arXiv:2303.07768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19977;&#20803;&#32858;&#31867;&#20013;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;&#65292;&#24182;&#21487;&#20197;&#33719;&#24471;&#19982; MSC &#31639;&#27861;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19977;&#32500;&#25968;&#25454;&#30340;&#19977;&#20803;&#32858;&#31867;&#65292;&#29616;&#26377;&#30340;&#20960;&#31181;&#26041;&#27861;&#38656;&#35201;&#25351;&#23450;&#27599;&#20010;&#32500;&#24230;&#30340;&#32858;&#31867;&#22823;&#23567;&#25110;&#32858;&#31867;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19977;&#20803;&#32858;&#31867;(MSC)&#31639;&#27861;&#21487;&#20197;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#25214;&#21040;&#20445;&#30041;&#20449;&#21495;&#30340;&#20999;&#29255;&#20197;&#20415;&#22522;&#20110;&#30456;&#20284;&#24230;&#38408;&#20540;&#25214;&#21040;&#32858;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; MSC-DBSCAN&#25193;&#23637;&#31639;&#27861;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20301;&#20110;&#19981;&#21516;&#23376;&#31354;&#38388;&#30340;&#19981;&#21516;&#20999;&#29255;&#32858;&#31867;(&#22914;&#26524;&#25968;&#25454;&#38598;&#26159;r&#20010;&#31209;&#19968;&#24352;&#37327;(r&gt;1)&#30340;&#24635;&#21644;)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#21644; MSC &#31639;&#27861;&#30456;&#21516;&#30340;&#36755;&#20837;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#31209;&#19968;&#24352;&#37327;&#25968;&#25454;&#26102;&#19982; MSC &#31639;&#27861;&#33719;&#24471;&#30456;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several methods for triclustering three-dimensional data require the cluster size or the number of clusters in each dimension to be specified. To address this issue, the Multi-Slice Clustering (MSC) for 3-order tensor finds signal slices that lie in a low dimensional subspace for a rank-one tensor dataset in order to find a cluster based on the threshold similarity. We propose an extension algorithm called MSC-DBSCAN to extract the different clusters of slices that lie in the different subspaces from the data if the dataset is a sum of r rank-one tensor (r &gt; 1). Our algorithm uses the same input as the MSC algorithm and can find the same solution for rank-one tensor data as MSC.
&lt;/p&gt;</description></item><item><title>NeurIPS 2022&#30340;Traffic4cast&#31454;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#21033;&#29992;&#31232;&#30095;&#33410;&#28857;&#25968;&#25454;&#39044;&#27979;&#25972;&#20010;&#36947;&#36335;&#22270;&#30340;&#21160;&#24577;&#26410;&#26469;&#20132;&#36890;&#29366;&#24577;&#65292;&#24182;&#23558;GPS&#25968;&#25454;&#30340;&#36895;&#24230;&#32423;&#21035;&#36716;&#21270;&#20026;&#19977;&#20010;&#25317;&#22581;&#31561;&#32423;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07758</link><description>&lt;p&gt;
NeurIPS 2022&#30340;Traffic4cast--&#39044;&#27979;&#22478;&#24066;&#20132;&#36890;&#21644;ETA&#30340;&#21160;&#24577;&#36335;&#32593;&#27169;&#22411;&#21033;&#29992;&#38745;&#27490;&#36710;&#36742;&#25506;&#27979;&#22120;&#30340;&#31232;&#30095;&#33410;&#28857;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic4cast at NeurIPS 2022 -- Predict Dynamics along Graph Edges from Sparse Node Data: Whole City Traffic and ETA from Stationary Vehicle Detectors. (arXiv:2303.07758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07758
&lt;/p&gt;
&lt;p&gt;
NeurIPS 2022&#30340;Traffic4cast&#31454;&#36187;&#35201;&#27714;&#21442;&#19982;&#32773;&#21033;&#29992;&#31232;&#30095;&#33410;&#28857;&#25968;&#25454;&#39044;&#27979;&#25972;&#20010;&#36947;&#36335;&#22270;&#30340;&#21160;&#24577;&#26410;&#26469;&#20132;&#36890;&#29366;&#24577;&#65292;&#24182;&#23558;GPS&#25968;&#25454;&#30340;&#36895;&#24230;&#32423;&#21035;&#36716;&#21270;&#20026;&#19977;&#20010;&#25317;&#22581;&#31561;&#32423;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#21270;&#21644;&#20010;&#20154;&#30340;&#20132;&#36890;&#19994;&#21153;&#36235;&#21183;&#20351;&#25105;&#20204;&#19981;&#24471;&#19981;&#37325;&#26032;&#32771;&#34385;&#25105;&#20204;&#29983;&#27963;&#21644;&#20351;&#29992;&#22478;&#24066;&#31354;&#38388;&#30340;&#26041;&#24335;&#12290;Traffic4cast&#31454;&#36187;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#30740;&#31350;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#36335;&#32593;&#65292;$10^{12}$ &#20010;&#25506;&#38024;&#25968;&#25454;&#28857;&#21644;&#19977;&#20010;&#22478;&#24066;&#20004;&#24180;&#30340;&#38745;&#27490;&#36710;&#36742;&#25506;&#27979;&#22120;&#30340;&#20449;&#24687;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#38745;&#27490;&#36710;&#36742;&#25506;&#27979;&#22120;&#21482;&#22312;&#23569;&#25968;&#20301;&#32622;&#19978;&#21487;&#29992;&#65292;&#23613;&#31649;&#23427;&#20204;&#26159;&#25429;&#33719;&#20132;&#36890;&#37327;&#26368;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;Traffic4cast 2022&#25506;&#32034;&#30340;&#27169;&#22411;&#26377;&#33021;&#21147;&#23558;&#21482;&#26377;&#20960;&#20010;&#33410;&#28857;&#19978;&#30340;&#26494;&#25955;&#20851;&#32852;&#30340;&#26102;&#38388;&#33410;&#28857;&#25968;&#25454;&#27010;&#25324;&#21040;&#25972;&#20010;&#36947;&#36335;&#22270;&#30340;&#21160;&#24577;&#26410;&#26469;&#20132;&#36890;&#29366;&#24577;&#30340;&#39044;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global trends of urbanization and increased personal mobility force us to rethink the way we live and use urban space. The Traffic4cast competition series tackles this problem in a data-driven way, advancing the latest methods in machine learning for modeling complex spatial systems over time. In this edition, our dynamic road graph data combine information from road maps, $10^{12}$ probe data points, and stationary vehicle detectors in three cities over the span of two years. While stationary vehicle detectors are the most accurate way to capture traffic volume, they are only available in few locations. Traffic4cast 2022 explores models that have the ability to generalize loosely related temporal vertex data on just a few nodes to predict dynamic future traffic states on the edges of the entire road graph. In the core challenge, participants are invited to predict the likelihoods of three congestion classes derived from the speed levels in the GPS data for the entire road graph in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#19977;&#38454;&#24352;&#37327;&#22810;&#36335;&#32858;&#31867;&#26041;&#27861;(MCAM)&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20284;&#30697;&#38453;&#24182;&#24212;&#29992;&#20808;&#36827;&#32858;&#31867;&#26041;&#27861;&#26368;&#32456;&#23454;&#29616;&#20102;&#22810;&#36335;&#32858;&#31867;&#20998;&#26512;&#12290;&#19982;&#20854;&#20182;&#24050;&#30693;&#31639;&#27861;&#30456;&#27604;&#65292;MCAM&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.07757</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#19977;&#38454;&#24352;&#37327;&#30340;&#22810;&#36335;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multiway clustering of 3-order tensor via affinity matrix. (arXiv:2303.07757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#19977;&#38454;&#24352;&#37327;&#22810;&#36335;&#32858;&#31867;&#26041;&#27861;(MCAM)&#65292;&#36890;&#36807;&#26500;&#24314;&#30456;&#20284;&#30697;&#38453;&#24182;&#24212;&#29992;&#20808;&#36827;&#32858;&#31867;&#26041;&#27861;&#26368;&#32456;&#23454;&#29616;&#20102;&#22810;&#36335;&#32858;&#31867;&#20998;&#26512;&#12290;&#19982;&#20854;&#20182;&#24050;&#30693;&#31639;&#27861;&#30456;&#27604;&#65292;MCAM&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#38454;&#24352;&#37327;&#30340;&#22810;&#36335;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30456;&#20284;&#30697;&#38453;&#30340;&#22810;&#36335;&#32858;&#31867;&#65288;MCAM&#65289;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#24352;&#37327;&#20999;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#27599;&#20010;&#20999;&#29255;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#26500;&#24314;&#19968;&#20010;&#30456;&#20284;&#30697;&#38453;&#65292;&#24182;&#22312;&#20854;&#19978;&#24212;&#29992;&#20808;&#36827;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;&#19977;&#31181;&#27169;&#24335;&#30340;&#25152;&#26377;&#32858;&#31867;&#30340;&#32452;&#21512;&#21487;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#22810;&#36335;&#32858;&#31867;&#12290;&#26368;&#21518;&#65292;MCAM&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#19982;&#20854;&#20182;&#24050;&#30693;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method of multiway clustering for 3-order tensors via affinity matrix (MCAM). Based on a notion of similarity between the tensor slices and the spread of information of each slice, our model builds an affinity/similarity matrix on which we apply advanced clustering methods. The combination of all clusters of the three modes delivers the desired multiway clustering. Finally, MCAM achieves competitive results compared with other known algorithms on synthetics and real datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#25968;&#23383;&#21270;&#38754;&#35797;&#24773;&#22659;&#26469;&#35825;&#21457;&#21387;&#21147;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#36830;&#32493;&#26631;&#27880;&#21644;&#22522;&#20934;&#20998;&#31867;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#38598;&#12290;&#26368;&#20339;&#34920;&#29616;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;88.3%&#21644;87.5%&#12290;</title><link>http://arxiv.org/abs/2303.07742</link><description>&lt;p&gt;
ForDigitStress&#65306;&#19968;&#31181;&#37319;&#29992;&#25968;&#23383;&#21270;&#38754;&#35797;&#24773;&#22659;&#30340;&#22810;&#27169;&#24577;&#21387;&#21147;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario. (arXiv:2303.07742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#25968;&#23383;&#21270;&#38754;&#35797;&#24773;&#22659;&#26469;&#35825;&#21457;&#21387;&#21147;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#25968;&#25454;&#12289;&#36830;&#32493;&#26631;&#27880;&#21644;&#22522;&#20934;&#20998;&#31867;&#22120;&#30340;&#21387;&#21147;&#25968;&#25454;&#38598;&#12290;&#26368;&#20339;&#34920;&#29616;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;88.3%&#21644;87.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#21387;&#21147;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25968;&#23383;&#21270;&#38754;&#35797;&#26469;&#35825;&#21457;&#21387;&#21147;&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;40&#21517;&#21442;&#19982;&#32773;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#65288;&#21160;&#20316;&#25429;&#25417;&#12289;&#38754;&#37096;&#35782;&#21035;&#12289;&#30524;&#21160;&#36861;&#36394;&#65289;&#20197;&#21450;&#29983;&#29702;&#20449;&#24687;&#65288;&#20809;&#30005;&#33033;&#25615;&#22270;&#12289;&#30382;&#32932;&#30005;&#21453;&#24212;&#65289;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#20102;&#21387;&#21147;&#21644;&#21457;&#29983;&#24773;&#32490;&#65288;&#22914;&#32670;&#32827;&#12289;&#24868;&#24594;&#12289;&#28966;&#34385;&#12289;&#24778;&#35766;&#65289;&#30340;&#26102;&#38388;&#36830;&#32493;&#26631;&#27880;&#12290;&#20026;&#20102;&#24314;&#31435;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#38024;&#23545;&#20108;&#20803;&#21387;&#21147;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#35780;&#20272;&#20102;&#20116;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65288;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;K&#36817;&#37051;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65289;&#12290;&#26368;&#20339;&#34920;&#29616;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#20026;88.3%&#21644;87.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-modal stress dataset that uses digital job interviews to induce stress. The dataset provides multi-modal data of 40 participants including audio, video (motion capturing, facial recognition, eye tracking) as well as physiological information (photoplethysmography, electrodermal activity). In addition to that, the dataset contains time-continuous annotations for stress and occurred emotions (e.g. shame, anger, anxiety, surprise). In order to establish a baseline, five different machine learning classifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest, Long-Short-Term Memory Network) have been trained and evaluated on the proposed dataset for a binary stress classification task. The best-performing classifier achieved an accuracy of 88.3% and an F1-score of 87.5%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#21457;&#29616;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#26102;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;</title><link>http://arxiv.org/abs/2303.07735</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#33021;&#20570;&#31639;&#26415;&#21527;&#65311;&#23545;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23383;&#25216;&#33021;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Can neural networks do arithmetic? A survey on the elementary numerical skills of state-of-the-art deep learning models. (arXiv:2303.07735v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#21457;&#29616;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#26102;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#33021;&#23637;&#31034;&#22797;&#26434;&#25512;&#29702;&#25216;&#33021;&#30340;&#23398;&#20064;&#27169;&#22411;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#32780;&#25968;&#23398;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#35780;&#20272;&#31185;&#23398;&#36827;&#27493;&#26041;&#21521;&#30340;&#30446;&#26631;&#39046;&#22495;&#20043;&#19968;&#12290;&#36807;&#21435;&#20960;&#24180;&#37324;&#24050;&#32463;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#35774;&#35745;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12289;&#25968;&#20540;&#31215;&#20998;&#21644;&#26032;&#29468;&#24819;&#25110;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#30340;&#21457;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#32489;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25968;&#37327;&#21644;&#31526;&#21495;&#25968;&#23383;&#30340;&#22522;&#26412;&#29702;&#35299;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#23457;&#26680;&#65292;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22312;&#38754;&#23545;&#30456;&#23545;&#31616;&#21333;&#30340;&#27979;&#35797;&#22522;&#26412;&#25968;&#20540;&#21644;&#31639;&#26415;&#30693;&#35782;&#30340;&#20219;&#21153;&#26102;&#65292;&#20063;&#32463;&#24120;&#26080;&#27861;&#32988;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating learning models that can exhibit sophisticated reasoning skills is one of the greatest challenges in deep learning research, and mathematics is rapidly becoming one of the target domains for assessing scientific progress in this direction. In the past few years there has been an explosion of neural network architectures, data sets, and benchmarks specifically designed to tackle mathematical problems, reporting notable success in disparate fields such as automated theorem proving, numerical integration, and discovery of new conjectures or matrix multiplication algorithms. However, despite these impressive achievements it is still unclear whether deep learning models possess an elementary understanding of quantities and symbolic numbers. In this survey we critically examine the recent literature, concluding that even state-of-the-art architectures often fall short when probed with relatively simple tasks designed to test basic numerical and arithmetic knowledge.
&lt;/p&gt;</description></item><item><title>Reinforcer&#27169;&#22411;&#20351;&#29992;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20013;&#25991;G2P&#20013;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21477;&#23376;&#36807;&#20110;&#26222;&#36941;&#21644;&#35789;&#36793;&#30028;&#20998;&#21106;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07726</link><description>&lt;p&gt;
&#20165;&#38656;&#22909;&#30340;&#37051;&#23621;&#65306;&#22522;&#20110;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#30340;&#20013;&#25991;&#23383;&#32032;&#21040;&#38899;&#32032;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion. (arXiv:2303.07726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07726
&lt;/p&gt;
&lt;p&gt;
Reinforcer&#27169;&#22411;&#20351;&#29992;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20013;&#25991;G2P&#20013;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21477;&#23376;&#36807;&#20110;&#26222;&#36941;&#21644;&#35789;&#36793;&#30028;&#20998;&#21106;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20013;&#25991;&#23383;&#32032;&#21040;&#38899;&#32032;&#65288;G2P&#65289;&#31995;&#32479;&#37319;&#29992;&#19977;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#24207;&#21015;&#36716;&#21270;&#20026;&#23383;&#31526;&#23884;&#20837;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#28982;&#21518;&#22522;&#20110;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#39044;&#27979;&#38899;&#32032;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#35821;&#35328;&#30693;&#35782;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#32534;&#30721;&#21477;&#23376;&#30340;&#36807;&#20110;&#26222;&#36941;&#30340;&#32467;&#26500;&#65292;&#24182;&#26410;&#28085;&#30422;&#20351;&#29992;&#35821;&#38899;&#30693;&#35782;&#25152;&#38656;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#25163;&#24037;&#21518;&#22788;&#29702;&#31995;&#32479;&#26469;&#35299;&#20915;&#19982;&#23383;&#31526;&#38899;&#35843;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#22312;&#35789;&#36793;&#30028;&#20998;&#21106;&#19978;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;G2P&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Reinforcer&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#37051;&#36817;&#23383;&#31526;&#20043;&#38388;&#30340;&#35821;&#38899;&#20449;&#24687;&#26469;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24378;&#22823;&#30340;&#24341;&#23548;&#20559;&#24046;&#65292;&#20197;&#24110;&#21161;&#28040;&#38500;&#21457;&#38899;&#30340;&#27495;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#20013;&#25991;G2P&#31995;&#32479;&#30456;&#27604;&#65292;Reinforcer&#27169;&#22411;&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework that first transforms input sequences into character embeddings, obtains linguistic information using language models, and then predicts the phonemes based on global context about the entire input sequence. However, linguistic knowledge alone is often inadequate. Language models frequently encode overly general structures of a sentence and fail to cover specific cases needed to use phonetic knowledge. Also, a handcrafted post-processing system is needed to address the problems relevant to the tone of the characters. However, the system exhibits inconsistency in the segmentation of word boundaries which consequently degrades the performance of the G2P system. To address these issues, we propose the Reinforcer that provides strong inductive bias for language models by emphasizing the phonological information between neighboring characters to help disambiguate pronunciations. Experimental results show that the R
&lt;/p&gt;</description></item><item><title>DisCoHead&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#29983;&#25104;&#35848;&#35805;&#22836;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#31163;&#21644;&#25511;&#21046;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#30340;&#20960;&#20309;&#21464;&#25442;&#21644;&#31070;&#32463;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#23558;&#23494;&#38598;&#36816;&#21160;&#20272;&#35745;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#32534;&#30721;&#22120;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#22836;&#37096;&#36816;&#21160;&#20855;&#26377;&#26356;&#39640;&#30340;&#36136;&#37327;&#21644;&#21516;&#27493;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07697</link><description>&lt;p&gt;
DisCoHead:&#36890;&#36807;&#20998;&#31163;&#25511;&#21046;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#38899;&#35270;&#39057;&#29983;&#25104;&#35848;&#35805;&#22836;
&lt;/p&gt;
&lt;p&gt;
DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions. (arXiv:2303.07697v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07697
&lt;/p&gt;
&lt;p&gt;
DisCoHead&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#38899;&#35270;&#39057;&#29983;&#25104;&#35848;&#35805;&#22836;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#31163;&#21644;&#25511;&#21046;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#30340;&#20960;&#20309;&#21464;&#25442;&#21644;&#31070;&#32463;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#23558;&#23494;&#38598;&#36816;&#21160;&#20272;&#35745;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#32534;&#30721;&#22120;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#22836;&#37096;&#36816;&#21160;&#20855;&#26377;&#26356;&#39640;&#30340;&#36136;&#37327;&#21644;&#21516;&#27493;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36924;&#30495;&#30340;&#35848;&#35805;&#22836;&#29983;&#25104;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#33258;&#28982;&#30340;&#22836;&#37096;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DisCoHead&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#30417;&#30563;&#21363;&#21487;&#20998;&#31163;&#21644;&#25511;&#21046;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#12290;DisCoHead&#20351;&#29992;&#21333;&#19968;&#30340;&#20960;&#20309;&#21464;&#25442;&#20316;&#20026;&#29942;&#39048;&#65292;&#20174;&#22836;&#37096;&#39537;&#21160;&#35270;&#39057;&#20013;&#38548;&#31163;&#24182;&#25552;&#21462;&#22836;&#37096;&#36816;&#21160;&#12290;&#21487;&#20197;&#20351;&#29992;&#20223;&#23556;&#21464;&#25442;&#25110;&#34180;&#26495;&#26679;&#26465;&#21464;&#25442;&#65292;&#20004;&#32773;&#37117;&#21487;&#20197;&#20316;&#20026;&#20960;&#20309;&#29942;&#39048;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23494;&#38598;&#36816;&#21160;&#20272;&#35745;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#32534;&#30721;&#22120;&#38598;&#25104;&#21040;&#19968;&#36215;&#26469;&#22686;&#24378;DisCoHead&#30340;&#25928;&#29575;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#32534;&#30721;&#22120;&#38544;&#24335;&#22320;&#20272;&#35745;&#21644;&#24212;&#29992;&#23494;&#38598;&#36816;&#21160;&#12290;&#22312;&#23558;&#24050;&#20998;&#31163;&#30340;&#22836;&#37096;&#36816;&#21160;&#24212;&#29992;&#20110;&#28304;&#36523;&#20221;&#20043;&#21518;&#65292;DisCoHead&#26681;&#25454;&#35821;&#38899;&#38899;&#39057;&#25511;&#21046;&#22068;&#37096;&#21306;&#22495;&#65292;&#26681;&#25454;&#21333;&#29420;&#30340;&#39537;&#21160;&#35270;&#39057;&#30504;&#30524;&#21644;&#31227;&#21160;&#30473;&#27611;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DisCoHead&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#21516;&#27493;&#31934;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For realistic talking head generation, creating natural head motion while maintaining accurate lip synchronization is essential. To fulfill this challenging task, we propose DisCoHead, a novel method to disentangle and control head pose and facial expressions without supervision. DisCoHead uses a single geometric transformation as a bottleneck to isolate and extract head motion from a head-driving video. Either an affine or a thin-plate spline transformation can be used and both work well as geometric bottlenecks. We enhance the efficiency of DisCoHead by integrating a dense motion estimator and the encoder of a generator which are originally separate modules. Taking a step further, we also propose a neural mix approach where dense motion is estimated and applied implicitly by the encoder. After applying the disentangled head motion to a source identity, DisCoHead controls the mouth region according to speech audio, and it blinks eyes and moves eyebrows following a separate driving vid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07693</link><description>&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Policy Learning for Offline-to-Online Reinforcement Learning. (arXiv:2303.07693v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#19968;&#20010;&#29615;&#22659;&#26469;&#25910;&#38598;&#26032;&#40092;&#30340;&#25968;&#25454;&#65292;&#20294;&#24403;&#22312;&#32447;&#20132;&#20114;&#25104;&#26412;&#39640;&#26114;&#26102;&#19981;&#20999;&#23454;&#38469;&#12290;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#30452;&#25509;&#20174;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#22914;&#26524;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24046;&#65292;&#23558;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#31163;&#32447;&#21040;&#22312;&#32447;&#30340;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#20195;&#29702;&#39318;&#20808;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#28982;&#21518;&#20877;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#31574;&#30053;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26174;&#24335;&#32771;&#34385;&#20102;&#22312;&#32447;&#21644;&#31163;&#32447;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#20102;&#33258;&#36866;&#24212;&#26356;&#26032;&#31574;&#30053;&#65292;&#21363;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#37319;&#29992;&#24754;&#35266;&#26356;&#26032;&#31574;&#30053;&#65292;&#32780;&#23545;&#22312;&#32447;&#25968;&#25454;&#38598;&#37319;&#29992;&#20048;&#35266;/&#36138;&#24515;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#24182;&#23454;&#29616;&#20004;&#32773;&#26368;&#20339;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#20122;&#32447;&#24615;&#21518;&#24724;&#30028;&#65292;&#19982;&#26377;&#21516;&#26102;&#35775;&#38382;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#30340;oracle&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;&#20960;&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.07685</link><description>&lt;p&gt;
FPTN:&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FPTN: Fast Pure Transformer Network for Traffic Flow Forecasting. (arXiv:2303.07685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24555;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#35270;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22826;&#22810;&#30340;&#20256;&#24863;&#22120;&#20250;&#23548;&#33268;&#19968;&#20010;&#22823;&#20110;800&#30340;&#21521;&#37327;&#65292;&#36825;&#24456;&#38590;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#35774;&#35745;&#20102;&#22797;&#26434;&#30340;&#26426;&#21046;&#26469;&#25429;&#33719;MTS&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#39044;&#27979;&#36895;&#24230;&#32531;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32431;Transformer&#32593;&#32476;&#65288;FPTN&#65289;&#12290;&#39318;&#20808;&#65292;&#23558;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#27839;&#20256;&#24863;&#22120;&#32500;&#24230;&#32780;&#38750;&#26102;&#38388;&#32500;&#24230;&#21010;&#20998;&#20026;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20805;&#20998;&#34920;&#31034;&#22797;&#26434;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#23884;&#20837;&#26041;&#24335;&#23558;&#36825;&#20123;&#21521;&#37327;&#25237;&#24433;&#21040;&#36866;&#24403;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#20043;&#21518;&#65292;&#20026;&#20102;&#21516;&#26102;&#25429;&#33719;&#36825;&#20123;&#21521;&#37327;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Transformer&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#39044;&#27979;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FPTN&#19981;&#20165;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36816;&#34892;&#36895;&#24230;&#24555;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow forecasting is challenging due to the intricate spatio-temporal correlations in traffic flow data. Existing Transformer-based methods usually treat traffic flow forecasting as multivariate time series (MTS) forecasting. However, too many sensors can cause a vector with a dimension greater than 800, which is difficult to process without information loss. In addition, these methods design complex mechanisms to capture spatial dependencies in MTS, resulting in slow forecasting speed. To solve the abovementioned problems, we propose a Fast Pure Transformer Network (FPTN) in this paper. First, the traffic flow data are divided into sequences along the sensor dimension instead of the time dimension. Then, to adequately represent complex spatio-temporal correlations, Three types of embeddings are proposed for projecting these vectors into a suitable vector space. After that, to capture the complex spatio-temporal correlations simultaneously in these vectors, we utilize Transforme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Brain-Score&#35780;&#20272;64&#20010;CNN&#27169;&#22411;&#20013;&#19982;&#22270;&#20687;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.07679</link><description>&lt;p&gt;
&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#25152;&#38656;&#30340;&#29305;&#24449;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Feature representations useful for predicting image memorability. (arXiv:2303.07679v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Brain-Score&#35780;&#20272;64&#20010;CNN&#27169;&#22411;&#20013;&#19982;&#22270;&#20687;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#24050;&#32463;&#21560;&#24341;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#24050;&#32463;&#25509;&#36817;&#22522;&#20110;&#20154;&#31867;&#19968;&#33268;&#24615;&#20272;&#35745;&#30340;&#32463;&#39564;&#19978;&#38480;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#23884;&#20837;&#22312;CNN&#27169;&#22411;&#20013;&#30340;&#21738;&#20123;&#29305;&#24449;&#34920;&#31034;&#23545;&#20110;&#35760;&#24518;&#21147;&#30340;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#36127;&#36131;&#20173;&#28982;&#26159;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#33041;&#37096;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;CNN&#27169;&#22411;&#20013;&#19982;&#35760;&#24518;&#21147;&#26377;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;Brain-Score&#35780;&#20272;&#20102;&#22312;64&#20010;&#29992;&#20110;&#29289;&#20307;&#35782;&#21035;&#30340;CNN&#27169;&#22411;&#30340;16,860&#23618;&#20013;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#33041;&#37096;&#30456;&#20284;&#24615;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#20998;&#26512;&#26174;&#31034;&#20986;&#19968;&#20010;&#26126;&#26174;&#30340;&#36235;&#21183;&#65292;&#21363;&#20855;&#26377;&#39640;&#35760;&#24518;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23618;&#19982;&#26368;&#39640;&#38454;&#27573;&#30340;&#39070;&#19979;&#30382;&#36136;&#65288;IT&#65289;&#30340;&#33041;&#37096;&#30456;&#20284;&#24615;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#23545;64&#20010;CNN&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#39044;&#27979;&#22270;&#20687;&#35760;&#24518;&#21147;&#24182;&#27809;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;CNN&#27169;&#22411;&#24050;&#32463;&#21253;&#21547;&#20102;&#19982;&#35760;&#24518;&#21147;&#30456;&#20851;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting image memorability has attracted interest in various fields. Consequently, prediction accuracy with convolutional neural network (CNN) models has been approaching the empirical upper bound estimated based on human consistency. However, identifying which feature representations embedded in CNN models are responsible for such high prediction accuracy of memorability remains an open question. To tackle this problem, this study sought to identify memorability-related feature representations in CNN models using brain similarity. Specifically, memorability prediction accuracy and brain similarity were examined and assessed by Brain-Score across 16,860 layers in 64 CNN models pretrained for object recognition. A clear tendency was shown in this comprehensive analysis that layers with high memorability prediction accuracy had higher brain similarity with the inferior temporal (IT) cortex, which is the highest stage in the ventral visual pathway. Furthermore, fine-tuning the 64 CNN m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#27010;&#29575;&#36136;&#37327;&#27969;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#31038;&#20132;&#32593;&#32476;&#35774;&#32622;&#20013;&#31038;&#21306;&#28436;&#21464;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07675</link><description>&lt;p&gt;
Sinkhorn-Flow: &#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#27010;&#29575;&#36136;&#37327;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems Using Optimal Transport. (arXiv:2303.07675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#27010;&#29575;&#36136;&#37327;&#27969;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#31038;&#20132;&#32593;&#32476;&#35774;&#32622;&#20013;&#31038;&#21306;&#28436;&#21464;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#39044;&#27979;&#31163;&#25955;&#21464;&#37327;&#20998;&#24067;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#38598;&#20013;&#20110;&#39044;&#27979;&#38543;&#21518;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#65292;&#32780;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#36825;&#31181;&#27010;&#29575;&#36136;&#37327;&#22312;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#38543;&#26102;&#38388;&#27969;&#21160;&#30340;&#26041;&#24335;&#26159;&#19968;&#39033;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#39044;&#27979;&#36825;&#31181;&#36136;&#37327;&#27969;&#37327;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#39044;&#27979;&#36755;&#36816;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#29992; Sinkhorn &#36845;&#20195;&#26367;&#25442;&#26631;&#20934;&#30340; softmax &#25805;&#20316;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#22312;&#31038;&#20132;&#32593;&#32476;&#35774;&#32622;&#20013;&#39044;&#27979;&#31038;&#21306;&#23558;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#20219;&#21153;&#65292;&#24182;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25913;&#36827;&#26367;&#20195;&#39044;&#27979;&#26041;&#27861;&#26041;&#38754;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#20102;&#22312;&#39044;&#27979;&#20044;&#20811;&#20848;&#35758;&#20250;&#25237;&#31080;&#20013;&#27966;&#31995;&#28436;&#21464;&#20219;&#21153;&#26041;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how distributions over discrete variables vary over time is a common task in time series forecasting. But whereas most approaches focus on merely predicting the distribution at subsequent time steps, a crucial piece of information in many settings is to determine how this probability mass flows between the different elements over time. We propose a new approach to predicting such mass flow over time using optimal transport. Specifically, we propose a generic approach to predicting transport matrices in end-to-end deep learning systems, replacing the standard softmax operation with Sinkhorn iterations. We apply our approach to the task of predicting how communities will evolve over time in social network settings, and show that the approach improves substantially over alternative prediction methods. We specifically highlight results on the task of predicting faction evolution in Ukrainian parliamentary voting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#32763;&#35793;&#30340;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;ceT1&#25195;&#25551;&#36716;&#25442;&#20026;hrT2&#25195;&#25551;&#65292;&#26469;&#36827;&#34892;&#26410;&#26631;&#35760;&#30340;hrT2&#25195;&#25551;&#30340;Koos&#20998;&#31867;&#65292;&#36991;&#20813;&#20102;&#26080;&#27880;&#37322;hrT2&#25195;&#25551;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07674</link><description>&lt;p&gt;
&#22522;&#20110;&#24433;&#20687;&#32763;&#35793;&#30340;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#36827;&#34892;Koos&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Koos Classification of Vestibular Schwannoma via Image Translation-Based Unsupervised Cross-Modality Domain Adaptation. (arXiv:2303.07674v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#32763;&#35793;&#30340;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;ceT1&#25195;&#25551;&#36716;&#25442;&#20026;hrT2&#25195;&#25551;&#65292;&#26469;&#36827;&#34892;&#26410;&#26631;&#35760;&#30340;hrT2&#25195;&#25551;&#30340;Koos&#20998;&#31867;&#65292;&#36991;&#20813;&#20102;&#26080;&#27880;&#37322;hrT2&#25195;&#25551;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koos&#20998;&#32423;&#26631;&#20934;&#26159;&#35786;&#26029;&#21069;&#24237;&#31070;&#32463;&#38808;&#30244;&#65288;VS&#65289;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#25551;&#36848;&#30244;&#20307;&#21450;&#20854;&#23545;&#30456;&#37051;&#33041;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#23545;&#27604;&#22686;&#24378;T1&#65288;ceT1&#65289;&#25195;&#25551;&#21644;&#39640;&#20998;&#36776;&#29575;T2&#65288;hrT2&#65289;&#25195;&#25551;&#22343;&#21487;&#29992;&#20110;Koos&#20998;&#31867;&#65292;&#20294;&#30001;&#20110;hrT2&#25195;&#25551;&#26356;&#23433;&#20840;&#12289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#65292;&#22240;&#27492;hrT2&#25195;&#25551;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;hrT2&#25195;&#25551;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#20250;&#22240;&#26080;&#30417;&#30563;&#23398;&#20064;&#32780;&#24615;&#33021;&#19979;&#38477;&#12290;&#22914;&#26524;ceT1&#25195;&#25551;&#21450;&#20854;&#27880;&#37322;&#21487;&#29992;&#20110;hrT2&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;hrT2&#25195;&#25551;&#36827;&#34892;Koos&#20998;&#31867;&#30340;&#24615;&#33021;&#23558;&#22823;&#22823;&#25552;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#32763;&#35793;&#30340;&#26080;&#30417;&#30563;&#36328;&#27169;&#24577;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#23558;&#24102;&#27880;&#37322;&#30340;ceT1&#25195;&#25551;&#36716;&#25442;&#20026;hrT2&#25195;&#25551;&#65292;&#28982;&#21518;&#29992;&#20854;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#36827;&#34892;&#26410;&#26631;&#35760;&#30340;hrT2&#25195;&#25551;&#30340;Koos&#20998;&#31867;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#27880;&#37322;&#26102;&#20351;&#29992;hrT2&#25195;&#25551;&#21487;&#23454;&#29616;&#31934;&#30830;&#30340;Koos&#20998;&#31867;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koos grading scale is a classification system for vestibular schwannoma (VS) used to characterize the tumor and its effects on adjacent brain structures. The Koos classification captures many of the characteristics of treatment deci-sions and is often used to determine treatment plans. Although both contrast-enhanced T1 (ceT1) scanning and high-resolution T2 (hrT2) scanning can be used for Koos Classification, hrT2 scanning is gaining interest because of its higher safety and cost-effectiveness. However, in the absence of annotations for hrT2 scans, deep learning methods often inevitably suffer from performance deg-radation due to unsupervised learning. If ceT1 scans and their annotations can be used for unsupervised learning of hrT2 scans, the performance of Koos classifi-cation using unlabeled hrT2 scans will be greatly improved. In this regard, we propose an unsupervised cross-modality domain adaptation method based on im-age translation by transforming annotated ceT1 scans into
&lt;/p&gt;</description></item><item><title>AutoTransfer&#26159;&#19968;&#31181;&#24102;&#26377;&#30693;&#35782;&#36716;&#31227;&#30340;AutoML&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20808;&#21069;&#30340;GNN&#26550;&#26500;&#35774;&#35745;&#30693;&#35782;&#36716;&#31227;&#21040;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20219;&#21153;&#23884;&#20837;&#21644;&#20219;&#21153;&#27169;&#22411;&#24211;&#26469;&#23547;&#25214;&#29702;&#24819;&#27169;&#22411;&#30340;&#35774;&#35745;&#20808;&#39564;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07669</link><description>&lt;p&gt;
AutoTransfer: &#24102;&#26377;&#30693;&#35782;&#36716;&#31227;&#30340;AutoML&#8212;&#8212;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AutoTransfer: AutoML with Knowledge Transfer -- An Application to Graph Neural Networks. (arXiv:2303.07669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07669
&lt;/p&gt;
&lt;p&gt;
AutoTransfer&#26159;&#19968;&#31181;&#24102;&#26377;&#30693;&#35782;&#36716;&#31227;&#30340;AutoML&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20808;&#21069;&#30340;GNN&#26550;&#26500;&#35774;&#35745;&#30693;&#35782;&#36716;&#31227;&#21040;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20219;&#21153;&#23884;&#20837;&#21644;&#20219;&#21153;&#27169;&#22411;&#24211;&#26469;&#23547;&#25214;&#29702;&#24819;&#27169;&#22411;&#30340;&#35774;&#35745;&#20808;&#39564;&#65292;&#20197;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AutoML&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#35780;&#20215;&#25351;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#23547;&#25214;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;AutoML&#25216;&#26415;&#29420;&#31435;&#22320;&#32771;&#34385;&#27599;&#20010;&#20219;&#21153;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#35768;&#22810;&#32467;&#26500;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTransfer&#65292;&#36825;&#26159;&#19968;&#31181;AutoML&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#21521;&#24863;&#20852;&#36259;&#30340;&#26032;&#20219;&#21153;&#36716;&#31227;&#20808;&#21069;&#30340;&#24314;&#31569;&#35774;&#35745;&#30693;&#35782;&#26469;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#19968;&#20010;&#20219;&#21153;&#27169;&#22411;&#24211;&#65292;&#23427;&#25429;&#25417;&#20102;&#23545;&#21508;&#31181;&#19981;&#21516;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#21644;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20197;&#21450;&#19968;&#31181;&#33021;&#22815;&#20934;&#30830;&#34913;&#37327;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#35745;&#31639;&#26377;&#25928;&#20219;&#21153;&#23884;&#20837;&#12290;&#22522;&#20110;&#20219;&#21153;&#27169;&#22411;&#24211;&#21644;&#20219;&#21153;&#23884;&#20837;&#65292;&#36890;&#36807;&#32858;&#21512;&#19982;&#20219;&#21153;&#30456;&#20284;&#30340;&#39030;&#32423;&#35774;&#35745;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#26032;&#20219;&#21153;&#30340;&#29702;&#24819;&#27169;&#22411;&#30340;&#35774;&#35745;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational cost. Here we propose AutoTransfer, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23558;&#25968;&#25454;&#28857;&#21644;&#20219;&#21153;&#36830;&#25509;&#36215;&#26469;&#30340;&#30693;&#35782;&#22270;&#65292;&#21033;&#29992;&#26469;&#33258;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#25454;&#28857;&#26631;&#31614;&#26469;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07666</link><description>&lt;p&gt;
&#20851;&#31995;&#22810;&#20219;&#21153;&#23398;&#20064;&#65306;&#24314;&#27169;&#25968;&#25454;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Relational Multi-Task Learning: Modeling Relations between Data and Tasks. (arXiv:2303.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23558;&#25968;&#25454;&#28857;&#21644;&#20219;&#21153;&#36830;&#25509;&#36215;&#26469;&#30340;&#30693;&#35782;&#22270;&#65292;&#21033;&#29992;&#26469;&#33258;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#25454;&#28857;&#26631;&#31614;&#26469;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#26159;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#22810;&#20219;&#21153;&#27169;&#22411;&#21482;&#33021;&#35775;&#38382;&#32473;&#23450;&#25968;&#25454;&#28857;&#65292;&#32780;&#19981;&#33021;&#35775;&#38382;&#26469;&#33258;&#20854;&#20182;&#20219;&#21153;&#30340;&#25968;&#25454;&#28857;&#26631;&#31614;&#12290;&#36825;&#20026;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#25193;&#23637;&#21040;&#21033;&#29992;&#26469;&#33258;&#20854;&#20182;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#25454;&#28857;&#26631;&#31614;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#24182;&#22312;&#36825;&#31181;&#26041;&#24335;&#19978;&#25552;&#39640;&#26032;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#36741;&#21161;&#20219;&#21153;&#30340;&#25968;&#25454;&#28857;&#26631;&#31614;&#26469;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;MetaLink&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#26500;&#24314;&#19968;&#20010;&#36830;&#25509;&#25968;&#25454;&#28857;&#21644;&#20219;&#21153;&#30340;&#30693;&#35782;&#22270;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#36741;&#21161;&#20219;&#21153;&#30340;&#26631;&#31614;&#12290;&#30693;&#35782;&#22270;&#30001;&#20004;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#32452;&#25104;&#65306;&#65288;1&#65289;&#25968;&#25454;&#33410;&#28857;&#65292;&#20854;&#20013;&#33410;&#28857;&#29305;&#24449;&#26159;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#25968;&#25454;&#23884;&#20837;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#33410;&#28857;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#12290;&#35813;&#30693;&#35782;&#22270;&#20013;&#30340;&#36793;&#25429;&#25417;&#25968;&#25454;&#20219;&#21153;&#20851;&#31995;&#65292;&#36793;&#26631;&#31614;&#34920;&#31034;&#36825;&#20123;&#20851;&#31995;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point's labels from other tasks. This presents an opportunity to extend multi-task learning to utilize data point's labels from other auxiliary tasks, and this way improves performance on the new task. Here we introduce a novel relational multi-task learning setting where we leverage data point labels from auxiliary tasks to make more accurate predictions on the new task. We develop MetaLink, where our key innovation is to build a knowledge graph that connects data points and tasks and thus allows us to leverage labels from auxiliary tasks. The knowledge graph consists of two types of nodes: (1) data nodes, where node features are data embeddings computed by the neural network, and (2) task nodes, with the last layer's weights for each task as node features. The edges in this knowledge graph capture data-task relationships, and the edge la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.07647</link><description>&lt;p&gt;
&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advances and Applications of Machine Learning in Experimental Solid Mechanics: A Review. (arXiv:2303.07647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#29289;&#29702;&#24863;&#30693;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#31561;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#22312;&#34920;&#24449;&#21644;&#29702;&#35299;&#22825;&#28982;&#21644;&#26032;&#26448;&#26009;&#30340;&#21147;&#23398;&#24615;&#36136;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#21253;&#25324;&#23454;&#39564;&#35774;&#35745;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21453;&#38382;&#39064;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#22240;&#27492;&#21450;&#26102;&#36827;&#34892;&#20840;&#38754;&#21644;&#26356;&#26032;&#30340;&#32508;&#36848;&#65292;&#23545;&#20110;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#19982;&#35813;&#32508;&#36848;&#30456;&#20851;&#30340;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#26415;&#35821;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#29702;&#23398;&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20840;&#38754;&#28085;&#30422;&#20102;&#23454;&#39564;&#21147;&#23398;&#20256;&#32479;&#21644;&#26032;&#20852;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#21253;&#25324;&#26029;&#35010;&#21147;&#23398;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#32435;&#31859;&#21644;&#24494;&#35266;&#21147;&#23398;&#12289;&#26500;&#24314;&#26448;&#26009;&#21644;&#20108;&#32500;&#26448;&#26009;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#27963;&#36291;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#22266;&#20307;&#21147;&#23398;&#26410;&#26469;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many decades, experimental solid mechanics has played a crucial role in characterizing and understanding the mechanical properties of natural and novel materials. Recent advances in machine learning (ML) provide new opportunities for the field, including experimental design, data analysis, uncertainty quantification, and inverse problems. As the number of papers published in recent years in this emerging field is exploding, it is timely to conduct a comprehensive and up-to-date review of recent ML applications in experimental solid mechanics. Here, we first provide an overview of common ML algorithms and terminologies that are pertinent to this review, with emphasis placed on physics-informed and physics-based ML methods. Then, we provide thorough coverage of recent ML applications in traditional and emerging areas of experimental mechanics, including fracture mechanics, biomechanics, nano- and micro-mechanics, architected materials, and 2D material. Finally, we highlight some curr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#32431;&#22797;&#21512;&#20307;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20108;&#38454;&#21333;&#32431;&#22797;&#21512;&#20307;&#26469;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20851;&#31995;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#21035;&#30340;&#32593;&#32476;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Cheeger&#19981;&#31561;&#24335;&#30340;&#21333;&#32431;&#20809;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07646</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#32431;&#22797;&#21512;&#20307;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering with Simplicial Complexes. (arXiv:2303.07646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#32431;&#22797;&#21512;&#20307;&#36827;&#34892;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20108;&#38454;&#21333;&#32431;&#22797;&#21512;&#20307;&#26469;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20851;&#31995;&#20197;&#23454;&#29616;&#26356;&#39640;&#32423;&#21035;&#30340;&#32593;&#32476;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Cheeger&#19981;&#31561;&#24335;&#30340;&#21333;&#32431;&#20809;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#22522;&#20110;&#20108;&#38454;&#21333;&#32431;&#22797;&#21512;&#20307;(&#20063;&#31216;&#20026;&#22635;&#20805;&#19977;&#35282;&#24418;)&#23545;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#65292;&#20197;&#21033;&#29992;&#26356;&#39640;&#32423;&#21035;&#30340;&#32593;&#32476;&#20132;&#20114;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21333;&#32431;&#23548;&#32435;&#20989;&#25968;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#35813;&#20989;&#25968;&#65292;&#24471;&#21040;&#39640;&#23494;&#24230;&#30340;&#22635;&#20805;&#19977;&#35282;&#24418;&#22312;&#38598;&#21512;&#20869;&#65292;&#32780;&#22312;&#38598;&#21512;&#20043;&#38388;&#30340;&#22635;&#20805;&#19977;&#35282;&#24418;&#23494;&#24230;&#36739;&#23567;&#30340;&#26368;&#20248;&#20998;&#32452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#32431;&#37051;&#25509;&#31639;&#23376;&#65292;&#36890;&#36807;&#20108;&#38454;&#21333;&#32431;&#22797;&#21512;&#20307;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#33879;&#21517;&#30340;Cheeger&#19981;&#31561;&#24335;&#25193;&#23637;&#21040;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#32858;&#31867;&#20013;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;Cheeger&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#32431;&#20809;&#35889;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#25253;&#21578;&#23454;&#39564;&#32467;&#26524;&#65292;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a new clustering algorithm to group nodes in networks based on second-order simplices (aka filled triangles) to leverage higher-order network interactions. We define a simplicial conductance function, which on minimizing, yields an optimal partition with a higher density of filled triangles within the set while the density of filled triangles is smaller across the sets. To this end, we propose a simplicial adjacency operator that captures the relation between the nodes through second-order simplices. This allows us to extend the well-known Cheeger inequality to cluster a simplicial complex. Then, leveraging the Cheeger inequality, we propose the simplicial spectral clustering algorithm. We report results from numerical experiments on synthetic and real-world network data to demonstrate the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#31232;&#26377;&#20107;&#20214;&#20013;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25552;&#20379;&#27491;&#30830;&#33218;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#30053;&#24494;&#22686;&#21152;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07627</link><description>&lt;p&gt;
&#31232;&#26377;&#20107;&#20214;&#20013;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Best arm identification in rare events. (arXiv:2303.07627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#31232;&#26377;&#20107;&#20214;&#20013;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#25552;&#20379;&#27491;&#30830;&#33218;&#30340;&#36873;&#25321;&#65292;&#21516;&#26102;&#30053;&#24494;&#22686;&#21152;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#33218;&#37117;&#26377;&#19968;&#20010;&#24494;&#23567;&#30340;&#27010;&#29575;&#23454;&#29616;&#22823;&#37327;&#22238;&#25253;&#65292;&#32780;&#20197;&#21387;&#20498;&#24615;&#30340;&#27010;&#29575;&#22238;&#25253;&#20026;&#38646;&#12290;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#24212;&#29992;&#26159;&#22312;&#32447;&#24191;&#21578;&#65292;&#20854;&#20013;&#24191;&#21578;&#30340;&#28857;&#20987;&#29575;&#21487;&#33021;&#20165;&#21344;&#19981;&#21040;&#30334;&#20998;&#20043;&#19968;&#65292;&#32780;&#26368;&#32456;&#36716;&#21270;&#20026;&#38144;&#21806;&#30340;&#21033;&#28070;&#21487;&#33021;&#20877;&#27425;&#21482;&#26159;&#28857;&#20987;&#29575;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26368;&#36817;&#65292;&#24050;&#24320;&#21457;&#20102;&#35299;&#20915;BAI&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#22312;&#27491;&#30830;&#36873;&#25321;&#33218;&#26102;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#27491;&#22914;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#37027;&#26679;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#27599;&#20010;&#33218;&#30340;&#22870;&#21169;&#36807;&#31243;&#21487;&#20197;&#24456;&#22909;&#22320;&#36817;&#20284;&#20026;&#22797;&#21512;&#27850;&#26494;&#36807;&#31243;&#30340;&#20107;&#23454;&#65292;&#25552;&#20986;&#20102;&#26356;&#24555;&#30340;&#31639;&#27861;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#30053;&#26377;&#22686;&#21152;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#36827;&#34892;&#20998;&#26512;&#65292;&#24403;&#22870;&#21169;&#21457;&#29983;&#30340;&#31232;&#26377;&#24615;&#38477;&#20302;&#20026;&#38646;&#65292;&#22870;&#21169;&#37329;&#39069;&#22686;&#21152;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the best arm identification problem in the stochastic multi-armed bandit framework where each arm has a tiny probability of realizing large rewards while with overwhelming probability the reward is zero. A key application of this framework is in online advertising where click rates of advertisements could be a fraction of a single percent and final conversion to sales, while highly profitable, may again be a small fraction of the click rates. Lately, algorithms for BAI problems have been developed that minimise sample complexity while providing statistical guarantees on the correct arm selection. As we observe, these algorithms can be computationally prohibitive. We exploit the fact that the reward process for each arm is well approximated by a Compound Poisson process to arrive at algorithms that are faster, with a small increase in sample complexity. We analyze the problem in an asymptotic regime as rarity of reward occurrence reduces to zero, and reward amounts increase 
&lt;/p&gt;</description></item><item><title>RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.07622</link><description>&lt;p&gt;
RE-MOVE&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#21160;&#24577;&#29615;&#22659;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07622
&lt;/p&gt;
&lt;p&gt;
RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#22312;&#23454;&#26102;&#37096;&#32626;&#26399;&#38388;&#36866;&#24212;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RE-MOVE&#65288;&#35831;&#27714;&#24110;&#21161;&#24182;&#31227;&#21160;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#26469;&#35843;&#25972;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20197;&#36866;&#24212;&#29615;&#22659;&#30340;&#23454;&#26102;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#35831;&#27714;&#21453;&#39304;&#24182;&#22914;&#20309;&#23558;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20013;&#12290;RE-MOVE&#21033;&#29992;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#35831;&#27714;&#20154;&#31867;&#21453;&#39304;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21512;&#25104;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#27979;&#35797;&#26102;&#38388;&#21160;&#24577;&#23548;&#33322;&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#26631;&#31614;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;CE&#25439;&#22833;&#20989;&#25968;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#32467;&#26524;&#24471;&#20986;&#20102;&#21487;&#20197;&#20351;&#27169;&#22411;&#20559;&#21521;&#20110;&#23569;&#25968;&#31867;&#30340;&#38544;&#24335;&#20559;&#24046;&#29702;&#35770;&#24182;&#25512;&#23548;&#20986;&#20102;&#20998;&#31867;&#22120;&#21644;&#23884;&#20837;&#30340;&#38381;&#24335;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.07608</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#20132;&#21449;&#29109;&#21442;&#25968;&#21270;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#20013;&#30340;&#38544;&#24335;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Geometry of Cross-Entropy Parameterizations for Label-Imbalanced Data. (arXiv:2303.07608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#26631;&#31614;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;CE&#25439;&#22833;&#20989;&#25968;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#32467;&#26524;&#24471;&#20986;&#20102;&#21487;&#20197;&#20351;&#27169;&#22411;&#20559;&#21521;&#20110;&#23569;&#25968;&#31867;&#30340;&#38544;&#24335;&#20559;&#24046;&#29702;&#35770;&#24182;&#25512;&#23548;&#20986;&#20102;&#20998;&#31867;&#22120;&#21644;&#23884;&#20837;&#30340;&#38381;&#24335;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22312;&#38646;&#35757;&#32451;&#35823;&#24046;&#21306;&#22495;&#20197;&#22806;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#22823;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#32463;&#36807;&#36923;&#36753;&#35843;&#25972;&#30340;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25439;&#22833;&#21442;&#25968;&#21270;&#20316;&#20026;&#21152;&#26435;CE&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20123;&#35774;&#35745;&#32972;&#21518;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#26159;&#38544;&#24335;&#20559;&#24046;&#29702;&#35770;&#65292;&#23545;&#20110;&#32447;&#24615;&#21644;&#32447;&#24615;&#21270;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#23427;&#20204;&#20026;&#20160;&#20040;&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#20248;&#21270;&#36335;&#24452;&#19978;&#35825;&#23548;&#20986;&#20559;&#24046;&#65292;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#26377;&#21033;&#20110;&#23569;&#25968;&#20154;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#29702;&#35770;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;CE&#21442;&#25968;&#21270;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#21644;&#23884;&#20837;&#30340;&#38544;&#24335;&#20960;&#20309;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25104;&#26524;&#26159;&#34920;&#24449;&#38750;&#20984;&#25935;&#24863;&#25104;&#26412;SVM&#20998;&#31867;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#35813;&#20998;&#31867;&#22120;&#26159;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#20998;&#31867;&#22120;&#21644;&#23884;&#20837;&#30340;&#35282;&#24230;&#21644;&#33539;&#25968;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#36825;&#20123;&#20844;&#24335;&#26159;&#31867;&#21035;&#25968;&#12289;&#22833;&#34913;&#21644;&#23569;&#25968;&#20154;&#27604;&#20363;&#20197;&#21450;&#25439;&#22833;&#36229;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#20511;&#21161;&#36825;&#20123;&#20844;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various logit-adjusted parameterizations of the cross-entropy (CE) loss have been proposed as alternatives to weighted CE for training large models on label-imbalanced data far beyond the zero train error regime. The driving force behind those designs has been the theory of implicit bias, which for linear(ized) models, explains why they successfully induce bias on the optimization path towards solutions that favor minorities. Aiming to extend this theory to non-linear models, we investigate the implicit geometry of classifiers and embeddings that are learned by different CE parameterizations. Our main result characterizes the global minimizers of a non-convex cost-sensitive SVM classifier for the unconstrained features model, which serves as an abstraction of deep nets. We derive closed-form formulas for the angles and norms of classifiers and embeddings as a function of the number of classes, the imbalance and the minority ratios, and the loss hyperparameters. Using these, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#23545;COVID-19&#24863;&#26579;&#24773;&#20917;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.07600</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#24863;&#26579;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Forecasting COVID-19 Infections in Gulf Cooperation Council (GCC) Countries using Machine Learning. (arXiv:2303.07600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#22269;&#23478;&#30340;COVID-19&#30123;&#24773;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#23545;COVID-19&#24863;&#26579;&#24773;&#20917;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#33258;&#21435;&#24180;&#39318;&#27425;&#26816;&#27979;&#20197;&#26469;&#65292;&#24050;&#32463;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24863;&#26579;&#20102;&#36229;&#36807;6800&#19975;&#20154;&#12290;&#26102;&#38388;&#24207;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;COVID-19&#30340;&#24863;&#26579;&#24773;&#20917;&#12290;&#26412;&#25991;&#20351;&#29992;Johns Hopkins&#30340;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#38024;&#23545;&#28023;&#28286;&#21512;&#20316;&#22996;&#21592;&#20250;&#65288;GCC&#65289;&#22269;&#23478;&#24320;&#21457;&#20102;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;2020&#24180;1&#26376;22&#26085;&#33267;2021&#24180;1&#26376;22&#26085;&#20043;&#38388;&#30340;&#19968;&#24180;&#32047;&#35745;COVID-19&#30149;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#24863;&#26579;&#25968;&#25454;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#20026;&#25152;&#30740;&#31350;&#30340;&#22269;&#23478;&#24320;&#21457;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;COVID-19&#30340;&#24863;&#26579;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 has infected more than 68 million people worldwide since it was first detected about a year ago. Machine learning time series models have been implemented to forecast COVID-19 infections. In this paper, we develop time series models for the Gulf Cooperation Council (GCC) countries using the public COVID-19 dataset from Johns Hopkins. The dataset set includes the one-year cumulative COVID-19 cases between 22/01/2020 to 22/01/2021. We developed different models for the countries under study based on the spatial distribution of the infection data. Our experimental results show that the developed models can forecast COVID-19 infections with high precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65288;CKTF&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20013;&#38388;&#34920;&#31034;&#30340;&#22810;&#20010;&#23545;&#27604;&#30446;&#26631;&#65292;&#20351;&#24471;CKTF&#33021;&#22815;&#20256;&#36882;&#36275;&#22815;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#25552;&#39640;&#20102;&#29616;&#26377;KT&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07599</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#27604;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#19982;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Contrastive Knowledge Transfer Framework for Model Compression and Transfer Learning. (arXiv:2303.07599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65288;CKTF&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20013;&#38388;&#34920;&#31034;&#30340;&#22810;&#20010;&#23545;&#27604;&#30446;&#26631;&#65292;&#20351;&#24471;CKTF&#33021;&#22815;&#20256;&#36882;&#36275;&#22815;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#24182;&#25552;&#39640;&#20102;&#29616;&#26377;KT&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36801;&#31227;&#65288;KT&#65289;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;KT&#24037;&#20316;&#36890;&#36807;&#26368;&#23567;&#21270;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#21644;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#20043;&#38388;&#26465;&#20214;&#29420;&#31435;&#36755;&#20986;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#20174;&#22823;&#27169;&#22411;&#20013;&#20256;&#36755;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#25945;&#24072;&#27169;&#22411;&#20013;&#20013;&#38388;&#34920;&#31034;&#30340;&#39640;&#32500;&#32467;&#26500;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#26377;&#25928;&#24615;&#26377;&#38480;&#65292;&#24182;&#19988;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#21508;&#31181;&#21551;&#21457;&#24335;&#30452;&#35273;&#30340;&#21551;&#21457;&#65292;&#20351;&#24471;&#26222;&#36866;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65288;CKTF&#65289;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#20854;&#20043;&#38388;&#30340;&#20013;&#38388;&#34920;&#31034;&#30340;&#22810;&#20010;&#23545;&#27604;&#30446;&#26631;&#65292;&#20351;&#25945;&#24072;&#21521;&#23398;&#29983;&#20256;&#36882;&#36275;&#22815;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;CKTF&#23545;&#29616;&#26377;&#30340;KT&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#24191;&#20041;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#23548;&#20986;&#39640;&#24615;&#33021;&#20351;&#20854;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer (KT) achieves competitive performance and is widely used for image classification tasks in model compression and transfer learning. Existing KT works transfer the information from a large model ("teacher") to train a small model ("student") by minimizing the difference of their conditionally independent output distributions. However, these works overlook the high-dimension structural knowledge from the intermediate representations of the teacher, which leads to limited effectiveness, and they are motivated by various heuristic intuitions, which makes it difficult to generalize. This paper proposes a novel Contrastive Knowledge Transfer Framework (CKTF), which enables the transfer of sufficient structural knowledge from the teacher to the student by optimizing multiple contrastive objectives across the intermediate representations between them. Also, CKTF provides a generalized agreement to existing KT techniques and increases their performance significantly by derivi
&lt;/p&gt;</description></item><item><title>AdPE&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;&#65292;&#25197;&#26354;&#23616;&#37096;&#32467;&#26500;&#65292;&#24378;&#21046;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07598</link><description>&lt;p&gt;
AdPE&#65306;&#36890;&#36807;MAE+&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+. (arXiv:2303.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07598
&lt;/p&gt;
&lt;p&gt;
AdPE&#26041;&#27861;&#36890;&#36807;&#23545;&#25239;&#20301;&#32622;&#23884;&#20837;&#65292;&#25197;&#26354;&#23616;&#37096;&#32467;&#26500;&#65292;&#24378;&#21046;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#35270;&#35273;Transformer&#26088;&#22312;&#36890;&#36807;&#39044;&#35774;&#20219;&#21153;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#39044;&#20808;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;Masked Image Modeling&#65288;MIM&#65289;&#65292;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;Transformer&#39044;&#27979;&#25513;&#34109;&#34917;&#19969;&#30456;&#23545;&#24212;&#12290;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20934;&#21017;&#26159;&#39044;&#35774;&#20219;&#21153;&#38656;&#35201;&#36275;&#22815;&#38590;&#20197;&#38450;&#27490;Transformer&#23398;&#20064;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#19981;&#36275;&#36947;&#30340;&#20302;&#23618;&#27425;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Adversarial Positional Embedding&#65288;AdPE&#65289;&#26041;&#27861; - &#36890;&#36807;&#25200;&#21160;&#20301;&#32622;&#32534;&#30721;&#26469;&#25197;&#26354;&#23616;&#37096;&#35270;&#35273;&#32467;&#26500;&#65292;&#20197;&#20351;&#24471;&#23398;&#20064;&#30340;Transformer&#19981;&#33021;&#20165;&#20351;&#29992;&#23616;&#37096;&#30456;&#20851;&#30340;&#34917;&#19969;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#34917;&#19969;&#12290;&#25105;&#20204;&#20551;&#35774;&#23427;&#36843;&#20351;Transformer&#32534;&#30721;&#22120;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#20855;&#26377;&#24046;&#21035;&#24615;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#32477;&#23545;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65292;&#20854;&#20013;&#23545;&#25239;&#20301;&#32622;&#21487;&#20197;&#27169;&#25311;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning of vision transformers seeks to pretrain an encoder via pretext tasks without labels. Among them is the Masked Image Modeling (MIM) aligned with pretraining of language transformers by predicting masked patches as a pretext task. A criterion in unsupervised pretraining is the pretext task needs to be sufficiently hard to prevent the transformer encoder from learning trivial low-level features not generalizable well to downstream tasks. For this purpose, we propose an Adversarial Positional Embedding (AdPE) approach -- It distorts the local visual structures by perturbing the position encodings so that the learned transformer cannot simply use the locally correlated patches to predict the missing ones. We hypothesize that it forces the transformer encoder to learn more discriminative features in a global context with stronger generalizability to downstream tasks. We will consider both absolute and relative positional encodings, where adversarial positions can be im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fast GS-DOT&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30697;&#38453;&#30340;&#22359;&#23545;&#35282;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20256;&#32479;&#26041;&#27861;&#30456;&#21516;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#20013;&#31867;&#21035;&#26631;&#31614;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07597</link><description>&lt;p&gt;
&#24555;&#36895;&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#20013;&#30340;&#32676;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fast Regularized Discrete Optimal Transport with Group-Sparse Regularizers. (arXiv:2303.07597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fast GS-DOT&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30697;&#38453;&#30340;&#22359;&#23545;&#35282;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20256;&#32479;&#26041;&#27861;&#30456;&#21516;&#31934;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#20013;&#31867;&#21035;&#26631;&#31614;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26159;&#34913;&#37327;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#22495;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#36317;&#31163;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#34429;&#28982;&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#20854;&#20013;&#19968;&#20010;&#22495;&#30340;&#26679;&#26412;&#25968;&#25454;&#20250;&#26377;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32676;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#32463;&#24120;&#34987;&#29992;&#20316;&#27491;&#21017;&#21270;&#39033;&#26469;&#22788;&#29702;&#31867;&#21035;&#26631;&#31614;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#21516;&#31867;&#21035;&#26631;&#31614;&#30340;&#25968;&#25454;&#26679;&#26412;&#23545;&#24212;&#21040;&#19968;&#20010;&#32676;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#20013;&#65292;&#23427;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#26679;&#26412;&#19978;&#30340;&#26631;&#31614;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#24102;&#26377;&#26799;&#24230;&#30340;&#27491;&#21017;&#21270;&#20248;&#21270;&#38382;&#39064;&#26469;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#36317;&#31163;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#24403;&#31867;&#21035;&#25110;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#36739;&#22823;&#26102;&#65292;&#23545;&#20110;&#27491;&#21017;&#21270;&#39033;&#30340;&#25968;&#37327;&#21450;&#20854;&#30456;&#24212;&#26799;&#24230;&#25968;&#30340;&#22686;&#38271;&#65292;&#35745;&#31639;&#26799;&#24230;&#30340;&#25104;&#26412;&#21464;&#24471;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#32676;&#31232;&#30095;&#27491;&#21017;&#21270;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#65288;Fast GS-DOT&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30697;&#38453;&#30340;&#22359;&#23545;&#35282;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20256;&#32479;&#26041;&#27861;&#30456;&#21516;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#36895;&#24230;&#21644;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularized discrete optimal transport (OT) is a powerful tool to measure the distance between two discrete distributions that have been constructed from data samples on two different domains. While it has a wide range of applications in machine learning, in some cases the sampled data from only one of the domains will have class labels such as unsupervised domain adaptation. In this kind of problem setting, a group-sparse regularizer is frequently leveraged as a regularization term to handle class labels. In particular, it can preserve the label structure on the data samples by corresponding the data samples with the same class label to one group-sparse regularization term. As a result, we can measure the distance while utilizing label information by solving the regularized optimization problem with gradient-based algorithms. However, the gradient computation is expensive when the number of classes or data samples is large because the number of regularization terms and their respectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#19977;&#20803;&#20915;&#31574;&#30340;&#21333;&#38544;&#34255;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;STWD-SFNN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#21644;&#36830;&#32493;&#35843;&#25972;&#38408;&#20540;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#26356;&#39640;&#25928;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.07589</link><description>&lt;p&gt;
&#21333;&#38544;&#34255;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#19977;&#20803;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Sequential three-way decisions with a single hidden layer feedforward neural network. (arXiv:2303.07589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07589
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#19977;&#20803;&#20915;&#31574;&#30340;&#21333;&#38544;&#34255;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;STWD-SFNN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#21644;&#36830;&#32493;&#35843;&#25972;&#38408;&#20540;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#26356;&#39640;&#25928;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#19977;&#20803;&#20915;&#31574;&#65288;STWD&#65289;&#30340;&#21333;&#38544;&#34255;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;STWD-SFNN&#65289;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#30340;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22810;&#31890;&#24230;&#32423;&#21035;&#21160;&#24577;&#23398;&#20064;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#65292;&#24182;&#35774;&#32622;&#36830;&#32493;&#30340;&#38408;&#20540;&#21442;&#25968;&#65292;&#32771;&#34385;&#20102;&#22788;&#29702;&#25104;&#26412;&#65292;&#21487;&#26356;&#39640;&#25928;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The three-way decisions strategy has been employed to construct network topology in a single hidden layer feedforward neural network (SFNN). However, this model has a general performance, and does not consider the process costs, since it has fixed threshold parameters. Inspired by the sequential three-way decisions (STWD), this paper proposes STWD with an SFNN (STWD-SFNN) to enhance the performance of networks on structured datasets. STWD-SFNN adopts multi-granularity levels to dynamically learn the number of hidden layer nodes from coarse to fine, and set the sequential threshold parameters. Specifically, at the coarse granular level, STWD-SFNN handles easy-to-classify instances by applying strict threshold conditions, and with the increasing number of hidden layer nodes at the fine granular level, STWD-SFNN focuses more on disposing of the difficult-to-classify instances by applying loose threshold conditions, thereby realizing the classification of instances. Moreover, STWD-SFNN con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#23884;&#20837;&#24335;&#35745;&#31639;&#30340;&#23454;&#26102;&#37096;&#32626;&#65292;&#36895;&#24230;&#36798;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;100&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.07586</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#21152;&#36895;&#22120;&#30340;&#38647;&#36798;&#24863;&#30693;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Knowledge Distillation for Radar Perception on Embedded Accelerators. (arXiv:2303.07586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#23884;&#20837;&#24335;&#35745;&#31639;&#30340;&#23454;&#26102;&#37096;&#32626;&#65292;&#36895;&#24230;&#36798;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35768;&#22810;&#29992;&#20110;&#36947;&#36335;&#23433;&#20840;&#24863;&#30693;&#30340;&#38647;&#36798;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#37117;&#26080;&#27861;&#24456;&#22909;&#22320;&#36816;&#34892;&#22312;&#29992;&#20110;&#27773;&#36710;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#12290;&#30456;&#21453;&#65292;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#22320;&#21033;&#29992;&#20102;&#19987;&#38376;&#21152;&#36895;&#22120;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#32423;&#21035;&#38647;&#36798;&#24863;&#30693;&#20219;&#21153;&#30340;&#24072;&#29983;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#29992;&#20110;&#38745;&#24577;&#30446;&#26631;&#26816;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#23398;&#29983;&#27169;&#22411;&#12290;&#35813;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#21033;&#29992;&#23884;&#20837;&#24335;&#35745;&#31639;&#36827;&#34892;&#23454;&#26102;&#37096;&#32626;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#27604;&#25945;&#24072;&#27169;&#22411;&#24555;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many radar signal processing methodologies are being developed for critical road safety perception tasks. Unfortunately, these signal processing algorithms are often poorly suited to run on embedded hardware accelerators used in automobiles. Conversely, end-to-end machine learning (ML) approaches better exploit the performance gains brought by specialized accelerators. In this paper, we propose a teacher-student knowledge distillation approach for low-level radar perception tasks. We utilize a hybrid model for stationary object detection as a teacher to train an end-to-end ML student model. The student can efficiently harness embedded compute for real-time deployment. We demonstrate that the proposed student model runs at speeds 100x faster than the teacher model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#20803;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#36825;&#20123;&#21306;&#22495;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#65307;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI&#25351;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.07580</link><description>&lt;p&gt;
&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#21487;&#35299;&#37322;AI&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Sensitive Region-based Metamorphic Testing Framework using Explainable AI. (arXiv:2303.07580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07580
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#21306;&#22495;&#30340;&#20803;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#36825;&#20123;&#21306;&#22495;&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#65307;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#21464;&#24577;&#27979;&#35797;&#26469;&#26816;&#27979;&#38169;&#35823;&#20998;&#31867;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#35752;&#35770;&#21464;&#24577;&#20851;&#31995;(MR)&#65292;&#20294;&#24456;&#23569;&#35752;&#35770;&#24212;&#35813;&#36716;&#25442;&#21738;&#20123;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#23384;&#22312;&#25935;&#24863;&#21306;&#22495;&#65292;&#21363;&#20351;&#36827;&#34892;&#23567;&#30340;&#36716;&#25442;&#20063;&#20250;&#23481;&#26131;&#25913;&#21464;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21464;&#24577;&#27979;&#35797;&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#25442;&#25935;&#24863;&#21306;&#22495;&#26377;&#25928;&#22320;&#26816;&#27979;&#26131;&#20986;&#29616;&#38169;&#35823;&#20998;&#31867;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25935;&#24863;&#21306;&#22495;&#21487;&#20197;&#30001;&#21487;&#35299;&#37322;AI(XAI)&#25351;&#23450;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#26816;&#27979;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is one of the most popular research topics in machine learning and DL-driven image recognition systems have developed rapidly. Recent research has used metamorphic testing (MT) to detect misclassified images. Most of them discuss metamorphic relations (MR), with little discussion on which regions should be transformed. We focus on the fact that there are sensitive regions where even a small transformation can easily change the prediction results and propose an MT framework that efficiently tests for regions prone to misclassification by transforming the sensitive regions. Our evaluation showed that the sensitive regions can be specified by Explainable AI (XAI) and our framework effectively detects faults.
&lt;/p&gt;</description></item><item><title>VANI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22810;&#35821;&#35328;&#21475;&#38899;&#21487;&#25511;&#21046;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#25903;&#25345;&#23545;&#35821;&#38899;&#30340;&#21475;&#38899;&#12289;&#35821;&#35328;&#12289;&#35828;&#35805;&#32773;&#12289;F0&#21644;&#33021;&#37327;&#29305;&#24449;&#36827;&#34892;&#26174;&#24335;&#25511;&#21046;&#65292;&#24182;&#33021;&#22312;&#20445;&#30041;&#26412;&#22320;&#21475;&#38899;&#30340;&#24773;&#20917;&#19979;&#26356;&#25442;&#35828;&#35805;&#32773;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2303.07578</link><description>&lt;p&gt;
VANI&#65306;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#21487;&#23545;&#26412;&#22320;&#21644;&#38750;&#26412;&#22320;&#35821;&#35328;&#20351;&#29992;&#32773;&#36827;&#34892;&#21457;&#38899;&#25511;&#21046;&#65292;&#24182;&#20445;&#30041;&#20854;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
VANI: Very-lightweight Accent-controllable TTS for Native and Non-native speakers with Identity Preservation. (arXiv:2303.07578v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07578
&lt;/p&gt;
&lt;p&gt;
VANI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22810;&#35821;&#35328;&#21475;&#38899;&#21487;&#25511;&#21046;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#25903;&#25345;&#23545;&#35821;&#38899;&#30340;&#21475;&#38899;&#12289;&#35821;&#35328;&#12289;&#35828;&#35805;&#32773;&#12289;F0&#21644;&#33021;&#37327;&#29305;&#24449;&#36827;&#34892;&#26174;&#24335;&#25511;&#21046;&#65292;&#24182;&#33021;&#22312;&#20445;&#30041;&#26412;&#22320;&#21475;&#38899;&#30340;&#24773;&#20917;&#19979;&#26356;&#25442;&#35828;&#35805;&#32773;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VANI&#65292;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#22810;&#35821;&#35328;&#21475;&#38899;&#21487;&#25511;&#21046;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;RADMMM&#20013;&#25552;&#20986;&#30340;&#35299;&#32544;&#25918;&#31574;&#30053;&#65292;&#25903;&#25345;&#21475;&#38899;&#12289;&#35821;&#35328;&#12289;&#35828;&#35805;&#32773;&#12289;&#32454;&#31890;&#24230;F0&#21644;&#33021;&#37327;&#29305;&#24449;&#30340;&#26174;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;Indic&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#22312;3&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#21512;&#25104;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#22312;&#20445;&#30041;&#35828;&#35805;&#32773;&#22768;&#38899;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21475;&#38899;&#30340;&#24773;&#20917;&#19979;&#36716;&#25442;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#27604;&#36187;&#30340;Track 2&#21644;3&#20013;&#20351;&#29992;&#20102;&#36731;&#37327;&#32423;VANI&#27169;&#22411;&#65292;&#22312;Track 1&#20013;&#20351;&#29992;&#20102;&#22823;&#21442;&#25968;&#30340;RADMMM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VANI, a very lightweight multi-lingual accent controllable speech synthesis system. Our model builds upon disentanglement strategies proposed in RADMMM and supports explicit control of accent, language, speaker and fine-grained $F_0$ and energy features for speech synthesis. We utilize the Indic languages dataset, released for LIMMITS 2023 as part of ICASSP Signal Processing Grand Challenge, to synthesize speech in 3 different languages. Our model supports transferring the language of a speaker while retaining their voice and the native accent of the target language. We utilize the large-parameter RADMMM model for Track $1$ and lightweight VANI model for Track $2$ and $3$ of the competition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#36827;&#34892;&#31354;&#38388;&#29289;&#20307;&#35782;&#21035;&#21644;&#20301;&#32622;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21152;&#24030;&#27225;&#21439;&#23454;&#29616;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#23545;&#20572;&#36710;&#26631;&#24535;&#21644;&#28040;&#38450;&#26643;&#30340;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.07560</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#21152;&#24030;&#27225;&#21439;&#31354;&#38388;AI&#29289;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Computer Vision Applications for Spatial AI Object Recognition in Orange County, California. (arXiv:2303.07560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#36827;&#34892;&#31354;&#38388;&#29289;&#20307;&#35782;&#21035;&#21644;&#20301;&#32622;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21152;&#24030;&#27225;&#21439;&#23454;&#29616;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#23545;&#20572;&#36710;&#26631;&#24535;&#21644;&#28040;&#38450;&#26643;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38598;&#25104;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#23454;&#29616;&#27225;&#21439;&#31354;&#38388;&#29289;&#20307;&#35782;&#21035;&#21644;&#20301;&#32622;&#26816;&#27979;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22810;&#20256;&#24863;&#22120;&#12289;&#39640;&#20998;&#36776;&#29575;&#37326;&#22806;&#25968;&#25454;&#33719;&#21462;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#20197;&#21450;&#37326;&#22806;&#21518;&#22788;&#29702;&#21644;&#39044;&#20998;&#26512;&#22788;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#31639;&#27861;&#20844;&#24335;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#21367;&#31215;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#19982;360{\deg}&#20840;&#26223;&#22270;&#20687;&#20013;&#26816;&#27979;&#30340;&#29289;&#20307;&#20301;&#32622;&#20272;&#35745;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#27225;&#21439;&#30340;&#20004;&#20010;&#21306;&#22495;&#30340;&#29031;&#29255;&#29699;&#22270;&#20687;&#20013;&#22788;&#29702;&#36229;&#36807; 80 &#19975;&#20010;&#26041;&#20301;&#35282;&#30340;&#24212;&#29992;&#23454;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#20572;&#36710;&#26631;&#24535;&#21644;&#28040;&#38450;&#26643;&#35782;&#21035;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#19982;&#27492;&#26041;&#27861;&#30456;&#20851;&#30340;&#26356;&#24191;&#27867;&#30340;&#24615;&#33021;&#21644;&#26410;&#26469;&#25216;&#26415;&#21019;&#26032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an integrated and systematic automation approach to spatial object recognition and positional detection using AI machine learning and computer vision algorithms for Orange County, California. We describe a comprehensive methodology for multi-sensor, high-resolution field data acquisition, along with post-field processing and pre-analysis processing tasks. We developed a series of algorithmic formulations and workflows that integrate convolutional deep neural network learning with detected object positioning estimation in 360{\deg} equirectancular photosphere imagery. We provide examples of application processing more than 800 thousand cardinal directions in photosphere images across two areas in Orange County, and present detection results for stop-sign and fire hydrant object recognition. We discuss the efficiency and effectiveness of our approach, along with broader inferences related to the performance and implications of this approach for future technological innovations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.07557</link><description>&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;: &#26032;&#30340;&#25361;&#25112;&#12289;&#35270;&#35282;&#21644;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#35774;&#35745;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#20013;&#65292;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#32456;&#36523;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#36235;&#21183;&#65292;&#23427;&#33021;&#22815;&#28385;&#36275;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#19981;&#26029;&#36866;&#24212;&#26032;&#25361;&#25112;&#24182;&#20445;&#30041;&#36807;&#21435;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#20154;&#33268;&#21147;&#20110;&#24314;&#31435;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#30784;&#65292;&#36825;&#19982;&#26356;&#24191;&#27867;&#25506;&#32034;&#30340;&#20998;&#31867;&#35774;&#32622;&#23384;&#22312;&#26412;&#36136;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#12289;&#38416;&#36848;&#21644;&#35752;&#35770;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#65292;&#35797;&#22270;&#20026;&#20854;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#24314;&#31435;&#22522;&#30784;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#24456;&#37325;&#35201;&#65292;&#23450;&#20041;&#20102;&#24212;&#23545;&#32456;&#36523;&#23398;&#20064;&#22797;&#26434;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#35774;&#35745;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#35774;&#32622;&#21644;&#22330;&#26223;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#36827;&#34892;&#32456;&#36523;&#24322;&#24120;&#26816;&#27979;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is of paramount importance in many real-world domains, characterized by evolving behavior. Lifelong learning represents an emerging trend, answering the need for machine learning models that continuously adapt to new challenges in dynamic environments while retaining past knowledge. However, limited efforts are dedicated to building foundations for lifelong anomaly detection, which provides intrinsically different challenges compared to the more widely explored classification setting. In this paper, we face this issue by exploring, motivating, and discussing lifelong anomaly detection, trying to build foundations for its wider adoption. First, we explain why lifelong anomaly detection is relevant, defining challenges and opportunities to design anomaly detection methods that deal with lifelong learning complexities. Second, we characterize learning settings and a scenario generation procedure that enables researchers to experiment with lifelong anomaly detection using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07546</link><description>&lt;p&gt;
&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#21450;&#20854;&#22312;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#32508;&#36848;&#65288;arXiv:2303.07546v1 [cs.SE]&#65289;
&lt;/p&gt;
&lt;p&gt;
Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (arXiv:2303.07546v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#31181;&#26032;&#25216;&#26415;&#37117;&#20250;&#22686;&#21152;&#38544;&#21547;&#30340;&#28431;&#27934;&#65292;&#35753;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#32476;&#25915;&#20987;&#32773;&#21033;&#29992;&#12290;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#21487;&#20197;&#25104;&#20026;&#24555;&#36895;&#20998;&#26512;&#25968;&#21315;&#34892;&#20195;&#30721;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#30053;&#24494;&#20462;&#25913;&#21151;&#33021;&#29305;&#23450;&#30340;&#27979;&#35797;&#25968;&#25454;&#26469;&#36935;&#21040;&#22810;&#20010;&#28431;&#27934;&#21644;&#25915;&#20987;&#21521;&#37327;&#12290;&#36825;&#20010;&#36807;&#31243;&#19982;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#21463;&#38480;&#24615;&#23545;&#25239;&#24615;&#31034;&#20363;&#30456;&#20284;&#65292;&#22240;&#27492;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#21040;&#33258;&#21160;&#21270;&#27979;&#35797;&#24037;&#20855;&#20013;&#21487;&#33021;&#20250;&#26377;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#22240;&#27492;&#65292;&#26412;&#31995;&#32479;&#32508;&#36848;&#20391;&#37325;&#20110;&#38480;&#21046;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22312;&#23545;&#25239;&#23398;&#20064;&#21644;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#30340;&#24403;&#21069;&#26368;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#27979;&#35797;&#24037;&#20855;&#65292;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#24377;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23545;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#29616;&#21463;&#38480;&#21046;&#30340;&#25968;&#25454;&#29983;&#25104;&#24212;&#29992;&#26159;&#31995;&#32479;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every novel technology adds hidden vulnerabilities ready to be exploited by a growing number of cyber-attacks. Automated software testing can be a promising solution to quickly analyze thousands of lines of code by generating and slightly modifying function-specific testing data to encounter a multitude of vulnerabilities and attack vectors. This process draws similarities to the constrained adversarial examples generated by adversarial learning methods, so there could be significant benefits to the integration of these methods in automated testing tools. Therefore, this systematic review is focused on the current state-of-the-art of constrained data generation methods applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance testing tools with adversarial learning methods and improve the resilience and robustness of their digital systems. The found constrained data generation applications for adversarial machine learning were systemat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#29305;&#24449;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07540</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#39044;&#27979;&#24515;&#33039;MRI&#20013;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI. (arXiv:2303.07540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#29305;&#24449;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34928;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#29983;&#21629;&#23041;&#32961;&#30142;&#30149;&#65292;&#20250;&#23548;&#33268;&#24038;&#24515;&#23460;&#21387;&#21147;&#21319;&#39640;&#12290;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20195;&#29702;&#26631;&#24535;&#65292;&#34920;&#31034;&#24038;&#24515;&#23460;&#30340;&#39640;&#21387;&#12290;PAWP &#30001;&#21491;&#24515;&#23548;&#31649;&#26816;&#26597;&#65288;RHC&#65289;&#30830;&#23450;&#65292;&#20294;&#23427;&#26159;&#19968;&#31181;&#26377;&#21019;&#24615;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#20174;&#22823;&#37327;&#20154;&#32676;&#20013;&#35782;&#21035;&#39640;&#21361;&#24739;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;PAWP&#12290;&#36825;&#20010;&#27969;&#31243;&#25552;&#21462;&#39640;&#32500;&#25195;&#25551;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#20026;&#20102;&#36136;&#37327;&#25511;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20026;&#22522;&#30784;&#30340;&#20998;&#32452;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#36136;&#37327;&#24046;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29305;&#24449;&#65306;&#24515;&#33039;MRI&#19982;&#30701;&#36724;&#21644;&#22235;&#33108;&#35270;&#22270;&#20197;&#21450;&#30005;&#23376;&#30149;&#21382;&#65292;&#23398;&#20064;&#20114;&#34917;&#20449;&#24687;&#12290;&#36825;&#39033;&#23454;&#39564;&#20998;&#26512;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HiSSNet&#30340;&#23618;&#27425;&#21407;&#22411;&#32593;&#32476;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#22836;&#25140;&#24335;&#32819;&#26426;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#20854;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20132;&#20114;&#23398;&#20064;&#29305;&#23450;&#30340;&#37325;&#35201;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.07538</link><description>&lt;p&gt;
HiSSNet: &#38754;&#21521;&#20302;&#36164;&#28304;&#22836;&#25140;&#24335;&#32819;&#26426;&#30340;&#23618;&#27425;&#21407;&#22411;&#32593;&#32476;&#36827;&#34892;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#19982;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
HiSSNet: Sound Event Detection and Speaker Identification via Hierarchical Prototypical Networks for Low-Resource Headphones. (arXiv:2303.07538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HiSSNet&#30340;&#23618;&#27425;&#21407;&#22411;&#32593;&#32476;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#22836;&#25140;&#24335;&#32819;&#26426;&#30340;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#20854;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20132;&#20114;&#23398;&#20064;&#29305;&#23450;&#30340;&#37325;&#35201;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#38477;&#22122;&#32819;&#26426;&#36890;&#36807;&#28040;&#38500;&#19981;&#38656;&#35201;&#30340;&#32972;&#26223;&#22122;&#38899;&#26174;&#30528;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#21548;&#35273;&#20307;&#39564;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#20250;&#38459;&#25377;&#29992;&#25143;&#20851;&#27880;&#30340;&#22768;&#38899;&#12290;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#65288;SID&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#32819;&#26426;&#26377;&#36873;&#25321;&#24615;&#22320;&#36890;&#36807;&#37325;&#35201;&#30340;&#22768;&#38899;&#65292;&#20294;&#23558;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20026;&#29992;&#25143;&#20013;&#24515;&#30340;&#20307;&#39564;&#23384;&#22312;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;HiSSNet&#65288;Hierarchical SED&#21644;SID Network&#65289;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern noise-cancelling headphones have significantly improved users' auditory experiences by removing unwanted background noise, but they can also block out sounds that matter to users. Machine learning (ML) models for sound event detection (SED) and speaker identification (SID) can enable headphones to selectively pass through important sounds; however, implementing these models for a user-centric experience presents several unique challenges. First, most people spend limited time customizing their headphones, so the sound detection should work reasonably well out of the box. Second, the models should be able to learn over time the specific sounds that are important to users based on their implicit and explicit interactions. Finally, such models should have a small memory footprint to run on low-power headphones with limited on-chip memory. In this paper, we propose addressing these challenges using HiSSNet (Hierarchical SED and SID Network). HiSSNet is an SEID (SED and SID) model th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#20998;&#25968;&#38454;&#21160;&#21147;&#23398;&#20998;&#26512;&#26469;&#36827;&#34892;COPD&#30340;&#35786;&#26029;&#65292;&#21487;&#20197;&#20174;&#29983;&#29702;&#20449;&#21495;&#20013;&#25552;&#21462;&#19981;&#21516;&#38454;&#27573;&#30340;&#29305;&#24449;&#20449;&#21495;&#65292;&#36827;&#32780;&#21457;&#23637;&#20986;&#21487;&#39044;&#27979;COPD&#20998;&#32423;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.07537</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#38454;&#21160;&#21147;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#21161;&#20110;COPD&#20998;&#32423;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fractional dynamics foster deep learning of COPD stage prediction. (arXiv:2303.07537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#20998;&#25968;&#38454;&#21160;&#21147;&#23398;&#20998;&#26512;&#26469;&#36827;&#34892;COPD&#30340;&#35786;&#26029;&#65292;&#21487;&#20197;&#20174;&#29983;&#29702;&#20449;&#21495;&#20013;&#25552;&#21462;&#19981;&#21516;&#38454;&#27573;&#30340;&#29305;&#24449;&#20449;&#21495;&#65292;&#36827;&#32780;&#21457;&#23637;&#20986;&#21487;&#39044;&#27979;COPD&#20998;&#32423;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#65288;COPD&#65289;&#26159;&#20840;&#29699;&#27515;&#20129;&#21407;&#22240;&#20043;&#19968;&#12290;&#24120;&#35268;&#30340;COPD&#35786;&#26029;&#26041;&#27861;&#65288;&#22914;&#32954;&#21151;&#33021;&#26816;&#27979;&#65289;&#21487;&#33021;&#19981;&#22826;&#21487;&#38752;&#65292;&#22240;&#20026;&#27979;&#35797;&#38656;&#35201;&#27979;&#35797;&#32773;&#21644;&#34987;&#27979;&#35797;&#32773;&#30340;&#20805;&#20998;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;&#26089;&#26399;&#35786;&#26029;&#20063;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#29983;&#29702;&#20449;&#21495;&#25968;&#25454;&#38598;&#65288;&#26469;&#33258;WestRo COPD&#25968;&#25454;&#38598;&#30340;54&#21517;&#24739;&#32773;&#30340;4432&#20221;&#35760;&#24405;&#21644;&#26469;&#33258;WestRo Porti COPD&#25968;&#25454;&#38598;&#30340;534&#21517;&#24739;&#32773;&#30340;13824&#20221;&#30149;&#21382;&#65289;&#65292;&#25581;&#31034;&#20854;&#22797;&#26434;&#30340;&#32806;&#21512;&#20998;&#24418;&#21160;&#21147;&#23398;&#29305;&#24615;&#24182;&#36827;&#34892;&#20998;&#25968;&#38454;&#21160;&#21147;&#23398;&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#65292;&#29992;&#20110;COPD&#30340;&#35786;&#26029;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#20998;&#25968;&#38454;&#21160;&#24577;&#24314;&#27169;&#21487;&#20197;&#20174;&#29983;&#29702;&#20449;&#21495;&#20013;&#25552;&#21462;&#20986;&#21508;&#20010;COPD&#38454;&#27573;&#30340;&#29420;&#29305;&#20449;&#21495;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20123;&#20449;&#21495;&#26500;&#24314;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#39044;&#27979;COPD&#20998;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chronic obstructive pulmonary disease (COPD) is one of the leading causes of death worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable because the test depends on an adequate effort from the tester and testee. Moreover, the early diagnosis of COPD is challenging. We address COPD detection by constructing two novel physiological signals datasets (4432 records from 54 patients in the WestRo COPD dataset and 13824 medical records from 534 patients in the WestRo Porti COPD dataset). The authors demonstrate their complex coupled fractal dynamical characteristics and perform a fractional-order dynamics deep learning analysis to diagnose COPD. The authors found that the fractional-order dynamical modeling can extract distinguishing signatures from the physiological signals across patients with all COPD stages from stage 0 (healthy) to stage 4 (very severe). They use the fractional signatures to develop and train a deep neural network that predicts COPD stages based on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#25972;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#21644;1.48&#20493;&#30340;&#24179;&#22343;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.07535</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#35268;&#21010;&#65306;&#19968;&#31181;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path Planning using Reinforcement Learning: A Policy Iteration Approach. (arXiv:2303.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#25972;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#21644;1.48&#20493;&#30340;&#24179;&#22343;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23454;&#26102;&#22788;&#29702;&#30340;&#24433;&#21709;&#22312;&#26368;&#36817;&#34987;&#35748;&#35782;&#21040;&#65292;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#39640;&#25928;&#23454;&#29616;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#23613;&#31649;RL&#31639;&#27861;&#20013;&#21033;&#29992;Bellman&#26041;&#31243;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#26159;&#35774;&#35745;&#21442;&#25968;&#30340;&#22823;&#25628;&#32034;&#31354;&#38388;&#20063;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#38416;&#26126;&#19982;&#24378;&#21270;&#23398;&#20064;&#21442;&#25968;&#30456;&#20851;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#31574;&#30053;&#36845;&#20195;&#26041;&#38754;&#12290;&#32771;&#34385;&#21040;&#24494;&#35843;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21442;&#25968;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#35843;&#33410;&#22120;&#30340;&#24207;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#25506;&#32034;&#36825;&#20123;&#21442;&#25968;&#30340;&#36807;&#31243;&#65292;&#24182;&#21152;&#36895;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;1.82&#20493;&#30340;&#23792;&#20540;&#21152;&#36895;&#65292;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;1.48&#20493;&#65292;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the impact of real-time processing being realized in the recent past, the need for efficient implementations of reinforcement learning algorithms has been on the rise. Albeit the numerous advantages of Bellman equations utilized in RL algorithms, they are not without the large search space of design parameters.  This research aims to shed light on the design space exploration associated with reinforcement learning parameters, specifically that of Policy Iteration. Given the large computational expenses of fine-tuning the parameters of reinforcement learning algorithms, we propose an auto-tuner-based ordinal regression approach to accelerate the process of exploring these parameters and, in return, accelerate convergence towards an optimal policy. Our approach provides 1.82x peak speedup with an average of 1.48x speedup over the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#38477;&#20302;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#24182;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07527</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#23454;&#29616;&#39046;&#22495;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization via Nuclear Norm Regularization. (arXiv:2303.07527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#38477;&#20302;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#24182;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#20165;&#26377;&#26377;&#38480;&#22495;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20855;&#22791;&#23545;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#29305;&#24449;&#26680;&#33539;&#25968;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#30452;&#35266;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20943;&#23569;&#20102;&#29615;&#22659;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#40723;&#21169;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#20026;&#20160;&#20040;&#30456;&#27604;&#20110;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#25110;&#20854;&#20182;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#26356;&#21152;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;&#23454;&#39564;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#35797;&#39564;&#35777;&#26126;&#65292;&#26680;&#33539;&#25968;&#27491;&#21017;&#21270;&#22312;&#24191;&#27867;&#30340;&#39046;&#22495;&#36890;&#29992;&#24615;&#20219;&#21153;&#20013;&#19982;&#22522;&#32447;&#30456;&#27604;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;ERM&#21644;SWAD&#65292;&#19988;&#34920;&#29616;&#25345;&#32493;&#25552;&#39640;&#65292;&#20363;&#22914;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#21644;0.9&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalize to unseen domains is crucial for machine learning systems deployed in the real world, especially when we only have data from limited training domains. In this paper, we propose a simple and effective regularization method based on the nuclear norm of the learned features for domain generalization. Intuitively, the proposed regularizer mitigates the impacts of environmental features and encourages learning domain-invariant features. Theoretically, we provide insights into why nuclear norm regularization is more effective compared to ERM and alternative regularization methods. Empirically, we conduct extensive experiments on both synthetic and real datasets. We show that nuclear norm regularization achieves strong performance compared to baselines in a wide range of domain generalization tasks. Moreover, our regularizer is broadly applicable with various methods such as ERM and SWAD with consistently improved performance, e.g., 1.7% and 0.9% test accuracy improv
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24222;&#22823;&#30340;&#24320;&#28304;&#20989;&#25968;&#25968;&#25454;&#38598;&#26469;&#33258;&#21160;&#26816;&#27979;&#36719;&#20214;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2303.07525</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33258;&#21160;&#26816;&#27979;&#36719;&#20214;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Automated Vulnerability Detection in Source Code Using Quantum Natural Language Processing. (arXiv:2303.07525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#24222;&#22823;&#30340;&#24320;&#28304;&#20989;&#25968;&#25968;&#25454;&#38598;&#26469;&#33258;&#21160;&#26816;&#27979;&#36719;&#20214;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28304;&#20195;&#30721;&#20013;&#28431;&#27934;&#30340;&#23384;&#22312;&#26159;&#36719;&#20214;&#20195;&#30721;&#23457;&#35745;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#36825;&#20123;&#32570;&#38519;&#24456;&#21487;&#33021;&#34987;&#25915;&#20987;&#24182;&#23548;&#33268;&#31995;&#32479;&#22949;&#21327;&#12289;&#25968;&#25454;&#27844;&#28431;&#25110;&#26381;&#21153;&#25298;&#32477;&#12290;&#25105;&#20204;&#20351;&#29992;C&#21644;C++&#24320;&#28304;&#20195;&#30721;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#20197;&#29992;&#20110;&#21151;&#33021;&#32423;&#28431;&#27934;&#35782;&#21035;&#12290;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#30001;&#25968;&#30334;&#19975;&#20010;&#24320;&#28304;&#20989;&#25968;&#32452;&#25104;&#30340;&#24222;&#22823;&#25968;&#25454;&#38598;&#65292;&#21487;&#25351;&#21521;&#28508;&#22312;&#28431;&#27934;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;Long Short Term Memory (LSTM)&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;Long Short Term Memory (QLSTM)&#21019;&#24314;&#20102;&#19968;&#31181;&#39640;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20174;&#28304;&#20195;&#30721;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#28304;&#20195;&#30721;&#39318;&#20808;&#34987;&#36716;&#25442;&#20026;&#26368;&#23567;&#20013;&#38388;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#28040;&#38500;&#26080;&#20851;&#32452;&#20214;&#24182;&#32553;&#30701;&#20381;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20445;&#25345;&#35821;&#20041;&#21644;&#21477;&#27861;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges in the field of software code audit is the presence of vulnerabilities in software source code. These flaws are highly likely ex-ploited and lead to system compromise, data leakage, or denial of ser-vice. C and C++ open source code are now available in order to create a large-scale, classical machine-learning and quantum machine-learning system for function-level vulnerability identification. We assembled a siz-able dataset of millions of open-source functions that point to poten-tial exploits. We created an efficient and scalable vulnerability detection method based on a deep neural network model Long Short Term Memory (LSTM), and quantum machine learning model Long Short Term Memory (QLSTM), that can learn features extracted from the source codes. The source code is first converted into a minimal intermediate representation to remove the pointless components and shorten the de-pendency. Therefore, We keep the semantic and syntactic information usi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#30446;&#26631;&#25513;&#27169;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20998;&#21106;&#21644;&#26080;&#30417;&#30563;&#27880;&#20876;&#32593;&#32476;&#65292;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20351;&#24471;&#37197;&#20934;&#21644;&#20998;&#21106;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07517</link><description>&lt;p&gt;
SuperMask&#65306;&#20174;&#22810;&#35270;&#35282;&#19981;&#23545;&#40784;&#30340;&#20302;&#20998;&#36776;&#29575;&#30913;&#20849;&#25391;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30446;&#26631;&#25513;&#27169;
&lt;/p&gt;
&lt;p&gt;
SuperMask: Generating High-resolution object masks from multi-view, unaligned low-resolution MRIs. (arXiv:2303.07517v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#22810;&#20010;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#30446;&#26631;&#25513;&#27169;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20998;&#21106;&#21644;&#26080;&#30417;&#30563;&#27880;&#20876;&#32593;&#32476;&#65292;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20351;&#24471;&#37197;&#20934;&#21644;&#20998;&#21106;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;MRI&#65289;&#20013;&#30340;&#20998;&#21106;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#39640;&#20998;&#36776;&#29575;&#30340;&#31561;&#21521;&#24615;MRI&#24456;&#23569;&#65292;&#32780;&#20856;&#22411;&#30340;MRI&#26159;&#38750;&#31561;&#21521;&#24615;&#30340;&#65292;&#20854;&#20013;&#30340;&#24179;&#38754;&#20998;&#36776;&#29575;&#36828;&#20302;&#20110;&#20307;&#32032;&#20998;&#36776;&#29575;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#24120;&#24120;&#22312;&#19981;&#21516;&#30340;&#24179;&#38754;&#19978;&#33719;&#21462;&#22810;&#20010;&#24207;&#21015;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#24207;&#21015;&#19981;&#27491;&#20132;&#20110;&#24444;&#27492;&#65292;&#38480;&#21046;&#20102;&#35768;&#22810;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#37325;&#26500;&#22810;&#20010;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20197;&#24471;&#21040;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#20174;&#22810;&#20010;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#25513;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#20351;&#24471;&#37197;&#20934;&#21644;&#20998;&#21106;&#30456;&#20114;&#22686;&#24378;&#65292;&#32467;&#21512;&#20102;&#20998;&#21106;&#21644;&#26080;&#30417;&#30563;&#27880;&#20876;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#22810;&#35270;&#35282;&#34701;&#21512;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#30446;&#26631;&#23545;&#35937;&#25513;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Three-dimensional segmentation in magnetic resonance images (MRI), which reflects the true shape of the objects, is challenging since high-resolution isotropic MRIs are rare and typical MRIs are anisotropic, with the out-of-plane dimension having a much lower resolution. A potential remedy to this issue lies in the fact that often multiple sequences are acquired on different planes. However, in practice, these sequences are not orthogonal to each other, limiting the applicability of many previous solutions to reconstruct higher-resolution images from multiple lower-resolution ones. We propose a weakly-supervised deep learning-based solution to generating high-resolution masks from multiple low-resolution images. Our method combines segmentation and unsupervised registration networks by introducing two new regularizations to make registration and segmentation reinforce each other. Finally, we introduce a multi-view fusion method to generate high-resolution target object masks. The exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#22312;&#25191;&#34892;&#19968;&#31995;&#21015;&#28216;&#25103;&#26102;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#31232;&#30095;&#23548;&#33268;&#26799;&#24230;&#21464;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.07507</link><description>&lt;p&gt;
&#36830;&#32493;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss of Plasticity in Continual Deep Reinforcement Learning. (arXiv:2303.07507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#22312;&#25191;&#34892;&#19968;&#31995;&#21015;&#28216;&#25103;&#26102;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#31232;&#30095;&#23548;&#33268;&#26799;&#24230;&#21464;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#20856;&#22522;&#20110;&#20540;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#24490;&#29615;&#25191;&#34892;Atari 2600&#28216;&#25103;&#26102;&#65292;&#23427;&#20204;&#22833;&#21435;&#20102;&#23398;&#20064;&#33391;&#22909;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#26435;&#37325;&#12289;&#26799;&#24230;&#21644;&#28608;&#27963;&#22312;&#26102;&#38388;&#19978;&#22914;&#20309;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#36275;&#36857;&#21464;&#24471;&#26356;&#21152;&#31232;&#30095;&#65292;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn continually is essential in a complex and changing world. In this paper, we characterize the behavior of canonical value-based deep reinforcement learning (RL) approaches under varying degrees of non-stationarity. In particular, we demonstrate that deep RL agents lose their ability to learn good policies when they cycle through a sequence of Atari 2600 games. This phenomenon is alluded to in prior work under various guises -e.g., loss of plasticity, implicit under-parameterization, primacy bias, and capacity loss. We investigate this phenomenon closely at scale and analyze how the weights, gradients, and activations change over time in several experiments with varying dimensions (e.g., similarity between games, number of games, number of frames per game), with some experiments spanning 50 days and 2 billion environment interactions. Our analysis shows that the activation footprint of the network becomes sparser, contributing to the diminishing gradients. We inves
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20803;&#23398;&#20064;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07502</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#27861;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#36817;&#26399;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Meta-learning approaches for few-shot learning: A survey of recent advances. (arXiv:2303.07502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20803;&#23398;&#20064;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#23398;&#20064;&#22810;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20854;&#19987;&#27880;&#20110;&#21516;&#20998;&#24067;&#39044;&#27979;&#65292;&#23548;&#33268;&#20854;&#22312;&#26032;&#30340;&#26410;&#35265;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#22240;&#26679;&#26412;&#19981;&#36275;&#32780;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24212;&#23569;&#37327;&#26679;&#26412;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65292;&#28982;&#21518;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;(I) &#22522;&#20110;&#24230;&#37327;&#30340;&#12289;(II) &#22522;&#20110;&#35760;&#24518;&#30340;&#12289;(III) &#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its astounding success in learning deeper multi-dimensional data, the performance of deep learning declines on new unseen tasks mainly due to its focus on same-distribution prediction. Moreover, deep learning is notorious for poor generalization from few samples. Meta-learning is a promising approach that addresses these issues by adapting to new tasks with few-shot datasets. This survey first briefly introduces meta-learning and then investigates state-of-the-art meta-learning methods and recent advances in: (I) metric-based, (II) memory-based, (III), and learning-based methods. Finally, current challenges and insights for future researches are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#25674;&#38144;&#28508;&#22312;&#21464;&#37327;&#30340;&#29305;&#24615;&#19982;&#20256;&#32479;&#26174;&#24335;&#34920;&#31034;&#26041;&#27861;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2303.07487</link><description>&lt;p&gt;
&#20351;&#29992;VAE&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#65306;&#22312;cryo-EM&#20013;&#30340;&#24212;&#29992;&#35266;&#23519;&#65288;arXiv:2303.07487v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
Using VAEs to Learn Latent Variables: Observations on Applications in cryo-EM. (arXiv:2303.07487v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#25674;&#38144;&#28508;&#22312;&#21464;&#37327;&#30340;&#29305;&#24615;&#19982;&#20256;&#32479;&#26174;&#24335;&#34920;&#31034;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#36817;&#20284;&#20998;&#24067;&#12290;VAE&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#29992;&#20110;&#35748;&#35777;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#65292;&#20026;&#25968;&#25454;&#26679;&#26412;&#29983;&#25104;&#28508;&#22312;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;VAEs&#24050;&#29992;&#20110;&#34920;&#24449;&#29289;&#29702;&#21644;&#29983;&#29289;&#31995;&#32479;&#12290;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;VAE&#22312;&#29983;&#29289;&#24212;&#29992;&#20013;&#30340;&#25674;&#38144;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#24212;&#29992;&#20013;&#65292;&#32534;&#30721;&#22120;&#19982;&#26356;&#20256;&#32479;&#30340;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#34920;&#31034;&#20855;&#26377;&#23450;&#24615;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are a popular generative model used to approximate distributions. The encoder part of the VAE is used in amortized learning of latent variables, producing a latent representation for data samples. Recently, VAEs have been used to characterize physical and biological systems. In this case study, we qualitatively examine the amortization properties of a VAE used in biological applications. We find that in this application the encoder bears a qualitative resemblance to more traditional explicit representation of latent variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24341;&#23548;&#24335;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#21407;&#22987;&#40614;&#20811;&#39118;&#19982;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#36755;&#20986;&#22343;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#35757;&#32451;&#20174;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23450;&#21521;&#25298;&#32477;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#38477;&#22122;&#21644;&#21435;&#28151;&#21709;&#31561;&#36890;&#29992;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.07486</link><description>&lt;p&gt;
&#24341;&#23548;&#24335;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Guided Speech Enhancement Network. (arXiv:2303.07486v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#24341;&#23548;&#24335;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#21407;&#22987;&#40614;&#20811;&#39118;&#19982;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#36755;&#20986;&#22343;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#35757;&#32451;&#20174;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23450;&#21521;&#25298;&#32477;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#38477;&#22122;&#21644;&#21435;&#28151;&#21709;&#31561;&#36890;&#29992;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35821;&#38899;&#37319;&#38598;&#30340;&#36136;&#37327;&#65292;&#22810;&#40614;&#20811;&#39118;&#35821;&#38899;&#22686;&#24378;&#25216;&#26415;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#21407;&#22987;&#40614;&#20811;&#39118;&#19982;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#36755;&#20986;&#22343;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#36755;&#20837;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#27169;&#22411;&#20174;&#27874;&#26463;&#24418;&#25104;&#22120;&#30340;&#25552;&#31034;&#20013;&#23398;&#20064;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23450;&#21521;&#25298;&#32477;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#38477;&#22122;&#21644;&#21435;&#28151;&#21709;&#31561;&#36890;&#29992;&#20219;&#21153;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#32463;&#20856;&#30340;&#31354;&#38388;&#28388;&#27874;&#31639;&#27861;&#65292;&#32780;&#38750;&#19982;&#20043;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
High quality speech capture has been widely studied for both voice communication and human computer interface reasons. To improve the capture performance, we can often find multi-microphone speech enhancement techniques deployed on various devices. Multi-microphone speech enhancement problem is often decomposed into two decoupled steps: a beamformer that provides spatial filtering and a single-channel speech enhancement model that cleans up the beamformer output. In this work, we propose a speech enhancement solution that takes both the raw microphone and beamformer outputs as the input for an ML model. We devise a simple yet effective training scheme that allows the model to learn from the cues of the beamformer by contrasting the two inputs and greatly boost its capability in spatial rejection, while conducting the general tasks of denoising and dereverberation. The proposed solution takes advantage of classical spatial filtering algorithms instead of competing with them. By design, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.07475</link><description>&lt;p&gt;
&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23548;&#33268;&#65288;&#36817;&#20284;&#65289;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#30340;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#20197;&#36817;&#20284;&#22320;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#36817;&#20284;&#20110;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#26469;&#33258;&#20110;&#23545;&#24179;&#26041;&#25439;&#22833;&#30340;&#35757;&#32451;&#12290;&#19982;&#20043;&#21069;&#19987;&#38376;&#38024;&#23545;&#25351;&#25968;&#23614;&#25439;&#22833;&#24182;&#20351;&#29992;&#20013;&#38388;&#25903;&#25345;&#21521;&#37327;&#26426;&#20844;&#24335;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#22522;&#20110;Ji&#21644;Telgarsky&#65288;2021&#65289;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#30340;&#26032;&#36817;&#20284;&#31561;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#24674;&#22797;&#20102;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#19979;&#25351;&#25968;&#23614;&#25439;&#22833;&#30340;&#29616;&#26377;&#31934;&#30830;&#31561;&#25928;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#32039;&#23494;&#24615;&#30340;&#35777;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35777;&#25454;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef
&lt;/p&gt;</description></item><item><title>X-Former&#26159;&#19968;&#31181;&#20869;&#23384;&#21152;&#36895;&#22120;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#65292;&#26174;&#33879;&#21152;&#36895;Transformer&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07470</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#20013;&#21152;&#36895;Transformer
&lt;/p&gt;
&lt;p&gt;
X-Former: In-Memory Acceleration of Transformers. (arXiv:2303.07470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07470
&lt;/p&gt;
&lt;p&gt;
X-Former&#26159;&#19968;&#31181;&#20869;&#23384;&#21152;&#36895;&#22120;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#30697;&#38453;&#30340;&#31232;&#30095;&#24615;&#65292;&#26174;&#33879;&#21152;&#36895;Transformer&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#23558;&#27599;&#20010;&#21333;&#35789;&#30456;&#23545;&#20110;&#24207;&#21015;&#20013;&#30340;&#20854;&#20182;&#21333;&#35789;&#20998;&#37197;&#19968;&#20010;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;Transformer&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#22823;&#65292;&#24448;&#24448;&#36798;&#21040;&#25968;&#21315;&#20159;&#20010;&#21442;&#25968;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#30340;DRAM&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#65292;&#22914;GPU&#21644;TPU&#65292;&#22312;&#39640;&#25928;&#22788;&#29702;Transformer&#26041;&#38754;&#38754;&#20020;&#38480;&#21046;&#12290;&#22522;&#20110;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#30340;&#20869;&#23384;&#21152;&#36895;&#22120;&#26377;&#26395;&#25104;&#20026;&#35299;&#20915;&#27492;&#25361;&#25112;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#39640;&#23384;&#20648;&#23494;&#24230;&#65292;&#21516;&#26102;&#22312;&#23384;&#20648;&#22120;&#38453;&#21015;&#20869;&#25191;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;CNN&#21644;RNN&#19981;&#21516;&#65292;Transformer&#20013;&#32463;&#24120;&#20351;&#29992;&#27880;&#24847;&#24471;&#20998;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#65288;MVM&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#30340;&#20004;&#20010;&#25805;&#20316;&#25968;&#37117;&#20250;&#21160;&#24577;&#26356;&#25913;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;NVM&#30340;&#21152;&#36895;&#22120;&#20250;&#30001;&#20110;&#25152;&#38656;&#30340;&#20869;&#23384;&#35775;&#38382;&#37327;&#22686;&#21152;&#32780;&#20135;&#29983;&#39640;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-Former&#65292;&#36825;&#26159;&#19968;&#31181;&#20869;&#23384;&#21152;&#36895;&#22120;&#65292;&#21033;&#29992;&#27880;&#24847;&#30697;&#38453;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20943;&#23569;MVM&#25805;&#20316;&#25152;&#38656;&#30340;&#20869;&#23384;&#35775;&#38382;&#27425;&#25968;&#12290;&#22810;&#20010;Transformer&#27169;&#22411;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;X-Former&#26174;&#30528;&#21152;&#36895;Transformer&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved great success in a wide variety of natural language processing (NLP) tasks due to the attention mechanism, which assigns an importance score for every word relative to other words in a sequence. However, these models are very large, often reaching hundreds of billions of parameters, and therefore require a large number of DRAM accesses. Hence, traditional deep neural network (DNN) accelerators such as GPUs and TPUs face limitations in processing Transformers efficiently. In-memory accelerators based on non-volatile memory promise to be an effective solution to this challenge, since they provide high storage density while performing massively parallel matrix vector multiplications within memory arrays. However, attention score computations, which are frequently used in Transformers (unlike CNNs and RNNs), require matrix vector multiplications (MVM) where both operands change dynamically for each input. As a result, conventional NVM-based accelerators incur hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22312;&#20302;&#31471;&#21040;&#20013;&#31471;&#35774;&#22791;&#19978;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2303.07452</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Network Anomaly Detection Using Federated Learning. (arXiv:2303.07452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22312;&#20302;&#31471;&#21040;&#20013;&#31471;&#35774;&#22791;&#19978;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32593;&#32476;&#27969;&#37327;&#30340;&#30495;&#23454;&#24615;&#21644;&#24322;&#26500;&#24615;&#65292;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20840;&#23616;&#26381;&#21153;&#22120;&#19978;&#30340;&#35745;&#31639;&#36127;&#33655;&#22312;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#20171;&#32461;&#19968;&#20010;&#24378;&#22823;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26469;&#35299;&#20915;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#38382;&#39064;&#65292;&#22810;&#20010;&#21442;&#19982;&#32773;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#12290;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26550;&#26500;&#19981;&#21516;&#65292;&#32852;&#37030;&#23398;&#20064;&#19981;&#38656;&#35201;&#21442;&#19982;&#32773;&#23558;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#38450;&#27490;&#25915;&#20987;&#32773;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#20110;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#65292;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#19981;&#20805;&#20998;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20302;&#31471;&#21040;&#20013;&#31471;&#35774;&#22791;&#19978;&#26816;&#27979;&#32593;&#32476;&#24322;&#24120;&#24182;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the veracity and heterogeneity in network traffic, detecting anomalous events is challenging. The computational load on global servers is a significant challenge in terms of efficiency, accuracy, and scalability. Our primary motivation is to introduce a robust and scalable framework that enables efficient network anomaly detection. We address the issue of scalability and efficiency for network anomaly detection by leveraging federated learning, in which multiple participants train a global model jointly. Unlike centralized training architectures, federated learning does not require participants to upload their training data to the server, preventing attackers from exploiting the training data. Moreover, most prior works have focused on traditional centralized machine learning, making federated machine learning under-explored in network anomaly detection. Therefore, we propose a deep neural network framework that could work on low to mid-end devices detecting network anomalies wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19982;&#30456;&#20301;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#30450;&#30446;&#20272;&#35745;&#25152;&#35859;&#30340;&#8220;&#28151;&#21709;&#25351;&#32441;&#8221;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#24133;&#24230;&#35889;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07449</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#20301;&#29305;&#24449;&#30340;&#30450;&#38899;&#39057;&#25151;&#38388;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Blind Acoustic Room Parameter Estimation Using Phase Features. (arXiv:2303.07449v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19982;&#30456;&#20301;&#30456;&#20851;&#30340;&#29305;&#24449;&#36827;&#34892;&#30450;&#30446;&#20272;&#35745;&#25152;&#35859;&#30340;&#8220;&#28151;&#21709;&#25351;&#32441;&#8221;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#24133;&#24230;&#35889;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#22330;&#36827;&#34892;&#25151;&#38388;&#22768;&#23398;&#24314;&#27169;&#38656;&#35201;&#20174;&#22024;&#26434;&#21644;&#28151;&#21709;&#30340;&#38899;&#39057;&#20013;&#36827;&#34892;&#19968;&#23450;&#31243;&#24230;&#30340;&#30450;&#30446;&#21442;&#25968;&#20272;&#35745;&#12290;&#29616;&#20195;&#26041;&#27861;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#19982;&#26102;&#39057;&#34920;&#24449;&#30456;&#32467;&#21512;&#12290;&#20351;&#29992;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#26469;&#24320;&#21457;&#36825;&#20123;&#31867;&#20284;&#20110;&#39057;&#35889;&#22270;&#30340;&#29305;&#24449;&#34920;&#26126;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#30456;&#20301;&#39046;&#22495;&#38544;&#24335;&#20002;&#24323;&#20102;&#22823;&#37327;&#38899;&#39057;&#20449;&#24687;&#12290;&#21463;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#30340;&#19982;&#30456;&#20301;&#30456;&#20851;&#30340;&#29305;&#24449;&#25193;&#23637;&#26368;&#36817;&#30340;&#26041;&#27861;&#20197;&#30450;&#30446;&#22320;&#20272;&#35745;&#25152;&#35859;&#30340;"&#28151;&#21709;&#25351;&#32441;"&#21442;&#25968;&#65292;&#21363;&#38899;&#37327;&#21644;RT60&#12290;&#36825;&#20123;&#29305;&#24449;&#30340;&#28155;&#21152;&#22312;&#21508;&#31181;&#22768;&#23398;&#31354;&#38388;&#20013;&#20248;&#20110;&#20165;&#20381;&#36182;&#20110;&#24133;&#24230;&#22522;&#30784;&#39057;&#35889;&#29305;&#24449;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#26032;&#25968;&#25454;&#38598;&#22312;&#21333;&#21442;&#25968;&#21644;&#22810;&#21442;&#25968;&#20272;&#35745;&#31574;&#30053;&#20013;&#35780;&#20272;&#20102;&#37096;&#32626;&#36825;&#20123;&#26032;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling room acoustics in a field setting involves some degree of blind parameter estimation from noisy and reverberant audio. Modern approaches leverage convolutional neural networks (CNNs) in tandem with time-frequency representation. Using short-time Fourier transforms to develop these spectrogram-like features has shown promising results, but this method implicitly discards a significant amount of audio information in the phase domain. Inspired by recent works in speech enhancement, we propose utilizing novel phase-related features to extend recent approaches to blindly estimate the so-called "reverberation fingerprint" parameters, namely, volume and RT60. The addition of these features is shown to outperform existing methods that rely solely on magnitude-based spectral features across a wide range of acoustics spaces. We evaluate the effectiveness of the deployment of these novel features in both single-parameter and multi-parameter estimation strategies, using a novel dataset th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PO-ST-DIM&#65292;&#25913;&#36827;&#20102;ST-DIM&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07437</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#21487;&#35266;&#27979;Atari&#28216;&#25103;&#20013;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representation Learning in Partially Observable Atari Games. (arXiv:2303.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;PO-ST-DIM&#65292;&#25913;&#36827;&#20102;ST-DIM&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;Atari&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#29615;&#22659;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#22312;&#20197;&#21069;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#27604;&#29983;&#25104;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24847;&#35782;&#21040;&#25513;&#34109;&#22270;&#20687;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#34920;&#31034;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#25513;&#27169;&#20316;&#20026;&#22686;&#24378;&#25216;&#26415;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#21033;&#29992;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20180;&#32454;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#23436;&#20840;&#21487;&#35266;&#23519;&#29615;&#22659;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#37096;&#20998;&#21487;&#35266;&#27979;&#29366;&#24577;&#21019;&#24314;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#26089;&#26399;&#30340;Atari 2600&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#28145;&#24230;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;ST-DIM&#65289;&#30340;&#23545;&#27604;&#26041;&#27861;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#20854;&#30417;&#30563;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;ST-DIM&#65288;PO-ST-DIM&#65289;&#65292;&#36890;&#36807;&#21512;&#24182;&#37096;&#20998;&#21487;&#35266;&#27979;&#26041;&#26696;&#26469;&#25913;&#36827;&#23545;&#27604;&#26041;&#27861;&#12290;PO-ST-DIM&#20248;&#20110;ST-DIM&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
State representation learning aims to capture latent factors of an environment. Contrastive methods have performed better than generative models in previous state representation learning research. Although some researchers realize the connections between masked image modeling and contrastive representation learning, the effort is focused on using masks as an augmentation technique to represent the latent generative factors better. Partially observable environments in reinforcement learning have not yet been carefully studied using unsupervised state representation learning methods.  In this article, we create an unsupervised state representation learning scheme for partially observable states. We conducted our experiment on a previous Atari 2600 framework designed to evaluate representation learning models. A contrastive method called Spatiotemporal DeepInfomax (ST-DIM) has shown state-of-the-art performance on this benchmark but remains inferior to its supervised counterpart. Our appr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#20214;&#31649;&#29702;&#26102;&#38388;&#65292;&#25552;&#39640;&#23384;&#20648;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07407</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#23384;&#20648;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Study on the Data Storage Technology of Mini-Airborne Radar Based on Machine Learning. (arXiv:2303.07407v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#20214;&#31649;&#29702;&#26102;&#38388;&#65292;&#25552;&#39640;&#23384;&#20648;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26816;&#27979;&#24212;&#29992;&#20013;&#65292;&#26426;&#36733;&#38647;&#36798;&#30340;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#36828;&#39640;&#20110;&#26080;&#32447;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#65292;&#22240;&#27492;&#36890;&#24120;&#20351;&#29992;&#26426;&#36733;&#25968;&#25454;&#23384;&#20648;&#31995;&#32479;&#26469;&#23384;&#20648;&#38647;&#36798;&#25968;&#25454;&#12290;&#20855;&#26377;&#33391;&#22909;&#25239;&#38663;&#24615;&#33021;&#30340;&#25968;&#25454;&#23384;&#20648;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;NAND Flash&#20316;&#20026;&#23384;&#20648;&#20171;&#36136;&#65292;&#20294;&#23384;&#22312;&#38271;&#26102;&#38388;&#30340;&#25991;&#20214;&#31649;&#29702;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#25968;&#25454;&#23384;&#20648;&#36895;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#21488;&#23567;&#22411;&#21270;&#30340;&#38480;&#21046;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#20102;&#23384;&#20648;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#38647;&#36798;&#25968;&#25454;&#65292;&#37319;&#29992;&#35813;&#27169;&#22411;&#23545;&#25991;&#20214;&#31649;&#29702;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30830;&#23450;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#38647;&#36798;&#25968;&#25454;&#30340;&#23384;&#20648;&#12290;&#20026;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#23545;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#30340;&#25968;&#25454;&#23384;&#20648;&#31995;&#32479;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#26174;&#33879;&#38477;&#20302;&#25991;&#20214;&#31649;&#29702;&#26102;&#38388;&#65292;&#26377;&#25928;&#25552;&#39640;&#23567;&#22411;&#26426;&#36733;&#38647;&#36798;&#25968;&#25454;&#23384;&#20648;&#31995;&#32479;&#30340;&#25968;&#25454;&#23384;&#20648;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The data rate of airborne radar is much higher than the wireless data transfer rate in many detection applications, so the onboard data storage systems are usually used to store the radar data. Data storage systems with good seismic performance usually use NAND Flash as storage medium, and there is a widespread problem of long file management time, which seriously affects the data storage speed, especially under the limitation of platform miniaturization. To solve this problem, a data storage method based on machine learning is proposed for mini-airborne radar. The storage training model is established based on machine learning, and could process various kinds of radar data. The file management methods are classified and determined using the model, and then are applied to the storage of radar data. To verify the performance of the proposed method, a test was carried out on the data storage system of a mini-airborne radar. The experimental results show that the method based on machine l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#25552;&#21319;&#26641;&#30340;&#21442;&#25968;&#35843;&#20248;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;R&#21253;EZtune&#33258;&#21160;&#35843;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07400</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#26469;&#35843;&#35797;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#25552;&#21319;&#26641;
&lt;/p&gt;
&lt;p&gt;
Tuning support vector machines and boosted trees using optimization algorithms. (arXiv:2303.07400v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#25552;&#21319;&#26641;&#30340;&#21442;&#25968;&#35843;&#20248;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;R&#21253;EZtune&#33258;&#21160;&#35843;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#20013;&#24456;&#22810;&#26041;&#27861;&#38656;&#35201;&#23545;&#21442;&#25968;&#36827;&#34892;&#35843;&#20248;&#25165;&#33021;&#21457;&#25381;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;Adaboost&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#21442;&#25968;&#35843;&#20248;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#32593;&#26684;&#25628;&#32034;&#26469;&#30830;&#23450;&#21442;&#25968;&#35843;&#20248;&#30340;&#33539;&#22260;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#30001;&#20248;&#21270;&#31639;&#27861;&#36873;&#25321;&#30340;&#27169;&#22411;&#19982;&#32593;&#26684;&#25628;&#32034;&#33719;&#24471;&#30340;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#36873;&#25321;&#25928;&#26524;&#20248;&#31168;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#24320;&#21457;&#20102;&#19968;&#20010;R&#21253;EZtune&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35797;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#25552;&#21319;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning methods have been growing in popularity in recent years. Many of these procedures have parameters that must be tuned for models to perform well. Research has been extensive in neural networks, but not for many other learning methods. We looked at the behavior of tuning parameters for support vector machines, gradient boosting machines, and adaboost in both a classification and regression setting. We used grid search to identify ranges of tuning parameters where good models can be found across many different datasets. We then explored different optimization algorithms to select a model across the tuning parameter space. Models selected by the optimization algorithm were compared to the best models obtained through grid search to select well performing algorithms. This information was used to create an R package, EZtune, that automatically tunes support vector machines and boosted trees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07393</link><description>&lt;p&gt;
&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#22312;&#20107;&#20214;&#26102;&#38388;&#19979;&#30340;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#24066;&#22330;&#29983;&#24577;&#31995;&#32479;&#65292;&#30001;&#19977;&#20010;&#33829;&#20859;&#32423;&#21035;&#20195;&#34920;&#65306;&#26368;&#20248;&#25191;&#34892;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#26368;&#23567;&#26234;&#33021;&#30340;&#27969;&#21160;&#24615;&#38656;&#35201;&#32773;&#21644;&#24555;&#36895;&#30340;&#30005;&#23376;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#12290;&#26368;&#20248;&#25191;&#34892;&#20195;&#29702;&#31867;&#21035;&#21253;&#25324;&#20080;&#20837;&#21644;&#21334;&#20986;&#20195;&#29702;&#65292;&#21487;&#20197;&#20351;&#29992;&#38480;&#20215;&#21333;&#21644;&#24066;&#20215;&#21333;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#24066;&#20215;&#21333;&#36827;&#34892;&#20132;&#26131;&#12290;&#22870;&#21169;&#20989;&#25968;&#26126;&#30830;&#24179;&#34913;&#20102;&#20132;&#26131;&#25191;&#34892;&#24046;&#20215;&#19982;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#31454;&#20105;&#23398;&#20064;&#26234;&#33021;&#20307;&#22914;&#20309;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#30340;&#22823;&#23567;&#21644;&#29992;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#20989;&#25968;&#24433;&#21709;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#31354;&#38388;&#22270;&#26469;&#30740;&#31350;ABM&#30340;&#21160;&#24577;&#65292;&#24403;&#29305;&#23450;&#35268;&#33539;&#34987;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#23454;&#29616;&#39640;&#25928;&#36125;&#21494;&#26031;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21453;&#38382;&#39064;&#30340;&#26032;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.07392</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian Physics Informed Neural Networks for Inverse Problems via Ensemble Kalman Inversion. (arXiv:2303.07392v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;&#23454;&#29616;&#39640;&#25928;&#36125;&#21494;&#26031;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21453;&#38382;&#39064;&#30340;&#26032;&#25512;&#26029;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#29992;&#20110;&#25512;&#26029;&#29289;&#29702;&#21442;&#25968;&#21644;&#23398;&#20064;&#27491;&#21521;&#35299;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#20026;&#39640;&#32500;&#21518;&#39564;&#25512;&#26029;&#24102;&#26469;&#20102;&#35745;&#31639;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#20363;&#22914;&#22522;&#20110;&#31890;&#23376;&#25110;&#26041;&#24046;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#35201;&#20040;&#22312;&#39640;&#32500;&#21518;&#39564;&#25512;&#26029;&#26102;&#35745;&#31639;&#26114;&#36149;&#65292;&#35201;&#20040;&#25552;&#20379;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;B-PINNs&#25512;&#26029;&#31639;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#21345;&#23572;&#26364;&#21453;&#28436;(EKI)&#22788;&#29702;&#39640;&#32500;&#25512;&#26029;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22823;&#22823;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#19982;&#22522;&#20110;&#21704;&#23494;&#39039;&#33945;&#29305;&#21345;&#32599;&#65288;HMC&#65289;&#30340;B-PINNs&#26041;&#27861;&#30456;&#24403;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35768;&#22810;&#21453;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Physics Informed Neural Networks (B-PINNs) have gained significant attention for inferring physical parameters and learning the forward solutions for problems based on partial differential equations. However, the overparameterized nature of neural networks poses a computational challenge for high-dimensional posterior inference. Existing inference approaches, such as particle-based or variance inference methods, are either computationally expensive for high-dimensional posterior inference or provide unsatisfactory uncertainty estimates. In this paper, we present a new efficient inference algorithm for B-PINNs that uses Ensemble Kalman Inversion (EKI) for high-dimensional inference tasks. We find that our proposed method can achieve inference results with informative uncertainty estimates comparable to Hamiltonian Monte Carlo (HMC)-based B-PINNs with a much reduced computational cost. These findings suggest that our proposed approach has great potential for uncertainty quantifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#20803;&#36827;&#21270;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026; NeuroFS&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#20855;&#26377;&#26368;&#39640;&#25490;&#21517;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.07200</link><description>&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#36827;&#21270;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Supervised Feature Selection with Neuron Evolution in Sparse Neural Networks. (arXiv:2303.07200v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#20803;&#36827;&#21270;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026; NeuroFS&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#20855;&#26377;&#26368;&#39640;&#25490;&#21517;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#20174;&#25968;&#25454;&#20013;&#36873;&#25321;&#20449;&#24687;&#37327;&#39640;&#30340;&#21464;&#37327;&#23376;&#38598;&#65292;&#19981;&#20165;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#32780;&#19988;&#21487;&#20197;&#20943;&#36731;&#36164;&#28304;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#24212;&#29992;&#20110;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#26102;&#20250;&#21463;&#21040;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#36827;&#21270;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#36164;&#28304;&#26377;&#25928;&#30340;&#30417;&#30563;&#24335;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026; "NeuroFS"&#12290;&#36890;&#36807;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#20462;&#21098;&#26080;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;NeuroFS&#26377;&#25928;&#22320;&#27966;&#29983;&#20986;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;&#36890;&#36807;&#23545; $11$ &#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20302;&#32500;&#21644;&#39640;&#32500;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#22810;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; NeuroFS &#22312;&#32771;&#34385;&#21040;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#26102;&#20855;&#26377;&#26368;&#39640;&#30340;&#25490;&#21517;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection that selects an informative subset of variables from data not only enhances the model interpretability and performance but also alleviates the resource demands. Recently, there has been growing attention on feature selection using neural networks. However, existing methods usually suffer from high computational costs when applied to high-dimensional datasets. In this paper, inspired by evolution processes, we propose a novel resource-efficient supervised feature selection method using sparse neural networks, named \enquote{NeuroFS}. By gradually pruning the uninformative features from the input layer of a sparse neural network trained from scratch, NeuroFS derives an informative subset of features efficiently. By performing several experiments on $11$ low and high-dimensional real-world benchmarks of different types, we demonstrate that NeuroFS achieves the highest ranking-based score among the considered state-of-the-art supervised feature selection models. The code 
&lt;/p&gt;</description></item><item><title>&#29992;&#20803;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#35823;&#24046;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07127</link><description>&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#20248;&#21270;&#25913;&#36827;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving physics-informed neural networks with meta-learned optimization. (arXiv:2303.07127v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07127
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#30340;&#35823;&#24046;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#26126;&#65292;&#21033;&#29992;&#20803;&#23398;&#20064;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36825;&#20123;&#32593;&#32476;&#29992;&#20110;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#31995;&#32479;&#26102;&#30340;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#22320;&#20351;&#29992;&#22266;&#23450;&#30340;&#25163;&#24037;&#21046;&#20316;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22522;&#20110;&#19968;&#20010;&#27973;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20803;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#22312;&#25968;&#23398;&#29289;&#29702;&#23398;&#20013;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#26041;&#31243;&#30340;&#20803;&#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#21253;&#25324;&#32447;&#24615;&#24179;&#27969;&#26041;&#31243;&#12289;&#27850;&#26494;&#26041;&#31243;&#12289;Korteweg-de Vries&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20803;&#23398;&#20064;&#20248;&#21270;&#22120;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#21363;&#29992;&#20110;&#19968;&#20010;&#24494;&#20998;&#26041;&#31243;&#30340;&#20803;&#35757;&#32451;&#20248;&#21270;&#22120;&#20063;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#21478;&#19968;&#20010;&#24494;&#20998;&#26041;&#31243;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the error achievable using physics-informed neural networks for solving systems of differential equations can be substantially reduced when these networks are trained using meta-learned optimization methods rather than to using fixed, hand-crafted optimizers as traditionally done. We choose a learnable optimization method based on a shallow multi-layer perceptron that is meta-trained for specific classes of differential equations. We illustrate meta-trained optimizers for several equations of practical relevance in mathematical physics, including the linear advection equation, Poisson's equation, the Korteweg--de Vries equation and Burgers' equation. We also illustrate that meta-learned optimizers exhibit transfer learning abilities, in that a meta-trained optimizer on one differential equation can also be successfully deployed on another differential equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PANIC&#65292;&#19968;&#20010;&#22522;&#20110;&#21407;&#22411;&#21152;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;AD&#20998;&#31867;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;3D&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#24182;&#19988;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30452;&#25509;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25552;&#21462;&#20986;&#20102;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;AD&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.07125</link><description>&lt;p&gt;
&#19981;&#35201;&#24778;&#24908;&#65306;&#22522;&#20110;&#21407;&#22411;&#21152;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Don't PANIC: Prototypical Additive Neural Network for Interpretable Classification of Alzheimer's Disease. (arXiv:2303.07125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PANIC&#65292;&#19968;&#20010;&#22522;&#20110;&#21407;&#22411;&#21152;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;AD&#20998;&#31867;&#27169;&#22411;&#65292;&#25972;&#21512;&#20102;3D&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#24182;&#19988;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#30452;&#25509;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25552;&#21462;&#20986;&#20102;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;AD&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#20855;&#26377;&#22797;&#26434;&#21644;&#22810;&#22240;&#32032;&#30340;&#30149;&#22240;&#23398;&#65292;&#38656;&#35201;&#25972;&#21512;&#31070;&#32463;&#35299;&#21078;&#23398;&#12289;&#36951;&#20256;&#23398;&#21644;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20449;&#24687;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20102;&#22270;&#20687;&#21644;&#34920;&#26684;&#20449;&#24687;&#20197;&#25552;&#39640;&#35786;&#26029;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#20173;&#28982;&#26159;&#20020;&#24202;&#24212;&#29992;&#30340;&#38556;&#30861;&#65292;&#20854;&#20013;&#29702;&#35299;&#24322;&#36136;&#27169;&#22411;&#30340;&#20915;&#31574;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PANIC&#30340;&#21407;&#22411;&#21152;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#30340;AD&#20998;&#31867;&#65292;&#20854;&#25972;&#21512;&#20102;3D&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#23427;&#26159;&#36890;&#36807;&#35774;&#35745;&#26469;&#21487;&#35299;&#37322;&#30340;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23581;&#35797;&#36924;&#36817;&#32593;&#32476;&#20915;&#31574;&#30340;&#20107;&#21518;&#35299;&#37322;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PANIC&#22312;AD&#20998;&#31867;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#30452;&#25509;&#25552;&#20379;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PANIC&#25552;&#21462;&#20986;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;AD&#29305;&#24449;&#65292;&#24182;&#28385;&#36275;&#20102;&#19968;&#23450;&#30340;s&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) has a complex and multifactorial etiology, which requires integrating information about neuroanatomy, genetics, and cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep learning approaches combined image and tabular information to improve diagnostic performance. However, the black-box nature of such neural networks is still a barrier for clinical applications, in which understanding the decision of a heterogeneous model is integral. We propose PANIC, a prototypical additive neural network for interpretable AD classification that integrates 3D image and tabular data. It is interpretable by design and, thus, avoids the need for post-hoc explanations that try to approximate the decision of a network. Our results demonstrate that PANIC achieves state-of-the-art performance in AD classification, while directly providing local and global explanations. Finally, we show that PANIC extracts biologically meaningful signatures of AD, and satisfies a s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07122</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#37327;&#21270;&#21271;&#26497;&#25918;&#22823;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Quantifying Causes of Arctic Amplification via Deep Learning based Time-series Causal Inference. (arXiv:2303.07122v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;TCINet&#65292;&#29992;&#20110;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21271;&#26497;&#21464;&#26262;&#65292;&#20063;&#31216;&#21271;&#26497;&#25918;&#22823;&#65292;&#30001;&#22810;&#31181;&#22823;&#27668;&#21644;&#28023;&#27915;&#22240;&#32032;&#23548;&#33268;&#65292;&#20294;&#20854;&#22522;&#30784;&#28909;&#21147;&#22240;&#32032;&#30340;&#35814;&#32454;&#24773;&#20917;&#20173;&#19981;&#28165;&#26970;&#12290;&#20351;&#29992;&#22266;&#23450;&#27835;&#30103;&#25928;&#24212;&#31574;&#30053;&#25512;&#26029;&#22823;&#27668;&#36807;&#31243;&#23545;&#28023;&#20912;&#34701;&#21270;&#30340;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;&#19981;&#29616;&#23454;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#26102;&#38388;&#21464;&#21270;&#30340;&#28151;&#28102;&#30340;&#24433;&#21709;&#32780;&#24341;&#36215;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TCINet - &#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65292;&#20197;&#36830;&#32493;&#27835;&#30103;&#26041;&#24335;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#22914;&#20309;&#22823;&#22823;&#25552;&#39640;&#37327;&#21270;&#21271;&#26497;&#28023;&#20912;&#34701;&#21270;&#30340;&#20027;&#35201;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The warming of the Arctic, also known as Arctic amplification, is led by several atmospheric and oceanic drivers, however, the details of its underlying thermodynamic causes are still unknown. Inferring the causal effects of atmospheric processes on sea ice melt using fixed treatment effect strategies leads to unrealistic counterfactual estimations. Such models are also prone to bias due to time-varying confoundedness. In order to tackle these challenges, we propose TCINet - time-series causal inference model to infer causation under continuous treatment using recurrent neural networks. Through experiments on synthetic and observational data, we show how our research can substantially improve the ability to quantify the leading causes of Arctic sea ice melt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06965</link><description>&lt;p&gt;
Uni-RXN: &#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24357;&#21512;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#26377;&#26426;&#21270;&#23398;&#30740;&#31350;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21270;&#23398;&#21453;&#24212;&#22522;&#26412;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#21453;&#24212;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#21463;&#26377;&#26426;&#21270;&#23398;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24402;&#32435;&#20559;&#35265;&#32435;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#35813;&#26694;&#26550;&#21487;&#24212;&#29992;&#20110;&#22522;&#20110;&#21453;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#21487;&#21512;&#25104;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;PyTorch&#21644;Firedrake&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23454;&#29616;&#23545;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.06871</link><description>&lt;p&gt;
&#21033;&#29992;PyTorch&#21644;Firedrake&#23454;&#29616;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-driven machine learning models coupling PyTorch and Firedrake. (arXiv:2303.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;PyTorch&#21644;Firedrake&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23454;&#29616;&#23545;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25551;&#36848;&#21644;&#24314;&#27169;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#20013;&#35768;&#22810;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;PDE&#24314;&#27169;&#20165;&#25552;&#20379;&#20102;&#29289;&#29702;&#27169;&#22411;&#30340;&#19981;&#23436;&#25972;&#25551;&#36848;&#12290;&#22522;&#20110;PDE&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;PDE&#29992;&#20316;&#24402;&#32435;&#20559;&#32622;&#65292;&#20351;&#32806;&#21512;&#27169;&#22411;&#33021;&#22815;&#20381;&#36182;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23558;PDE&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#39640;&#24615;&#33021;&#27169;&#25311;&#37096;&#32626;&#21040;&#22797;&#26434;&#38382;&#39064;&#20013;&#38656;&#35201;&#32452;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;PDE&#30340;&#26694;&#26550;&#25552;&#20379;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32806;&#21512;&#26041;&#24335;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;PyTorch&#21644;PDE&#31995;&#32479;Firedrake&#30456;&#32467;&#21512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25351;&#23450;&#32806;&#21512;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#23545;&#29616;&#26377;&#20195;&#30721;&#36827;&#34892;&#24494;&#19981;&#36275;&#36947;&#30340;&#26356;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are central to describing and modelling complex physical systems that arise in many disciplines across science and engineering. However, in many realistic applications PDE modelling provides an incomplete description of the physics of interest. PDE-based machine learning techniques are designed to address this limitation. In this approach, the PDE is used as an inductive bias enabling the coupled model to rely on fundamental physical laws while requiring less training data. The deployment of high-performance simulations coupling PDEs and machine learning to complex problems necessitates the composition of capabilities provided by machine learning and PDE-based frameworks. We present a simple yet effective coupling between the machine learning framework PyTorch and the PDE system Firedrake that provides researchers, engineers and domain specialists with a high productive way of specifying coupled models while only requiring trivial changes to existi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#31614;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#65292;&#24674;&#22797;&#26631;&#31614;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06836</link><description>&lt;p&gt;
&#26631;&#31614;&#20449;&#24687;&#29942;&#39048;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Label Information Bottleneck for Label Enhancement. (arXiv:2303.06836v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#31614;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;&#65292;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#65292;&#24674;&#22797;&#26631;&#31614;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;&#26631;&#31614;&#22686;&#24378;&#38382;&#39064;&#65292;&#21363;&#20174;&#36923;&#36753;&#26631;&#31614;&#24674;&#22797;&#26631;&#31614;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#65288;LIB&#65289;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#19981;&#30456;&#20851;&#30340;&#26631;&#31614;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#24674;&#22797;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21162;&#21147;&#25366;&#25496;&#20851;&#38190;&#30340;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#20197;&#25552;&#39640;&#24674;&#22797;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#26631;&#31614;&#22686;&#24378;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#32852;&#21512;&#36807;&#31243;&#65306;1) &#23398;&#20064;&#20855;&#26377;&#20851;&#38190;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;2) &#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24674;&#22797;&#26631;&#31614;&#20998;&#24067;&#12290;&#20851;&#38190;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#30340;&#8220;&#29942;&#39048;&#8221;&#25366;&#25496;&#20986;&#26469;&#12290;&#26174;&#33879;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26631;&#31614;&#20998;&#37197;&#21644;&#26631;&#31614;&#24046;&#24322;&#30340;&#20851;&#38190;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the challenging problem of Label Enhancement (LE), which aims to exactly recover label distributions from logical labels, and present a novel Label Information Bottleneck (LIB) method for LE. For the recovery process of label distributions, the label irrelevant information contained in the dataset may lead to unsatisfactory recovery performance. To address this limitation, we make efforts to excavate the essential label relevant information to improve the recovery performance. Our method formulates the LE problem as the following two joint processes: 1) learning the representation with the essential label relevant information, 2) recovering label distributions based on the learned representation. The label relevant information can be excavated based on the "bottleneck" formed by the learned representation. Significantly, both the label relevant information about the label assignments and the label relevant information about the label gaps can be explored in ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21322;&#33258;&#20027;&#20195;&#29702;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65292;&#26377;&#25928;&#38477;&#20302;&#19987;&#23478;&#35843;&#29992;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06710</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20026;&#20154;-&#26426;&#21327;&#21516;&#26426;&#22120;&#20154;&#26234;&#33021;&#20915;&#31574;&#25552;&#20379;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decision Making for Human-in-the-loop Robotic Agents via Uncertainty-Aware Reinforcement Learning. (arXiv:2303.06710v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21322;&#33258;&#20027;&#20195;&#29702;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65292;&#26377;&#25928;&#38477;&#20302;&#19987;&#23478;&#35843;&#29992;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;-&#26426;&#21327;&#21516;&#33539;&#24335;&#20013;&#65292;&#26426;&#22120;&#20154;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#22823;&#37096;&#20998;&#26102;&#38388;&#20869;&#33258;&#20027;&#23436;&#25104;&#20219;&#21153;&#65292;&#24403;&#38656;&#35201;&#24110;&#21161;&#26102;&#65292;&#21487;&#20197;&#21521;&#22806;&#37096;&#19987;&#23478;&#23547;&#27714;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#26159;&#35201;&#30693;&#36947;&#20309;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#65306;&#22826;&#23569;&#30340;&#35831;&#27714;&#20250;&#23548;&#33268;&#26426;&#22120;&#20154;&#29359;&#38169;&#65292;&#20294;&#22826;&#22810;&#30340;&#35831;&#27714;&#20250;&#20351;&#19987;&#23478;&#36807;&#36733;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21322;&#33258;&#20027;&#20195;&#29702;&#22312;&#23545;&#20219;&#21153;&#25104;&#21151;&#32467;&#26524;&#30340;&#20449;&#24515;&#20302;&#26102;&#35831;&#27714;&#22806;&#37096;&#24110;&#21161;&#12290;&#32622;&#20449;&#24230;&#26159;&#36890;&#36807;&#20272;&#35745;&#24403;&#21069;&#29366;&#24577;&#30340;&#22238;&#25253;&#26041;&#24046;&#26469;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20272;&#35745;&#21487;&#20197;&#36890;&#36807;&#31867;&#20284;&#36125;&#23572;&#26364;&#36882;&#24402;&#30340;&#35757;&#32451;&#36807;&#31243;&#36880;&#27493;&#25913;&#36827;&#12290;&#22312;&#20855;&#26377;&#23436;&#20840;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29366;&#24577;&#20449;&#24687;&#30340;&#31163;&#25955;&#23548;&#33322;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#19987;&#23478;&#35843;&#29992;&#39044;&#31639;&#26102;&#26159;&#26377;&#25928;&#30340;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#27809;&#26377;&#35775;&#38382;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a Human-in-the-Loop paradigm, a robotic agent is able to act mostly autonomously in solving a task, but can request help from an external expert when needed. However, knowing when to request such assistance is critical: too few requests can lead to the robot making mistakes, but too many requests can overload the expert. In this paper, we present a Reinforcement Learning based approach to this problem, where a semi-autonomous agent asks for external assistance when it has low confidence in the eventual success of the task. The confidence level is computed by estimating the variance of the return from the current state. We show that this estimate can be iteratively improved during training using a Bellman-like recursion. On discrete navigation problems with both fully- and partially-observable state information, we show that our method makes effective use of a limited budget of expert calls at run-time, despite having no access to the expert at training time.
&lt;/p&gt;</description></item><item><title>CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06274</link><description>&lt;p&gt;
CoNIC&#25361;&#25112;&#65306;&#25512;&#21160;&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35745;&#25968;&#30340;&#21069;&#27839;&#65288;arXiv:2303.06274v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06274
&lt;/p&gt;
&lt;p&gt;
CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.
&lt;/p&gt;
&lt;p&gt;
&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#24418;&#24577;&#27979;&#37327;&#26159;&#24110;&#21161;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#32452;&#32455;&#23398;&#21644;&#24739;&#32773;&#39044;&#21518;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#19968;&#20010;&#31038;&#21306;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#20197;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#21517;&#20026;CoNIC&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#20849;&#25490;&#34892;&#27036;&#19978;&#36827;&#34892;&#23454;&#26102;&#32467;&#26524;&#26816;&#26597;&#12290;&#25105;&#20204;&#22522;&#20110;1,658&#20010;&#32467;&#32928;&#32452;&#32455;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21518;&#25361;&#25112;&#20998;&#26512;&#12290;&#27599;&#20010;&#27169;&#22411;&#26816;&#27979;&#21040;&#32422;7&#20159;&#20010;&#32454;&#32990;&#26680;&#65292;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#19981;&#33391;&#22686;&#29983;&#20998;&#32423;&#21644;&#29983;&#23384;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25361;&#25112;&#23545;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#25913;&#36827;&#23548;&#33268;&#20102;&#19979;&#28216;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#34920;&#26126;&#65292;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06021</link><description>&lt;p&gt;
&#20307;&#32946;&#21338;&#24425;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#39044;&#27979;&#27169;&#22411;&#24212;&#20248;&#21270;&#20934;&#30830;&#24615;&#36824;&#26159;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sports betting: should forecasting models be optimised for accuracy or calibration?. (arXiv:2303.06021v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#26657;&#20934;&#24615;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#26368;&#36817;&#23545;&#20307;&#32946;&#21338;&#24425;&#36827;&#34892;&#20102;&#32852;&#37030;&#21512;&#27861;&#21270;&#65292;&#36825;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#30456;&#36935;&#12290;&#22914;&#26524;&#21338;&#24425;&#32773;&#33021;&#22815;&#21033;&#29992;&#25968;&#25454;&#20934;&#30830;&#22320;&#39044;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20182;&#20204;&#21487;&#20197;&#35748;&#35782;&#21040;&#20309;&#26102;&#20070;maker&#30340;&#36180;&#29575;&#23545;&#20182;&#20204;&#26377;&#21033;&#12290;&#30001;&#20110;&#20307;&#32946;&#21338;&#24425;&#20165;&#22312;&#32654;&#22269;&#30340;&#24066;&#22330;&#19978;&#23601;&#26159;&#19968;&#20010;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#34892;&#19994;&#65292;&#22240;&#27492;&#25214;&#21040;&#36825;&#26679;&#30340;&#26426;&#20250;&#21487;&#33021;&#20250;&#38750;&#24120;&#26377;&#21033;&#21487;&#22270;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20307;&#32946;&#36187;&#26524;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#20934;&#30830;&#24230;&#26469;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20551;&#35774;&#65292;&#23545;&#20110;&#20307;&#32946;&#21338;&#24425;&#38382;&#39064;&#65292;&#27169;&#22411;&#26657;&#20934;&#27604;&#20934;&#30830;&#24230;&#26356;&#37325;&#35201;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#36187;&#23395;&#30340;NBA&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21333;&#20010;&#36187;&#23395;&#19978;&#20351;&#29992;&#24050;&#21457;&#24067;&#30340;&#36180;&#29575;&#36827;&#34892;&#21338;&#24425;&#23454;&#39564;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#21338;&#24425;&#31995;&#32479;&#65292;&#25105;&#20204;&#34920;&#26126;&#20248;&#21270;&#26657;&#20934;&#30340;&#39044;&#27979;&#27169;&#22411;&#27604;&#20248;&#21270;&#20934;&#30830;&#24230;&#24179;&#22343;&#24102;&#26469;&#26356;&#39640;&#30340;&#22238;&#25253;&#29575;&#65288;&#25237;&#36164;&#22238;&#25253;&#29575;&#20026;$110.42&#65285;$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to accurately predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of forecasting models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. Evaluating various betting systems, we show that optimising the forecasting model for calibration leads to greater returns than optimising for accuracy, on average (return on investment of $110.42\%$ v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2303.05735</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#24418;&#30340;&#30828;&#20214;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Hardware Acceleration of Neural Graphics. (arXiv:2303.05735v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#24418;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65292;&#21457;&#29616;&#24403;&#21069;GPU&#24615;&#33021;&#26080;&#27861;&#28385;&#36275;&#23545;4K&#20998;&#36776;&#29575;60FPS&#28210;&#26579;&#30340;&#38656;&#27714;&#65292;&#19988;&#22312;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#24615;&#33021;&#32570;&#21475;&#26356;&#22823;&#12290;&#20316;&#32773;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#28210;&#26579;&#21644;&#21453;&#28210;&#26579;&#31639;&#27861;&#24050;&#34987;&#31070;&#32463;&#34920;&#31034;&#65288;NR&#65289;&#25152;&#21462;&#20195;&#12290;NR&#26368;&#36817;&#34987;&#29992;&#20110;&#23398;&#20064;&#22330;&#26223;&#30340;&#20960;&#20309;&#21644;&#26448;&#36136;&#23646;&#24615;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#21512;&#25104;&#30495;&#23454;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#25215;&#35834;&#29992;&#21487;&#20280;&#32553;&#30340;&#36136;&#37327;&#21644;&#21487;&#39044;&#27979;&#30340;&#24615;&#33021;&#26367;&#25442;&#20256;&#32479;&#30340;&#28210;&#26579;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#38382;&#39064;&#65306;&#31070;&#32463;&#22270;&#24418;&#65288;NG&#65289;&#26159;&#21542;&#38656;&#35201;&#30828;&#20214;&#25903;&#25345;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#34920;&#24615;&#30340;NG&#24212;&#29992;&#31243;&#24207;&#65292;&#21457;&#29616;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#24403;&#21069;&#30340;GPU&#19978;&#20197;60FPS&#28210;&#26579;4K&#20998;&#36776;&#29575;&#65292;&#21017;&#25152;&#38656;&#24615;&#33021;&#19982;&#24403;&#21069;GPU&#30340;&#23454;&#38469;&#24615;&#33021;&#23384;&#22312;1.5&#20493;&#33267;55&#20493;&#30340;&#24046;&#36317;&#12290;&#23545;&#20110;&#22686;&#24378;&#29616;&#23454;/&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#25152;&#38656;&#24615;&#33021;&#19982;&#25152;&#38656;&#31995;&#32479;&#21151;&#29575;&#20043;&#38388;&#23384;&#22312;&#26356;&#22823;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30830;&#23450;&#36755;&#20837;&#32534;&#30721;&#21644;MLP&#20869;&#26680;&#26159;&#24615;&#33021;&#29942;&#39048;&#65292;&#23545;&#20110;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#12289;&#22810;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#21644;&#20302;&#20998;&#36776;&#29575;&#23494;&#38598;&#32593;&#26684;&#65292;&#23427;&#20204;&#21344;&#24212;&#29992;&#31243;&#24207;&#26102;&#38388;&#30340;72&#65285;&#12289;60&#65285;&#21644;59&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rendering and inverse-rendering algorithms that drive conventional computer graphics have recently been superseded by neural representations (NR). NRs have recently been used to learn the geometric and the material properties of the scenes and use the information to synthesize photorealistic imagery, thereby promising a replacement for traditional rendering algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (NG) need hardware support? We studied representative NG applications showing that, if we want to render 4k res. at 60FPS there is a gap of 1.5X-55X in the desired performance on current GPUs. For AR/VR applications, there is an even larger gap of 2-4 OOM between the desired performance and the required system power. We identify that the input encoding and the MLP kernels are the performance bottlenecks, consuming 72%,60% and 59% of application time for multi res. hashgrid, multi res. densegrid and low res. densegrid 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.05660</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#30456;&#20851;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23454;&#29616;&#20132;&#36890;&#37327;&#20272;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#35299;&#20915;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards better traffic volume estimation: Tackling both underdetermined and non-equilibrium problems via a correlation-adaptive graph convolution network. (arXiv:2303.05660v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#21644;&#38750;&#24179;&#34913;&#38382;&#39064;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#37327;&#26159;&#20132;&#36890;&#31649;&#29702;&#21644;&#25511;&#21046;&#25552;&#20379;&#32454;&#31890;&#24230;&#20449;&#24687;&#19981;&#21487;&#25110;&#32570;&#30340;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20256;&#24863;&#22120;&#30340;&#26377;&#38480;&#37096;&#32626;&#65292;&#33719;&#21462;&#20840;&#38754;&#30340;&#20132;&#36890;&#37327;&#20449;&#24687;&#24182;&#19981;&#23481;&#26131;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#29305;&#23450;&#26041;&#27861;&#30340;&#25972;&#20307;&#20272;&#35745;&#20934;&#30830;&#24615;&#19978;&#65292;&#24573;&#30053;&#20102;&#20132;&#36890;&#37327;&#20272;&#35745;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#27492;&#22312;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#36890;&#37327;&#20272;&#35745;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;: (1) &#30001;&#26410;&#26816;&#27979;&#21040;&#30340;&#34892;&#21160;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#20132;&#36890;&#27969;&#65292;&#20197;&#21450; (2) &#30001;&#25317;&#22581;&#20256;&#25773;&#24341;&#36215;&#30340;&#38750;&#24179;&#34913;&#20132;&#36890;&#27969;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26080;&#27169;&#22411;&#30340;&#21644;&#30456;&#20851;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20934;&#30830;&#30340;&#20840;&#38754;&#20132;&#36890;&#37327;&#20272;&#35745;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#37327;&#21270;&#20132;&#36890;&#36895;&#24230;&#21644;&#27969;&#37327;&#20043;&#38388;&#30340;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#24314;&#31435;&#20132;&#36890;&#27969;&#22270;&#30340;&#30456;&#20851;&#22270;&#21367;&#31215;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic volume is an indispensable ingredient to provide fine-grained information for traffic management and control. However, due to limited deployment of traffic sensors, obtaining full-scale volume information is far from easy. Existing works on this topic primarily focus on improving the overall estimation accuracy of a particular method and ignore the underlying challenges of volume estimation, thereby having inferior performances on some critical tasks. This paper studies two key problems with regard to traffic volume estimation: (1) underdetermined traffic flows caused by undetected movements, and (2) non-equilibrium traffic flows arise from congestion propagation. Here we demonstrate a graph-based deep learning method that can offer a data-driven, model-free and correlation adaptive approach to tackle the above issues and perform accurate network-wide traffic volume estimation. Particularly, in order to quantify the dynamic and nonlinear relationships between traffic speed and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AdaOFUL&#21644;VARA&#20004;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;AdaOFUL&#20855;&#26377;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;VARA&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.05606</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37325;&#23614;&#22870;&#21169;&#26041;&#24046;&#24863;&#30693;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variance-aware robust reinforcement learning with linear function approximation under heavy-tailed rewards. (arXiv:2303.05606v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AdaOFUL&#21644;VARA&#20004;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;AdaOFUL&#20855;&#26377;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;VARA&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;AdaOFUL&#21644;VARA&#65292;&#29992;&#20110;&#22312;&#20165;&#23384;&#22312;&#26377;&#38480;&#26041;&#24046;&#30340;&#37325;&#23614;&#22870;&#21169;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#12290;&#23545;&#20110;&#32447;&#24615;&#38543;&#26426;&#36172;&#24466;&#65292;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#33258;&#36866;&#24212;Huber&#22238;&#24402;&#24182;&#25552;&#20986;AdaOFUL&#26469;&#35299;&#20915;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#12290;AdaOFUL&#36798;&#21040;&#20102;&#29366;&#24577;-of-the-art&#30340;&#36951;&#25022;&#30028;&#65292;&#21363;$ \widetilde{O}\big&#65288;d\big(\sum_{t=1}^T\nu_{t}^2\big)^{1/2}+d\big)$&#65292;&#20854;&#20013;$\nu_{t}^2$&#26159;&#31532;$t$&#36718;&#22870;&#21169;&#35266;&#27979;&#21040;&#30340;&#26465;&#20214;&#26041;&#24046;&#65292;$d$&#26159;&#29305;&#24449;&#32500;&#24230;&#65292;$\widetilde{O}&#65288;\cdot&#65289;$ &#38544;&#34255;&#23545;&#25968;&#20381;&#36182;&#24615;&#12290;&#22312;AdaOFUL&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VARA&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#23427;&#36798;&#21040;&#20102;&#26356;&#32039;&#23494;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#65292;&#21363; $ \widetilde{O}(d\sqrt{HG^*K})$&#12290;&#36825;&#37324;&#65292;$H$&#26159;&#20107;&#20214;&#30340;&#38271;&#24230;&#65292;$K$&#26159;&#20107;&#20214;&#30340;&#25968;&#37327;&#65292;$G^*$&#26159;&#36739;&#23567;&#30340;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#37327;&#65292;&#24403;&#22312;&#20854;&#20182;&#23454;&#20363;&#30456;&#20851;&#37327;&#34987;&#38480;&#21046;&#26102;&#65292;&#23427;&#21487;&#20197;&#34987;&#36793;&#30028;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two algorithms, AdaOFUL and VARA, for online sequential decision-making in the presence of heavy-tailed rewards with only finite variances. For linear stochastic bandits, we address the issue of heavy-tailed rewards by modifying the adaptive Huber regression and proposing AdaOFUL. AdaOFUL achieves a state-of-the-art regret bound of $\widetilde{O}\big(d\big(\sum_{t=1}^T \nu_{t}^2\big)^{1/2}+d\big)$ as if the rewards were uniformly bounded, where $\nu_{t}^2$ is the observed conditional variance of the reward at round $t$, $d$ is the feature dimension, and $\widetilde{O}(\cdot)$ hides logarithmic dependence. Building upon AdaOFUL, we propose VARA for linear MDPs, which achieves a tighter variance-aware regret bound of $\widetilde{O}(d\sqrt{HG^*K})$. Here, $H$ is the length of episodes, $K$ is the number of episodes, and $G^*$ is a smaller instance-dependent quantity that can be bounded by other instance-dependent quantities when additional structural conditions on the 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSeaProbLog&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#65292;&#25903;&#25345;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25512;&#26029;&#21644;&#23398;&#20064;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.04660</link><description>&lt;p&gt;
&#31163;&#25955;-&#36830;&#32493;&#22495;&#20013;&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Probabilistic Logic Programming in Discrete-Continuous Domains. (arXiv:2303.04660v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04660
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSeaProbLog&#30340;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#65292;&#25903;&#25345;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25512;&#26029;&#21644;&#23398;&#20064;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#36923;&#36753;&#24418;&#24335;&#30340;&#31526;&#21495;&#32972;&#26223;&#30693;&#35782;&#12290;NeSy&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#33539;&#22260;&#20869;&#26377;&#21161;&#20110;&#23398;&#20064;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#26029;&#38750;&#20998;&#24067;&#25968;&#25454;&#12290;&#27010;&#29575;NeSy&#30528;&#37325;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#36923;&#36753;&#21644;&#27010;&#29575;&#35770;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#36824;&#20801;&#35768;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#23398;&#20064;&#12290;&#24403;&#21069;&#27010;&#29575;NeSy&#31995;&#32479;&#65288;&#22914;DeepProbLog&#65289;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#23616;&#38480;&#20110;&#26377;&#38480;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28145;&#24230;&#27010;&#29575;&#32534;&#31243;&#65288;DPP&#65289;&#22312;&#24314;&#27169;&#21644;&#20248;&#21270;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeaProbLog&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#35821;&#35328;&#65292;&#23558;DPP&#25216;&#26415;&#32435;&#20837;NeSy&#20013;&#12290;&#36825;&#26679;&#20570;&#30340;&#32467;&#26524;&#26159;&#22312;&#36923;&#36753;&#32422;&#26463;&#26465;&#20214;&#19979;&#25903;&#25345;&#31163;&#25955;&#21644;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#30340;&#25512;&#26029;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;1&#65289;DeepSeaProbLog&#30340;&#35821;&#20041;&#65292;2&#65289;&#23427;&#30340;&#39640;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;3&#65289;&#23427;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;4&#65289;&#19968;&#32452;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#39046;&#22495;&#30340;&#24314;&#27169;&#12289;&#25512;&#26029;&#21644;&#23398;&#20064;&#26041;&#38754;&#30456;&#23545;&#20110;&#24403;&#21069;NeSy&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic AI (NeSy) allows neural networks to exploit symbolic background knowledge in the form of logic. It has been shown to aid learning in the limited data regime and to facilitate inference on out-of-distribution data. Probabilistic NeSy focuses on integrating neural networks with both logic and probability theory, which additionally allows learning under uncertainty. A major limitation of current probabilistic NeSy systems, such as DeepProbLog, is their restriction to finite probability distributions, i.e., discrete random variables. In contrast, deep probabilistic programming (DPP) excels in modelling and optimising continuous probability distributions. Hence, we introduce DeepSeaProbLog, a neural probabilistic logic programming language that incorporates DPP techniques into NeSy. Doing so results in the support of inference and learning of both discrete and continuous probability distributions under logical constraints. Our main contributions are 1) the semantics of DeepS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#39044;&#27979;&#31867;&#26631;&#31614;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22806;&#22495;&#26816;&#27979;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312; AUROC &#21644; AUPR &#26041;&#38754;&#23454;&#29616;&#32479;&#35745;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.04115</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22806;&#22495;&#26816;&#27979;&#30340;&#39044;&#27979;&#23884;&#20837;&#21151;&#29575;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Predicted Embedding Power Regression for Large-Scale Out-of-Distribution Detection. (arXiv:2303.04115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#30340;&#39044;&#27979;&#31867;&#26631;&#31614;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22806;&#22495;&#26816;&#27979;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312; AUROC &#21644; AUPR &#26041;&#38754;&#23454;&#29616;&#32479;&#35745;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22495;&#36755;&#20837;&#21487;&#33021;&#20250;&#21361;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29992;&#20110;&#22806;&#22495;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#23569;&#37327;&#31867;&#21035;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20026;&#22823;&#35268;&#27169;&#22806;&#22495;&#26816;&#27979;&#24320;&#21457;&#30340;&#26041;&#27861;&#24456;&#23569;&#12290;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26368;&#22823;&#20998;&#31867;&#27010;&#29575;&#65292;&#20363;&#22914;&#26368;&#20808;&#36827;&#30340;&#20998;&#32452; softmax &#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#20998;&#24067;&#35745;&#31639;&#39044;&#27979;&#31867;&#26631;&#31614;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#20165;&#22312;&#35745;&#31639;&#25104;&#26412;&#19978;&#31245;&#26377;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;14&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312; AUROC&#65288;84.2 vs 82.4&#65289;&#21644; AUPR&#65288;96.2 vs 93.7&#65289;&#26041;&#38754;&#23454;&#29616;&#20102;&#32479;&#35745;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) inputs can compromise the performance and safety of real world machine learning systems. While many methods exist for OOD detection and work well on small scale datasets with lower resolution and few classes, few methods have been developed for large-scale OOD detection. Existing large-scale methods generally depend on maximum classification probability, such as the state-of-the-art grouped softmax method. In this work, we develop a novel approach that calculates the probability of the predicted class label based on label distributions learned during the training process. Our method performs better than current state-of-the-art methods with only a negligible increase in compute cost. We evaluate our method against contemporary methods across $14$ datasets and achieve a statistically significant improvement with respect to AUROC (84.2 vs 82.4) and AUPR (96.2 vs 93.7).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03052</link><description>&lt;p&gt;
&#25513;&#34109;&#22270;&#20687;&#26159;&#40065;&#26834;&#24494;&#35843;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Masked Images Are Counterfactual Samples for Robust Fine-tuning. (arXiv:2303.03052v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#32780;&#21463;&#21040;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#23637;&#29616;&#20102;&#31354;&#21069;&#30340;&#40065;&#26834;&#24615;&#26469;&#24212;&#23545;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#20998;&#24067;&#20869;&#24615;&#33021;&#21644;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#22788;&#29702;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#22240;&#26524;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#25513;&#34109;&#22270;&#20687;&#20316;&#20026;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#24494;&#35843;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#31867;&#28608;&#27963;&#22270;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20851;&#25110;&#35821;&#20041;&#26080;&#20851;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#65292;&#20197;&#25171;&#30772;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#20854;&#20182;&#22270;&#20687;&#30340;&#34917;&#19969;&#26469;&#37325;&#26032;&#22635;&#20805;&#25513;&#34109;&#30340;&#34917;&#19969;&#12290;&#36825;&#20123;&#21453;&#20107;&#23454;&#26679;&#26412;&#21017;&#29992;&#20110;&#29305;&#24449;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are challenged by the distribution shift between the training data and test data. Recently, the large models pre-trained on diverse data demonstrate unprecedented robustness to various distribution shifts. However, fine-tuning on these models can lead to a trade-off between in-distribution (ID) performance and out-of-distribution (OOD) robustness. Existing methods for tackling this trade-off do not explicitly address the OOD robustness problem. In this paper, based on causal analysis on the aforementioned problems, we propose a novel fine-tuning method, which use masked images as counterfactual samples that help improving the robustness of the fine-tuning model. Specifically, we mask either the semantics-related or semantics-unrelated patches of the images based on class activation map to break the spurious correlation, and refill the masked patches with patches from other images. The resulting counterfactual samples are used in feature-based distillation with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#23545;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#39640;&#32500;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22266;&#23450;&#39044;&#27979;&#38382;&#39064;&#30340;&#21452;&#19979;&#38477;&#26354;&#32447;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.01372</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#32447;&#24615;&#22238;&#24402;&#21452;&#19979;&#38477;&#30340;&#39640;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-dimensional analysis of double descent for linear regression with random projections. (arXiv:2303.01372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#23545;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#39640;&#32500;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22266;&#23450;&#39044;&#27979;&#38382;&#39064;&#30340;&#21452;&#19979;&#38477;&#26354;&#32447;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38543;&#26426;&#25237;&#24433;&#30340;&#25968;&#37327;&#21464;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#22266;&#23450;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#21452;&#19979;&#38477;&#26354;&#32447;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;Ridge&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#24182;&#20351;&#29992;&#38750;&#21442;&#25968;&#32479;&#35745;&#20013;&#30340;&#32463;&#20856;&#27010;&#24565;&#65288;&#21363;&#33258;&#30001;&#24230;&#65292;&#21448;&#31216;&#26377;&#25928;&#32500;&#25968;&#65289;&#26469;&#22238;&#39038;&#26089;&#26399;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#38543;&#26426;&#25237;&#24433;&#30340;&#26368;&#23567;&#33539;&#25968;&#26368;&#23567;&#20108;&#20056;&#25311;&#21512;&#30340;&#24191;&#20041;&#24615;&#33021;&#65288;&#20197;&#24179;&#26041;&#20559;&#24046;&#21644;&#26041;&#24046;&#34920;&#31034;&#65289;&#30340;&#28176;&#36817;&#31561;&#20215;&#29289;&#65292;&#20026;&#21452;&#19979;&#38477;&#29616;&#35937;&#25552;&#20379;&#31616;&#21333;&#30340;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and review earlier results using classical notions from non-parametric statistics, namely degrees of freedom, also known as effective dimensionality. We then compute asymptotic equivalents of the generalization performance (in terms of squared bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.00515</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#21487;&#35299;&#37322;&#27700;&#20301;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#22411;&#27700;&#20301;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#21644;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#36816;&#29992;&#20110;&#27721;&#27743;&#25968;&#25454;&#38598;&#30340;&#23454;&#38469;&#20998;&#26512;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27721;&#27743;&#27700;&#20301;&#23545;&#20110;&#20132;&#36890;&#25511;&#21046;&#21644;&#36991;&#20813;&#33258;&#28982;&#28798;&#23475;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#28041;&#21450;&#22810;&#31181;&#21464;&#37327;&#24182;&#30456;&#20114;&#22797;&#26434;&#22320;&#32852;&#31995;&#30528;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#36716;&#25442;&#22120;&#65292;&#21033;&#29992;&#21464;&#37327;&#20808;&#21069;&#30693;&#35782;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#65292;&#39044;&#27979;&#27721;&#27743;&#27982;&#24030;&#26725;&#30340;&#27700;&#20301;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#21040;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#26524;&#20851;&#31995;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#24418;&#24335;&#21270;&#20026;&#22810;&#23618;&#32593;&#32476;&#24182;&#20351;&#29992;&#33945;&#29256;&#26041;&#27861;&#12290;&#20973;&#20511;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#26681;&#25454;&#20808;&#21069;&#30340;&#30693;&#35782;&#33719;&#24471;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;2016&#24180;&#33267;2021&#24180;&#30340;&#27721;&#27743;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
&lt;/p&gt;</description></item><item><title>TimeMAE&#26159;&#19968;&#31181;&#26032;&#22411;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#21033;&#29992;transformer&#32593;&#32476;&#23558;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#25104;&#19968;&#31995;&#21015;&#19981;&#37325;&#21472;&#30340;&#23376;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25513;&#30721;&#31574;&#30053;&#35206;&#30422;&#26412;&#22320;&#21270;&#23376;&#24207;&#21015;&#30340;&#35821;&#20041;&#21333;&#20803;&#65292;&#20197;&#23398;&#20064;&#21040;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.00320</link><description>&lt;p&gt;
TimeMAE: &#22522;&#20110;&#35299;&#32806;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TimeMAE: Self-Supervised Representations of Time Series with Decoupled Masked Autoencoders. (arXiv:2303.00320v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00320
&lt;/p&gt;
&lt;p&gt;
TimeMAE&#26159;&#19968;&#31181;&#26032;&#22411;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#21033;&#29992;transformer&#32593;&#32476;&#23558;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#22788;&#29702;&#25104;&#19968;&#31995;&#21015;&#19981;&#37325;&#21472;&#30340;&#23376;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25513;&#30721;&#31574;&#30053;&#35206;&#30422;&#26412;&#22320;&#21270;&#23376;&#24207;&#21015;&#30340;&#35821;&#20041;&#21333;&#20803;&#65292;&#20197;&#23398;&#20064;&#21040;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#20165;&#22312;&#31232;&#30095;&#36880;&#28857;&#36755;&#20837;&#21333;&#20803;&#19978;&#36827;&#34892;&#21333;&#21521;&#32534;&#30721;&#65292;&#24403;&#21069;&#26041;&#27861;&#19981;&#33021;&#23398;&#20064;&#21040;&#26368;&#20248;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeMAE&#65292;&#19968;&#31181;&#22522;&#20110;transformer&#32593;&#32476;&#30340;&#23398;&#20064;&#21487;&#20256;&#36882;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;TimeMAE&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#23558;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#31383;&#21475;&#20999;&#29255;&#20998;&#21306;&#22788;&#29702;&#25104;&#19968;&#31995;&#21015;&#19981;&#37325;&#21472;&#30340;&#23376;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#38543;&#26426;&#25513;&#30721;&#31574;&#30053;&#35206;&#30422;&#26412;&#22320;&#21270;&#23376;&#24207;&#21015;&#30340;&#35821;&#20041;&#21333;&#20803;&#12290;&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#36798;&#21040;&#19968;&#20030;&#19977;&#24471;&#30340;&#30446;&#26631;&#65292;&#21363;&#65288;1&#65289;&#23398;&#20064;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65307;
&lt;/p&gt;
&lt;p&gt;
Enhancing the expressive capacity of deep learning-based time series models with self-supervised pre-training has become ever-increasingly prevalent in time series classification. Even though numerous efforts have been devoted to developing self-supervised models for time series data, we argue that the current methods are not sufficient to learn optimal time series representations due to solely unidirectional encoding over sparse point-wise input units. In this work, we propose TimeMAE, a novel self-supervised paradigm for learning transferrable time series representations based on transformer networks. The distinct characteristics of the TimeMAE lie in processing each time series into a sequence of non-overlapping sub-series via window-slicing partitioning, followed by random masking strategies over the semantic units of localized sub-series. Such a simple yet effective setting can help us achieve the goal of killing three birds with one stone, i.e., (1) learning enriched contextual r
&lt;/p&gt;</description></item><item><title>Dish-TS&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#23558;Lookback&#31383;&#21475;&#20316;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#20316;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23558;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#21644;&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#20004;&#31867;&#12290;</title><link>http://arxiv.org/abs/2302.14829</link><description>&lt;p&gt;
Dish-TS: &#19968;&#31181;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#36890;&#29992;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Dish-TS: A General Paradigm for Alleviating Distribution Shift in Time Series Forecasting. (arXiv:2302.14829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14829
&lt;/p&gt;
&lt;p&gt;
Dish-TS&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#23558;Lookback&#31383;&#21475;&#20316;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#20316;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23558;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#21644;&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#20004;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#25351;&#30340;&#26159;&#26102;&#38388;&#24207;&#21015;&#22312;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23427;&#24456;&#22823;&#31243;&#24230;&#19978;&#38459;&#30861;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#24067;&#20559;&#31227;&#30340;&#30740;&#31350;&#22823;&#22810;&#23616;&#38480;&#20110;&#20998;&#24067;&#37327;&#21270;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;Lookback&#21644;Horizon&#20043;&#38388;&#30340;&#28508;&#22312;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#24635;&#32467;&#20026;&#20004;&#31867;&#12290;&#23558;Lookback&#31383;&#21475;&#35270;&#20026;&#36755;&#20837;&#31354;&#38388;&#65292;Horizon&#31383;&#21475;&#35270;&#20026;&#36755;&#20986;&#31354;&#38388;&#65292;&#23384;&#22312;(i)&#20869;&#37096;&#31354;&#38388;&#20559;&#31227;&#65292;&#21363;&#22312;&#36755;&#20837;&#31354;&#38388;&#20869;&#30340;&#20998;&#24067;&#38543;&#26102;&#38388;&#20445;&#25345;&#20559;&#31227;&#65292;&#20197;&#21450;(ii)&#19981;&#21516;&#31354;&#38388;&#20559;&#31227;&#65292;&#22312;&#36755;&#20837;&#31354;&#38388;&#21644;&#36755;&#20986;&#31354;&#38388;&#20043;&#38388;&#20998;&#24067;&#20559;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Dish-TS&#30340;&#36890;&#29992;&#31070;&#32463;&#27169;&#22411;&#26469;&#32531;&#35299;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20272;&#35745;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31995;&#25968;&#32593;&#32476;&#65288;CONET&#65289;&#65292;&#23427;&#21487;&#20197;&#26159;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26144;&#23556;&#36755;&#20837;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution shift in Time Series Forecasting (TSF), indicating series distribution changes over time, largely hinders the performance of TSF models. Existing works towards distribution shift in time series are mostly limited in the quantification of distribution and, more importantly, overlook the potential shift between lookback and horizon windows. To address above challenges, we systematically summarize the distribution shift in TSF into two categories. Regarding lookback windows as input-space and horizon windows as output-space, there exist (i) intra-space shift, that the distribution within the input-space keeps shifted over time, and (ii) inter-space shift, that the distribution is shifted between input-space and output-space. Then we introduce, Dish-TS, a general neural paradigm for alleviating distribution shift in TSF. Specifically, for better distribution estimation, we propose the coefficient net (CONET), which can be any neural architectures, to map input sequences in
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2302.08893</link><description>&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on online active learning. (arXiv:2302.08893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08893
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12289;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#24335;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#26631;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#25968;&#25454;&#20165;&#20197;&#26410;&#26631;&#35760;&#24418;&#24335;&#21487;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#26368;&#23567;&#21270;&#19982;&#25910;&#38598;&#26631;&#35760;&#35266;&#27979;&#30456;&#20851;&#30340;&#25104;&#26412;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26631;&#27880;&#27599;&#20010;&#35266;&#27979;&#21487;&#20197;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#20351;&#24471;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#24050;&#32463;&#25552;&#20986;&#65292;&#26088;&#22312;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35266;&#27979;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#22320;&#20998;&#20026;&#20004;&#31867;&#65306;&#38745;&#24577;&#22522;&#20110;&#27744;&#30340;&#21644;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;&#28041;&#21450;&#20174;&#23553;&#38381;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;&#35266;&#27979;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#35843;&#26597;&#21644;&#25991;&#29486;&#32508;&#36848;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#27169;&#22411;&#36866;&#24212;&#26032;&#36827;&#25968;&#25454;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22522;&#20110;&#27969;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#26426;&#36935;&#12289;&#29992;&#20110;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#30340;&#31574;&#30053;&#20197;&#21450;&#35813;&#33539;&#20363;&#20013;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. Howev
&lt;/p&gt;</description></item><item><title>OpenHLS&#26159;&#19968;&#20010;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#30340;&#24320;&#28304;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#31185;&#23398;&#39046;&#22495;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#20013;&#20302;&#24310;&#36831;&#22788;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06751</link><description>&lt;p&gt;
OpenHLS&#65306;&#36866;&#29992;&#20110;&#23454;&#39564;&#31185;&#23398;&#30340;&#20302;&#24310;&#36831;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#32423;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
OpenHLS: High-Level Synthesis for Low-Latency Deep Neural Networks for Experimental Science. (arXiv:2302.06751v3 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06751
&lt;/p&gt;
&lt;p&gt;
OpenHLS&#26159;&#19968;&#20010;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#30340;&#24320;&#28304;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#23454;&#39564;&#31185;&#23398;&#39046;&#22495;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#20013;&#20302;&#24310;&#36831;&#22788;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#39564;&#39537;&#21160;&#30340;&#31185;&#23398;&#39046;&#22495;&#65292;&#20363;&#22914;&#39640;&#33021;&#29289;&#29702;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#65292;&#39640;&#25968;&#25454;&#29575;&#23454;&#39564;&#23545;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#26045;&#21152;&#30828;&#24615;&#32422;&#26463;&#65306;&#25910;&#38598;&#30340;&#25968;&#25454;&#24517;&#39035;&#26080;&#24046;&#21035;&#22320;&#23384;&#20648;&#20197;&#36827;&#34892;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#38656;&#35201;&#22823;&#23481;&#37327;&#23384;&#20648;&#65292;&#25110;&#32773;&#22312;&#23454;&#26102;&#20934;&#30830;&#36807;&#28388;&#26102;&#65292;&#20174;&#32780;&#38656;&#35201;&#20302;&#24310;&#36831;&#22788;&#29702;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#20854;&#20182;&#36807;&#28388;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#35774;&#35745;&#21644;&#37096;&#32626;&#22256;&#38590;&#65292;&#23578;&#26410;&#24191;&#27867;&#24212;&#29992;&#20110;&#27492;&#31867;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#32534;&#35793;&#22120;&#26694;&#26550;OpenHLS&#65292;&#22522;&#20110;&#39640;&#32423;&#32508;&#21512;&#25216;&#26415;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32423;&#34920;&#31034;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#31561;&#36817;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#20302;&#32423;&#34920;&#31034;&#65292;&#20854;&#20013;&#27809;&#26377;&#20219;&#20309;&#19987;&#26377;&#20381;&#36182;&#39033;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24037;&#20316;&#36127;&#36733;&#19978;&#35780;&#20272;OpenHLS&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26696;&#20363;&#30740;&#31350;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many experiment-driven scientific domains, such as high-energy physics, material science, and cosmology, high data rate experiments impose hard constraints on data acquisition systems: collected data must either be indiscriminately stored for post-processing and analysis, thereby necessitating large storage capacity, or accurately filtered in real-time, thereby necessitating low-latency processing. Deep neural networks, effective in other filtering tasks, have not been widely employed in such data acquisition systems, due to design and deployment difficulties. We present an open source, lightweight, compiler framework, without any proprietary dependencies, OpenHLS, based on high-level synthesis techniques, for translating high-level representations of deep neural networks to low-level representations, suitable for deployment to near-sensor devices such as field-programmable gate arrays. We evaluate OpenHLS on various workloads and present a case-study implementation of a deep neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#32447;&#24615;Ridge Bandits&#20013;&#29420;&#29305;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#21644;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;UCB&#21644;&#22522;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#37117;&#26159;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.06025</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;Ridge Bandits&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#21644;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits. (arXiv:2302.06025v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#32447;&#24615;Ridge Bandits&#20013;&#29420;&#29305;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#21644;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;UCB&#21644;&#22522;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#37117;&#26159;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#24179;&#22343;&#32467;&#26524;&#26159;&#25152;&#36873;&#25321;&#21160;&#20316;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#38750;&#32447;&#24615;&#27169;&#22411;&#26377;&#20004;&#31181;&#22855;&#29305;&#29616;&#35937;&#65306;&#39318;&#20808;&#65292;&#38500;&#20102;&#20855;&#26377;&#26631;&#20934;&#21442;&#25968;&#29575;&#30340;&#8220;&#23398;&#20064;&#38454;&#27573;&#8221;&#20197;&#36827;&#34892;&#20272;&#35745;&#25110;&#21518;&#24724;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#30001;&#38750;&#32447;&#24615;&#20989;&#25968;&#30830;&#23450;&#30340;&#22266;&#23450;&#25104;&#26412;&#30340;&#8220;&#28903;&#24405;&#26399;&#8221;; &#20854;&#27425;&#65292;&#23454;&#29616;&#26368;&#23567;&#28903;&#24405;&#25104;&#26412;&#38656;&#35201;&#26032;&#30340;&#25506;&#32034;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#31867;&#21517;&#20026;ridge&#20989;&#25968;&#30340;&#29305;&#27530;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#25512;&#23548;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#65292;&#27492;&#22806;&#36824;&#25512;&#23548;&#20102;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#19978;&#19979;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#20808;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#21021;&#22987;&#34892;&#21160;&#65292;&#28982;&#21518;&#23558;&#38382;&#39064;&#35270;&#20026;&#23616;&#37096;&#32447;&#24615;&#65292;&#36825;&#26159;&#32479;&#35745;&#19978;&#26368;&#20248;&#30340;&#12290;&#30456;&#21453;&#65292;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#65292;&#20363;&#22914;UCB&#21644;&#20381;&#36182;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#65292;&#20854;&#21487;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the "learning phase" with a standard parametric rate for estimation or regret, there is an "burn-in period" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.02314</link><description>&lt;p&gt;
CECT&#65306;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#29992;&#20110;COVID-19&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image Classification. (arXiv:2302.02314v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#21033;&#29992;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#21518;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#19968;&#26032;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;Transformer&#24320;&#21457;&#65292;&#21069;&#32773;&#65288;&#21518;&#32773;&#65289;&#21487;&#20197;&#25429;&#25417;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#24615;&#33021;&#21463;&#23616;&#37096;&#65288;&#20840;&#23616;&#65289;&#29305;&#24449;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#32593;&#32476;CECT&#65292;&#36890;&#36807;&#21487;&#25511;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#36827;&#34892;&#32452;&#21512;&#12290;CECT&#30001;&#21367;&#31215;&#32534;&#30721;&#22359;&#12289;&#36716;&#32622;&#21367;&#31215;&#35299;&#30721;&#22359;&#21644;Transformer&#20998;&#31867;&#22359;&#32452;&#25104;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#25110;Transformer&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;CECT&#21487;&#20197;&#21516;&#26102;&#25429;&#25417;&#22810;&#20010;&#23616;&#37096;&#21644;&#20840;&#23616;&#23610;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#38598;&#21512;&#31995;&#25968;&#21487;&#20197;&#25511;&#21046;&#19981;&#21516;&#23610;&#24230;&#23616;&#37096;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;COVID-19&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;CECT&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29305;&#24449;&#25429;&#25417;&#33021;&#21147;&#65292;&#25105;&#20204;&#30456;&#20449;CECT&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#32780;&#26377;&#25928;&#30340;&#24037;&#20855;&#25193;&#23637;&#21040;&#20854;&#20182;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most computer vision models are developed based on either convolutional neural network (CNN) or transformer, while the former (latter) method captures local (global) features. To relieve model performance limitations due to the lack of global (local) features, we develop a novel classification network CECT by controllable ensemble CNN and transformer. CECT is composed of a convolutional encoder block, a transposed-convolutional decoder block, and a transformer classification block. Different from conventional CNN- or transformer-based methods, our CECT can capture features at both multi-local and global scales. Besides, the contribution of local features at different scales can be controlled with the proposed ensemble coefficients. We evaluate CECT on two public COVID-19 datasets and it outperforms existing state-of-the-art methods on all evaluation metrics. With remarkable feature capture ability, we believe CECT can be extended to other medical image classification scenarios as a dia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2301.13060</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38646;&#19968;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Zero-One Laws of Graph Neural Networks. (arXiv:2301.13060v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#30740;&#31350;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#24403;&#22270;&#33410;&#28857;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;GNN&#30340;&#34892;&#20026;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35777;&#26126;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#65292;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#29992;&#20110;&#23545;&#22270;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26631;&#20934;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#30340;&#34920;&#31034;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#65292;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#33410;&#28857;&#30340;&#25968;&#37327;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;&#65292;GNNs&#30340;&#34892;&#20026;&#22914;&#20309;&#65311;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25105;&#20204;&#20174;Erd&#337;s-R&#233;nyi&#27169;&#22411;&#20013;&#32472;&#21046;&#19981;&#26029;&#22686;&#22823;&#30340;&#22270;&#26102;&#65292;&#36825;&#20123;&#22270;&#26144;&#23556;&#21040;GNN&#20998;&#31867;&#22120;&#30340;&#29305;&#23450;&#36755;&#20986;&#30340;&#27010;&#29575;&#36235;&#20110;&#38646;&#25110;&#19968;&#12290;&#36825;&#20010;&#31867;&#21253;&#25324;&#27969;&#34892;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#12290;&#36825;&#20010;&#32467;&#26524;&#24314;&#31435;&#20102;&#36825;&#20123;GNN&#30340;&#38646;&#19968;&#23450;&#24459;&#65292;&#24182;&#19988;&#31867;&#27604;&#20110;&#20854;&#20182;&#25910;&#25947;&#23450;&#24459;&#65292;&#24102;&#26469;&#20102;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#35266;&#23519;&#21040;&#29702;&#35770;&#19982;&#23454;&#36341;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are de facto standard deep learning architectures for machine learning on graphs. This has led to a large body of work analyzing the capabilities and limitations of these models, particularly pertaining to their representation and extrapolation capacity. We offer a novel theoretical perspective on the representation and extrapolation capacity of GNNs, by answering the question: how do GNNs behave as the number of graph nodes become very large? Under mild assumptions, we show that when we draw graphs of increasing size from the Erd\H{o}s-R\'enyi model, the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. This class includes the popular graph convolutional network architecture. The result establishes 'zero-one laws' for these GNNs, and analogously to other convergence laws, entails theoretical limitations on their capacity. We empirically verify our results, observing that the theoret
&lt;/p&gt;</description></item><item><title>SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;</title><link>http://arxiv.org/abs/2301.07074</link><description>&lt;p&gt;
SegViz&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07074
&lt;/p&gt;
&lt;p&gt;
SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22810;&#20010;&#19979;&#28216;&#20020;&#24202;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#25163;&#21160;&#27880;&#37322;&#26159;&#32791;&#26102;&#30340;&#12289;&#38656;&#35201;&#39640;&#25216;&#33021;&#30340;&#12289;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;3D&#22270;&#20687;&#12290;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#22810;&#20010;&#32452;&#30340;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#30693;&#35782;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegViz&#65292;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#23558;SegViz&#30340;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#38598;&#20013;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
&lt;/p&gt;</description></item><item><title>WuYun&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#26059;&#24459;&#20998;&#35299;&#25104;&#39592;&#26550;&#21644;&#21160;&#24577;&#35013;&#39280;&#38899;&#31526;&#20004;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#26059;&#24459;&#39592;&#26550;&#65292;&#25552;&#20379;&#36741;&#21161;&#25351;&#23548;&#26469;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#38271;&#26399;&#32467;&#26500;&#21644;&#38899;&#20048;&#24615;&#30340;&#26059;&#24459;&#12290;</title><link>http://arxiv.org/abs/2301.04488</link><description>&lt;p&gt;
WuYun: &#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#25506;&#32034;&#23618;&#27425;&#32467;&#26500;&#39592;&#26550;&#24341;&#23548;&#30340;&#26059;&#24459;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WuYun: Exploring hierarchical skeleton-guided melody generation using knowledge-enhanced deep learning. (arXiv:2301.04488v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04488
&lt;/p&gt;
&lt;p&gt;
WuYun&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#26059;&#24459;&#20998;&#35299;&#25104;&#39592;&#26550;&#21644;&#21160;&#24577;&#35013;&#39280;&#38899;&#31526;&#20004;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#26059;&#24459;&#39592;&#26550;&#65292;&#25552;&#20379;&#36741;&#21161;&#25351;&#23548;&#26469;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#38271;&#26399;&#32467;&#26500;&#21644;&#38899;&#20048;&#24615;&#30340;&#26059;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#38761;&#26032;&#20102;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#65292;&#28982;&#32780;&#30446;&#21069;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#26059;&#24459;&#29983;&#25104;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#31471;&#21040;&#31471;&#30340;&#19968;&#20010;&#38899;&#31526;&#25509;&#19968;&#20010;&#38899;&#31526;&#30340;&#29983;&#25104;&#27169;&#24335;&#65292;&#32780;&#19988;&#22312;&#23545;&#24453;&#27599;&#20010;&#38899;&#31526;&#26102;&#37117;&#19968;&#35270;&#21516;&#20161;&#12290;&#36825;&#37324;&#25105;&#20204;&#25552;&#20986; WuYun&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#25913;&#21892;&#29983;&#25104;&#26059;&#24459;&#30340;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#26368;&#20855;&#32467;&#26500;&#37325;&#35201;&#24615;&#30340;&#38899;&#31526;&#20197;&#26500;&#36896;&#26059;&#24459;&#39592;&#26550;&#65292;&#28982;&#21518;&#22312;&#39592;&#26550;&#30340;&#22522;&#30784;&#19978;&#22635;&#20805;&#21160;&#24577;&#35013;&#39280;&#38899;&#31526;&#65292;&#29983;&#25104;&#23436;&#25972;&#30340;&#26059;&#24459;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#26059;&#24459;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#37325;&#24314;&#26059;&#24459;&#39592;&#26550;&#65292;&#26059;&#24459;&#39592;&#26550;&#20316;&#20026;&#38468;&#21152;&#30693;&#35782;&#20026;&#26059;&#24459;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#36741;&#21161;&#25351;&#23548;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102; WuYun &#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#38271;&#26399;&#32467;&#26500;&#21644;&#38899;&#20048;&#24615;&#30340;&#26059;&#24459;&#65292;&#24182;&#22312;&#25152;&#26377;&#20027;&#35266;&#35780;&#20215;&#24230;&#37327;&#26631;&#20934;&#19978;&#24179;&#22343;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;0.51 &#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#19968;&#31181;&#22810;&#23398;&#31185;&#35270;&#35282;&#26469;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#26059;&#24459;&#23618;&#27425;&#32467;&#26500;&#65292;&#35813;&#24605;&#36335;&#21487;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#38899;&#20048;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning has revolutionized music generation, existing methods for structured melody generation follow an end-to-end left-to-right note-by-note generative paradigm and treat each note equally. Here, we present WuYun, a knowledge-enhanced deep learning architecture for improving the structure of generated melodies, which first generates the most structurally important notes to construct a melodic skeleton and subsequently infills it with dynamically decorative notes into a full-fledged melody. Specifically, we use music domain knowledge to extract melodic skeletons and employ sequence learning to reconstruct them, which serve as additional knowledge to provide auxiliary guidance for the melody generation process. We demonstrate that WuYun can generate melodies with better long-term structure and musicality and outperforms other state-of-the-art methods by 0.51 on average on all subjective evaluation metrics. Our study provides a multidisciplinary lens to design melodic hie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22686;&#21152;&#20102;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;PIBConv&#22359;&#26469;&#20943;&#23569;&#35745;&#31639;&#21344;&#29992;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2301.01286</link><description>&lt;p&gt;
DARTS&#25628;&#32034;&#31354;&#38388;&#30340;&#20266;&#21453;&#36716;&#29942;&#39048;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Inverted Bottleneck Convolution for DARTS Search Space. (arXiv:2301.01286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22686;&#21152;&#20102;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;PIBConv&#22359;&#26469;&#20943;&#23569;&#35745;&#31639;&#21344;&#29992;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#33258;&#24341;&#20837;DARTS&#20197;&#26469;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#35745;&#21407;&#21017;&#36880;&#27493;&#25193;&#23637;DARTS&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;ConvNeXt&#30340;&#24494;&#23567;&#35774;&#35745;&#21464;&#21270;&#26469;&#36880;&#27493;&#25193;&#20805;DARTS&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30740;&#31350;&#20934;&#30830;&#24615;&#12289;&#35780;&#20272;&#23618;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21453;&#36716;&#29942;&#39048;&#21367;&#31215;&#65288;PIBConv&#65289;&#22359;&#65292;&#26088;&#22312;&#20943;&#23569;ConvNeXt&#20013;&#25552;&#20986;&#30340;&#21453;&#36716;&#29942;&#39048;&#22359;&#30340;&#35745;&#31639;&#21344;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#23545;&#20110;&#35780;&#20272;&#23618;&#25968;&#30340;&#25935;&#24863;&#24230;&#35201;&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#23618;&#25968;&#20165;&#20026;2&#26102;&#26174;&#30528;&#20248;&#20110;&#19968;&#20010;&#20855;&#26377;&#31867;&#20284;&#35268;&#27169;&#30340;DARTS&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#23618;&#25968;&#65292;&#23427;&#19981;&#20165;&#22312;&#36739;&#20302;&#30340;GMACs&#21644;&#21442;&#25968;&#25968;&#37327;&#19979;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615; &#65292;&#35745;&#31639;&#21344;&#29992;&#20063;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Architecture Search (DARTS) has attracted considerable attention as a gradient-based neural architecture search method. Since the introduction of DARTS, there has been little work done on adapting the action space based on state-of-art architecture design principles for CNNs. In this work, we aim to address this gap by incrementally augmenting the DARTS search space with micro-design changes inspired by ConvNeXt and studying the trade-off between accuracy, evaluation layer count, and computational cost. We introduce the Pseudo-Inverted Bottleneck Conv (PIBConv) block intending to reduce the computational footprint of the inverted bottleneck block proposed in ConvNeXt. Our proposed architecture is much less sensitive to evaluation layer count and outperforms a DARTS network with similar size significantly, at layer counts as small as 2. Furthermore, with less layers, not only does it achieve higher accuracy with lower computational footprint (measured in GMACs) and parame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;TriNet&#37319;&#29992;&#19977;&#20998;&#25903;&#32467;&#26500;&#65292;&#21487;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#30340;&#23849;&#28291;&#65292;&#24182;&#22312;&#19979;&#28216;ASR&#20219;&#21153;&#20013;&#27604;SOTA&#26041;&#27861;Data2vec&#23454;&#29616;&#20102;6.06%&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;</title><link>http://arxiv.org/abs/2301.00656</link><description>&lt;p&gt;
TriNet&#65306;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#23436;&#20840;&#25110;&#32531;&#24930;&#23849;&#28291;&#30340;&#31283;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR. (arXiv:2301.00656v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;TriNet&#37319;&#29992;&#19977;&#20998;&#25903;&#32467;&#26500;&#65292;&#21487;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#30340;&#23849;&#28291;&#65292;&#24182;&#22312;&#19979;&#28216;ASR&#20219;&#21153;&#20013;&#27604;SOTA&#26041;&#27861;Data2vec&#23454;&#29616;&#20102;6.06%&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#31361;&#28982;&#30340;&#20449;&#24687;&#23849;&#28291;&#25110;&#32531;&#24930;&#30340;&#32500;&#24230;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TriNet&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20998;&#25903;&#32467;&#26500;&#26469;&#38450;&#27490;&#23849;&#28291;&#21644;&#31283;&#23450;&#39044;&#35757;&#32451;&#12290;TriNet&#23398;&#20064;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#22312;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#21040;&#19968;&#20010;&#26356;&#39640;&#32423;&#21035;&#30340;&#31354;&#38388;&#20013;&#20197;&#39044;&#27979;&#30001;&#20923;&#32467;&#30340;&#25945;&#24072;&#29983;&#25104;&#30340;&#34394;&#20551;&#30446;&#26631;&#21521;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#31283;&#23450;&#21644;&#21152;&#36895;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19979;&#28216;&#22522;&#20934;ASR&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Data2vec&#23454;&#29616;&#20102;6.06&#65285;&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;&#25105;&#20204;&#20250;&#22312;https://github.com/tencent-ailab/ &#19978;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) models confront challenges of abrupt informational collapse or slow dimensional collapse. We propose TriNet, which introduces a novel triple-branch architecture for preventing collapse and stabilizing the pre-training. TriNet learns the SSL latent embedding space and incorporates it to a higher level space for predicting pseudo target vectors generated by a frozen teacher. Our experimental results show that the proposed method notably stabilizes and accelerates pre-training and achieves a relative word error rate reduction (WERR) of 6.06% compared to the state-of-the-art (SOTA) Data2vec for a downstream benchmark ASR task. We will release our code at https://github.com/tencent-ailab/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.00503</link><description>&lt;p&gt;
&#25903;&#20184;&#23453;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#24050;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#37096;&#32626;&#65292;&#20026;&#36229;&#36807;1&#20159;&#27963;&#36291;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;AlipayKG&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#25551;&#36848;&#29992;&#25143;&#24847;&#22270;&#30340;&#31163;&#32447;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#12289;&#20016;&#23500;&#30340;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#19987;&#23478;&#35268;&#21017;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#25512;&#26029;&#22312;&#32447;&#29992;&#25143;&#30340;&#19979;&#19968;&#27493;&#24847;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;(SGG)&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#38271;&#23614;&#20998;&#24067;&#23545;&#26080;&#20559;&#35859;&#35789;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#37319;&#29992;&#20559;&#26012;&#31867;&#24179;&#34913;&#37325;&#21152;&#26435;(SCR)&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#26435;&#34913;&#22823;&#22810;&#25968;&#35859;&#35789;&#21644;&#23569;&#25968;&#35859;&#35789;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SCR&#26041;&#27861;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.00351</link><description>&lt;p&gt;
&#32771;&#34385;&#38271;&#23614;&#20998;&#24067;&#22240;&#32032;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Skew Class-balanced Re-weighting for Unbiased Scene Graph Generation. (arXiv:2301.00351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;(SGG)&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#32771;&#34385;&#20102;&#38271;&#23614;&#20998;&#24067;&#23545;&#26080;&#20559;&#35859;&#35789;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#37319;&#29992;&#20559;&#26012;&#31867;&#24179;&#34913;&#37325;&#21152;&#26435;(SCR)&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#26435;&#34913;&#22823;&#22810;&#25968;&#35859;&#35789;&#21644;&#23569;&#25968;&#35859;&#35789;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SCR&#26041;&#27861;&#36229;&#36234;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#22330;&#26223;&#22270;&#29983;&#25104;(SGG)&#31639;&#27861;&#65292;&#31216;&#20026;&#20559;&#26012;&#31867;&#24179;&#34913;&#37325;&#21152;&#26435;(SCR)&#65292;&#26088;&#22312;&#32771;&#34385;&#38271;&#23614;&#20998;&#24067;&#25152;&#23548;&#33268;&#30340;&#26080;&#20559;&#35859;&#35789;&#39044;&#27979;&#12290;&#20197;&#24448;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32531;&#35299;&#36739;&#23567;&#30340;&#35859;&#35789;&#39044;&#27979;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#26174;&#31034;&#20102;&#24613;&#21095;&#19979;&#38477;&#30340;&#21484;&#22238;&#24471;&#20998;&#65292;&#21363;&#22833;&#21435;&#20102;&#22823;&#22810;&#25968;&#35859;&#35789;&#30340;&#24615;&#33021;&#12290;&#36825;&#36824;&#27809;&#26377;&#27491;&#30830;&#20998;&#26512;&#26377;&#38480;SGG&#25968;&#25454;&#38598;&#20013;&#22810;&#25968;&#35859;&#35789;&#24615;&#33021;&#21644;&#23569;&#25968;&#35859;&#35789;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#20559;&#26012;&#31867;&#24179;&#34913;&#37325;&#21152;&#26435;(SCR)&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24212;&#29992;&#20110;&#26080;&#20559;SGG&#27169;&#22411;&#12290;&#21033;&#29992;&#20559;&#26012;&#35859;&#35789;&#39044;&#27979;&#30340;&#26012;&#29575;&#65292;SCR&#20272;&#35745;&#30446;&#26631;&#35859;&#35789;&#26435;&#37325;&#31995;&#25968;&#65292;&#28982;&#21518;&#37325;&#26032;&#20026;&#20559;&#26012;&#35859;&#35789;&#21152;&#26435;&#65292;&#20197;&#26356;&#22909;&#22320;&#26435;&#34913;&#22823;&#22810;&#25968;&#35859;&#35789;&#21644;&#23569;&#25968;&#35859;&#35789;&#12290;&#22312;&#26631;&#20934;Visual Genome&#25968;&#25454;&#38598;&#21644;Oxford Visual Geometry Group(VGG)&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;SCR&#26041;&#27861;&#22312;&#21484;&#22238;&#21644;mAP&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#20026;SGG&#20219;&#21153;&#25552;&#20379;&#20102;&#26356;&#31283;&#20581;&#21644;&#26080;&#20559;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
An unbiased scene graph generation (SGG) algorithm referred to as Skew Class-balanced Re-weighting (SCR) is proposed for considering the unbiased predicate prediction caused by the long-tailed distribution. The prior works focus mainly on alleviating the deteriorating performances of the minority predicate predictions, showing drastic dropping recall scores, i.e., losing the majority predicate performances. It has not yet correctly analyzed the trade-off between majority and minority predicate performances in the limited SGG datasets. In this paper, to alleviate the issue, the Skew Class-balanced Re-weighting (SCR) loss function is considered for the unbiased SGG models. Leveraged by the skewness of biased predicate predictions, the SCR estimates the target predicate weight coefficient and then re-weights more to the biased predicates for better trading-off between the majority predicates and the minority ones. Extensive experiments conducted on the standard Visual Genome dataset and O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; ShapeNet &#21512;&#25104;&#25968;&#25454;&#38598; SPARF&#65292;&#21253;&#25324;&#36229;&#36807; 100 &#19975;&#20010;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#30340;&#36752;&#23556;&#22330;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447; SuRFNet&#65292;&#36890;&#36807;&#23398;&#20064;&#23569;&#37327;&#35270;&#22270;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;</title><link>http://arxiv.org/abs/2212.09100</link><description>&lt;p&gt;
SPARF&#65306;&#20174;&#23569;&#37327;&#36755;&#20837;&#22270;&#20687;&#20013;&#23398;&#20064;&#22823;&#35268;&#27169;&#30340; 3D &#31232;&#30095;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input Images. (arXiv:2212.09100v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; ShapeNet &#21512;&#25104;&#25968;&#25454;&#38598; SPARF&#65292;&#21253;&#25324;&#36229;&#36807; 100 &#19975;&#20010;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#30340;&#36752;&#23556;&#22330;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447; SuRFNet&#65292;&#36890;&#36807;&#23398;&#20064;&#23569;&#37327;&#35270;&#22270;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRFs) &#30340;&#36827;&#23637;&#23558;&#26032;&#35270;&#35282;&#21512;&#25104;&#38382;&#39064;&#30475;&#20316;&#26159;&#31232;&#30095;&#36752;&#23556;&#22330; (SRF) &#20248;&#21270;&#65292;&#20351;&#29992;&#31232;&#30095;&#20307;&#32032;&#36827;&#34892;&#39640;&#25928;&#24555;&#36895;&#28210;&#26579; (plenoxels, InstantNGP)&#12290;&#20026;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#37319;&#29992; SRF &#20316;&#20026; 3D &#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SPARF&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110; ShapeNet &#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#65292;&#30001; $\sim$ 17 &#30334;&#19975;&#24352;&#22270;&#20687;&#32452;&#25104;&#65292;&#20174;&#36817; 40,000 &#20010;&#39640;&#20998;&#36776;&#29575;&#24418;&#29366;&#28210;&#26579;&#32780;&#26469; (400 X 400 &#20687;&#32032;)&#12290;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#21253;&#25324;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20855;&#26377;&#22810;&#20010;&#20307;&#32032;&#20998;&#36776;&#29575;&#30340; 3D &#20248;&#21270;&#36807;&#30340;&#36752;&#23556;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31649;&#32447;&#65288;SuRFNet&#65289;&#65292;&#23427;&#20174;&#23569;&#37327;&#35270;&#22270;&#20013;&#23398;&#20064;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#25910;&#38598;&#30340; SPARF &#25968;&#25454;&#38598;&#21644; 3D &#31232;&#30095;&#21367;&#31215;&#26469;&#23454;&#29616;&#30340;&#12290;SuRFNet &#20351;&#29992;&#23569;&#37327;/&#21333;&#20010;&#22270;&#20687;&#30340;&#37096;&#20998; SRF &#21644;&#29305;&#23450;&#30340; SRF &#25439;&#22833;&#26469;&#23398;&#20064;&#29983;&#25104;&#31232;&#30095;&#20307;&#32032;&#36752;&#23556;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel view synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels for efficient and fast rendering (plenoxels,InstantNGP). In order to leverage machine learning and adoption of SRFs as a 3D representation, we present SPARF, a large-scale ShapeNet-based synthetic dataset for novel view synthesis consisting of $\sim$ 17 million images rendered from nearly 40,000 shapes at high resolution (400 X 400 pixels). The dataset is orders of magnitude larger than existing synthetic datasets for novel view synthesis and includes more than one million 3D-optimized radiance fields with multiple voxel resolutions. Furthermore, we propose a novel pipeline (SuRFNet) that learns to generate sparse voxel radiance fields from only few views. This is done by using the densely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs partial SRFs from few/one images and a specialized SRF loss to learn to gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;7&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24037;&#31243;&#21147;&#23398;&#39046;&#22495;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#27169;&#22411;&#26657;&#20934;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.00881</link><description>&lt;p&gt;
&#25506;&#31350;&#26426;&#26800;&#23398;&#31185;&#20013;&#20998;&#31867;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Investigating Deep Learning Model Calibration for Classification Problems in Mechanics. (arXiv:2212.00881v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;7&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24037;&#31243;&#21147;&#23398;&#39046;&#22495;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#27169;&#22411;&#26657;&#20934;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#31243;&#21147;&#23398;&#38382;&#39064;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#20204;&#23545;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#26448;&#26009;&#21644;&#32467;&#26500;&#30340;&#21147;&#23398;&#34892;&#20026;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#20174;&#24037;&#31243;&#22797;&#21512;&#26448;&#26009;&#21040;&#20960;&#20309;&#22797;&#26434;&#30340;&#36229;&#26448;&#26009;&#20877;&#21040;&#24322;&#36136;&#29983;&#29289;&#32452;&#32455;&#31561;&#21508;&#31181;&#31995;&#32479;&#30340;&#26426;&#26800;&#34892;&#20026;&#65292;&#24182;&#19988;&#35823;&#24046;&#36739;&#20302;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26657;&#20934;&#65292;&#21363;&#39044;&#27979;&#32467;&#23616;&#30340;&#27010;&#29575;&#21644;&#30495;&#23454;&#32467;&#23616;&#27010;&#29575;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#26426;&#26800;&#38382;&#39064;&#30340;&#19971;&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#27169;&#22411;&#26657;&#20934;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in applying machine learning methods to problems in engineering mechanics. In particular, there has been significant interest in applying deep learning techniques to predicting the mechanical behavior of heterogeneous materials and structures. Researchers have shown that deep learning methods are able to effectively predict mechanical behavior with low error for systems ranging from engineered composites, to geometrically complex metamaterials, to heterogeneous biological tissue. However, there has been comparatively little attention paid to deep learning model calibration, i.e., the match between predicted probabilities of outcomes and the true probabilities of outcomes. In this work, we perform a comprehensive investigation into ML model calibration across seven open access engineering mechanics datasets that cover three distinct types of mechanical problems. Specifically, we evaluate both model and model calibration error for multiple mach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24037;&#31243;&#22270;&#32440;&#32452;&#20214;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;2D&#24037;&#31243;&#38646;&#20214;&#22270;&#32440;&#30340;&#30690;&#37327;&#21270;&#21644;&#26426;&#22120;&#35299;&#35835;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32452;&#20214;&#20998;&#21106;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.00290</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24037;&#31243;&#22270;&#32440;&#32452;&#20214;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Component Segmentation of Engineering Drawings Using Graph Convolutional Networks. (arXiv:2212.00290v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24037;&#31243;&#22270;&#32440;&#32452;&#20214;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;2D&#24037;&#31243;&#38646;&#20214;&#22270;&#32440;&#30340;&#30690;&#37327;&#21270;&#21644;&#26426;&#22120;&#35299;&#35835;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32452;&#20214;&#20998;&#21106;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#33258;&#21160;&#23436;&#25104;2D&#24037;&#31243;&#38646;&#20214;&#22270;&#32440;&#30340;&#30690;&#37327;&#21270;&#21644;&#26426;&#22120;&#35299;&#35835;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#24037;&#31243;&#22270;&#32440;&#20013;&#27599;&#20010;&#30690;&#37327;&#31508;&#30011;&#30340;&#35821;&#20041;&#31867;&#22411;&#65292;&#20197;&#35782;&#21035;&#32452;&#20214;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32452;&#20214;&#20998;&#21106;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven framework to automate the vectorization and machine interpretation of 2D engineering part drawings. In industrial settings, most manufacturing engineers still rely on manual reads to identify the topological and manufacturing requirements from drawings submitted by designers. The interpretation process is laborious and time-consuming, which severely inhibits the efficiency of part quotation and manufacturing tasks. While recent advances in image-based computer vision methods have demonstrated great potential in interpreting natural images through semantic segmentation approaches, the application of such methods in parsing engineering technical drawings into semantically accurate components remains a significant challenge. The severe pixel sparsity in engineering drawings also restricts the effective featurization of image-based data-driven methods. To overcome these challenges, we propose a deep learning based framework that predicts the semantic type of each v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#27867;&#21270;&#30340;&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#27963;&#21160;&#38750;&#24179;&#31283;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05634</link><description>&lt;p&gt;
&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of generative model for neuronal ensemble inference method. (arXiv:2211.05634v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05634
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#27867;&#21270;&#30340;&#31070;&#32463;&#20803;&#38598;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#27963;&#21160;&#38750;&#24179;&#31283;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#31070;&#32463;&#20803;&#30340;&#30456;&#20114;&#20316;&#29992;&#26500;&#25104;&#20102;&#32500;&#25345;&#29983;&#21629;&#27963;&#21160;&#25152;&#24517;&#38656;&#30340;&#21508;&#31181;&#33041;&#21151;&#33021;&#65292;&#22240;&#27492;&#65292;&#20998;&#26512;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#38416;&#26126;&#22823;&#33041;&#21151;&#33021;&#30340;&#26426;&#21046;&#65292;&#21253;&#25324;&#31070;&#32463;&#31185;&#23398;&#21508;&#20010;&#39046;&#22495;&#22312;&#20869;&#30340;&#35768;&#22810;&#30740;&#31350;&#27491;&#22312;&#31215;&#26497;&#24320;&#23637;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#21644;&#20013;&#24515;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#21644;&#20013;&#24515;&#30340;&#23384;&#22312;&#26377;&#21161;&#20110;&#25552;&#39640;&#20449;&#24687;&#22788;&#29702;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#31070;&#32463;&#20803;&#27963;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#21151;&#33021;&#31070;&#32463;&#20803;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#24314;&#31435;&#27963;&#21160;&#27169;&#22411;&#23384;&#22312;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#27963;&#21160;&#29305;&#24449;&#21462;&#20915;&#20110;&#29983;&#29702;&#23454;&#39564;&#26465;&#20214;&#65292;&#22240;&#27492;&#20854;&#20855;&#26377;&#38750;&#24179;&#31283;&#24615;&#12290;&#32467;&#26524;&#65292;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#27169;&#22411;&#20013;&#20551;&#35774;&#30340;&#24179;&#31283;&#24615;&#20250;&#22952;&#30861;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various brain functions that are necessary to maintain life activities materialize through the interaction of countless neurons. Therefore, it is important to analyze the structure of functional neuronal network. To elucidate the mechanism of brain function, many studies are being actively conducted on the structure of functional neuronal ensemble and hub, including all areas of neuroscience. In addition, recent study suggests that the existence of functional neuronal ensembles and hubs contributes to the efficiency of information processing. For these reasons, there is a demand for methods to infer functional neuronal ensembles from neuronal activity data, and methods based on Bayesian inference have been proposed. However, there is a problem in modeling the activity in Bayesian inference. The features of each neuron's activity have non-stationarity depending on physiological experimental conditions. As a result, the assumption of stationarity in Bayesian inference model impedes infer
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#24050;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#27468;&#26354;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#23454;&#29616;&#20102;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.02247</link><description>&lt;p&gt;
&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65306;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35299;&#24320;&#38899;&#39057;&#25928;&#26524;&#30340;&#32039;&#23494;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Music Mixing Style Transfer: A Contrastive Learning Approach to Disentangle Audio Effects. (arXiv:2211.02247v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#24050;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#27468;&#26354;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#31995;&#32479;&#23454;&#29616;&#20102;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#38899;&#20048;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#31995;&#32479;&#65292;&#23558;&#36755;&#20837;&#22810;&#36712;&#28151;&#38899;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#20026;&#21442;&#32771;&#27468;&#26354;&#30340;&#39118;&#26684;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#32463;&#36807;&#23545;&#27604;&#30446;&#26631;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#30340;&#65292;&#35813;&#32534;&#30721;&#22120;&#20174;&#21442;&#32771;&#38899;&#20048;&#24405;&#38899;&#20013;&#25552;&#21462;&#20165;&#19982;&#38899;&#39057;&#25928;&#26524;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#27169;&#22411;&#37117;&#26159;&#33258;&#30417;&#30563;&#26041;&#24335;&#35757;&#32451;&#30340;&#65292;&#20351;&#29992;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#20174;&#24050;&#22788;&#29702;&#30340;&#28287;&#24230;&#22810;&#36712;&#25968;&#25454;&#38598;&#20013;&#32531;&#35299;&#20102;&#33719;&#21462;&#26410;&#22788;&#29702;&#24178;&#29157;&#25968;&#25454;&#30340;&#25968;&#25454;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20854;&#20998;&#31163;&#38899;&#39057;&#25928;&#26524;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#39564;&#35777;&#20854;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;&#20174;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#19981;&#20165;&#21487;&#20197;&#23558;&#22810;&#36712;&#38899;&#39057;&#30340;&#28151;&#38899;&#39118;&#26684;&#36716;&#25442;&#20026;&#21442;&#32771;&#39118;&#26684;&#65292;&#32780;&#19988;&#22312;&#20351;&#29992;&#38899;&#20048;&#28304;&#20998;&#31163;&#27169;&#22411;&#26102;&#20063;&#20855;&#26377;&#28151;&#21512;&#39118;&#26684;&#36716;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end music mixing style transfer system that converts the mixing style of an input multitrack to that of a reference song. This is achieved with an encoder pre-trained with a contrastive objective to extract only audio effects related information from a reference music recording. All our models are trained in a self-supervised manner from an already-processed wet multitrack dataset with an effective data preprocessing method that alleviates the data scarcity of obtaining unprocessed dry data. We analyze the proposed encoder for the disentanglement capability of audio effects and also validate its performance for mixing style transfer through both objective and subjective evaluations. From the results, we show the proposed system not only converts the mixing style of multitrack audio close to a reference but is also robust with mixture-wise style transfer upon using a music source separation model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#20197;&#25429;&#33719;&#23454;&#20307;&#38388;&#30340;&#20869;&#22312;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2211.01717</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#23398;&#20064;&#20449;&#21495;&#30340;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Hypergraphs From Signals With Dual Smoothness Prior. (arXiv:2211.01717v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#20197;&#25429;&#33719;&#23454;&#20307;&#38388;&#30340;&#20869;&#22312;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26159;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#20869;&#22312;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#24403;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21487;&#29992;&#30340;&#36229;&#22270;&#25299;&#25169;&#32467;&#26500;&#26102;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;HGSL&#65292;&#36890;&#36807;&#25226;&#27599;&#20010;&#36229;&#36793;&#19982;&#20855;&#26377;&#33410;&#28857;&#20449;&#21495;&#24179;&#28369;&#24615;&#21644;&#36793;&#36830;&#25509;&#24615;&#30340;&#23376;&#22270;&#23545;&#24212;&#36215;&#26469;&#65292;&#25581;&#31034;&#20102;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#20449;&#21495;&#21644;&#36229;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph structure learning, which aims to learn the hypergraph structures from the observed signals to capture the intrinsic high-order relationships among the entities, becomes crucial when a hypergraph topology is not readily available in the datasets. There are two challenges that lie at the heart of this problem: 1) how to handle the huge search space of potential hyperedges, and 2) how to define meaningful criteria to measure the relationship between the signals observed on nodes and the hypergraph structure. In this paper, for the first challenge, we adopt the assumption that the ideal hypergraph structure can be derived from a learnable graph structure that captures the pairwise relations within signals. Further, we propose a hypergraph structure learning framework HGSL with a novel dual smoothness prior that reveals a mapping between the observed node signals and the hypergraph structure, whereby each hyperedge corresponds to a subgraph with both node signal smoothness and e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#21435;&#22122;&#27169;&#22411;&#38598;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#25991;&#26412;-&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#26159;&#19968;&#20010;&#28176;&#36827;&#30340;&#36807;&#31243;&#65292;&#32780;&#22312;&#29983;&#25104;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#23427;&#30340;&#21512;&#25104;&#34892;&#20026;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#19981;&#21516;&#38454;&#27573;&#30340;&#19987;&#38376;&#27169;&#22411;&#30340;&#26500;&#24819;&#12290;</title><link>http://arxiv.org/abs/2211.01324</link><description>&lt;p&gt;
eDiff-I: &#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#21435;&#22122;&#27169;&#22411;&#38598;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. (arXiv:2211.01324v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#21435;&#22122;&#27169;&#22411;&#38598;&#21512;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#25991;&#26412;-&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#30340;&#29983;&#25104;&#26159;&#19968;&#20010;&#28176;&#36827;&#30340;&#36807;&#31243;&#65292;&#32780;&#22312;&#29983;&#25104;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#23427;&#30340;&#21512;&#25104;&#34892;&#20026;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#19981;&#21516;&#38454;&#27573;&#30340;&#19987;&#38376;&#27169;&#22411;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#20123;&#25991;&#26412;-&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20174;&#38543;&#26426;&#22122;&#22768;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#36880;&#27493;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#21512;&#25104;&#34892;&#20026;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20250;&#23450;&#24615;&#22320;&#25913;&#21464;&#65306;&#22312;&#37319;&#26679;&#26089;&#26399;&#65292;&#29983;&#25104;&#24378;&#28872;&#20381;&#36182;&#20110;&#25991;&#26412;&#25552;&#31034;&#20197;&#29983;&#25104;&#38024;&#23545;&#25991;&#26412;&#30340;&#20869;&#23481;&#65292;&#32780;&#22312;&#21518;&#26399;&#65292;&#25991;&#26412;&#26465;&#20214;&#20960;&#20046;&#34987;&#24573;&#30053;&#20102;&#12290;&#36825;&#34920;&#26126;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21487;&#33021;&#24182;&#19981;&#29702;&#24819;&#12290;&#22240;&#27492;&#65292;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#35757;&#32451;&#19968;&#32452;&#38024;&#23545;&#19981;&#21516;&#21512;&#25104;&#38454;&#27573;&#19987;&#38376;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#38598;&#21512;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#26368;&#21021;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#20998;&#25104;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#20026;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#30340;&#29305;&#23450;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#25193;&#25955;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.14685</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#30340;&#28436;&#31034;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Demonstrations with Latent Space Priors. (arXiv:2210.14685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#26469;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#20013;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#29366;&#24577;&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20855;&#26377;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#25216;&#33021;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#26469;&#21033;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#20174;&#19968;&#20010;&#23398;&#20064;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;&#24320;&#22987;&#65292;&#25105;&#20204;&#20998;&#21035;&#35757;&#32451;&#28436;&#31034;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#20302;&#23618;&#31574;&#30053;&#12290;&#24207;&#21015;&#27169;&#22411;&#24418;&#25104;&#20102;&#28508;&#22312;&#31354;&#38388;&#23545;&#21512;&#29702;&#30340;&#28436;&#31034;&#34892;&#20026;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#21152;&#36895;&#39640;&#23618;&#27425;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#20165;&#29366;&#24577;&#30340;&#36816;&#21160;&#25429;&#25417;&#28436;&#31034;&#20013;&#33719;&#21462;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25506;&#32034;&#20102;&#20960;&#31181;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#36716;&#31227;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#28508;&#22312;&#31354;&#38388;&#20808;&#39564;&#30693;&#35782;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#26041;&#38754;&#37117;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#20855;&#26377;&#22797;&#26434;&#12289;&#27169;&#25311;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#25361;&#25112;&#24615;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations provide insight into relevant state or action space regions, bearing great potential to boost the efficiency and practicality of reinforcement learning agents. In this work, we propose to leverage demonstration datasets by combining skill learning and sequence modeling. Starting with a learned joint latent space, we separately train a generative model of demonstration sequences and an accompanying low-level policy. The sequence model forms a latent space prior over plausible demonstration behaviors to accelerate learning of high-level policies. We show how to acquire such priors from state-only motion capture demonstrations and explore several methods for integrating them into policy learning on transfer tasks. Our experimental results confirm that latent space priors provide significant gains in learning speed and final performance. We benchmark our approach on a set of challenging sparse-reward environments with a complex, simulated humanoid, and on offline RL benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#22810;&#39033;&#24335;Zonotopes&#23454;&#29616;&#34892;&#21160;&#25237;&#24433;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#36755;&#20837;&#38480;&#21046;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;</title><link>http://arxiv.org/abs/2210.10691</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#22810;&#39033;&#24335;Zonotopes&#23454;&#29616;&#34892;&#21160;&#25237;&#24433;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Reinforcement Learning via Action Projection using Reachability Analysis and Polynomial Zonotopes. (arXiv:2210.10691v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#21644;&#22810;&#39033;&#24335;Zonotopes&#23454;&#29616;&#34892;&#21160;&#25237;&#24433;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#36755;&#20837;&#38480;&#21046;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20135;&#29983;&#20102;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#23433;&#20840;&#20445;&#38556;&#65292;&#36825;&#38459;&#27490;&#20102;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#32447;&#24615;&#36830;&#32493;&#31995;&#32479;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#21040;&#36798;-&#36991;&#20813;&#20219;&#21153;&#30340;&#23433;&#20840;&#23631;&#38556;&#65292;&#24182;&#36890;&#36807;&#23558;&#25552;&#20986;&#30340;&#21160;&#20316;&#25237;&#24433;&#21040;&#26368;&#25509;&#36817;&#30340;&#23433;&#20840;&#21160;&#20316;&#26469;&#38450;&#27490;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24212;&#29992;&#21487;&#33021;&#19981;&#23433;&#20840;&#30340;&#21160;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#34892;&#21160;&#25237;&#24433;&#65292;&#26159;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#23454;&#29616;&#30340;&#12290;&#34892;&#21160;&#25237;&#24433;&#30340;&#23433;&#20840;&#32422;&#26463;&#26159;&#36890;&#36807;&#24212;&#29992;&#21442;&#25968;&#21270;&#21487;&#36798;&#24615;&#20998;&#26512;&#20351;&#29992;&#22810;&#39033;&#24335;Zonotopes&#24471;&#21040;&#30340;&#65292;&#36825;&#20351;&#24471;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21160;&#20316;&#23545;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#24433;&#21709;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#34892;&#21160;&#25237;&#24433;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23433;&#20840;&#23631;&#38556;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#36755;&#20837;&#38480;&#21046;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#31354;&#38388;&#32500;&#24230;&#30340;&#25972;&#21512;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning produces very promising results for many applications, its main disadvantage is the lack of safety guarantees, which prevents its use in safety-critical systems. In this work, we address this issue by a safety shield for nonlinear continuous systems that solve reach-avoid tasks. Our safety shield prevents applying potentially unsafe actions from a reinforcement learning agent by projecting the proposed action to the closest safe action. This approach is called action projection and is implemented via mixed-integer optimization. The safety constraints for action projection are obtained by applying parameterized reachability analysis using polynomial zonotopes, which enables to accurately capture the nonlinear effects of the actions on the system. In contrast to other state-of-the-art approaches for action projection, our safety shield can efficiently handle input constraints and dynamic obstacles, eases incorporation of the spatial robot dimensions into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#28165;&#27905;&#21518;&#38376;&#25915;&#20987;&#25152;&#20256;&#26579;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#34920;&#31034;&#26469;&#35782;&#21035;&#34987;&#27602;&#21270;&#30340;&#21644;&#24178;&#20928;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#30693;&#36947;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10272</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#35757;&#32451;&#38598;&#28165;&#27905;&#28040;&#38500;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Training set cleansing of backdoor poisoning by self-supervised representation learning. (arXiv:2210.10272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#28165;&#27905;&#21518;&#38376;&#25915;&#20987;&#25152;&#20256;&#26579;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#34920;&#31034;&#26469;&#35782;&#21035;&#34987;&#27602;&#21270;&#30340;&#21644;&#24178;&#20928;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#30693;&#36947;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25110;&#29305;&#27931;&#20234;&#25915;&#20987;&#26159;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#19968;&#31181;&#37325;&#35201;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#31867;&#22411;&#65292;&#22312;&#35813;&#25915;&#20987;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#27602;&#21270;&#20102;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#37117;&#20855;&#26377;&#21518;&#38376;&#27169;&#24335;&#65288;&#36890;&#24120;&#26159;&#19981;&#21487;&#23519;&#35273;&#30340;&#25110;&#26080;&#23475;&#30340;&#27169;&#24335;&#65289;&#65292;&#24182;&#34987;&#26631;&#35760;&#20026;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#31867;&#21035;&#12290;&#24403;&#22312;&#21518;&#38376;&#27602;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;DNN&#22312;&#22823;&#22810;&#25968;&#33391;&#24615;&#27979;&#35797;&#26679;&#26412;&#19978;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#27979;&#35797;&#26679;&#26412;&#20013;&#21253;&#21547;&#21253;&#21547;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#27169;&#24335;&#26102;&#65288;&#21363;&#21547;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#65289;&#65292;&#23427;&#20250;&#21521;&#30446;&#26631;&#31867;&#21035;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#30417;&#30563;&#24335;&#35757;&#32451;&#21487;&#33021;&#20250;&#26500;&#24314;&#26356;&#24378;&#30340;&#21518;&#38376;&#27169;&#24335;&#19982;&#20851;&#32852;&#30340;&#30446;&#26631;&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#32780;&#19981;&#26159;&#27491;&#24120;&#29305;&#24449;&#19982;&#30495;&#23454;&#36215;&#28304;&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24573;&#30053;&#26679;&#26412;&#26631;&#31614;&#24182;&#22522;&#20110;&#22270;&#20687;&#30340;&#35821;&#20041;&#20869;&#23481;&#23398;&#20064;&#29305;&#24449;&#23884;&#20837;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26469;&#28165;&#38500;&#21518;&#38376;&#27602;&#21270;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#26469;&#21306;&#20998;&#27602;&#21270;&#21644;&#24178;&#20928;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#30693;&#35782;&#65292;&#22312;&#25915;&#20987;&#32773;&#23558;&#35302;&#21457;&#22120;&#36866;&#24212;&#20110;&#19982;&#27491;&#24120;&#22270;&#20687;&#20855;&#26377;&#39640;&#24863;&#30693;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#21518;&#38376;&#20934;&#30830;&#24615;&#21644;&#24178;&#20928;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here we focus on image classification tasks and show that supervised training may build stronger association between the backdoor pattern and the associated target class than that between normal features and the true class of origin. By contrast, self-supervised representation learning ignores the labels of samples and learns a feature embedding based on images' semantic content. %We thus propose to us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#21644;&#24102;&#25513;&#33180;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#20110;&#35757;&#32451;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24102;&#26377;&#25513;&#33180;&#30340;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04150</link><description>&lt;p&gt;
Mask-adapted CLIP&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#21644;&#24102;&#25513;&#33180;&#30340;&#20307;&#31995;&#32467;&#26500;&#30340;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#20110;&#35757;&#32451;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#24102;&#26377;&#25513;&#33180;&#30340;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#30446;&#30340;&#26159;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;&#35821;&#20041;&#21306;&#22495;&#65292;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#34987;&#35266;&#23519;&#21040;&#12290;&#26368;&#36817;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#19981;&#32771;&#34385;&#31867;&#30340;&#25513;&#30721;&#25552;&#35758;&#65292;&#28982;&#21518;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#23545;&#25513;&#30721;&#21306;&#22495;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#19968;&#33539;&#20363;&#30340;&#24615;&#33021;&#29942;&#39048;&#26159;&#39044;&#35757;&#32451;CLIP&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#22312;&#36974;&#34109;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#19968;&#32452;&#24102;&#26377;&#25513;&#30721;&#22270;&#20687;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#25968;&#25454;&#19978;&#23545;CLIP&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;CLIP&#23558;&#25513;&#30721;&#22270;&#20687;&#21306;&#22495;&#19982;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#21517;&#35789;&#21305;&#37197;&#26469;&#25366;&#25496;&#29616;&#26377;&#22270;&#20687;-&#26631;&#39064;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO Captions&#65289;&#26469;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#19982;&#26356;&#31934;&#30830;&#19988;&#25163;&#21160;&#27880;&#37322;&#30340;&#22266;&#23450;&#31867;&#21035;&#20998;&#21106;&#26631;&#31614;&#65288;&#20363;&#22914;COCO-Stuff&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#22024;&#26434;&#20294;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#33021;&#26356;&#22909;&#22320;&#20445;&#30041;CLIP&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38500;&#20102;&#24494;&#35843;&#25972;&#20010;&#27169;&#22411;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#24212;&#25513;&#30721;&#30340;CLIP&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#33945;&#29256;&#36807;&#31243;&#26469;&#26356;&#22909;&#22320;&#22788;&#29702;&#24102;&#26377;&#25513;&#30721;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;OpenImages&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.02390</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Prompt Learning for Image-Language Model Generalization. (arXiv:2210.02390v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#30340;&#22270;&#20687;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#39640;&#25928;&#30340;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#25552;&#31034;&#23398;&#20064;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25552;&#31034;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#35270;&#20026;&#21487;&#35757;&#32451;&#30340;&#65292;&#21516;&#26102;&#20923;&#32467;&#20854;&#20313;&#37096;&#20998;&#65292;&#24182;&#20248;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#24050;&#30693;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#24433;&#21709;&#20102;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#27491;&#21017;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#25552;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#21046;&#23450;&#20026;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20943;&#23569;&#23545;&#24050;&#35265;&#25552;&#31034;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#26410;&#35265;&#25552;&#31034;&#30340;&#25552;&#31034;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20197;&#27010;&#29575;&#30340;&#26041;&#24335;&#23545;&#36755;&#20837;&#25552;&#31034;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20316;&#20026;&#20808;&#39564;&#20998;&#24067;&#65292;&#20351;&#25105;&#20204;&#30340;&#25552;&#35758;&#19982;&#22522;&#20110;&#22270;&#20687;&#26080;&#26465;&#20214;&#25110;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26412;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Minimization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt generalizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#25910;&#32553;&#25439;&#22833;&#21644;Mixup&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#36328;&#22495;&#25554;&#20540;&#20013;&#20986;&#29616;&#30340;&#24179;&#28369;&#24615;&#38382;&#39064;&#65307;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23450;&#37327;&#35780;&#20272;&#25554;&#20540;&#24179;&#28369;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.00841</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#31354;&#38388;&#25554;&#20540;&#30340;&#24179;&#28369;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Smooth image-to-image translations with latent space interpolations. (arXiv:2210.00841v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#25910;&#32553;&#25439;&#22833;&#21644;Mixup&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#20687;&#36328;&#22495;&#25554;&#20540;&#20013;&#20986;&#29616;&#30340;&#24179;&#28369;&#24615;&#38382;&#39064;&#65307;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23450;&#37327;&#35780;&#20272;&#25554;&#20540;&#24179;&#28369;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22495;&#22270;&#20687;&#36716;&#25442;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#22495;&#30340;&#39118;&#26684;&#23558;&#28304;&#22270;&#20687;&#36827;&#34892;&#36716;&#25442;&#12290;&#36825;&#20123;&#21464;&#25442;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#26159;graduality&#65292;&#21363;&#24403;&#23427;&#20204;&#21508;&#33258;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#34987;&#32447;&#24615;&#25554;&#20540;&#26102;&#65292;&#28304;&#22270;&#20687;&#21644;&#30446;&#26631;&#22270;&#20687;&#20043;&#38388;&#30340;&#24179;&#28369;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#22312;&#20869;&#37096;&#22495;&#19978;&#32447;&#24615;&#25554;&#20540;&#24471;&#21040;&#38750;&#24120;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#35780;&#20272;&#36328;&#22495;&#25554;&#20540;&#26102;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#32463;&#24120;&#20135;&#29983;&#22806;&#35266;&#31361;&#21464;&#25110;&#38750;&#30495;&#23454;&#20013;&#38388;&#22270;&#20687;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#36328;&#22495;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65306;&#19968;&#31181;&#26032;&#30340;&#25910;&#32553;&#25439;&#22833;&#65292;&#23427;&#21487;&#20197;&#21387;&#32553;&#28508;&#22312;&#31354;&#38388;&#65307;&#19968;&#31181;Mixup&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#20351;&#22495;&#20043;&#38388;&#30340;&#26679;&#24335;&#34920;&#31034;&#21464;&#24471;&#24179;&#22374;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23450;&#37327;&#35780;&#20272;&#25554;&#20540;&#24179;&#28369;&#24230;&#65292;&#36890;&#36807;&#27979;&#37327;&#20013;&#38388;&#22270;&#20687;&#19982;&#19968;&#31181;&#26356;&#26114;&#36149;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#24863;&#30693;&#30456;&#20284;&#24230;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain image-to-image (I2I) translations can transform a source image according to the style of a target domain. One important, desired characteristic of these transformations, is their graduality, which corresponds to a smooth change between the source and the target image when their respective latent-space representations are linearly interpolated. However, state-of-the-art methods usually perform poorly when evaluated using inter-domain interpolations, often producing abrupt changes in the appearance or non-realistic intermediate images. In this paper, we argue that one of the main reasons behind this problem is the lack of sufficient inter-domain training data and we propose two different regularization methods to alleviate this issue: a new shrinkage loss, which compacts the latent space, and a Mixup data-augmentation strategy, which flattens the style representations between domains. We also propose a new metric to quantitatively evaluate the degree of the interpolation smo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#35813;&#31639;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.15382</link><description>&lt;p&gt;
&#24102;&#26377;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#32447;&#24615;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization. (arXiv:2209.15382v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#35777;&#26126;&#20102;&#19968;&#23450;&#26465;&#20214;&#19979;&#35813;&#31639;&#27861;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#26410;&#27491;&#21017;&#21270;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#22312;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#65292;&#24403; Q-&#20540;&#26159;&#24050;&#30693;&#30340;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24050;&#30693;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#36924;&#36817;&#19968;&#20010;&#20559;&#24046;&#35823;&#24046;&#26102;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#19968;&#31181;&#20960;&#20309;&#22686;&#38271;&#27493;&#38271;&#21487;&#20197;&#32447;&#24615;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#26679;&#26412;&#20026;&#22522;&#30784;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#24050;&#30693;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#20013;&#65292;Q &#20540;&#20989;&#25968;&#30340;&#26368;&#20339;&#34920;&#31034;&#24050;&#30693;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#20272;&#35745;&#35823;&#24046;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#32447;&#24615;&#20445;&#35777;&#65292;&#30452;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#20272;&#35745;&#35823;&#24046;&#12289;&#20559;&#24046;&#35823;&#24046;&#21644;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#26465;&#20214;&#25968;&#30340;&#35823;&#24046;&#39033;&#20026;&#27490;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#24182;&#25193;&#23637;&#20102;&#20808;&#21069;&#38024;&#23545; softmax &#34920;&#21442;&#25968;&#21270;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14609</link><description>&lt;p&gt;
&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation Using Parameter Pruning. (arXiv:2209.14609v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21442;&#25968;&#20462;&#21098;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#33719;&#24471;&#20808;&#36827;&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#23384;&#20648;&#21644;&#27169;&#22411;&#35757;&#32451;&#21464;&#24471;&#26114;&#36149;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#21512;&#25104;&#20445;&#30041;&#21407;&#22987;&#22823;&#22411;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#20449;&#24687;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#21305;&#37197;&#32593;&#32476;&#21442;&#25968;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#21442;&#25968;&#30340;&#32500;&#24230;&#36890;&#24120;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#21442;&#25968;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#38590;&#20197;&#21305;&#37197;&#65292;&#38477;&#20302;&#20102;&#33976;&#39311;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#20462;&#21098;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#20462;&#21098;&#38590;&#20197;&#21305;&#37197;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#21512;&#25104;&#26356;&#21152;&#31283;&#20581;&#30340;&#33976;&#39311;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many fields, the acquisition of advanced models depends on large datasets, making data storage and model training expensive. As a solution, dataset distillation can synthesize a small dataset that preserves most information of the original large dataset. The recently proposed dataset distillation method by matching network parameters has been proven effective for several datasets. However, the dimensions of network parameters are typically large. Furthermore, some parameters are difficult to match during the distillation process, degrading distillation performance. Based on this observation, this study proposes a novel dataset distillation method based on parameter pruning that solves the problem. The proposed method can synthesize more robust distilled datasets and improve distillation performance by pruning difficult-to-match parameters during the distillation process. Experimental results on three datasets show that the proposed method outperforms other state-of-the-art dataset d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#20809;&#27969;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#26469;&#35299;&#20915;&#33033;&#20914;&#28040;&#22833;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#39033;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.11741</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33033;&#20914;&#32593;&#32476;&#65306;&#20351;&#29992;&#21487;&#23398;&#20064;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20107;&#20214;&#39537;&#21160;&#20809;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics. (arXiv:2209.11741v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#20809;&#27969;&#20272;&#35745;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#21160;&#21147;&#23398;&#26469;&#35299;&#20915;&#33033;&#20914;&#28040;&#22833;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#39033;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20107;&#20214;&#39537;&#21160;&#30456;&#26426;&#20973;&#20511;&#20854;&#24322;&#27493;&#25429;&#25417;&#20016;&#23500;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#23637;&#29616;&#20986;&#39640;&#36895;&#36816;&#21160;&#20272;&#35745;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20973;&#20511;&#20854;&#31070;&#32463;&#21551;&#21457;&#24335;&#20107;&#20214;&#39537;&#21160;&#22788;&#29702;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#27492;&#31867;&#24322;&#27493;&#25968;&#25454;&#65292;&#32780;&#31867;&#20284;&#28431;&#30005;&#25972;&#27969;&#19982;&#25918;&#30005;&#65288;LIF&#65289;&#30340;&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#36319;&#36394;&#36755;&#20837;&#20013;&#21253;&#21547;&#30340;&#20851;&#38190;&#26102;&#38388;&#20449;&#24687;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22312;&#31070;&#32463;&#20803;&#20869;&#23384;&#20013;&#32500;&#25252;&#21160;&#24577;&#29366;&#24577;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#21516;&#26102;&#38543;&#30528;&#26102;&#38388;&#27969;&#36893;&#36951;&#24536;&#20887;&#20313;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#21516;&#26679;&#22823;&#23567;&#30340;&#27169;&#25311;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23558;&#20801;&#35768;&#22312;&#39034;&#24207;&#22238;&#24402;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21518;&#38754;&#30340;&#31070;&#32463;&#20803;&#22788;&#30340;&#33033;&#20914;&#28040;&#22833;&#38382;&#39064;&#65292;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24456;&#38590;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#31070;&#32463;&#20803;&#21160;&#24577;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#38408;&#20540;&#65292;&#36890;&#36807;&#26799;&#24230;&#34928;&#20943;&#21453;&#21521;&#20256;&#25773;&#35299;&#20915;&#20102;&#33033;&#20914;&#28040;&#22833;&#38382;&#39064;&#12290;&#22312;&#20809;&#27969;&#20272;&#35745;&#36825;&#19968;&#39640;&#36895;&#36816;&#21160;&#20272;&#35745;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35768;&#22810;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#20351;&#29992;&#30340;&#26041;&#24335;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpro
&lt;/p&gt;</description></item><item><title>NIERT&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#30340;&#25968;&#20540;&#20869;&#25554;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#23545;&#35266;&#23519;&#28857;&#21644;&#30446;&#26631;&#28857;&#36827;&#34892;&#23884;&#20837;&#21644;&#32479;&#19968;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20869;&#25554;&#21644;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.09078</link><description>&lt;p&gt;
NIERT: &#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#32479;&#19968;&#25955;&#20081;&#25968;&#25454;&#34920;&#31034;&#20197;&#23454;&#29616;&#20934;&#30830;&#25968;&#20540;&#20869;&#25554;
&lt;/p&gt;
&lt;p&gt;
NIERT: Accurate Numerical Interpolation through Unifying Scattered Data Representations using Transformer Encoder. (arXiv:2209.09078v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09078
&lt;/p&gt;
&lt;p&gt;
NIERT&#26159;&#19968;&#31181;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#30340;&#25968;&#20540;&#20869;&#25554;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#23545;&#35266;&#23519;&#28857;&#21644;&#30446;&#26631;&#28857;&#36827;&#34892;&#23884;&#20837;&#21644;&#32479;&#19968;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20869;&#25554;&#21644;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#20081;&#25968;&#25454;&#30340;&#20869;&#25554;&#26159;&#25968;&#20540;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24736;&#20037;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#36129;&#29486;&#21382;&#21490;&#12290;&#36817;&#26399;&#30340;&#36827;&#23637;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#20869;&#25554;&#22120;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#19988;&#21487;&#25512;&#24191;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#22312;&#20004;&#20010;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65306;\textbf{1&#65289;&#34920;&#31034;&#23398;&#20064;&#19981;&#36275;}&#65292;&#22240;&#20026;&#22312;&#27969;&#34892;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#65292;&#35266;&#23519;&#28857;&#21644;&#30446;&#26631;&#28857;&#30340;&#23884;&#20837;&#26159;&#20998;&#24320;&#30340;&#65307;\textbf{2&#65289;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;}&#65292;&#22240;&#20026;&#24573;&#35270;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30340;&#20808;&#21069;&#20869;&#25554;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;\textbf{T}ransformer&#30340;\textbf{E}ncoder \textbf{R}epresentation&#23454;&#29616;\textbf{N}umerical \textbf{I}nterpolation&#65288;&#31216;&#20026;\textbf{NIERT}&#65289;&#30340;&#26041;&#27861;&#12290;NIERT&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#36825;&#26679;&#65292;NIERT&#21487;&#20197;&#23558;&#35266;&#23519;&#28857;&#21644;&#30446;&#26631;&#28857;&#23884;&#20837;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#32534;&#30721;&#22120;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20869;&#25554;&#65292;&#26356;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26356;&#31616;&#27905;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpolation for scattered data is a classical problem in numerical analysis, with a long history of theoretical and practical contributions. Recent advances have utilized deep neural networks to construct interpolators, exhibiting excellent and generalizable performance. However, they still fall short in two aspects: \textbf{1) inadequate representation learning}, resulting from separate embeddings of observed and target points in popular encoder-decoder frameworks and \textbf{2) limited generalization power}, caused by overlooking prior interpolation knowledge shared across different domains. To overcome these limitations, we present a \textbf{N}umerical \textbf{I}nterpolation approach using \textbf{E}ncoder \textbf{R}epresentation of \textbf{T}ransformers (called \textbf{NIERT}). On one hand, NIERT utilizes an encoder-only framework rather than the encoder-decoder structure. This way, NIERT can embed observed and target points into a unified encoder representation space, thus effec
&lt;/p&gt;</description></item><item><title>DC-Art-GAN &#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340; DC-GAN &#36827;&#34892;&#33402;&#26415;&#20316;&#21697;&#30340;&#31283;&#23450;&#29983;&#25104;&#21644;&#22810;&#26679;&#24615;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#34394;&#25311;&#29616;&#23454;&#20013;&#19981;&#23384;&#22312;&#30340;&#36924;&#30495;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2209.02847</link><description>&lt;p&gt;
DC-Art-GAN: &#20351;&#29992; DC-GAN &#36827;&#34892;&#25968;&#23383;&#33402;&#26415;&#30340;&#31283;&#23450;&#36807;&#31243;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DC-Art-GAN: Stable Procedural Content Generation using DC-GANs for Digital Art. (arXiv:2209.02847v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02847
&lt;/p&gt;
&lt;p&gt;
DC-Art-GAN &#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340; DC-GAN &#36827;&#34892;&#33402;&#26415;&#20316;&#21697;&#30340;&#31283;&#23450;&#29983;&#25104;&#21644;&#22810;&#26679;&#24615;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#34394;&#25311;&#29616;&#23454;&#20013;&#19981;&#23384;&#22312;&#30340;&#36924;&#30495;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#33402;&#26415;&#26159;&#20351;&#29992;&#25968;&#23383;&#25216;&#26415;&#20316;&#20026;&#29983;&#25104;&#25110;&#21019;&#36896;&#36807;&#31243;&#30340;&#33402;&#26415;&#26041;&#27861;&#12290;&#38543;&#30528;&#25968;&#23383;&#36135;&#24065;&#21644; NFT&#65288;&#19981;&#21487;&#26367;&#20195;&#20195;&#24065;&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#25968;&#23383;&#33402;&#26415;&#30340;&#38656;&#27714;&#27491;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#25991;&#20027;&#35201;&#20513;&#23548;&#20351;&#29992;&#20855;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#26469;&#23454;&#29616;&#31283;&#23450;&#21644;&#22810;&#26679;&#30340;&#33402;&#26415;&#29983;&#25104;&#12290;&#35813;&#24037;&#20316;&#20027;&#35201;&#20391;&#37325;&#20110;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;DC-GAN&#65289;&#65292;&#24182;&#25506;&#32034;&#20102;&#35299;&#20915; GAN &#35757;&#32451;&#20013;&#24120;&#35265;&#38382;&#39064;&#30340;&#25216;&#24039;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181; DC-GAN &#30340;&#26550;&#26500;&#21644;&#35774;&#35745;&#65292;&#20197;&#30830;&#23450;&#19968;&#31181;&#21487;&#25512;&#33616;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#23454;&#29616;&#31283;&#23450;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#24037;&#20316;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#29983;&#25104;&#29616;&#23454;&#20013;&#19981;&#23384;&#22312;&#20294;&#30001;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#30340;&#36924;&#30495;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#30340;&#21160;&#29289;&#38754;&#37096;&#22270;&#20687;&#30340;&#35270;&#35273;&#32467;&#26524;&#65288;&#19968;&#20123;&#35777;&#25454;&#26174;&#31034;&#20102;&#19981;&#21516;&#29289;&#31181;&#30340;&#28151;&#21512;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851; DC-Art-GAN &#27169;&#22411;&#30340;&#22521;&#35757;&#12289;&#26550;&#26500;&#21644;&#35774;&#35745;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art is an artistic method of using digital technologies as a part of the generative or creative process. With the advent of digital currency and NFTs (Non-Fungible Token), the demand for digital art is growing aggressively. In this manuscript, we advocate the concept of using deep generative networks with adversarial training for a stable and variant art generation. The work mainly focuses on using the Deep Convolutional Generative Adversarial Network (DC-GAN) and explores the techniques to address the common pitfalls in GAN training. We compare various architectures and designs of DC-GANs to arrive at a recommendable design choice for a stable and realistic generation. The main focus of the work is to generate realistic images that do not exist in reality but are synthesised from random noise by the proposed model. We provide visual results of generated animal face images (some pieces of evidence showing a blend of species) along with recommendations for training, architecture and des
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#39044;&#27979;&#30495;&#23454;&#30340;3D&#21333;&#20010;&#32454;&#32990;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2208.14125</link><description>&lt;p&gt;
&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#39044;&#27979;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;3D&#32454;&#32990;&#24418;&#29366;
&lt;/p&gt;
&lt;p&gt;
A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images. (arXiv:2208.14125v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#39044;&#27979;&#30495;&#23454;&#30340;3D&#21333;&#20010;&#32454;&#32990;&#24418;&#29366;&#12290;&#35813;&#26041;&#27861;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#21512;&#25104;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;DISPR&#65292;&#29992;&#20110;&#35299;&#20915;&#20174;&#20108;&#32500;&#21333;&#20010;&#32454;&#32990;&#26174;&#24494;&#22270;&#20687;&#39044;&#27979;&#19977;&#32500;&#32454;&#32990;&#24418;&#29366;&#30340;&#21453;&#38382;&#39064;&#12290;&#21033;&#29992;&#20108;&#32500;&#26174;&#24494;&#22270;&#20687;&#20316;&#20026;&#20808;&#39564;&#26465;&#20214;&#65292;DISPR&#34987;&#35843;&#25972;&#20026;&#39044;&#27979;&#36924;&#30495;&#30340;&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#12290;&#25105;&#20204;&#20174;&#20845;&#20010;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20013;&#25552;&#21462;&#24418;&#24577;&#23398;&#29305;&#24449;&#65292;&#23637;&#31034;&#20102;DISPR&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#22312;&#22522;&#20110;&#29305;&#24449;&#30340;&#21333;&#20010;&#32454;&#32990;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23558;DISPR&#39044;&#27979;&#30340;&#29305;&#24449;&#28155;&#21152;&#21040;&#19977;&#20010;&#23569;&#25968;&#31867;&#20013;&#65292;&#23558;&#23439;F1&#24471;&#20998;&#20174;$F1_{macro}=55.2\pm4.6\%$&#25552;&#39640;&#21040;$F1_{macro}=72.2\pm4.9\%$&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#24212;&#29992;&#20110;&#21453;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#65292;&#24182;&#19988;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#20174;2D&#26174;&#24494;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#36924;&#30495;&#24418;&#24577;&#23398;&#29305;&#24449;&#30340;3D&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a special type of generative model, capable of synthesising new data from a learnt distribution. We introduce DISPR, a diffusion-based model for solving the inverse problem of three-dimensional (3D) cell shape prediction from two-dimensional (2D) single cell microscopy images. Using the 2D microscopy image as a prior, DISPR is conditioned to predict realistic 3D shape reconstructions. To showcase the applicability of DISPR as a data augmentation tool in a feature-based single cell classification task, we extract morphological features from the red blood cells grouped into six highly imbalanced classes. Adding features from the DISPR predictions to the three minority classes improved the macro F1 score from $F1_\text{macro} = 55.2 \pm 4.6\%$ to $F1_\text{macro} = 72.2 \pm 4.9\%$. We thus demonstrate that diffusion models can be successfully applied to inverse biomedical problems, and that they learn to reconstruct 3D shapes with realistic morphological features from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.14653</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#31995;&#21015;&#20013;&#30340;&#38598;&#25104;&#39044;&#27979;&#65306;&#20185;&#22659;&#20013;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble forecasts in reproducing kernel Hilbert space family: dynamical systems in Wonderland. (arXiv:2207.14653v2 [math-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#23884;&#20837;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#65292;&#24182;&#22312;&#35813;&#31354;&#38388;&#20013;&#20351;&#29992;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28023;&#27915;&#25110;&#22823;&#27668;&#27969;&#31561;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#30340;&#38598;&#21512;&#20272;&#35745;&#21644;&#27169;&#25311;&#30340;&#26041;&#27861;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#23558;&#35813;&#21160;&#21147;&#31995;&#32479;&#23884;&#20837;&#30001;&#21160;&#21147;&#39537;&#21160;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26063;&#20013;&#12290;&#36825;&#20010;&#23478;&#26063;&#22240;&#20854;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#32780;&#34987;&#21629;&#21517;&#20026;&#20185;&#22659;&#12290;&#22312;&#20185;&#22659;&#20013;&#65292;Koopman&#21644;Perron-Frobenius&#31639;&#23376;&#26159;&#37193;&#30340;&#21644;&#19968;&#33268;&#36830;&#32493;&#30340;&#12290;&#36825;&#20010;&#23646;&#24615;&#20445;&#35777;&#23427;&#20204;&#21487;&#20197;&#29992;&#23545;&#35282;&#21270;&#26377;&#30028;&#26080;&#31351;&#23567;&#29983;&#25104;&#22120;&#30340;&#25351;&#25968;&#32423;&#32423;&#25968;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#30452;&#25509;&#33719;&#24471;&#23545;Lyapunov&#25351;&#25968;&#21644;&#20999;&#32447;&#32447;&#24615;&#21160;&#24577;&#30340;&#31934;&#30830;&#38598;&#21512;&#24335;&#34920;&#36798;&#24335;&#12290;&#20185;&#22659;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#20986;&#26497;&#20854;&#31616;&#21333;&#30340;&#38598;&#21512;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#36712;&#36857;&#26679;&#26412;&#30340;&#24658;&#23450;&#26102;&#38388;&#32447;&#24615;&#32452;&#21512;&#26469;&#36827;&#34892;&#36712;&#36857;&#37325;&#26500;&#12290;&#36825;&#31181;&#20196;&#20154;&#23604;&#23596;&#30340;&#31616;&#21333;&#31574;&#30053;&#24471;&#20197;&#23454;&#29616;&#65292;&#26159;&#36890;&#36807;Hilbert&#31354;&#38388;&#35774;&#32622;&#20013;&#20999;&#32447;&#32447;&#24615;&#21160;&#21147;&#30340;&#23436;&#20840;&#21512;&#29702;&#30340;&#21472;&#21152;&#21407;&#29702;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A methodological framework for ensemble-based estimation and simulation of high dimensional dynamical systems such as the oceanic or atmospheric flows is proposed. To that end, the dynamical system is embedded in a family of reproducing kernel Hilbert spaces with kernel functions driven by the dynamics. This family is nicknamed Wonderland for its appealing properties. In Wonderland the Koopman and Perron-Frobenius operators are unitary and uniformly continuous. This property warrants they can be expressed in exponential series of diagonalizable bounded infinitesimal generators. Access to Lyapunov exponents and to exact ensemble based expressions of the tangent linear dynamics are directly available as well. Wonderland enables us the devise of strikingly simple ensemble data assimilation methods for trajectory reconstructions in terms of constant-in-time linear combinations of trajectory samples. Such an embarrassingly simple strategy is made possible through a fully justified superposi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#20102;&#25506;&#32034;&#26356;&#28145;&#20837;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20869;&#37096;&#20851;&#32852;&#65288;AIA&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#20808;&#21069;&#24573;&#35270;&#30340;&#31070;&#32463;&#20803;&#20869;&#37096;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20419;&#36827;&#26356;&#24378;&#30340;&#32852;&#24819;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;SNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.11670</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#29289;&#20223;&#29983;&#30340;&#33258;&#36866;&#24212;&#20869;&#37096;&#20851;&#32852;&#31070;&#32463;&#20803;&#35757;&#32451;&#26356;&#24378;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Stronger Spiking Neural Networks with Biomimetic Adaptive Internal Association Neurons. (arXiv:2207.11670v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#20102;&#25506;&#32034;&#26356;&#28145;&#20837;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20869;&#37096;&#20851;&#32852;&#65288;AIA&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#20808;&#21069;&#24573;&#35270;&#30340;&#31070;&#32463;&#20803;&#20869;&#37096;&#24433;&#21709;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20419;&#36827;&#26356;&#24378;&#30340;&#32852;&#24819;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;SNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#31532;&#19977;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#33268;&#21147;&#20110;&#25506;&#32034;&#26356;&#28145;&#20837;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#29983;&#29289;&#26234;&#33021;&#30340;&#25928;&#26524;&#12290;&#29983;&#29289;&#27169;&#25311;&#26426;&#21046;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;SNN&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#20851;&#32852;&#38271;&#26102;&#31243;&#22686;&#24378;&#65288;ALTP&#65289;&#29616;&#35937;&#34920;&#26126;&#65292;&#38500;&#20102;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#23398;&#20064;&#26426;&#21046;&#22806;&#65292;&#31070;&#32463;&#20803;&#20869;&#37096;&#20063;&#23384;&#22312;&#20851;&#32852;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#21069;&#32773;&#65292;&#32570;&#20047;&#23545;&#20869;&#37096;&#20851;&#32852;&#25928;&#24212;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20869;&#37096;&#20851;&#32852;&#65288;AIA&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#20808;&#21069;&#24573;&#35270;&#30340;&#31070;&#32463;&#20803;&#20869;&#37096;&#24433;&#21709;&#12290;&#19982;ALTP&#29616;&#35937;&#19968;&#33268;&#65292;AIA&#31070;&#32463;&#20803;&#27169;&#22411;&#23545;&#36755;&#20837;&#21050;&#28608;&#20855;&#26377;&#33258;&#36866;&#24212;&#24615;&#65292;&#21482;&#26377;&#22312;&#21516;&#26102;&#28608;&#21457;&#20004;&#20010;&#26641;&#31361;&#26102;&#25165;&#20250;&#21457;&#29983;&#20869;&#37096;&#32852;&#24819;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21152;&#26435;&#30340;&#26435;&#37325;&#26469;&#27979;&#37327;&#20869;&#37096;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20013;&#38388;&#32531;&#23384;&#26469;&#20943;&#23569;&#31361;&#35302;&#36164;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AIA&#31070;&#32463;&#20803;&#27169;&#22411;&#21487;&#20197;&#20419;&#36827;&#26356;&#24378;&#30340;&#32852;&#24819;&#23398;&#20064;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;SNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the third generation of neural networks, spiking neural networks (SNNs) are dedicated to exploring more insightful neural mechanisms to achieve near-biological intelligence. Intuitively, biomimetic mechanisms are crucial to understanding and improving SNNs. For example, the associative long-term potentiation (ALTP) phenomenon suggests that in addition to learning mechanisms between neurons, there are associative effects within neurons. However, most existing methods only focus on the former and lack exploration of the internal association effects. In this paper, we propose a novel Adaptive Internal Association~(AIA) neuron model to establish previously ignored influences within neurons. Consistent with the ALTP phenomenon, the AIA neuron model is adaptive to input stimuli, and internal associative learning occurs only when both dendrites are stimulated at the same time. In addition, we employ weighted weights to measure internal associations and introduce intermediate caches to redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26102;&#31354;&#29255;&#27573;&#65288;ESTF&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20351;&#29992;SNN&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;ESTF&#30340;SNN&#22312;CIFAR10-DVS&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;83.9\%&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2207.11659</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#31354;&#29255;&#27573;&#22312;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#19978;&#35757;&#32451;&#40065;&#26834;&#24615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Robust Spiking Neural Networks on Neuromorphic Data with Spatiotemporal Fragments. (arXiv:2207.11659v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26102;&#31354;&#29255;&#27573;&#65288;ESTF&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#25968;&#25454;&#65292;&#24182;&#25552;&#39640;&#20351;&#29992;SNN&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;ESTF&#30340;SNN&#22312;CIFAR10-DVS&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;83.9\%&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#20256;&#24863;&#22120;&#65288;&#20107;&#20214;&#30456;&#26426;&#65289;&#22825;&#29983;&#36866;&#21512;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#20026;&#36825;&#31181;&#20223;&#29983;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#25968;&#25454;&#12290;&#30001;&#20110;&#26102;&#31354;&#29305;&#24615;&#65292;&#38656;&#35201;&#20351;&#29992;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22788;&#29702;&#36825;&#20123;&#30456;&#26426;&#30340;&#38750;&#20256;&#32479;&#35270;&#35273;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20107;&#20214;&#26102;&#31354;&#29255;&#27573;&#65288;ESTF&#65289;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#28418;&#31227;&#25110;&#21453;&#36716;&#26102;&#31354;&#20107;&#20214;&#27969;&#30340;&#29255;&#27573;&#26469;&#27169;&#25311;&#20142;&#24230;&#21464;&#21270;&#30340;&#24178;&#25200;&#65292;&#20197;&#27492;&#20445;&#30041;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#30340;&#36830;&#32493;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#27969;&#34892;&#30340;&#31070;&#32463;&#24418;&#24577;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;ESTF&#27604;&#32431;&#20960;&#20309;&#21464;&#25442;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#20107;&#20214;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;ESTF&#30340;SNN&#22312;CIFAR10-DVS&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;83.9\%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic vision sensors (event cameras) are inherently suitable for spiking neural networks (SNNs) and provide novel neuromorphic vision data for this biomimetic model. Due to the spatiotemporal characteristics, novel data augmentations are required to process the unconventional visual signals of these cameras. In this paper, we propose a novel Event SpatioTemporal Fragments (ESTF) augmentation method. It preserves the continuity of neuromorphic data by drifting or inverting fragments of the spatiotemporal event stream to simulate the disturbance of brightness variations, leading to more robust spiking neural networks. Extensive experiments are performed on prevailing neuromorphic datasets. It turns out that ESTF provides substantial improvements over pure geometric transformations and outperforms other event data augmentation methods. It is worth noting that the SNNs with ESTF achieve the state-of-the-art accuracy of 83.9\% on the CIFAR10-DVS dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APOD&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;&#65292;&#35813;&#26694;&#26550;&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.10018</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#27880;&#37322;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Algorithmic Bias with Limited Annotations. (arXiv:2207.10018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;APOD&#30340;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#20943;&#23569;&#31639;&#27861;&#20559;&#35265;&#65292;&#35813;&#26694;&#26550;&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#24314;&#27169;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#23454;&#20363;&#30340;&#25935;&#24863;&#23646;&#24615;&#37117;&#26159;&#23436;&#20840;&#21487;&#29992;&#30340;&#65292;&#20294;&#30001;&#20110;&#33719;&#21462;&#25935;&#24863;&#20449;&#24687;&#30340;&#39640;&#25104;&#26412;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#26159;&#36825;&#26679;&#12290;&#24403;&#25935;&#24863;&#23646;&#24615;&#26410;&#20844;&#24320;&#25110;&#26080;&#27861;&#33719;&#24471;&#26102;&#65292;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#20197;&#20943;&#36731;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#25935;&#24863;&#32452;&#20043;&#38388;&#30340;&#20559;&#26012;&#20998;&#24067;&#20250;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#23376;&#38598;&#30340;&#20559;&#26012;&#24615;&#65292;&#36825;&#23548;&#33268;&#38750;&#26368;&#20248;&#30340;&#20559;&#24046;&#20943;&#36731;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Active Penalization Of Discrimination (APOD)&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#25351;&#23548;&#26377;&#38480;&#27880;&#37322;&#26368;&#22823;&#38480;&#24230;&#22320;&#28040;&#38500;&#31639;&#27861;&#20559;&#35265;&#12290;&#25152;&#25552;&#20986;&#30340;APOD&#23558;&#27495;&#35270;&#24809;&#32602;&#19982;&#20027;&#21160;&#23454;&#20363;&#36873;&#25321;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#38480;&#21046;&#31639;&#27861;&#20559;&#35265;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;APOD&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#27880;&#37322;&#25968;&#37327;&#26126;&#26174;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on fairness modeling commonly assumes that sensitive attributes for all instances are fully available, which may not be true in many real-world applications due to the high cost of acquiring sensitive information. When sensitive attributes are not disclosed or available, it is needed to manually annotate a small part of the training data to mitigate bias. However, the skewed distribution across different sensitive groups preserves the skewness of the original dataset in the annotated subset, which leads to non-optimal bias mitigation. To tackle this challenge, we propose Active Penalization Of Discrimination (APOD), an interactive framework to guide the limited annotations towards maximally eliminating the effect of algorithmic bias. The proposed APOD integrates discrimination penalization with active instance selection to efficiently utilize the limited annotation budget, and it is theoretically proved to be capable of bounding the algorithmic bias. According to the eval
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27431;&#25289;&#24377;&#24615;&#32441;&#29702;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20351;&#29992;&#28145;&#33021;&#37327;&#19982;&#28145;&#22270;&#20687;&#20808;&#39564;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#24418;&#29366;&#24674;&#22797;&#20219;&#21153;&#65292;&#24471;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#20462;&#22797;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.07921</link><description>&lt;p&gt;
&#22522;&#20110;CNN&#30340;&#27431;&#25289;&#24377;&#24615;&#32441;&#29702;&#20462;&#22797;&#31639;&#27861;&#32467;&#21512;&#28145;&#33021;&#37327;&#21644;&#28145;&#22270;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior. (arXiv:2207.07921v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27431;&#25289;&#24377;&#24615;&#32441;&#29702;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20351;&#29992;&#28145;&#33021;&#37327;&#19982;&#28145;&#22270;&#20687;&#20808;&#39564;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36827;&#34892;&#24418;&#29366;&#24674;&#22797;&#20219;&#21153;&#65292;&#24471;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#20462;&#22797;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#25289;&#24377;&#24615;&#32441;&#29702;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#21464;&#20998;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;&#65292;&#23427;&#19981;&#20165;&#28041;&#21450;&#24635;&#21464;&#24046;&#65292;&#36824;&#28041;&#21450;&#31561;&#39640;&#32447;&#26354;&#29575;&#65292;&#36825;&#20123;&#29305;&#28857;&#20351;&#23427;&#22312;&#24418;&#29366;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#26799;&#24230;&#27969;&#26159;&#19968;&#31181;&#22855;&#24322;&#12289;&#21508;&#21521;&#24322;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#22235;&#38454;PDE&#65292;&#36825;&#22312;&#25968;&#20540;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#24456;&#38590;&#25214;&#21040;&#26082;&#33021;&#25552;&#20379;&#23574;&#38160;&#36793;&#32536;&#21448;&#20855;&#26377;&#33391;&#22909;&#26059;&#36716;&#19981;&#21464;&#24615;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#27431;&#25289;&#24377;&#24615;&#36827;&#34892;&#20462;&#22797;&#30340;&#31070;&#32463;&#31639;&#27861;&#65306;&#21033;&#29992;&#28145;&#33021;&#37327;&#30340;&#27010;&#24565;&#65292;&#23558;&#21464;&#20998;&#33021;&#37327;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#28145;&#22270;&#20687;&#20808;&#39564;&#65292;&#20854;&#20013;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#23601;&#20805;&#24403;&#20102;&#20808;&#39564;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#25152;&#38656;&#30340;&#35299;&#65292;&#24471;&#21040;&#20102;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#20462;&#22797;&#25104;&#26524;&#12290;&#22312;&#24377;&#24615;&#32441;&#29702;&#30340;&#24418;&#29366;&#24674;&#22797;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#36136;&#37327;&#19978;&#19982;&#29616;&#26377;&#31639;&#27861;&#19981;&#30456;&#19978;&#19979;&#65292;&#33021;&#22815;&#32467;&#21512;&#33391;&#22909;&#30340;&#26059;&#36716;&#19981;&#21464;&#24615;&#21644;&#23574;&#38160;&#30340;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Euler's elastica constitute an appealing variational image inpainting model. It minimises an energy that involves the total variation as well as the level line curvature. These components are transparent and make it attractive for shape completion tasks. However, its gradient flow is a singular, anisotropic, and nonlinear PDE of fourth order, which is numerically challenging: It is difficult to find efficient algorithms that offer sharp edges and good rotation invariance. As a remedy, we design the first neural algorithm that simulates inpainting with Euler's Elastica. We use the deep energy concept which employs the variational energy as neural network loss. Furthermore, we pair it with a deep image prior where the network architecture itself acts as a prior. This yields better inpaintings by steering the optimisation trajectory closer to the desired solution. Our results are qualitatively on par with state-of-the-art algorithms on elastica-based shape completion. They combine good ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#35780;&#20215;&#26631;&#20934;&#38656;&#26681;&#25454;&#20219;&#21153;&#35268;&#23450;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2207.07506</link><description>&lt;p&gt;
&#25903;&#25345;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;softmax&#20449;&#24687;&#25299;&#23637;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmenting Softmax Information for Selective Classification with Out-of-Distribution Data. (arXiv:2207.07506v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#35780;&#20215;&#26631;&#20934;&#38656;&#26681;&#25454;&#20219;&#21153;&#35268;&#23450;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#26816;&#27979;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26159;&#19968;&#39033;&#27491;&#22312;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30740;&#31350;&#20851;&#27880;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#20219;&#21153;&#20013;&#21333;&#29420;&#35780;&#20272;&#30340;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#20854;&#22312;&#32852;&#21512;&#19979;&#28216;&#20219;&#21153;&#26102;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#22806;&#37096;&#20998;&#24067;&#25968;&#25454;&#26102;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;(SCOD)&#38382;&#39064;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#21160;&#26426;&#22312;&#20110;&#25298;&#32477;&#23427;&#20204;&#65292;&#20174;&#32780;&#38477;&#20302;&#23427;&#20204;&#23545;&#39044;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#35268;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#21644;&#21482;&#22312;OOD&#26816;&#27979;&#26102;&#35780;&#20272;&#26102;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20102;&#19981;&#21516;&#30340;&#24615;&#33021;&#12290;&#22240;&#20026;&#22914;&#26524;ID&#25968;&#25454;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#23558;ID&#25968;&#25454;&#19982;OOD&#25968;&#25454;&#28151;&#28102;&#23601;&#19981;&#20877;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;ID&#25968;&#25454;&#20013;&#27491;&#30830;&#39044;&#27979;&#21644;&#38169;&#35823;&#39044;&#27979;&#20043;&#38388;&#30340;&#28151;&#28102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SCOD&#26041;&#27861;&#65292;&#21363;&#20445;&#30041;softmax&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a task that is receiving an increasing amount of research attention in the domain of deep learning for computer vision. However, the performance of detection methods is generally evaluated on the task in isolation, rather than also considering potential downstream tasks in tandem. In this work, we examine selective classification in the presence of OOD data (SCOD). That is to say, the motivation for detecting OOD samples is to reject them so their impact on the quality of predictions is reduced. We show under this task specification, that existing post-hoc methods perform quite differently compared to when evaluated only on OOD detection. This is because it is no longer an issue to conflate in-distribution (ID) data with OOD data if the ID data is going to be misclassified. However, the conflation within ID data of correct and incorrect predictions becomes undesirable. We also propose a novel method for SCOD, Softmax Information Retaining Com
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#26631;&#31614;&#22122;&#22768;&#25554;&#20540;&#21487;&#20197;&#23548;&#33268;&#23545;&#25239;&#39118;&#38505;&#65292;&#22312;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#20013;&#26631;&#31614;&#22122;&#22768;&#19982;&#23545;&#25239;&#39118;&#38505;&#20043;&#38388;&#37117;&#23384;&#22312;&#19968;&#23450;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#22343;&#21248;&#26631;&#31614;&#22122;&#22768;&#30340;&#23545;&#25239;&#39118;&#38505;&#19982;&#26368;&#31967;&#30340;&#27745;&#26579;&#30456;&#24046;&#26080;&#20960;&#65292;&#24182;&#19988;&#27604;&#20856;&#22411;&#29616;&#23454;&#19990;&#30028;&#26631;&#31614;&#22122;&#22768;&#26356;&#20855;&#21361;&#23475;&#24615;&#12290;&#36827;&#34892;&#35686;&#24789;&#24182;&#28145;&#20837;&#30740;&#31350;&#27492;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#19981;&#23481;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2207.03933</link><description>&lt;p&gt;
&#23545;&#25239;&#39118;&#38505;&#12289;&#25554;&#20540;&#19982;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#39033;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
A law of adversarial risk, interpolation, and label noise. (arXiv:2207.03933v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#26631;&#31614;&#22122;&#22768;&#25554;&#20540;&#21487;&#20197;&#23548;&#33268;&#23545;&#25239;&#39118;&#38505;&#65292;&#22312;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#20013;&#26631;&#31614;&#22122;&#22768;&#19982;&#23545;&#25239;&#39118;&#38505;&#20043;&#38388;&#37117;&#23384;&#22312;&#19968;&#23450;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#22343;&#21248;&#26631;&#31614;&#22122;&#22768;&#30340;&#23545;&#25239;&#39118;&#38505;&#19982;&#26368;&#31967;&#30340;&#27745;&#26579;&#30456;&#24046;&#26080;&#20960;&#65292;&#24182;&#19988;&#27604;&#20856;&#22411;&#29616;&#23454;&#19990;&#30028;&#26631;&#31614;&#22122;&#22768;&#26356;&#20855;&#21361;&#23475;&#24615;&#12290;&#36827;&#34892;&#35686;&#24789;&#24182;&#28145;&#20837;&#30740;&#31350;&#27492;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#19981;&#23481;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#25968;&#25454;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#27979;&#35797;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25554;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25554;&#20540;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#23545;&#25239;&#24615;&#28431;&#27934;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#26631;&#31614;&#22122;&#22768;&#19982;&#23545;&#25239;&#39118;&#38505;&#20043;&#38388;&#30340;&#20851;&#31995;&#23450;&#29702;&#65292;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#19981;&#20570;&#20219;&#20309;&#24402;&#32435;&#20559;&#24046;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20998;&#24067;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#38750;&#22343;&#21248;&#26631;&#31614;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#26465;&#26032;&#30340;&#23450;&#29702;&#65292;&#34920;&#26126;&#20855;&#26377;&#30456;&#21516;&#22122;&#22768;&#29575;&#30340;&#22343;&#21248;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#23545;&#25239;&#39118;&#38505;&#19982;&#26368;&#31967;&#30340;&#27745;&#26579;&#24046;&#19981;&#22810;&#30456;&#21516;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22343;&#21248;&#26631;&#31614;&#22122;&#22768;&#27604;&#20856;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#26631;&#31614;&#22122;&#22768;&#26356;&#20855;&#21361;&#23475;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24402;&#32435;&#20559;&#24046;&#22914;&#20309;&#25918;&#22823;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#35770;&#35777;&#20102;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised learning, it has been shown that label noise in the data can be interpolated without penalties on test accuracy. We show that interpolating label noise induces adversarial vulnerability, and prove the first theorem showing the relationship between label noise and adversarial risk for any data distribution. Our results are almost tight if we do not make any assumptions on the inductive bias of the learning algorithm. We then investigate how different components of this problem affect this result, including properties of the distribution. We also discuss non-uniform label noise distributions; and prove a new theorem showing uniform label noise induces nearly as large an adversarial risk as the worst poisoning with the same noise rate. Then, we provide theoretical and empirical evidence that uniform label noise is more harmful than typical real-world label noise. Finally, we show how inductive biases amplify the effect of label noise and argue the need for future work in thi
&lt;/p&gt;</description></item><item><title>&#30123;&#24773;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20256;&#26579;&#30149;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24449;&#65292;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Epicasting&#30340;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#38598;&#25104;&#23567;&#27874;&#31070;&#32463;&#32593;&#32476;&#65288;EWNet&#65289;&#26469;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30123;&#24773;&#12290;</title><link>http://arxiv.org/abs/2206.10696</link><description>&lt;p&gt;
Epicasting&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30123;&#24773;&#30340;&#38598;&#25104;&#23567;&#27874;&#31070;&#32463;&#32593;&#32476;&#65288;EWNet&#65289;
&lt;/p&gt;
&lt;p&gt;
Epicasting: An Ensemble Wavelet Neural Network (EWNet) for Forecasting Epidemics. (arXiv:2206.10696v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10696
&lt;/p&gt;
&lt;p&gt;
&#30123;&#24773;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20256;&#26579;&#30149;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24449;&#65292;&#39044;&#27979;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Epicasting&#30340;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#38598;&#25104;&#23567;&#27874;&#31070;&#32463;&#32593;&#32476;&#65288;EWNet&#65289;&#26469;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#30149;&#20173;&#28982;&#26159;&#20840;&#29699;&#20154;&#31867;&#30142;&#30149;&#21644;&#27515;&#20129;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#20854;&#20013;&#35768;&#22810;&#30142;&#30149;&#20250;&#20135;&#29983;&#20256;&#26579;&#24615;&#24863;&#26579;&#30340;&#27969;&#34892;&#27874;&#12290;&#22823;&#22810;&#25968;&#36825;&#20123;&#27969;&#34892;&#30149;&#27809;&#26377;&#29305;&#23450;&#30340;&#33647;&#29289;&#21644;&#39044;&#38450;&#30123;&#33495;&#65292;&#20351;&#24773;&#20917;&#26356;&#21152;&#24694;&#21270;&#12290;&#36825;&#36843;&#20351;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20381;&#38752;&#21487;&#38752;&#21644;&#20934;&#30830;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#25152;&#29983;&#25104;&#30340;&#39044;&#35686;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27969;&#34892;&#30149;&#30340;&#20256;&#25773;&#27874;&#21160;&#22522;&#20110;&#23395;&#33410;&#20381;&#36182;&#24615;&#21644;&#23427;&#20204;&#30340;&#26412;&#36136;&#65292;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#36825;&#20123;&#27969;&#34892;&#30149;&#34920;&#29616;&#20986;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#22823;&#37325;&#21472;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;MODWT&#65289;&#20998;&#26512;&#20102;&#21508;&#31181;&#27969;&#34892;&#30149;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious diseases remain among the top contributors to human illness and death worldwide, among which many diseases produce epidemic waves of infection. The unavailability of specific drugs and ready-to-use vaccines to prevent most of these epidemics makes the situation worse. These force public health officials and policymakers to rely on early warning systems generated by reliable and accurate forecasts of epidemics. Accurate forecasts of epidemics can assist stakeholders in tailoring countermeasures, such as vaccination campaigns, staff scheduling, and resource allocation, to the situation at hand, which could translate to reductions in the impact of a disease. Unfortunately, most of these past epidemics exhibit nonlinear and non-stationary characteristics due to their spreading fluctuations based on seasonal-dependent variability and the nature of these epidemics. We analyse a wide variety of epidemic time series datasets using a maximal overlap discrete wavelet transform (MODWT)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#36755;&#20986;&#12289;&#22810;&#32500;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#26354;&#32447;&#21644;&#24418;&#29366;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#20026;&#21151;&#33021;&#23545;&#35937;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2206.09127</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#22810;&#24418;&#29366;&#24314;&#27169;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Multi-shape Modeling with Uncertainty Quantification. (arXiv:2206.09127v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#36755;&#20986;&#12289;&#22810;&#32500;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#26354;&#32447;&#21644;&#24418;&#29366;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#20026;&#21151;&#33021;&#23545;&#35937;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#21512;&#26354;&#32447;&#30340;&#24314;&#27169;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#24418;&#29366;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#33021;&#23545;&#38543;&#21518;&#30340;&#32479;&#35745;&#20219;&#21153;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#35768;&#22810;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21040;&#22810;&#20010;&#38381;&#21512;&#26354;&#32447;&#30340;&#38598;&#21512;&#65292;&#36825;&#20123;&#38598;&#21512;&#36890;&#24120;&#22312;&#22810;&#20010;&#23618;&#38754;&#19978;&#23637;&#29616;&#20986;&#32467;&#26500;&#19978;&#30340;&#30456;&#20284;&#24615;&#12290;&#20197;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#22320;&#32435;&#20837;&#36825;&#31181;&#26354;&#32447;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#24335;&#23545;&#22810;&#26465;&#38381;&#26354;&#32447;&#36827;&#34892;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#36755;&#20986;(&#20063;&#31216;&#20026;&#22810;&#36755;&#20986;)&#12289;&#22810;&#32500;&#39640;&#26031;&#36807;&#31243;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#36827;&#23637;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20960;&#20010;&#26354;&#32447;&#21644;&#24418;&#29366;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#30528;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#25928;&#29992;&#12290;&#36825;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#20869;&#26680;&#26500;&#36896;&#35299;&#20915;&#20102;&#23545;&#38381;&#21512;&#26354;&#32447;(&#20197;&#21450;&#23427;&#20204;&#30340;&#24418;&#29366;)&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#32780;&#19988;&#36824;&#20026;&#19968;&#33324;&#30340;&#21151;&#33021;&#23545;&#35937;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling and uncertainty quantification of closed curves is an important problem in the field of shape analysis, and can have significant ramifications for subsequent statistical tasks. Many of these tasks involve collections of closed curves, which often exhibit structural similarities at multiple levels. Modeling multiple closed curves in a way that efficiently incorporates such between-curve dependence remains a challenging problem. In this work, we propose and investigate a multiple-output (a.k.a. multi-output), multi-dimensional Gaussian process modeling framework. We illustrate the proposed methodological advances, and demonstrate the utility of meaningful uncertainty quantification, on several curve and shape-related tasks. This model-based approach not only addresses the problem of inference on closed curves (and their shapes) with kernel constructions, but also opens doors to nonparametric modeling of multi-level dependence for functional objects in general.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#23545;&#20110;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22270;&#65292;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2206.07883</link><description>&lt;p&gt;
&#22240;&#26524;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Pure Exploration of Causal Bandits. (arXiv:2206.07883v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22240;&#26524;&#27169;&#22411;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#20854;&#20013;&#23545;&#20110;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22270;&#65292;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#32452;&#21512;&#32431;&#25506;&#32034;&#26159;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#65306;&#22312;&#32473;&#23450;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#22240;&#26524;&#25512;&#26029;&#20998;&#24067;&#30340;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#21464;&#37327;&#26469;&#24178;&#39044;&#25110;&#19981;&#24178;&#39044;&#65292;&#24182;&#35266;&#23519;&#25152;&#26377;&#38543;&#26426;&#21464;&#37327;&#30340;&#38543;&#26426;&#32467;&#26524;&#65292;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#23569;&#30340;&#20351;&#29992;&#36718;&#25968;&#65292;&#20197;&#27010;&#29575;&#33267;&#23569;&#20026;$1-\delta$&#65292;&#36755;&#20986;&#33021;&#32473;&#20104;&#22870;&#21169;&#21464;&#37327;$Y$&#26368;&#20339;&#65288;&#25110;&#36817;&#20046;&#26368;&#20339;&#65289;&#26399;&#26395;&#32467;&#26524;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20854;&#20013;$\delta$&#26159;&#32473;&#23450;&#30340;&#32622;&#20449;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#31867;&#22411;&#22240;&#26524;&#27169;&#22411;&#8212;&#8212;&#20108;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;BGLM&#65289; &#21644;&#19968;&#33324;&#22270;&#30340;&#31532;&#19968;&#31181;&#38388;&#38548;&#20381;&#36182;&#24615;&#21644;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;&#23545;&#20110;BGLM&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#39318;&#27425;&#19987;&#38376;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#35774;&#35745;&#30340;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32780;&#25152;&#26377;&#29616;&#26377;&#30340;&#19968;&#33324;&#22270;&#31639;&#27861;&#37117;&#20855;&#26377;&#25351;&#25968;&#22797;&#26434;&#24230;&#21040;&#22270;&#22823;&#23567;&#25110;&#19968;&#20123;&#19981;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#23545;&#20110;&#19968;&#33324;&#22270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20165;&#23545;&#25968;&#20110;&#22270;&#22823;&#23567;&#21644;&#26410;&#30693;&#22240;&#26524;&#25928;&#24212;&#30340;&#25968;&#37327;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#25269;&#24481;&#38544;&#34255;&#24615;&#28151;&#28102;&#22240;&#32032;&#21644;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#19979;&#30028;&#65292;&#19982;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#34920;&#26126;&#20102;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combinatorial pure exploration of causal bandits is the following online learning task: given a causal graph with unknown causal inference distributions, in each round we choose a subset of variables to intervene or do no intervention, and observe the random outcomes of all random variables, with the goal that using as few rounds as possible, we can output an intervention that gives the best (or almost best) expected outcome on the reward variable $Y$ with probability at least $1-\delta$, where $\delta$ is a given confidence level. We provide the first gap-dependent and fully adaptive pure exploration algorithms on two types of causal models -- the binary generalized linear model (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed specifically for this setting and achieves polynomial sample complexity, while all existing algorithms for general graphs have either sample complexity exponential to the graph size or some unreasonable assumptions. For general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#35270;&#35273;Transformers&#20013;&#24341;&#20837;&#23616;&#37096;&#20559;&#32622;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#26469;&#40723;&#21169;&#31354;&#38388;&#32858;&#31867;&#20316;&#20026;&#35757;&#32451;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#30340;&#35821;&#20041;&#20998;&#21106;&#32467;&#26500;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26679;&#26412;&#25968;&#37327;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.04636</link><description>&lt;p&gt;
&#31354;&#38388;&#29109;&#20316;&#20026;&#35270;&#35273;Transformers&#30340;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Spatial Entropy as an Inductive Bias for Vision Transformers. (arXiv:2206.04636v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#35270;&#35273;Transformers&#20013;&#24341;&#20837;&#23616;&#37096;&#20559;&#32622;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#26469;&#40723;&#21169;&#31354;&#38388;&#32858;&#31867;&#20316;&#20026;&#35757;&#32451;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#30340;&#35821;&#20041;&#20998;&#21106;&#32467;&#26500;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26679;&#26412;&#25968;&#37327;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#35270;&#35273;Transformers&#65288;VT&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24341;&#20837;VT&#26550;&#26500;&#20013;&#30340;&#23616;&#37096;&#24402;&#32435;&#20559;&#32622;&#26377;&#21161;&#20110;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#26550;&#26500;&#20462;&#25913;&#20250;&#23548;&#33268;Transformer&#39592;&#24178;&#30340;&#36890;&#29992;&#24615;&#30340;&#25439;&#22833;&#65292;&#36825;&#37096;&#20998;&#36829;&#32972;&#20102;&#25512;&#21160;&#24320;&#21457;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20849;&#20139;&#30340;&#32479;&#19968;&#26550;&#26500;&#30340;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#34917;&#20805;&#26041;&#21521;&#65292;&#21363;&#20351;&#29992;&#36741;&#21161;&#33258;&#30417;&#30563;&#20219;&#21153;&#26469;&#24341;&#20837;&#23616;&#37096;&#20559;&#32622;&#65292;&#21516;&#26102;&#19982;&#26631;&#20934;&#30340;&#26377;&#30417;&#30563;&#35757;&#32451;&#19968;&#36215;&#36827;&#34892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;VT&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#22312;&#36827;&#34892;&#33258;&#30417;&#30563;&#35757;&#32451;&#26102;&#21487;&#33021;&#21253;&#21547;&#30340;&#35821;&#20041;&#20998;&#21106;&#32467;&#26500;&#65292;&#22312;&#26377;&#30417;&#30563;&#35757;&#32451;&#26102;&#19981;&#20250;&#33258;&#21160;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#40723;&#21169;&#31354;&#38388;&#32858;&#31867;&#20316;&#20026;&#35757;&#32451;&#27491;&#21017;&#21270;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Recent work on Vision Transformers (VTs) showed that introducing a local inductive bias in the VT architecture helps reducing the number of samples necessary for training. However, the architecture modifications lead to a loss of generality of the Transformer backbone, partially contradicting the push towards the development of uniform architectures, shared, e.g., by both the Computer Vision and the Natural Language Processing areas. In this work, we propose a different and complementary direction, in which a local bias is introduced using an auxiliary self-supervised task, performed jointly with standard supervised training. Specifically, we exploit the observation that the attention maps of VTs, when trained with self-supervision, can contain a semantic segmentation structure which does not spontaneously emerge when training is supervised. Thus, we explicitly encourage the emergence of this spatial clustering as a form of training regularization. In more detail, we exploit the assump
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#21435;&#38500;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#20195;&#29702;&#65292;&#24182;&#26681;&#25454;&#21435;&#38500;&#23398;&#20064;&#24847;&#22270;&#36827;&#34892;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2205.15567</link><description>&lt;p&gt;
&#27169;&#22411;&#21453;&#28436;&#23454;&#29616;&#23569;&#26679;&#26412;&#21435;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Unlearning by Model Inversion. (arXiv:2205.15567v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15567
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#21435;&#38500;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#20195;&#29702;&#65292;&#24182;&#26681;&#25454;&#21435;&#38500;&#23398;&#20064;&#24847;&#22270;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#26426;&#22120;&#21435;&#38500;&#23398;&#20064;&#24773;&#22659;&#65292;&#21363;&#21435;&#38500;&#19968;&#20010;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#20197;&#28040;&#38500;&#24050;&#35757;&#32451;&#27169;&#22411;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#20934;&#21435;&#38500;&#23398;&#20064;&#24773;&#22659;&#19979;&#65292;&#30446;&#26631;&#25968;&#25454;&#38598;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#21487;&#20197;&#23436;&#20840;&#30830;&#23450;&#30340;&#12290;&#20294;&#26159;&#22914;&#26524;&#35757;&#32451;&#25968;&#25454;&#38598;&#22312;&#21435;&#38500;&#23398;&#20064;&#26102;&#19981;&#21487;&#35775;&#38382;&#65292;&#21017;&#20960;&#20046;&#19981;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#35782;&#21035;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#32771;&#34385;&#23569;&#26679;&#26412;&#21435;&#38500;&#23398;&#20064;&#24773;&#22659;&#65292;&#21363;&#20165;&#26377;&#23569;&#37327;&#30446;&#26631;&#25968;&#25454;&#26679;&#26412;&#21487;&#29992;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#65288;i&#65289;&#36890;&#36807;&#27169;&#22411;&#21453;&#28436;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#20195;&#29702;&#65307;&#65288;ii&#65289;&#26681;&#25454;&#21435;&#38500;&#23398;&#20064;&#30340;&#24847;&#22270;&#35843;&#25972;&#20195;&#29702;&#65307;&#65288;iii&#65289;&#29992;&#35843;&#25972;&#21518;&#30340;&#20195;&#29702;&#26356;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a practical scenario of machine unlearning to erase a target dataset, which causes unexpected behavior from the trained model. The target dataset is often assumed to be fully identifiable in a standard unlearning scenario. Such a flawless identification, however, is almost impossible if the training dataset is inaccessible at the time of unlearning. Unlike previous approaches requiring a complete set of targets, we consider few-shot unlearning scenario when only a few samples of target data are available. To this end, we formulate the few-shot unlearning problem specifying intentions behind the unlearning request (e.g., purely unlearning, mislabel correction, privacy protection), and we devise a straightforward framework that (i) retrieves a proxy of the training data via model inversion fully exploiting information available in the context of unlearning; (ii) adjusts the proxy according to the unlearning intention; and (iii) updates the model with the adjusted proxy. We de
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item><item><title>CycleSense&#26159;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#21160;&#20316;&#20256;&#24863;&#22120;&#26816;&#27979;&#33258;&#34892;&#36710;&#20132;&#36890;&#20013;&#30340;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#34892;&#36710;&#25163;&#26356;&#36731;&#26494;&#22320;&#25253;&#21578;&#38505;&#24773;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#20351;&#29992;&#21487;&#20197;&#24110;&#21161;&#22478;&#24066;&#35268;&#21010;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#34892;&#36710;&#23433;&#20840;&#24863;&#38382;&#39064;&#65292;&#26377;&#26395;&#32531;&#35299;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.10416</link><description>&lt;p&gt;
CycleSense&#65306;&#21033;&#29992;&#31227;&#21160;&#21160;&#20316;&#20256;&#24863;&#22120;&#26816;&#27979;&#33258;&#34892;&#36710;&#20132;&#36890;&#20013;&#30340;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CycleSense: Detecting Near Miss Incidents in Bicycle Traffic from Mobile Motion Sensors. (arXiv:2204.10416v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10416
&lt;/p&gt;
&lt;p&gt;
CycleSense&#26159;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#21160;&#20316;&#20256;&#24863;&#22120;&#26816;&#27979;&#33258;&#34892;&#36710;&#20132;&#36890;&#20013;&#30340;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#34892;&#36710;&#25163;&#26356;&#36731;&#26494;&#22320;&#25253;&#21578;&#38505;&#24773;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#20351;&#29992;&#21487;&#20197;&#24110;&#21161;&#22478;&#24066;&#35268;&#21010;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#33258;&#34892;&#36710;&#23433;&#20840;&#24863;&#38382;&#39064;&#65292;&#26377;&#26395;&#32531;&#35299;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#30340;&#22478;&#24066;&#20013;&#65292;&#27773;&#36710;&#24341;&#36215;&#20102;&#20581;&#24247;&#21644;&#20132;&#36890;&#38382;&#39064;&#65292;&#22686;&#21152;&#33258;&#34892;&#36710;&#30340;&#20351;&#29992;&#29575;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#30001;&#20110;&#32570;&#20047;&#24863;&#30693;&#23433;&#20840;&#26469;&#36991;&#20813;&#39569;&#33258;&#34892;&#36710;&#12290;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#32773;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24456;&#38590;&#65292;&#22240;&#20026;&#20182;&#20204;&#32570;&#20047;&#20851;&#20110;&#33258;&#34892;&#36710;&#23433;&#20840;&#24863;&#30340;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#26679;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#20043;&#21069;&#25552;&#20986;&#20102;&#20247;&#21253;&#24179;&#21488;SimRa&#65292;&#23427;&#20801;&#35768;&#33258;&#34892;&#36710;&#25163;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#35760;&#24405;&#20854;&#39569;&#34892;&#24182;&#25253;&#21578;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CycleSense&#65292;&#36825;&#26159;&#19968;&#31181;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#26816;&#27979;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;&#65292;&#20174;&#32780;&#20351;&#25830;&#32937;&#32780;&#36807;&#20107;&#20214;&#25253;&#21578;&#26356;&#21152;&#23481;&#26131;&#12290;&#20351;&#29992;SimRa&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;SimRa&#20351;&#29992;&#30340;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;CycleSense&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#30528;&#25913;&#21892;&#20102;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cities worldwide, cars cause health and traffic problems whichcould be partly mitigated through an increased modal share of bicycles. Many people, however, avoid cycling due to a lack of perceived safety. For city planners, addressing this is hard as they lack insights intowhere cyclists feel safe and where they do not. To gain such insights,we have in previous work proposed the crowdsourcing platform SimRa,which allows cyclists to record their rides and report near miss incidentsvia a smartphone app. In this paper, we present CycleSense, a combination of signal pro-cessing and Machine Learning techniques, which partially automatesthe detection of near miss incidents, thus making the reporting of nearmiss incidents easier. Using the SimRa data set, we evaluate CycleSenseby comparing it to a baseline method used by SimRa and show that itsignificantly improves incident detection.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.13059</link><description>&lt;p&gt;
&#29992;&#29109;&#36817;&#20284;&#30340;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13059
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#26080;&#27861;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20197;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21333;&#23792;&#30340;&#39640;&#26031;&#20998;&#24067;&#36890;&#24120;&#34987;&#36873;&#25321;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#65292;&#24456;&#38590;&#36924;&#36817;&#22810;&#23792;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#22914;&#20309;&#36817;&#20284;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35745;&#31639;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30495;&#23454;&#29109;&#19982;&#36817;&#20284;&#29109;&#20043;&#38388;&#30340;&#36817;&#20284;&#35823;&#24046;&#65292;&#20197;&#20415;&#25581;&#31034;&#25105;&#20204;&#30340;&#36817;&#20284;&#20309;&#26102;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36817;&#20284;&#35823;&#24046;&#30001;&#39640;&#26031;&#28151;&#21512;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#19982;&#26041;&#24046;&#20043;&#21644;&#30340;&#27604;&#29575;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#24403;&#39640;&#26031;&#28151;&#21512;&#32452;&#20214;&#30340;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36817;&#20284;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;</title><link>http://arxiv.org/abs/2202.02113</link><description>&lt;p&gt;
&#20174;&#21306;&#20998;&#21040;&#29983;&#25104;&#65306;&#22522;&#20110;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#35299;&#20915;&#20102;&#25193;&#23637;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20316;GenKGC&#30340;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20197;&#21069;&#20855;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#65292;&#20379;&#30740;&#31350;&#30446;&#30340;&#20351;&#29992;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/GenKGC&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#26410;&#30693;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#26102;&#65292;&#23384;&#22312;&#36951;&#25022;&#19979;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#19979;&#38480;&#30340;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#36890;&#36807;&#23545;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20934;&#30830;&#25429;&#25417;&#65292;&#25105;&#20204;&#35777;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2201.01680</link><description>&lt;p&gt;
&#23398;&#20064;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#31995;&#32479;&#30340;&#36951;&#25022;&#19979;&#38480;
&lt;/p&gt;
&lt;p&gt;
Regret Lower Bounds for Learning Linear Quadratic Gaussian Systems. (arXiv:2201.01680v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#26410;&#30693;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#26102;&#65292;&#23384;&#22312;&#36951;&#25022;&#19979;&#38480;&#65292;&#24182;&#19988;&#36825;&#20010;&#19979;&#38480;&#30340;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#36890;&#36807;&#23545;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20934;&#30830;&#25429;&#25417;&#65292;&#25105;&#20204;&#35777;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#33258;&#36866;&#24212;&#25511;&#21046;&#26410;&#30693;&#30340;&#32447;&#24615;&#39640;&#26031;&#31995;&#32479;&#19982;&#20108;&#27425;&#20195;&#20215;&#24314;&#31435;&#36951;&#25022;&#19979;&#38480;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#23454;&#39564;&#35774;&#35745;&#12289;&#20272;&#35745;&#29702;&#35770;&#21644;&#26576;&#20123;&#20449;&#24687;&#30697;&#38453;&#30340;&#25200;&#21160;&#30028;&#38480;&#30340;&#24605;&#24819;&#65292;&#24471;&#21040;&#20102;&#20851;&#20110;&#26102;&#38388;&#36328;&#24230;$T$&#30340;&#36951;&#25022;&#19979;&#38480;&#65292;&#20854;&#27604;&#20363;&#23610;&#24230;&#32423;&#21035;&#20026; $\sqrt{T}$&#12290;&#25105;&#20204;&#30340;&#19979;&#38480;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#25511;&#21046;&#29702;&#35770;&#21442;&#25968;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#34920;&#26126;&#38590;&#20197;&#25511;&#21046;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#65307;&#24403;&#20855;&#20307;&#21270;&#20026;&#29366;&#24577;&#21453;&#39304;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#26089;&#26399;&#24037;&#20316;&#30340;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#25913;&#21892;&#20102;&#38543;&#31995;&#32479;&#29702;&#35770;&#24120;&#25968;&#65288;&#22914;&#31995;&#32479;&#25104;&#26412;&#21644;&#26684;&#25289;&#31859;&#24681;&#30697;&#38453;&#65289;&#30340;&#27604;&#20363;&#23610;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#19968;&#31867;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20855;&#26377;&#36739;&#24046;&#21487;&#35266;&#27979;&#32467;&#26500;&#30340;&#31995;&#32479;&#20063;&#38590;&#20197;&#23398;&#20064;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
TWe establish regret lower bounds for adaptively controlling an unknown linear Gaussian system with quadratic costs. We combine ideas from experiment design, estimation theory and a perturbation bound of certain information matrices to derive regret lower bounds exhibiting scaling on the order of magnitude $\sqrt{T}$ in the time horizon $T$. Our bounds accurately capture the role of control-theoretic parameters and we are able to show that systems that are hard to control are also hard to learn to control; when instantiated to state feedback systems we recover the dimensional dependency of earlier work but with improved scaling with system-theoretic constants such as system costs and Gramians. Furthermore, we extend our results to a class of partially observed systems and demonstrate that systems with poor observability structure also are hard to learn to control.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2110.10325</link><description>&lt;p&gt;
&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#21450;&#20854;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-Step Abductive Multi-Target Learning with Diverse Noisy Samples and Its Application to Tumour Segmentation for Breast Cancer. (arXiv:2110.10325v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#22788;&#29702;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#22312;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32467;&#21512;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#35825;&#23548;&#23398;&#20064;&#65292;&#22312;&#21457;&#26126;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;OSAMTL&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#21463;&#35825;&#23548;&#23398;&#20064;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#24335;&#31616;&#21333;&#22320;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21333;&#20010;&#22024;&#26434;&#26631;&#31614;&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#30340;&#26377;&#25928;&#24615;&#12290;&#20294;&#26159;&#65292;OSAMTL&#19981;&#36866;&#29992;&#20110;&#25552;&#20379;&#22810;&#31181;&#22024;&#26434;&#26679;&#26412;&#65288;DiNS&#65289;&#30340;&#23398;&#20064;&#20219;&#21153;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;DiNS&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#27493;&#24335;&#35825;&#23548;&#24335;&#22810;&#30446;&#26631;&#23398;&#20064;&#19982;DiNS&#65288;OSAMTL-DiNS&#65289;&#65292;&#20197;&#25193;&#23637;&#21407;&#22987;&#30340;OSAMTL&#20197;&#22788;&#29702;DiNS&#30340;&#22797;&#26434;&#22122;&#22768;&#26631;&#31614;&#12290;&#23558;OSAMTL-DiNS&#24212;&#29992;&#20110;MHWSIA&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the effectiveness of the combination of machine learning and logical reasoning, including data-driven logical reasoning, knowledge driven machine learning and abductive learning, in inventing advanced artificial intelligence technologies. One-step abductive multi-target learning (OSAMTL), an approach inspired by abductive learning, via simply combining machine learning and logical reasoning in a one-step balanced way, has as well shown its effectiveness in handling complex noisy labels of a single noisy sample in medical histopathology whole slide image analysis (MHWSIA). However, OSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are provided for a learning task. In this paper, giving definition of DiNS, we propose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to expand the original OSAMTL to handle complex noisy labels of DiNS. Applying OSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26399;&#26395;&#36317;&#31163;&#30340;&#20998;&#24067;&#32858;&#31867;&#25216;&#26415;&#65292;&#21487;&#20197;&#38477;&#20302;&#22122;&#22768;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2110.08871</link><description>&lt;p&gt;
&#22522;&#20110;&#26399;&#26395;&#36317;&#31163;&#30340;&#20998;&#24067;&#32858;&#31867;&#26041;&#27861;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expectation Distance-based Distributional Clustering for Noise-Robustness. (arXiv:2110.08871v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26399;&#26395;&#36317;&#31163;&#30340;&#20998;&#24067;&#32858;&#31867;&#25216;&#26415;&#65292;&#21487;&#20197;&#38477;&#20302;&#22122;&#22768;&#23545;&#32858;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#25216;&#26415;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#32858;&#31867;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#23558;&#25968;&#25454;&#20998;&#37197;&#21040;&#20854;&#20998;&#24067;&#30340;&#31751;&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#21363;&#26399;&#26395;&#36317;&#31163;&#65288;ED&#65289;&#65292;&#23427;&#36229;&#36234;&#20102;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#65288;$W_2$&#65289;&#30340;&#20998;&#24067;&#36317;&#31163;&#30340;&#29616;&#26377;&#25216;&#26415;&#65306;&#21518;&#32773;&#20165;&#20381;&#36182;&#20110;&#36793;&#32536;&#20998;&#24067;&#65292;&#32780;&#21069;&#32773;&#36824;&#20351;&#29992;&#32852;&#21512;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;&#21033;&#29992;ED&#65292;&#26412;&#25991;&#23558;&#20256;&#32479;&#30340;$K$-means&#21644;$K$-medoids&#32858;&#31867;&#25193;&#23637;&#21040;&#25968;&#25454;&#20998;&#24067;&#19978;&#65292;&#24182;&#20171;&#32461;&#20351;&#29992;$W_2$&#30340;$K$-medoids&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;$W_2$&#21644;ED&#36317;&#31163;&#24230;&#37327;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions. Using the ED, the paper extends the classical $K$-means and $K$-medoids clustering to those over data-distributions (rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also presents the closed-form expressions of the $W_2$ and ED distance measures. The implementation results of the proposed ED and the $W_2$ distance measures t
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21517;&#20026;Broad Ensemble Learning System (BELS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#21152;&#39640;&#25928;&#12289;&#31283;&#23450;&#22320;&#26356;&#26032;&#27169;&#22411;&#65292;&#25552;&#39640;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.03540</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#23545;&#25968;&#25454;&#27969;&#20998;&#31867;&#27010;&#24565;&#28418;&#31227;&#30340;&#24191;&#20041;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Broad Ensemble Learning System for Drifting Stream Classification. (arXiv:2110.03540v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03540
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21517;&#20026;Broad Ensemble Learning System (BELS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#27969;&#20998;&#31867;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#21152;&#39640;&#25928;&#12289;&#31283;&#23450;&#22320;&#26356;&#26032;&#27169;&#22411;&#65292;&#25552;&#39640;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#27969;&#29615;&#22659;&#20013;&#65292;&#20998;&#31867;&#27169;&#22411;&#24517;&#39035;&#26377;&#25928;&#22320;&#22788;&#29702;&#27010;&#24565;&#28418;&#31227;&#12290;&#38598;&#25104;&#26041;&#27861;&#26159;&#20026;&#27492;&#30446;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#22823;&#22359;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#35201;&#20040;&#36880;&#20010;&#23398;&#20064;&#25968;&#25454;&#12290;&#21069;&#32773;&#21487;&#33021;&#20250;&#38169;&#36807;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#21464;&#21270;&#65292;&#21518;&#32773;&#21017;&#21487;&#33021;&#21463;&#21040;&#25928;&#29575;&#21644;&#19981;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;Broad Learning System (BLS)&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#27599;&#27425;&#26356;&#26032;&#26102;&#20351;&#29992;&#23567;&#22359;&#25968;&#25454;&#12290;BLS&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#36731;&#37327;&#32423;&#31070;&#32463;&#32467;&#26500;&#65292;&#26368;&#36817;&#34987;&#24320;&#21457;&#29992;&#20110;&#22686;&#37327;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#36895;&#24230;&#24456;&#24555;&#65292;&#20294;&#23427;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#22359;&#26469;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#32780;&#19988;&#19981;&#33021;&#22788;&#29702;&#25968;&#25454;&#27969;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;Broad Ensemble Learning System (BELS)&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26356;&#26032;&#26041;&#24335;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#20351;&#29992;&#36755;&#20986;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
In a data stream environment, classification models must handle concept drift efficiently and effectively. Ensemble methods are widely used for this purpose; however, the ones available in the literature either use a large data chunk to update the model or learn the data one by one. In the former, the model may miss the changes in the data distribution, and in the latter, the model may suffer from inefficiency and instability. To address these issues, we introduce a novel ensemble approach based on the Broad Learning System (BLS), where mini chunks are used at each update. BLS is an effective lightweight neural architecture recently developed for incremental learning. Although it is fast, it requires huge data chunks for effective updates, and is unable to handle dynamic changes observed in data streams. Our proposed approach named Broad Ensemble Learning System (BELS) uses a novel updating method that significantly improves best-in-class model accuracy. It employs an ensemble of outpu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24182;&#24494;&#35843;Transformer&#27169;&#22411;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23569;1.5&#20010;&#25968;&#37327;&#32423;&#65292;&#19988;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.03501</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#31526;&#21495;&#25968;&#23398;&#27714;&#35299;&#22120;&#65281;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24182;&#24494;&#35843;Transformer&#27169;&#22411;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23569;1.5&#20010;&#25968;&#37327;&#32423;&#65292;&#19988;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#38382;&#39064;&#19968;&#30452;&#26159;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#21644;&#37325;&#22797;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;transformer&#20043;&#31867;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35757;&#32451;&#20026;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36825;&#20123;&#22823;&#22411;Transformer&#27169;&#22411;&#38656;&#35201;&#26497;&#20854;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#31526;&#21495;&#25968;&#23398;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26679;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#31526;&#21495;&#20219;&#21153;&#65292;&#39318;&#20808;&#36890;&#36807;&#35821;&#35328;&#32763;&#35793;&#23545;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20197;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#20351;&#29992;&#20102;&#22823;&#32422;1.5&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36798;&#21040;&#20102;&#19982;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19982;&#38024;&#23545;&#31526;&#21495;&#25968;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#24494;&#20998;&#26041;&#31243;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#20013;&#22235;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#29575;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2110.01303</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#20284;&#24230;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22686;&#37327;&#24335;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#20013;&#22235;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#29575;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#36827;&#34892;&#22686;&#37327;&#24335;&#23398;&#20064;&#26102;&#23481;&#26131;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#25506;&#32034;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23398;&#20064;&#31639;&#27861;&#31561;&#22240;&#32032;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#30456;&#20284;&#24615;&#23398;&#20064;&#36880;&#28176;&#34987;&#24212;&#29992;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22240;&#27492;&#20102;&#35299;&#30456;&#20284;&#24615;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#20013;&#30340;&#34920;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#22312;&#22235;&#31181;&#33879;&#21517;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#35282;&#24230;&#25439;&#22833;&#12289;&#23545;&#27604;&#25439;&#22833;&#12289;&#20013;&#24515;&#25439;&#22833;&#21644;&#19977;&#20803;&#25439;&#22833;&#65289;&#20013;&#36827;&#34892;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#29575;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#25968;&#25454;&#38598;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#29575;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;&#35282;&#24230;&#25439;&#22833;&#21463;&#24433;&#21709;&#26368;&#23567;&#65292;&#20854;&#27425;&#26159;&#23545;&#27604;&#25439;&#22833;&#12289;&#19977;&#20803;&#25439;&#22833;&#21644;&#20013;&#24515;&#25439;&#22833;&#65288;&#21033;&#29992;&#25366;&#25496;&#25216;&#26415;&#24615;&#33021;&#36739;&#22909;&#65289;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#65306;iCaRL&#12289;EWC&#21644;EBLL&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#19982;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21644;&#25193;&#23637;&#27979;&#35797;&#19982;&#25351;&#26631;&#65292;&#20855;&#26377;&#39640;&#25928;&#20934;&#30830;&#21644;&#26377;&#25928;&#24615;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2109.09500</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Estimation and Goodness-of-Fit for Large-Scale Confirmatory Item Factor Analysis. (arXiv:2109.09500v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#19982;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21644;&#25193;&#23637;&#27979;&#35797;&#19982;&#25351;&#26631;&#65292;&#20855;&#26377;&#39640;&#25928;&#20934;&#30830;&#21644;&#26377;&#25928;&#24615;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#35268;&#27169;&#39564;&#35777;&#39033;&#30446;&#22240;&#32032;&#20998;&#26512;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#21644;&#25311;&#21512;&#24230;&#26816;&#39564;&#26041;&#27861;&#12290;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#23558;Urban&#21644;Bauer&#65288;2021&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25193;&#23637;&#21040;&#39564;&#35777;&#24615;&#22240;&#32032;&#20998;&#26512;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22240;&#23376;&#36733;&#33655;&#21644;&#22240;&#23376;&#30456;&#20851;&#24615;&#30340;&#38480;&#21046;&#12290;&#23545;&#20110;&#25311;&#21512;&#24230;&#26816;&#39564;&#65292;&#25105;&#20204;&#25506;&#32034;&#22522;&#20110;&#27169;&#25311;&#30340;&#27979;&#35797;&#21644;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#20998;&#31867;&#22120;&#20004;&#20010;&#26679;&#26412;&#27979;&#35797;&#65288;C2ST&#65289;&#65292;&#35813;&#26041;&#27861;&#27979;&#35797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#21306;&#20998;&#26469;&#33258;&#25311;&#21512;&#30340;IFA&#27169;&#22411;&#30340;&#35266;&#27979;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#25193;&#23637;&#21253;&#25324;&#36817;&#20284;&#25311;&#21512;&#26816;&#39564;&#65292;&#20854;&#20013;&#29992;&#25143;&#25351;&#23450;&#35266;&#27979;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20013;&#24212;&#26377;&#22810;&#23569;&#21344;&#21487;&#21306;&#20998;&#37096;&#20998;&#30340;&#30334;&#20998;&#27604;&#65292;&#20197;&#21450;&#30456;&#23545;&#25311;&#21512;&#25351;&#25968;&#65288;RFI&#65289;&#65292;&#35813;&#25351;&#25968;&#31867;&#20284;&#20110;&#32467;&#26500;&#26041;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;RFI&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65306;&#65288;1&#65289;Urban&#21644;Bauer&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#25193;&#23637;&#21363;&#20351;&#23384;&#22312;&#39640;&#30456;&#20851;&#22240;&#23376;&#20063;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#65307;&#65288;2&#65289;&#25152;&#25552;&#20986;&#30340;&#25311;&#21512;&#24230;&#25351;&#26631;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#27169;&#22411;&#19981;&#33391;&#25311;&#21512;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#39564;&#35777;IFA&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate novel parameter estimation and goodness-of-fit (GOF) assessment methods for large-scale confirmatory item factor analysis (IFA) with many respondents, items, and latent factors. For parameter estimation, we extend Urban and Bauer's (2021) deep learning algorithm for exploratory IFA to the confirmatory setting by showing how to handle constraints on loadings and factor correlations. For GOF assessment, we explore simulation-based tests and indices that extend the classifier two-sample test (C2ST), a method that tests whether a deep neural network can distinguish between observed data and synthetic data sampled from a fitted IFA model. Proposed extensions include a test of approximate fit wherein the user specifies what percentage of observed and synthetic data should be distinguishable as well as a relative fit index (RFI) that is similar in spirit to the RFIs used in structural equation modeling. Via simulation studies, we show that: (1) the confirmatory extension of Urb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#28216;&#25103;&#20013;Nash&#24179;&#34913;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#32463;&#20856;&#27714;&#35299;&#22120;&#21152;&#36895;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2108.07472</link><description>&lt;p&gt;
Nash&#24179;&#34913;&#36924;&#36817;&#22120;&#21487;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Nash Equilibrium Approximator Learnable?. (arXiv:2108.07472v6 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#28216;&#25103;&#20013;Nash&#24179;&#34913;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#32463;&#20856;&#27714;&#35299;&#22120;&#21152;&#36895;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36924;&#36817;&#29983;&#25104;&#28216;&#25103;&#30340;&#20998;&#24067;&#20013;&#30340;Nash&#22343;&#34913;&#65288;NE&#65289;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;Probably Approximately Correct&#65288;PAC&#65289;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#27867;&#21270;&#36793;&#30028;&#12290;&#35813;&#36793;&#30028;&#25551;&#36848;&#20102;NE&#36924;&#36817;&#22120;&#30340;&#26399;&#26395;&#25439;&#22833;&#21644;&#32463;&#39564;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Nash&#36924;&#36817;&#22120;&#30340;agnostic PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#38500;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;NE&#36924;&#36817;&#22120;&#30340;&#19968;&#31181;&#24212;&#29992;&#12290;&#35757;&#32451;&#30340;NE&#36924;&#36817;&#22120;&#21487;&#20197;&#29992;&#20110;&#21551;&#21160;&#21644;&#21152;&#36895;&#32463;&#20856;NE&#27714;&#35299;&#22120;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20989;&#25968;&#36924;&#36817;&#21487;&#20197;&#23454;&#29616;&#36924;&#36817;NE&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the learnability of the function approximator that approximates Nash equilibrium (NE) for games generated from a distribution. First, we offer a generalization bound using the Probably Approximately Correct (PAC) learning model. The bound describes the gap between the expected loss and empirical loss of the NE approximator. Afterward, we prove the agnostic PAC learnability of the Nash approximator. In addition to theoretical analysis, we demonstrate an application of NE approximator in experiments. The trained NE approximator can be used to warm-start and accelerate classical NE solvers. Together, our results show the practicability of approximating NE through function approximation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.12220</link><description>&lt;p&gt;
&#24605;&#36335;&#27969;&#32593;&#32476;&#65306;&#20174;&#21333;&#19968;&#39044;&#27979;&#21040;&#27169;&#22411;&#24605;&#36335;&#30340;&#20018;&#32852;
&lt;/p&gt;
&lt;p&gt;
Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#36890;&#24120;&#20250;&#21019;&#24314;&#19968;&#31995;&#21015;&#24605;&#36335;&#65288;&#28041;&#21450;&#30452;&#35273;&#20915;&#31574;&#12289;&#21453;&#24605;&#12289;&#38169;&#35823;&#26356;&#27491;&#31561;&#65289;&#20197;&#36798;&#25104;&#20915;&#23450;&#12290;&#20294;&#26159;&#65292;&#22914;&#20170;&#30340;&#27169;&#22411;&#22823;&#22810;&#34987;&#35757;&#32451;&#20026;&#23558;&#36755;&#20837;&#26144;&#23556;&#21040;&#21333;&#19968;&#19988;&#22266;&#23450;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35753;&#27169;&#22411;&#26377;&#31532;&#20108;&#12289;&#31532;&#19977;&#21644;&#31532; k &#27425;&#24605;&#32771;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#20174;&#40657;&#26684;&#23572;&#30340;&#36777;&#35777;&#27861;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#24605;&#36335;&#27969;&#30340;&#27010;&#24565;&#65292;&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#22522;&#20110;&#27491;&#30830;&#24615;&#39044;&#27979;&#30340;&#26799;&#24230;&#25191;&#34892;&#36845;&#20195;&#39044;&#27979;&#26356;&#26032;&#12290;&#25105;&#20204;&#20197;&#38382;&#31572;&#20026;&#20363;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#65288;i&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32416;&#27491;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24605;&#36335;&#27969;&#30340;&#35821;&#20041;&#26657;&#39564;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans solve complex problems, they typically create a sequence of ideas (involving an intuitive decision, reflection, error correction, etc.) in order to reach a conclusive decision. Contrary to this, today's models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. Taking inspiration from Hegel's dialectics, we propose the concept of a thought flow which creates a sequence of predictions. We present a self-correction mechanism that is trained to estimate the model's correctness and performs iterative prediction updates based on the correctness prediction's gradient. We introduce our method at the example of question answering and conduct extensive experiments that demonstrate (i) our method's ability to correct its own predictions and (ii) its potential to notably improve model performances. In addition, we conduct a qualitative analysis of thought flow cor
&lt;/p&gt;</description></item><item><title>VIDA&#26159;&#19968;&#31181;&#38899;&#35270;&#39057;&#32467;&#21512;&#30340;&#21435;&#28151;&#21709;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21435;&#38500;&#28151;&#21709;&#65292;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; SoundSpaces-Speech&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#21508;&#31181;&#25151;&#38388;&#22768;&#23398;&#30340;&#36924;&#30495;&#35821;&#38899;&#22768;&#23398;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2106.07732</link><description>&lt;p&gt;
&#38899;&#39057;-&#35270;&#39057;&#21435;&#28151;&#21709;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Audio-Visual Dereverberation. (arXiv:2106.07732v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07732
&lt;/p&gt;
&lt;p&gt;
VIDA&#26159;&#19968;&#31181;&#38899;&#35270;&#39057;&#32467;&#21512;&#30340;&#21435;&#28151;&#21709;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21435;&#38500;&#28151;&#21709;&#65292;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290; SoundSpaces-Speech&#26159;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#30495;&#23454;&#19990;&#30028;&#20013;&#21508;&#31181;&#25151;&#38388;&#22768;&#23398;&#30340;&#36924;&#30495;&#35821;&#38899;&#22768;&#23398;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21709;&#19981;&#20165;&#20250;&#38477;&#20302;&#20154;&#31867;&#24863;&#30693;&#35821;&#38899;&#30340;&#36136;&#37327;&#65292;&#36824;&#20250;&#20005;&#37325;&#24433;&#21709;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#38899;&#35270;&#39057;&#35266;&#27979;&#23398;&#20064;&#21435;&#28151;&#21709;&#35828;&#35805;&#30340;&#24819;&#27861;&#12290;&#20154;&#31867;&#35828;&#35805;&#32773;&#21608;&#22260;&#30340;&#35270;&#35273;&#29615;&#22659;&#25581;&#31034;&#20102;&#20851;&#20110;&#25151;&#38388;&#20960;&#20309;&#24418;&#29366;&#12289;&#26448;&#26009;&#21644;&#35828;&#35805;&#32773;&#20301;&#32622;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#23427;&#20204;&#37117;&#20250;&#24433;&#21709;&#28151;&#21709;&#30340;&#31934;&#30830;&#25928;&#26524;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#38899;&#39057;&#21435;&#28151;&#21709;&#8221;&#65288;VIDA&#65289;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#21333;&#22768;&#36947;&#22768;&#38899;&#21644;&#35270;&#35273;&#22330;&#26223;&#21435;&#38500;&#28151;&#21709;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;SoundSpaces-Speech&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;3D&#25151;&#23627;&#25195;&#25551;&#20013;&#20351;&#29992;&#36924;&#30495;&#30340;&#35821;&#38899;&#22768;&#23398;&#28210;&#26579;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#25151;&#38388;&#22768;&#23398;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#24433;&#20687;&#19978;&#23637;&#31034;&#20102;VIDA&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;VIDA&#20248;&#20110;&#22522;&#20110;&#38899;&#39057;&#30340;&#20808;&#21069;&#26041;&#27861;&#65292;&#34701;&#21512;&#35270;&#35273;&#20449;&#24687;&#26377;&#21161;&#20110;&#25429;&#25417;&#37325;&#35201;&#32447;&#32034;&#65292;&#20174;&#32780;&#20351;&#21435;&#28151;&#21709;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reverberation not only degrades the quality of speech for human perception, but also severely impacts the accuracy of automatic speech recognition. Prior work attempts to remove reverberation based on the audio modality only. Our idea is to learn to dereverberate speech from audio-visual observations. The visual environment surrounding a human speaker reveals important cues about the room geometry, materials, and speaker location, all of which influence the precise reverberation effects. We introduce Visually-Informed Dereverberation of Audio (VIDA), an end-to-end approach that learns to remove reverberation based on both the observed monaural sound and visual scene. In support of this new task, we develop a large-scale dataset SoundSpaces-Speech that uses realistic acoustic renderings of speech in real-world 3D scans of homes offering a variety of room acoustics. Demonstrating our approach on both simulated and real imagery for speech enhancement, speech recognition, and speaker ident
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#24133;&#24230;&#24341;&#23548;&#30340;&#21160;&#24577;&#39640;&#25928;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;DEAT&#65289;&#65292;&#36880;&#28176;&#22686;&#21152;&#20102;&#23545;&#25239;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21152;&#36895;&#31574;&#30053;M+&#21152;&#36895;&#65292;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#19988;&#31526;&#21512;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2103.03076</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#24133;&#24230;&#24341;&#23548;&#30340;&#21160;&#24577;&#39640;&#25928;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Dynamic Efficient Adversarial Training Guided by Gradient Magnitude. (arXiv:2103.03076v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#24133;&#24230;&#24341;&#23548;&#30340;&#21160;&#24577;&#39640;&#25928;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65288;DEAT&#65289;&#65292;&#36880;&#28176;&#22686;&#21152;&#20102;&#23545;&#25239;&#36845;&#20195;&#27425;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21152;&#36895;&#31574;&#30053;M+&#21152;&#36895;&#65292;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#19988;&#31526;&#21512;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#40065;&#26834;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20197;&#25269;&#24481;&#24378;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#24456;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#20854;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#39640;&#25928;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;&#65288;DEAT&#65289;&#65292;&#20854;&#20013;&#36880;&#28176;&#22686;&#21152;&#20102;&#23545;&#25239;&#36845;&#20195;&#27425;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#26799;&#24230;&#24133;&#24230;&#19982;&#35757;&#32451;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#26354;&#29575;&#30456;&#20851;&#65292;&#20174;&#32780;&#21453;&#26144;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#26799;&#24230;&#24133;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21152;&#36895;&#31574;&#30053;&#65292;M+&#21152;&#36895;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#24182;&#39640;&#25928;&#22320;&#35843;&#25972;&#35757;&#32451;&#36807;&#31243;&#65292;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#23427;&#36866;&#29992;&#20110;DEAT&#24182;&#20860;&#23481;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;M+&#21152;&#36895;&#31574;&#30053;&#26174;&#33879;&#21152;&#24555;&#20102;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is an effective but time-consuming way to train robust deep neural networks that can withstand strong adversarial attacks. As a response to its inefficiency, we propose Dynamic Efficient Adversarial Training (DEAT), which gradually increases the adversarial iteration during training. We demonstrate that the gradient's magnitude correlates with the curvature of the trained model's loss landscape, allowing it to reflect the effect of adversarial training. Therefore, based on the magnitude of the gradient, we propose a general acceleration strategy, M+ acceleration, which enables an automatic and highly effective method of adjusting the training procedure. M+ acceleration is computationally efficient and easy to implement. It is suited for DEAT and compatible with the majority of existing adversarial training techniques. Extensive experiments have been done on CIFAR-10 and ImageNet datasets with various training environments. The results show that the proposed M+ acce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#22122;&#22768;&#23384;&#20648;&#35774;&#22791;&#26102;&#30340;&#21387;&#32553;&#19982;&#23384;&#20648;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#27169;&#25311;&#23384;&#20648;&#35774;&#22791;&#30340;&#40065;&#26834;&#21387;&#32553;&#25216;&#26415;&#65292;&#30456;&#23545;&#20110;&#25968;&#23383;&#21387;&#32553;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;NN&#30340;&#23384;&#20648;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2102.07725</link><description>&lt;p&gt;
&#22122;&#22768;&#23384;&#20648;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Neural Network Compression for Noisy Storage Devices. (arXiv:2102.07725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#22122;&#22768;&#23384;&#20648;&#35774;&#22791;&#26102;&#30340;&#21387;&#32553;&#19982;&#23384;&#20648;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#27169;&#25311;&#23384;&#20648;&#35774;&#22791;&#30340;&#40065;&#26834;&#21387;&#32553;&#25216;&#26415;&#65292;&#30456;&#23545;&#20110;&#25968;&#23383;&#21387;&#32553;&#26041;&#27861;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;NN&#30340;&#23384;&#20648;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21442;&#25968;&#30340;&#21387;&#32553;&#21644;&#39640;&#25928;&#23384;&#20648;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;NN&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;NN&#21442;&#25968;&#30340;&#23454;&#38469;&#29289;&#29702;&#23384;&#20648;&#36827;&#34892;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#31867;&#27604;&#23384;&#20648;&#35774;&#22791;&#20316;&#20026;&#25968;&#23383;&#20171;&#36136;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36825;&#31181;&#26041;&#27861;&#33258;&#28982;&#22320;&#25552;&#20379;&#20102;&#19968;&#31181;&#28155;&#21152;&#26356;&#22810;&#37325;&#35201;&#20301;&#20445;&#25252;&#30340;&#26041;&#27861;&#65292;&#20294;&#23384;&#22312;&#22122;&#22768;&#24182;&#21487;&#33021;&#25439;&#23475;&#23384;&#20648;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#38024;&#23545;&#27169;&#25311;&#23384;&#20648;&#35774;&#22791;&#23646;&#24615;&#30340;&#40065;&#26834;NN&#21387;&#32553;&#25216;&#26415;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#27880;&#20837;&#22122;&#22768;&#30340;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#23383;&#21387;&#32553;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23558;NN&#30340;&#23384;&#20648;&#25928;&#29575;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compression and efficient storage of neural network (NN) parameters is critical for applications that run on resource-constrained devices. Despite the significant progress in NN model compression, there has been considerably less investigation in the actual \textit{physical} storage of NN parameters. Conventionally, model compression and physical storage are decoupled, as digital storage media with error-correcting codes (ECCs) provide robust error-free storage. However, this decoupled approach is inefficient as it ignores the overparameterization present in most NNs and forces the memory device to allocate the same amount of resources to every bit of information regardless of its importance. In this work, we investigate analog memory devices as an alternative to digital media -- one that naturally provides a way to add more protection for significant bits unlike its counterpart, but is noisy and may compromise the stored model's performance if used naively. We develop a variety of rob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/1902.06239</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#36175;&#35774;&#35745;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#26032;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
A new Potential-Based Reward Shaping for Reinforcement Learning Agent. (arXiv:1902.06239v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.06239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#35774;&#35745;&#65288;PBRS&#65289;&#26159;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#21033;&#29992;&#39069;&#22806;&#30693;&#35782;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#20854;&#20013;&#65292;&#20256;&#36882;&#23398;&#20064;&#20013;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#26159;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#25910;&#38598;&#21040;&#30340;&#30693;&#35782;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#32463;&#39564;&#30340;&#22870;&#36175;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#24615;&#30693;&#35782;&#26469;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential-based reward shaping (PBRS) is a particular category of machine learning methods which aims to improve the learning speed of a reinforcement learning agent by extracting and utilizing extra knowledge while performing a task. There are two steps in the process of transfer learning: extracting knowledge from previously learned tasks and transferring that knowledge to use it in a target task. The latter step is well discussed in the literature with various methods being proposed for it, while the former has been explored less. With this in mind, the type of knowledge that is transmitted is very important and can lead to considerable improvement. Among the literature of both the transfer learning and the potential-based reward shaping, a subject that has never been addressed is the knowledge gathered during the learning process itself. In this paper, we presented a novel potential-based reward shaping method that attempted to extract knowledge from the learning process. The propo
&lt;/p&gt;</description></item></channel></rss>