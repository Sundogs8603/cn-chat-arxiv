<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.13042</link><description>&lt;p&gt;
MosaicFusion: &#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13042
&lt;/p&gt;
&lt;p&gt;
MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MosaicFusion&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#31614;&#30417;&#30563;&#12290;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26377;&#29992;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23545;&#35937;&#23454;&#20363;&#21644;&#33945;&#29256;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30011;&#24067;&#20998;&#20026;&#20960;&#20010;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;&#19968;&#36718;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#22522;&#20110;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22810;&#20010;&#23454;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#19982;&#23545;&#35937;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#36328;&#27880;&#24847;&#21147;&#22270;&#22312;&#23618;&#21644;&#25193;&#25955;&#26102;&#38388;&#27493;&#19978;&#65292;&#28982;&#21518;&#36827;&#34892;&#31616;&#21333;&#30340;&#38408;&#20540;&#22788;&#29702;&#21644;&#36793;&#32536;&#24863;&#30693;&#30340;&#32454;&#21270;&#22788;&#29702;&#65292;&#24471;&#21040;&#30456;&#24212;&#30340;&#23454;&#20363;&#33945;&#29256;&#12290;&#25105;&#20204;&#30340;MosaicFusion&#21487;&#20197;&#20026;&#31232;&#32570;&#21644;&#26032;&#39062;&#31867;&#21035;&#20135;&#29983;&#22823;&#37327;&#30340;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#22788;&#29702;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;LVIS&#38271;&#23614;&#21644;&#24320;&#25918;&#35789;&#27719;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;V-PTR&#12290;&#19982;&#20854;&#20182;&#23398;&#20064;&#35270;&#39057;&#25968;&#25454;&#26041;&#27861;&#30456;&#27604;&#65292;&#36890;&#36807;&#20215;&#20540;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.13041</link><description>&lt;p&gt;
&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#39044;&#35757;&#32451;&#20174;&#32593;&#32476;&#35270;&#39057;&#20013;&#36827;&#34892;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robotic Offline RL from Internet Videos via Value-Function Pre-Training. (arXiv:2309.13041v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;V-PTR&#12290;&#19982;&#20854;&#20182;&#23398;&#20064;&#35270;&#39057;&#25968;&#25454;&#26041;&#27861;&#30456;&#27604;&#65292;&#36890;&#36807;&#20215;&#20540;&#23398;&#20064;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#26159;&#24191;&#27867;&#25512;&#24191;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#37027;&#20040;&#65292;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20013;&#21551;&#29992;&#36825;&#26679;&#30340;&#33021;&#21147;&#38656;&#35201;&#20160;&#20040;&#65311;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#36827;&#20837;&#26426;&#22120;&#20154;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#35270;&#39057;&#25968;&#25454;&#65288;&#22914;Ego4D&#65289;&#23384;&#22312;"&#31867;&#22411;&#19981;&#21305;&#37197;"&#65292;&#32780;&#35270;&#39057;&#25968;&#25454;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#26368;&#22823;&#20808;&#21069;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#35270;&#39057;&#25552;&#20379;&#20165;&#26377;&#35266;&#23519;&#32463;&#39564;&#65292;&#27809;&#26377;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25152;&#38656;&#30340;&#21160;&#20316;&#25110;&#22870;&#21169;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#23436;&#20840;&#22522;&#20110;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#65292;&#20174;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#21033;&#29992;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20854;&#20182;&#20174;&#35270;&#39057;&#25968;&#25454;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20215;&#20540;&#23398;&#20064;&#23545;&#20110;&#19979;&#28216;&#26426;&#22120;&#20154;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26356;&#26377;&#21033;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#34987;&#31216;&#20026;V-PTR&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on Internet data has proven to be a key ingredient for broad generalization in many modern ML systems. What would it take to enable such capabilities in robotic reinforcement learning (RL)? Offline RL methods, which learn from datasets of robot experience, offer one way to leverage prior data into the robotic learning pipeline. However, these methods have a "type mismatch" with video data (such as Ego4D), the largest prior datasets available for robotics, since video offers observation-only experience without the action or reward annotations needed for RL methods. In this paper, we develop a system for leveraging large-scale human video datasets in robotic offline RL, based entirely on learning value functions via temporal-difference learning. We show that value learning on video datasets learns representations that are more conducive to downstream robotic offline RL than other approaches for learning from video data. Our system, called V-PTR, combines the benefits of pre-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#21040;Conformer&#27169;&#22411;&#20013;&#20197;&#35299;&#20915;&#38271;&#35805;&#35821;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#35805;&#35821;&#19978;&#20248;&#20110;&#22522;&#20934;Conformer&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13029</link><description>&lt;p&gt;
&#22686;&#24378;&#35760;&#24518;&#30340;Conformer&#29992;&#20110;&#25913;&#36827;&#31471;&#21040;&#31471;&#38271;&#31687;ASR
&lt;/p&gt;
&lt;p&gt;
Memory-augmented conformer for improved end-to-end long-form ASR. (arXiv:2309.13029v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13029
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#21040;Conformer&#27169;&#22411;&#20013;&#20197;&#35299;&#20915;&#38271;&#35805;&#35821;&#24773;&#20917;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#35805;&#35821;&#19978;&#20248;&#20110;&#22522;&#20934;Conformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20248;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#28982;&#32780;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31471;&#21040;&#31471;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#65289;&#22312;&#38271;&#35805;&#35821;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;Conformer&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#28155;&#21152;&#19968;&#20010;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#35760;&#24518;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#22806;&#37096;&#35760;&#24518;&#21487;&#20197;&#22686;&#24378;&#23545;&#38271;&#35805;&#35821;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#31995;&#32479;&#24490;&#29615;&#22320;&#23384;&#20648;&#21644;&#26816;&#32034;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#22270;&#28789;&#26426;&#65288;NTM&#65289;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Conformer-NTM&#27169;&#22411;&#26550;&#26500;&#29992;&#20110;ASR&#12290;&#20351;&#29992;Librispeech&#30340;train-clean-100&#21644;train-960&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38271;&#35805;&#35821;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;&#27809;&#26377;&#35760;&#24518;&#30340;&#22522;&#20934;Conformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformers have recently been proposed as a promising modelling approach for automatic speech recognition (ASR), outperforming recurrent neural network-based approaches and transformers. Nevertheless, in general, the performance of these end-to-end models, especially attention-based models, is particularly degraded in the case of long utterances. To address this limitation, we propose adding a fully-differentiable memory-augmented neural network between the encoder and decoder of a conformer. This external memory can enrich the generalization for longer utterances since it allows the system to store and retrieve more information recurrently. Notably, we explore the neural Turing machine (NTM) that results in our proposed Conformer-NTM model architecture for ASR. Experimental results using Librispeech train-clean-100 and train-960 sets show that the proposed system outperforms the baseline conformer without memory for long utterances.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#34920;&#31034;&#19977;&#32500;&#21152;&#22266;&#26495;&#30340;&#22270;&#23884;&#20837;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#39044;&#27979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#19979;&#21152;&#22266;&#26495;&#30340;&#24212;&#21147;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#27604;&#26377;&#38480;&#20803;-&#39030;&#28857;&#22270;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13022</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#21152;&#36733;&#19979;&#21152;&#22266;&#26495;&#24212;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network for Stress Predictions in Stiffened Panels Under Uniform Loading. (arXiv:2309.13022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#34920;&#31034;&#19977;&#32500;&#21152;&#22266;&#26495;&#30340;&#22270;&#23884;&#20837;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#39044;&#27979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#19979;&#21152;&#22266;&#26495;&#30340;&#24212;&#21147;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#27604;&#26377;&#38480;&#20803;-&#39030;&#28857;&#22270;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#20195;&#20215;&#36739;&#39640;&#30340;&#32467;&#26500;&#20998;&#26512;&#26041;&#27861;&#30340;&#38477;&#38454;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#27604;&#22914;&#26377;&#38480;&#20803;&#20998;&#26512;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#21487;&#20197;&#34920;&#31034;&#20026;&#22270;&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#22312;&#32467;&#26500;&#25110;&#20135;&#21697;&#30340;&#27010;&#24565;&#35774;&#35745;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#22797;&#26434;&#30340;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#34920;&#31034;&#19977;&#32500;&#21152;&#22266;&#26495;&#30340;&#26032;&#22411;&#22270;&#23884;&#20837;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#21333;&#29420;&#30340;&#26495;&#22495;&#20316;&#20026;&#39030;&#28857;&#36827;&#34892;&#32771;&#34385;&#12290;&#37319;&#29992;&#22270;&#37319;&#26679;&#21644;&#32858;&#21512; (GraphSAGE) &#26041;&#27861;&#39044;&#27979;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#19979;&#21152;&#22266;&#26495;&#30340;&#24212;&#21147;&#20998;&#24067;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#20803;-&#39030;&#28857;&#22270;&#34920;&#31034;&#30340;&#23545;&#27604;&#20197;&#35777;&#26126;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#21442;&#25968;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#21442;&#25968;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) and deep learning (DL) techniques have gained significant attention as reduced order models (ROMs) to computationally expensive structural analysis methods, such as finite element analysis (FEA). Graph neural network (GNN) is a particular type of neural network which processes data that can be represented as graphs. This allows for efficient representation of complex geometries that can change during conceptual design of a structure or a product. In this study, we propose a novel graph embedding technique for efficient representation of 3D stiffened panels by considering separate plate domains as vertices. This approach is considered using Graph Sampling and Aggregation (GraphSAGE) to predict stress distributions in stiffened panels with varying geometries. A comparison between a finite-element-vertex graph representation is conducted to demonstrate the effectiveness of the proposed approach. A comprehensive parametric study is performed to examine the effect of s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#20316;&#29289;&#21697;&#31181;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#20316;&#29289;&#20135;&#37327;&#23545;&#20110;&#29702;&#35299;&#20854;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.13021</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Deep Learning-based Approach for Optimal Genotype by Environment Selection. (arXiv:2309.13021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13021
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22522;&#20110;&#29615;&#22659;&#30340;&#22522;&#22240;&#22411;&#36873;&#25321;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#20316;&#29289;&#21697;&#31181;&#30340;&#22825;&#27668;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#22320;&#39044;&#27979;&#20316;&#29289;&#20135;&#37327;&#23545;&#20110;&#29702;&#35299;&#20854;&#36866;&#24212;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35770;&#25991;&#20013;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#23545;&#20110;&#25913;&#21892;&#20892;&#19994;&#23454;&#36341;&#21644;&#30830;&#20445;&#20316;&#29289;&#22312;&#19981;&#21516;&#27668;&#20505;&#26465;&#20214;&#19979;&#30340;&#38887;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;MLCAS2021&#20316;&#29289;&#20135;&#37327;&#39044;&#27979;&#25361;&#25112;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#21547;93,028&#20010;&#35757;&#32451;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#20102;10,337&#20010;&#27979;&#35797;&#35760;&#24405;&#30340;&#20135;&#37327;&#65292;&#22312;13&#24180;&#65288;2003-2015&#24180;&#65289;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#35206;&#30422;&#20102;&#32654;&#22269;28&#20010;&#24030;&#21644;&#21152;&#25343;&#22823;&#30465;&#20221;&#30340;159&#20010;&#22320;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;5,838&#20010;&#19981;&#21516;&#22522;&#22240;&#22411;&#30340;&#35814;&#32454;&#20449;&#24687;&#21644;&#20026;&#26399;214&#22825;&#30340;&#29983;&#38271;&#23395;&#33410;&#30340;&#27599;&#26085;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#20316;&#20026;&#33719;&#32988;&#22242;&#38431;&#20043;&#19968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65306;CNN-DNN&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#65307;&#20197;&#21450;CNN-LSTM-DNN&#27169;&#22411;&#65292;&#21152;&#20837;&#20102;LSTM&#23618;&#29992;&#20110;&#22825;&#27668;&#21464;&#37327;&#12290;&#21033;&#29992;&#24191;&#20041;&#38598;&#25104;&#26041;&#27861;&#65288;GEM&#65289;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#22522;&#22240;&#22411;&#19982;&#29615;&#22659;&#36873;&#25321;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise crop yield prediction is essential for improving agricultural practices and ensuring crop resilience in varying climates. Integrating weather data across the growing season, especially for different crop varieties, is crucial for understanding their adaptability in the face of climate change. In the MLCAS2021 Crop Yield Prediction Challenge, we utilized a dataset comprising 93,028 training records to forecast yields for 10,337 test records, covering 159 locations across 28 U.S. states and Canadian provinces over 13 years (2003-2015). This dataset included details on 5,838 distinct genotypes and daily weather data for a 214-day growing season, enabling comprehensive analysis. As one of the winning teams, we developed two novel convolutional neural network (CNN) architectures: the CNN-DNN model, combining CNN and fully-connected networks, and the CNN-LSTM-DNN model, with an added LSTM layer for weather variables. Leveraging the Generalized Ensemble Method (GEM), we determined opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.13016</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#29702;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#65288;DGL&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26799;&#24230;&#21521;&#37327;&#20013;&#24674;&#22797;&#31169;&#26377;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20855;&#26377;&#25935;&#24863;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20998;&#24067;&#24335;&#23398;&#20064;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#38656;&#35201;&#20849;&#20139;&#26799;&#24230;&#12290;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#38656;&#35201;&#20294;&#32570;&#20047;&#23545;&#38544;&#31169;&#27844;&#38706;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#26041;&#24335;&#30340;&#29702;&#35299;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#65288;I&#178;F&#65289;&#65292;&#36890;&#36807;&#38544;&#24335;&#35299;&#20915;DGL&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#38381;&#24335;&#36830;&#25509;&#12290;&#19982;&#30452;&#25509;&#35299;&#20915;DGL&#30456;&#27604;&#65292;I&#178;F&#22312;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20165;&#38656;&#35201;&#26799;&#24230;&#21644;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#30340;&#39044;&#35328;&#35775;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;I&#178;F&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#12289;&#25915;&#20987;&#23454;&#29616;&#21644;&#22522;&#20110;&#22122;&#22768;&#30340;&#38450;&#24481;&#20013;&#37117;&#33021;&#26377;&#25928;&#36817;&#20284;DGL&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#30340;&#26426;&#29702;&#21644;&#24212;&#23545;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#39640;&#25928;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21152;&#36895;&#22120;&#30828;&#20214;&#25903;&#25345;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#30340;DNN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2309.13015</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;N:M&#31232;&#30095;DNN&#35757;&#32451;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient N:M Sparse DNN Training Using Algorithm, Architecture, and Dataflow Co-Design. (arXiv:2309.13015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#39640;&#25928;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#31232;&#30095;&#21152;&#36895;&#22120;&#30828;&#20214;&#25903;&#25345;&#23454;&#29616;&#39640;&#31232;&#30095;&#27604;&#30340;DNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#26159;&#20943;&#23569;DNN&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#29305;&#21035;&#26159;N:M&#32454;&#31890;&#24230;&#32467;&#26500;&#31232;&#30095;&#65292;&#20854;&#20013;&#21482;&#26377;&#36830;&#32493;M&#20010;&#20803;&#32032;&#20013;&#30340;N&#20010;&#21487;&#20197;&#26159;&#38750;&#38646;&#20540;&#65292;&#22240;&#20854;&#23545;&#30828;&#20214;&#21451;&#22909;&#30340;&#27169;&#24335;&#21644;&#36798;&#21040;&#39640;&#31232;&#30095;&#27604;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21152;&#36895;N:M&#31232;&#30095;DNN&#35757;&#32451;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#65292;&#24182;&#19988;&#32570;&#20047;&#25903;&#25345;N:M&#31232;&#30095;&#35757;&#32451;&#30340;&#39640;&#25928;&#30828;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31639;&#27861;&#65292;&#26550;&#26500;&#21644;&#25968;&#25454;&#27969;&#21327;&#21516;&#35774;&#35745;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;N:M&#31232;&#30095;DNN&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#31639;&#27861;&#23618;&#38754;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#26435;&#37325;&#20462;&#21098;&#26041;&#27861;&#65288;BDWP&#65289;&#65292;&#21033;&#29992;DNN&#35757;&#32451;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#26435;&#37325;&#30340;N:M&#31232;&#30095;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#22312;&#26550;&#26500;&#23618;&#38754;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;DNN&#31232;&#30095;&#21152;&#36895;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#25968;&#25454;&#27969;&#21327;&#35758;&#65292;&#21033;&#29992;&#31232;&#30095;&#26435;&#37325;&#30340;&#32467;&#26500;&#27169;&#24335;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training is one of the promising techniques to reduce the computational cost of DNNs while retaining high accuracy. In particular, N:M fine-grained structured sparsity, where only N out of consecutive M elements can be nonzero, has attracted attention due to its hardware-friendly pattern and capability of achieving a high sparse ratio. However, the potential to accelerate N:M sparse DNN training has not been fully exploited, and there is a lack of efficient hardware supporting N:M sparse training. To tackle these challenges, this paper presents a computation-efficient training scheme for N:M sparse DNNs using algorithm, architecture, and dataflow co-design. At the algorithm level, a bidirectional weight pruning method, dubbed BDWP, is proposed to leverage the N:M sparsity of weights during both forward and backward passes of DNN training, which can significantly reduce the computational cost while maintaining model accuracy. At the architecture level, a sparse accelerator for DN
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13005</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#22495;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;
&lt;/p&gt;
&lt;p&gt;
Pursuing Counterfactual Fairness via Sequential Autoencoder Across Domains. (arXiv:2309.13005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#36861;&#27714;&#30340;&#21019;&#26032;&#26694;&#26550;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20844;&#24179;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#39046;&#22495;&#36716;&#31227;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#21508;&#31181;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#26102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#36830;&#32493;&#30340;&#24207;&#21015;&#39046;&#22495;&#20013;&#36880;&#28176;&#21464;&#21270;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22312;&#36825;&#20123;&#26032;&#39046;&#22495;&#20869;&#30340;&#27169;&#22411;&#25928;&#26524;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20855;&#26377;&#39034;&#24207;&#33258;&#32534;&#30721;&#22120;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65288;CDSAE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#29615;&#22659;&#20449;&#24687;&#21644;&#25935;&#24863;&#23646;&#24615;&#19982;&#20998;&#31867;&#29305;&#24449;&#30340;&#23884;&#20837;&#34920;&#31034;&#20998;&#24320;&#12290;&#36825;&#31181;&#24182;&#34892;&#20998;&#31163;&#19981;&#20165;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#21644;&#38476;&#29983;&#30340;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing the prevalence of domain shift as a common challenge in machine learning, various domain generalization (DG) techniques have been developed to enhance the performance of machine learning systems when dealing with out-of-distribution (OOD) data. Furthermore, in real-world scenarios, data distributions can gradually change across a sequence of sequential domains. While current methodologies primarily focus on improving model effectiveness within these new domains, they often overlook fairness issues throughout the learning process. In response, we introduce an innovative framework called Counterfactual Fairness-Aware Domain Generalization with Sequential Autoencoder (CDSAE). This approach effectively separates environmental information and sensitive attributes from the embedded representation of classification features. This concurrent separation not only greatly improves model generalization across diverse and unfamiliar domains but also effectively addresses challenges rela
&lt;/p&gt;</description></item><item><title>&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#20445;&#35777;&#27169;&#22411;&#21487;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#22256;&#38590;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13002</link><description>&lt;p&gt;
&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Expressive variational quantum circuits provide inherent privacy in federated learning. (arXiv:2309.13002v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13002
&lt;/p&gt;
&lt;p&gt;
&#34920;&#36798;&#24615;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25552;&#20379;&#22266;&#26377;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#36807;&#24230;&#21442;&#25968;&#21270;&#20445;&#35777;&#27169;&#22411;&#21487;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#35299;&#20915;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26041;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#19982;&#20013;&#22830;&#32858;&#21512;&#22120;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26631;&#20934;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#30340;&#26799;&#24230;&#30340;&#25968;&#25454;&#27844;&#38706;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20351;&#29992;&#34920;&#36798;&#24615;&#32534;&#30721;&#26144;&#23556;&#21644;&#36807;&#24230;&#21442;&#25968;&#21270;ans\"tze&#26500;&#24314;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#34920;&#36798;&#24615;&#26144;&#23556;&#23548;&#33268;&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#20855;&#26377;&#22266;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#32780;&#36807;&#24230;&#21442;&#25968;&#21270;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;&#25105;&#20204;&#30340;&#38544;&#31169;&#26694;&#26550;&#38598;&#20013;&#22312;&#36890;&#36807;&#37327;&#23376;&#30005;&#36335;&#26799;&#24230;&#29983;&#25104;&#30340;&#39640;&#27425;&#22810;&#20803;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#30340;&#35299;&#20915;&#22797;&#26434;&#24615;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#35770;&#28857;&#65292;&#24378;&#35843;&#22312;&#31934;&#30830;&#21644;&#36817;&#20284;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#22266;&#26377;&#22256;&#38590;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has emerged as a viable distributed solution to train machine learning models without the actual need to share data with the central aggregator. However, standard neural network-based federated learning models have been shown to be susceptible to data leakage from the gradients shared with the server. In this work, we introduce federated learning with variational quantum circuit model built using expressive encoding maps coupled with overparameterized ans\"atze. We show that expressive maps lead to inherent privacy against gradient inversion attacks, while overparameterization ensures model trainability. Our privacy framework centers on the complexity of solving the system of high-degree multivariate Chebyshev polynomials generated by the gradients of quantum circuit. We present compelling arguments highlighting the inherent difficulty in solving these equations, both in exact and approximate scenarios. Additionally, we delve into machine learning-based attack strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20113;&#32593;&#32476;&#65288;PCN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#23618;&#30340;&#25913;&#36827;&#65292;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#26550;&#26500;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#22823;&#24133;&#24230;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.12996</link><description>&lt;p&gt;
&#28857;&#20113;&#32593;&#32476;&#65306;&#32447;&#24615;&#23618;&#21442;&#25968;&#25968;&#37327;&#30340;&#25968;&#37327;&#32423;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Network: An Order of Magnitude Improvement in Linear Layer Parameter Count. (arXiv:2309.12996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28857;&#20113;&#32593;&#32476;&#65288;PCN&#65289;&#26550;&#26500;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#23618;&#30340;&#25913;&#36827;&#65292;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#26550;&#26500;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#22823;&#24133;&#24230;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28857;&#20113;&#32593;&#32476;&#65288;PCN&#65289;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#32447;&#24615;&#23618;&#30340;&#26032;&#22411;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#25903;&#25345;&#20854;&#22312;&#32447;&#24615;&#23618;&#20013;&#20248;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;MLP&#21644;PCN&#26550;&#26500;&#35757;&#32451;&#20102;&#20960;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#21407;&#22987;&#30340;AlexNet&#65292;&#20197;&#30452;&#25509;&#27604;&#36739;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#25910;&#38598;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21644;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;top-1&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;PCN&#31561;&#25928;&#20110;AlexNet&#30340;AlexNet-PCN16&#65292;&#22312;&#20854;&#32447;&#24615;&#23618;&#20013;&#21442;&#25968;&#20943;&#23569;99.5%&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#26550;&#26500;&#30456;&#24403;&#30340;&#25928;&#26524;&#65288;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290;&#25152;&#26377;&#35757;&#32451;&#37117;&#26159;&#22312;&#20113;RTX 4090 GPU&#19978;&#36827;&#34892;&#30340;&#65292;&#21033;&#29992;pytorch&#36827;&#34892;&#27169;&#22411;&#26500;&#24314;&#21644;&#35757;&#32451;&#12290;&#25552;&#20379;&#20102;&#20195;&#30721;&#20379;&#20219;&#20309;&#20154;&#22797;&#21046;&#26412;&#25991;&#20013;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Point Cloud Network (PCN) architecture, a novel implementation of linear layers in deep learning networks, and provides empirical evidence to advocate for its preference over the Multilayer Perceptron (MLP) in linear layers. We train several models, including the original AlexNet, using both MLP and PCN architectures for direct comparison of linear layers (Krizhevsky et al., 2012). The key results collected are model parameter count and top-1 test accuracy over the CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). AlexNet-PCN16, our PCN equivalent to AlexNet, achieves comparable efficacy (test accuracy) to the original architecture with a 99.5% reduction of parameters in its linear layers. All training is done on cloud RTX 4090 GPUs, leveraging pytorch for model construction and training. Code is provided for anyone to reproduce the trials from this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#27963;&#24615;&#29289;&#36136;&#31995;&#32479;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#27010;&#29575;&#27969;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#20013;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30452;&#25509;&#35745;&#31639;&#36825;&#20123;&#29289;&#29702;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12991</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27963;&#24615;&#29289;&#36136;&#20013;&#30340;&#27010;&#29575;&#27969;&#21644;&#29109;&#20135;&#29983;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning probability flows and entropy production rates in active matter. (arXiv:2309.12991v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#27963;&#24615;&#29289;&#36136;&#31995;&#32479;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#27010;&#29575;&#27969;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#20013;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30452;&#25509;&#35745;&#31639;&#36825;&#20123;&#29289;&#29702;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#29289;&#36136;&#31995;&#32479;&#65292;&#20174;&#33258;&#39537;&#33014;&#20307;&#21040;&#36816;&#21160;&#30340;&#32454;&#33740;&#65292;&#20854;&#29305;&#28857;&#26159;&#23558;&#33258;&#30001;&#33021;&#36716;&#21270;&#20026;&#24494;&#35266;&#23610;&#24230;&#30340;&#26377;&#25928;&#24037;&#20316;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#21040;&#36229;&#20986;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#33539;&#30068;&#30340;&#29289;&#29702;&#23398;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#38750;&#24179;&#34913;&#24577;&#36136;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#31283;&#24577;&#27010;&#29575;&#27969;&#30340;&#22823;&#23567;&#25552;&#20379;&#20102;&#37327;&#21270;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#27979;&#37327;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#30340;&#30772;&#32570;&#21644;&#38750;&#24179;&#34913;&#36755;&#36816;&#24378;&#24230;&#26469;&#29702;&#35299;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#25928;&#35745;&#31639;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#31995;&#32479;&#30340;&#26410;&#30693;&#21644;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#20272;&#35745;&#36825;&#31181;&#23494;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#25968;&#20197;&#21450;&#24494;&#35266;&#36816;&#21160;&#26041;&#31243;&#21487;&#20197;&#30452;&#25509;&#33719;&#21462;&#29109;&#20135;&#29983;&#36895;&#29575;&#65292;&#20854;&#27010;&#29575;&#27969;&#22823;&#23567;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active matter systems, from self-propelled colloids to motile bacteria, are characterized by the conversion of free energy into useful work at the microscopic scale. These systems generically involve physics beyond the reach of equilibrium statistical mechanics, and a persistent challenge has been to understand the nature of their nonequilibrium states. The entropy production rate and the magnitude of the steady-state probability current provide quantitative ways to do so by measuring the breakdown of time-reversal symmetry and the strength of nonequilibrium transport of measure. Yet, their efficient computation has remained elusive, as they depend on the system's unknown and high-dimensional probability density. Here, building upon recent advances in generative modeling, we develop a deep learning framework that estimates the score of this density. We show that the score, together with the microscopic equations of motion, gives direct access to the entropy production rate, the probabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12971</link><description>&lt;p&gt;
&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#22312;&#31616;&#21333;&#22797;&#21512;&#20307;&#19978;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#33457;&#29923;&#25289;&#26222;&#25289;&#26031;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#22797;&#21512;&#20307;&#26469;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#65292;&#22312;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#35782;&#21035;&#20869;&#22312;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#26469;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26222;&#36890;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#22522;&#20110;&#37197;&#23545;&#20132;&#20114;&#32593;&#32476;&#30340;&#22522;&#30784;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#20854;&#35782;&#21035;&#22797;&#26434;&#31995;&#32479;&#20013;&#28508;&#22312;&#39640;&#38454;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#33021;&#21147;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22797;&#26434;&#31995;&#32479;&#30340;&#39640;&#38454;&#20132;&#20114;&#24314;&#27169;&#30340;&#20016;&#23500;&#25968;&#23398;&#29702;&#35770;&#65292;&#21363;&#31616;&#21333;&#22797;&#21512;&#20307;&#65288;SCs&#65289;-&#19968;&#31181;&#23545;&#24314;&#27169;&#39640;&#38454;&#20132;&#20114;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#22522;&#20110;SC&#30340;GNNs&#23384;&#22312;&#22797;&#26434;&#24230;&#39640;&#21644;&#21051;&#26495;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#24378;&#24230;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;&#33457;&#29923;&#65288;FP&#65289;&#27169;&#22411;&#65292;&#23558;FP&#25289;&#26222;&#25289;&#26031;&#24341;&#20837;&#21040;SC&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;FP&#25289;&#26222;&#25289;&#26031;&#20026;&#22522;&#30784;&#30340;&#39640;&#38454;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HiGCN&#65289;&#65292;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;&#25299;&#25169;&#23610;&#24230;&#19978;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;FP&#25289;&#26222;&#25289;&#26031;&#22495;&#20869;&#30340;&#21442;&#25968;&#32452;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#19981;&#21516;&#27169;&#24335;&#30340;&#22270;&#26696;&#65292;&#20854;&#20013;&#28388;&#27874;&#22120;&#30340;&#26435;&#37325;&#29992;&#20316;&#25968;&#37327;&#21270;&#39640;&#38454;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent successes of vanilla Graph Neural Networks (GNNs) on many tasks, their foundation on pairwise interaction networks inherently limits their capacity to discern latent higher-order interactions in complex systems. To bridge this capability gap, we propose a novel approach exploiting the rich mathematical theory of simplicial complexes (SCs) - a robust tool for modeling higher-order interactions. Current SC-based GNNs are burdened by high complexity and rigidity, and quantifying higher-order interaction strengths remains challenging. Innovatively, we present a higher-order Flower-Petals (FP) model, incorporating FP Laplacians into SCs. Further, we introduce a Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians, capable of discerning intrinsic features across varying topological scales. By employing learnable graph filters, a parameter group within each FP Laplacian domain, we can identify diverse patterns where the filters' weights serve as a quan
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12931</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#30340;&#20998;&#21035;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12931
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#20013;&#65292;&#36890;&#36807;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21464;&#24418;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65288;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65289;&#36890;&#24120;&#20250;&#20026;[CLS]&#31526;&#21495;&#21644;&#26631;&#35760;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20026;&#26631;&#35760;&#21644;[CLS]&#31526;&#21495;&#20998;&#21035;&#20351;&#29992;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#24182;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#32531;&#35299;&#23558;&#30456;&#21516;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#25968;&#25454;&#24212;&#29992;&#20110;&#20004;&#31181;&#26631;&#35760;&#31867;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#25928;&#26524;&#65292;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#19982;&#23427;&#20204;&#21508;&#33258;&#30340;&#35282;&#33394;&#26368;&#20339;&#21305;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;[CLS]&#23884;&#20837;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#22659;&#20449;&#24687;&#65292;&#24182;&#22312;&#20854;&#38750;&#21508;&#21521;&#21516;&#24615;&#31354;&#38388;&#20013;&#20998;&#24067;&#26356;&#22343;&#21248;&#12290;&#24403;&#29992;&#36825;&#20004;&#20010;&#21333;&#29420;&#30340;&#24402;&#19968;&#21270;&#23618;&#26367;&#25442;&#24120;&#35268;&#30340;&#24402;&#19968;&#21270;&#23618;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#20102;2.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performa
&lt;/p&gt;</description></item><item><title>BayesDLL&#26159;&#19968;&#20010;&#29992;&#20110;PyTorch&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#24211;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#26080;&#38656;&#20462;&#25913;&#29992;&#25143;&#20195;&#30721;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#20316;&#20026;&#20808;&#39564;&#22343;&#20540;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.12928</link><description>&lt;p&gt;
BayesDLL: &#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BayesDLL: Bayesian Deep Learning Library. (arXiv:2309.12928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12928
&lt;/p&gt;
&lt;p&gt;
BayesDLL&#26159;&#19968;&#20010;&#29992;&#20110;PyTorch&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#24211;&#30456;&#27604;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#26080;&#38656;&#20462;&#25913;&#29992;&#25143;&#20195;&#30721;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#20316;&#20026;&#20808;&#39564;&#22343;&#20540;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PyTorch&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24211;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#24211;&#23454;&#29616;&#20102;&#20027;&#27969;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#26029;&#31639;&#27861;&#65306;&#21464;&#20998;&#25512;&#26029;&#12289;MC-dropout&#12289;&#38543;&#26426;&#26799;&#24230;MCMC&#21644;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#24211;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24211;&#26377;&#20197;&#19979;&#20027;&#35201;&#21306;&#21035;&#65306;1&#65289;&#25105;&#20204;&#30340;&#24211;&#21487;&#20197;&#22788;&#29702;&#21253;&#25324;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#22312;&#20869;&#30340;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;2&#65289;&#29992;&#25143;&#20960;&#20046;&#19981;&#38656;&#35201;&#20462;&#25913;&#20195;&#30721;&#65288;&#20363;&#22914;&#65292;&#39592;&#24178;&#32593;&#32476;&#23450;&#20041;&#20195;&#30721;&#26681;&#26412;&#19981;&#38656;&#35201;&#20462;&#25913;&#65289;&#12290;3&#65289;&#25105;&#20204;&#30340;&#24211;&#36824;&#20801;&#35768;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#20316;&#20026;&#20808;&#39564;&#22343;&#20540;&#65292;&#36825;&#23545;&#20110;&#20351;&#29992;&#20165;&#20165;&#20381;&#38752;&#19979;&#28216;&#25968;&#25454;&#38590;&#20197;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;ViTs&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: \url{https://github.com/SamsungLabs/BayesDLL}&#65288;&#22791;&#29992;&#23384;&#20648;&#24211;&#20063;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;\url{https://github.com/miny})
&lt;/p&gt;
&lt;p&gt;
We release a new Bayesian neural network library for PyTorch for large-scale deep networks. Our library implements mainstream approximate Bayesian inference algorithms: variational inference, MC-dropout, stochastic-gradient MCMC, and Laplace approximation. The main differences from other existing Bayesian neural network libraries are as follows: 1) Our library can deal with very large-scale deep networks including Vision Transformers (ViTs). 2) We need virtually zero code modifications for users (e.g., the backbone network definition codes do not neet to be modified at all). 3) Our library also allows the pre-trained model weights to serve as a prior mean, which is very useful for performing Bayesian inference with the large-scale foundation models like ViTs that are hard to optimise from scratch with the downstream data alone. Our code is publicly available at: \url{https://github.com/SamsungLabs/BayesDLL}\footnote{A mirror repository is also available at: \url{https://github.com/miny
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26174;&#33879;&#22270;&#20013;&#32771;&#34385;&#26799;&#24230;&#31526;&#21495;&#21644;&#24433;&#21709;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26356;&#22909;&#22320;&#35782;&#21035;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12913</link><description>&lt;p&gt;
&#24577;&#24230;&#38382;&#39064;&#65306;&#20851;&#27880;&#31215;&#26497;&#21644;&#20027;&#21160;&#26799;&#24230;&#26469;&#25552;&#21319;&#26174;&#33879;&#22270;
&lt;/p&gt;
&lt;p&gt;
A matter of attitude: Focusing on positive and active gradients to boost saliency maps. (arXiv:2309.12913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26174;&#33879;&#22270;&#20013;&#32771;&#34385;&#26799;&#24230;&#31526;&#21495;&#21644;&#24433;&#21709;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#26356;&#22909;&#22320;&#35782;&#21035;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#25552;&#20379;&#30340;&#35265;&#35299;&#36136;&#37327;&#65292;&#26174;&#33879;&#22270;&#24050;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#35265;&#35299;&#26159;&#21542;&#21487;&#20449;&#20173;&#23384;&#22312;&#19968;&#20123;&#30097;&#38382;&#65292;&#26159;&#21542;&#30495;&#27491;&#20195;&#34920;CNN&#29992;&#20110;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#20174;&#26174;&#33879;&#22270;&#20013;&#25405;&#25937;&#26799;&#24230;&#30340;&#31526;&#21495;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;CNN&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#32771;&#34385;&#27491;&#30830;&#31867;&#21035;&#30340;&#31526;&#21495;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20182;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#32593;&#32476;&#30495;&#27491;&#20851;&#27880;&#30340;&#22270;&#20687;&#20687;&#32032;&#12290;&#27492;&#22806;&#65292;&#36974;&#25377;&#25110;&#25913;&#21464;&#36825;&#20123;&#20687;&#32032;&#39044;&#35745;&#20250;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#20063;&#21464;&#24471;&#26356;&#21152;&#28165;&#26224;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saliency maps have become one of the most widely used interpretability techniques for convolutional neural networks (CNN) due to their simplicity and the quality of the insights they provide. However, there are still some doubts about whether these insights are a trustworthy representation of what CNNs use to come up with their predictions. This paper explores how rescuing the sign of the gradients from the saliency map can lead to a deeper understanding of multi-class classification problems. Using both pretrained and trained from scratch CNNs we unveil that considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on. Furthermore, how occluding or altering those pixels is expected to affect the outcome also becomes clearer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SLGNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21270;&#23398;&#20122;&#32467;&#26500;&#30340;&#22270;&#26469;&#34920;&#31034;&#33647;&#29289;&#20998;&#23376;&#65292;&#24182;&#32467;&#21512;&#24191;&#20041;&#34701;&#21512;&#22871;&#32034;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20197;&#35782;&#21035;&#23545;&#33647;&#29289;-&#34507;&#30333;&#36136;&#32467;&#21512;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#36830;&#25509;&#20122;&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.12906</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#23398;&#20064;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33647;&#29289;-&#34507;&#30333;&#36136;&#32467;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Building explainable graph neural network by sparse learning for the drug-protein binding prediction. (arXiv:2309.12906v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SLGNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21270;&#23398;&#20122;&#32467;&#26500;&#30340;&#22270;&#26469;&#34920;&#31034;&#33647;&#29289;&#20998;&#23376;&#65292;&#24182;&#32467;&#21512;&#24191;&#20041;&#34701;&#21512;&#22871;&#32034;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#20197;&#35782;&#21035;&#23545;&#33647;&#29289;-&#34507;&#30333;&#36136;&#32467;&#21512;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#36830;&#25509;&#20122;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#34987;&#24320;&#21457;&#24182;&#24212;&#29992;&#20110;&#33647;&#29289;-&#34507;&#30333;&#36136;&#32467;&#21512;&#39044;&#27979;&#65292;&#20197;&#35782;&#21035;&#19982;&#30446;&#26631;&#34507;&#30333;&#36136;&#26377;&#27963;&#36291;&#30456;&#20114;&#20316;&#29992;&#30340;&#33647;&#29289;&#20013;&#30340;&#20851;&#38190;&#21270;&#23398;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21487;&#35299;&#37322;&#30340;GNN&#27169;&#22411;&#25152;&#35782;&#21035;&#20986;&#30340;&#20851;&#38190;&#32467;&#26500;&#36890;&#24120;&#26159;&#21270;&#23398;&#19978;&#26080;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#19968;&#20010;&#38408;&#20540;&#26469;&#20174;&#20854;&#20182;&#32467;&#26500;&#20013;&#30830;&#23450;&#20851;&#38190;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#24403;&#21069;&#21487;&#35299;&#37322;&#30340;GNN&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLGNN&#65292;&#21363;&#20351;&#29992;&#31232;&#30095;&#23398;&#20064;&#26469;&#26500;&#24314;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;SLGNN&#20381;&#36182;&#20110;&#20351;&#29992;&#22522;&#20110;&#21270;&#23398;&#20122;&#32467;&#26500;&#30340;&#22270;&#65288;&#20854;&#20013;&#33410;&#28857;&#26159;&#21270;&#23398;&#20122;&#32467;&#26500;&#65289;&#26469;&#34920;&#31034;&#33647;&#29289;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;SLGNN&#23558;&#24191;&#20041;&#34701;&#21512;&#22871;&#32034;&#19982;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#23545;&#33647;&#29289;-&#34507;&#30333;&#36136;&#32467;&#21512;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#36830;&#25509;&#20122;&#22270;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#22522;&#20110;&#21270;&#23398;&#20122;&#32467;&#26500;&#30340;&#22270;&#65292;&#22240;&#27492;&#20445;&#35777;&#20102;&#33647;&#29289;&#20013;&#30340;&#20219;&#20309;&#20122;&#22270;&#21487;&#20197;&#30001;&#21270;&#23398;&#20122;&#32467;&#26500;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Networks (GNNs) have been developed and applied to drug-protein binding prediction to identify the key chemical structures in a drug that have active interactions with the target proteins. However, the key structures identified by the current explainable GNN models are typically chemically invalid. Furthermore, a threshold needs to be manually selected to pinpoint the key structures from the rest. To overcome the limitations of the current explainable GNN models, we propose our SLGNN, which stands for using Sparse Learning to Graph Neural Networks. Our SLGNN relies on using a chemical-substructure-based graph (where nodes are chemical substructures) to represent a drug molecule. Furthermore, SLGNN incorporates generalized fussed lasso with message-passing algorithms to identify connected subgraphs that are critical for the drug-protein binding prediction. Due to the use of the chemical-substructure-based graph, it is guaranteed that any subgraphs in a drug iden
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#35752;&#35770;&#26222;&#36866;&#35745;&#31639;&#30740;&#31350;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#31038;&#20250;&#12289;&#25216;&#26415;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#36890;&#36807;&#31038;&#20250;&#35282;&#24230;&#12289;&#25216;&#26415;&#35282;&#24230;&#21644;&#27861;&#24459;&#35282;&#24230;&#26469;&#25506;&#31350;&#20844;&#27491;&#30340;&#36884;&#24452;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#35268;&#21010;&#20986;&#26126;&#30830;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.12877</link><description>&lt;p&gt;
FairComp&#65306;&#20851;&#20110;&#26222;&#36866;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing. (arXiv:2309.12877v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#35752;&#35770;&#26222;&#36866;&#35745;&#31639;&#30740;&#31350;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#31038;&#20250;&#12289;&#25216;&#26415;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#36890;&#36807;&#31038;&#20250;&#35282;&#24230;&#12289;&#25216;&#26415;&#35282;&#24230;&#21644;&#27861;&#24459;&#35282;&#24230;&#26469;&#25506;&#31350;&#20844;&#27491;&#30340;&#36884;&#24452;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#35268;&#21010;&#20986;&#26126;&#30830;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#26222;&#36866;&#35745;&#31639;&#65288;UbiComp&#65289;&#30340;&#30740;&#31350;&#25104;&#26524;&#26082;&#20855;&#26377;&#36947;&#24503;&#65292;&#21448;&#20855;&#26377;&#20844;&#24179;&#24615;&#65311;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#26222;&#36866;&#35745;&#31639;&#20013;&#30340;&#20844;&#24179;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#27425;&#30740;&#35752;&#20250;&#26088;&#22312;&#35752;&#35770;&#26222;&#36866;&#35745;&#31639;&#30740;&#31350;&#20013;&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#31038;&#20250;&#12289;&#25216;&#26415;&#21644;&#27861;&#24459;&#24433;&#21709;&#12290;&#20174;&#31038;&#20250;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#20844;&#24179;&#24615;&#19982;&#26222;&#36866;&#35745;&#31639;&#30740;&#31350;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#30830;&#23450;&#30830;&#20445;&#26222;&#36866;&#25216;&#26415;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#25110;&#20405;&#29359;&#20010;&#20154;&#26435;&#21033;&#30340;&#36884;&#24452;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#23558;&#21457;&#36215;&#20851;&#20110;&#25968;&#25454;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#20197;&#24320;&#21457;&#36866;&#21512;&#26222;&#36866;&#35745;&#31639;&#30740;&#31350;&#30340;&#20559;&#35265;&#28040;&#20943;&#26041;&#27861;&#12290;&#20174;&#27861;&#24459;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#26032;&#25919;&#31574;&#22914;&#20309;&#22609;&#36896;&#25105;&#20204;&#31038;&#21306;&#30340;&#24037;&#20316;&#21644;&#26410;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#36127;&#36131;&#20219;&#26222;&#36866;&#35745;&#31639;&#20027;&#39064;&#30340;&#20805;&#28385;&#27963;&#21147;&#30340;&#31038;&#21306;&#65292;&#21516;&#26102;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21162;&#21147;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#35268;&#21010;&#20986;&#26126;&#30830;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are both ethical and fair? While fairness in machine learning (ML) has gained traction in recent years, fairness in UbiComp remains unexplored. This workshop aims to discuss fairness in UbiComp research and its social, technical, and legal implications. From a social perspective, we will examine the relationship between fairness and UbiComp research and identify pathways to ensure that ubiquitous technologies do not cause harm or infringe on individual rights. From a technical perspective, we will initiate a discussion on data practices to develop bias mitigation approaches tailored to UbiComp research. From a legal perspective, we will examine how new policies shape our community's work and future research. We aim to foster a vibrant community centered around the topic of responsible UbiComp, while also charting a clear path for future research endeavours in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20174;&#31034;&#25945;&#20013;&#23398;&#20064;&#22788;&#29702;&#36719;&#24615;&#39135;&#21697;&#23545;&#35937;&#30340;&#31283;&#20581;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;RGB-D&#22270;&#20687;&#21644;&#35302;&#35273;&#25968;&#25454;&#36827;&#34892;&#25235;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.12856</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20174;&#31034;&#25945;&#20013;&#23398;&#20064;&#22788;&#29702;&#36719;&#24615;&#39135;&#21697;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
Robotic Handling of Compliant Food Objects by Robust Learning from Demonstration. (arXiv:2309.12856v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20174;&#31034;&#25945;&#20013;&#23398;&#20064;&#22788;&#29702;&#36719;&#24615;&#39135;&#21697;&#23545;&#35937;&#30340;&#31283;&#20581;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;RGB-D&#22270;&#20687;&#21644;&#35302;&#35273;&#25968;&#25454;&#36827;&#34892;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#36719;&#24615;&#21644;&#21464;&#24418;&#30340;&#39135;&#21697;&#21407;&#26448;&#26009;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30446;&#21069;&#22312;&#28023;&#27915;&#31354;&#38388;&#12289;&#20892;&#19994;&#21644;&#39135;&#21697;&#34892;&#19994;&#20013;&#38656;&#27714;&#24040;&#22823;&#12290;&#36825;&#20123;&#34892;&#19994;&#20013;&#30340;&#35768;&#22810;&#20219;&#21153;&#26159;&#30001;&#20154;&#24037;&#25805;&#20316;&#21592;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#30001;&#20110;&#20219;&#21153;&#30340;&#32321;&#37325;&#21644;&#20047;&#21619;&#65292;&#25191;&#34892;&#30340;&#21464;&#21270;&#24615;&#24456;&#22823;&#65292;&#32467;&#26524;&#20063;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#30001;&#20110;&#24403;&#21069;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31574;&#30053;&#65292;&#24341;&#20837;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#36827;&#34892;&#22797;&#26434;&#22788;&#29702;&#20219;&#21153;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24076;&#26395;&#33021;&#22815;&#36890;&#36807;&#19968;&#20010;&#26356;&#19968;&#33268;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#29087;&#32451;&#30340;&#25805;&#20316;&#21592;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#38754;&#23545;&#19981;&#19968;&#33268;&#31034;&#25945;&#26102;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31034;&#25945;&#23398;&#20064;&#65288;LfD&#65289;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#36719;&#24615;&#39135;&#21697;&#23545;&#35937;&#30340;&#31283;&#20581;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;RGB-D&#22270;&#20687;&#21644;&#35302;&#35273;&#25968;&#25454;&#30340;&#21512;&#24182;&#26469;&#20272;&#35745;...
&lt;/p&gt;
&lt;p&gt;
The robotic handling of compliant and deformable food raw materials, characterized by high biological variation, complex geometrical 3D shapes, and mechanical structures and texture, is currently in huge demand in the ocean space, agricultural, and food industries. Many tasks in these industries are performed manually by human operators who, due to the laborious and tedious nature of their tasks, exhibit high variability in execution, with variable outcomes. The introduction of robotic automation for most complex processing tasks has been challenging due to current robot learning policies. A more consistent learning policy involving skilled operators is desired. In this paper, we address the problem of robot learning when presented with inconsistent demonstrations. To this end, we propose a robust learning policy based on Learning from Demonstration (LfD) for robotic grasping of food compliant objects. The approach uses a merging of RGB-D images and tactile data in order to estimate th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#32763;&#35793;&#19982;&#23545;&#40784; (CMTA) &#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#29983;&#23384;&#20998;&#26512;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#22312;&#30340;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#21644;&#20256;&#36882;&#20114;&#34917;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12855</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#32763;&#35793;&#19982;&#23545;&#40784;&#25216;&#26415;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Translation and Alignment for Survival Analysis. (arXiv:2309.12855v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#32763;&#35793;&#19982;&#23545;&#40784; (CMTA) &#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#30149;&#29702;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#29983;&#23384;&#20998;&#26512;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#22312;&#30340;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#21644;&#20256;&#36882;&#20114;&#34917;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29983;&#23384;&#20998;&#26512;&#30340;&#37325;&#28857;&#24050;&#32463;&#20174;&#30740;&#31350;&#20020;&#24202;&#25351;&#26631;&#36716;&#31227;&#21040;&#23558;&#22522;&#22240;&#32452;&#29305;&#24449;&#19982;&#30149;&#29702;&#22270;&#20687;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#30452;&#25509;&#23558;&#30149;&#29702;&#29305;&#24449;&#21644;&#22522;&#22240;&#32452;&#29305;&#24449;&#34701;&#21512;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#35201;&#20040;&#23558;&#22522;&#22240;&#32452;&#29305;&#24449;&#20316;&#20026;&#25351;&#23548;&#29992;&#20110;&#25972;&#21512;&#30149;&#29702;&#22270;&#20687;&#30340;&#29305;&#24449;&#12290;&#21069;&#32773;&#20250;&#24573;&#35270;&#20869;&#22312;&#30340;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#65292;&#32780;&#21518;&#32773;&#20250;&#20002;&#24323;&#19982;&#22522;&#22240;&#34920;&#36798;&#26080;&#20851;&#30340;&#30149;&#29702;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#32763;&#35793;&#19982;&#23545;&#40784; (CMTA) &#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#32034;&#20869;&#22312;&#30340;&#36328;&#27169;&#24577;&#30456;&#20851;&#24615;&#24182;&#20256;&#36882;&#28508;&#22312;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#24182;&#34892;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25972;&#21512;&#27169;&#24577;&#20869;&#20449;&#24687;&#24182;&#29983;&#25104;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#21033;&#29992;&#29983;&#25104;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#26469;&#22686;&#24378;&#29983;&#23384;&#20998;&#26512;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advances in high-throughput sequencing technologies, the focus of survival analysis has shifted from examining clinical indicators to incorporating genomic profiles with pathological images. However, existing methods either directly adopt a straightforward fusion of pathological features and genomic profiles for survival prediction, or take genomic profiles as guidance to integrate the features of pathological images. The former would overlook intrinsic cross-modal correlations. The latter would discard pathological information irrelevant to gene expression. To address these issues, we present a Cross-Modal Translation and Alignment (CMTA) framework to explore the intrinsic cross-modal correlations and transfer potential complementary information. Specifically, we construct two parallel encoder-decoder structures for multi-modal data to integrate intra-modal information and generate cross-modal representation. Taking the generated cross-modal representation to enhance an
&lt;/p&gt;</description></item><item><title>DeepOPF-U &#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12849</link><description>&lt;p&gt;
DeepOPF-U: &#19968;&#31181;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#32593;&#32476;&#20013;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
DeepOPF-U: A Unified Deep Neural Network to Solve AC Optimal Power Flow in Multiple Networks. (arXiv:2309.12849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12849
&lt;/p&gt;
&lt;p&gt;
DeepOPF-U &#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#27867;&#21270;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#22810;&#29992;&#20110;&#32473;&#23450;&#30340;&#30005;&#21147;&#32593;&#32476;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#24403;&#21069;&#20855;&#26377;&#19981;&#21516;&#25299;&#25169;&#21644;&#26085;&#30410;&#22686;&#38271;&#30340;&#21363;&#25554;&#21363;&#29992;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#30005;&#21147;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepOPF-U&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#19981;&#21516;&#30005;&#21147;&#32593;&#32476;&#20013;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#20010;&#19981;&#26029;&#25193;&#23637;&#30340;&#30005;&#21147;&#32593;&#32476;&#38598;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#19981;&#21516;&#32593;&#32476;&#20013;&#32473;&#23450;&#36127;&#33655;&#21644;&#26368;&#20248;&#21151;&#29575;&#27969;&#35299;&#30340;&#21521;&#37327;&#35774;&#35745;&#20102;&#24377;&#24615;&#36755;&#20837;&#23618;&#21644;&#36755;&#20986;&#23618;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#27597;&#32447;&#12289;&#32447;&#36335;&#12289;&#36127;&#33655;&#21644;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#25968;&#37327;&#12290;IEEE 57/118/300&#27597;&#32447;&#27979;&#35797;&#31995;&#32479;&#30340;&#20223;&#30495;&#20197;&#21450;&#19968;&#20010;&#20174;73&#20010;&#27597;&#32447;&#22686;&#38271;&#21040;118&#20010;&#27597;&#32447;&#30340;&#32593;&#32476;&#39564;&#35777;&#20102;DeepOPF-U&#30456;&#27604;&#24050;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional machine learning models to solve optimal power flow (OPF) are mostly trained for a given power network and lack generalizability to today's power networks with varying topologies and growing plug-and-play distributed energy resources (DERs). In this paper, we propose DeepOPF-U, which uses one unified deep neural network (DNN) to solve alternating-current (AC) OPF problems in different power networks, including a set of power networks that is successively expanding. Specifically, we design elastic input and output layers for the vectors of given loads and OPF solutions with varying lengths in different networks. The proposed method, using a single unified DNN, can deal with different and growing numbers of buses, lines, loads, and DERs. Simulations of IEEE 57/118/300-bus test systems and a network growing from 73 to 118 buses verify the improved performance of DeepOPF-U compared to existing DNN-based solution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#32676;&#27169;&#25311;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#26368;&#23567;&#21270;&#33021;&#28304;&#20351;&#29992;&#24182;&#37197;&#21512;&#36866;&#24403;&#30340;&#24341;&#23548;&#21183;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#19981;&#21516;&#22870;&#21169;&#32452;&#25104;&#23545;&#27169;&#25311;&#20154;&#32676;&#34892;&#20026;&#20855;&#26377;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26032;&#30340;&#20154;&#32676;&#27169;&#25311;&#25216;&#26415;&#30340;&#24320;&#21457;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12841</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#32676;&#27169;&#25311;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reward Function Design for Crowd Simulation via Reinforcement Learning. (arXiv:2309.12841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#32676;&#27169;&#25311;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#26368;&#23567;&#21270;&#33021;&#28304;&#20351;&#29992;&#24182;&#37197;&#21512;&#36866;&#24403;&#30340;&#24341;&#23548;&#21183;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#19981;&#21516;&#22870;&#21169;&#32452;&#25104;&#23545;&#27169;&#25311;&#20154;&#32676;&#34892;&#20026;&#20855;&#26377;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26032;&#30340;&#20154;&#32676;&#27169;&#25311;&#25216;&#26415;&#30340;&#24320;&#21457;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#32676;&#27169;&#25311;&#23545;&#20110;&#35270;&#39057;&#28216;&#25103;&#35774;&#35745;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#22312;&#34394;&#25311;&#19990;&#30028;&#20013;&#25918;&#32622;&#20855;&#26377;&#20154;&#31867;&#33324;&#34892;&#20026;&#30340;&#33258;&#20027;&#35282;&#33394;&#12290;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#25311;&#34394;&#25311;&#20154;&#32676;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22870;&#21169;&#20989;&#25968;&#30340;&#35774;&#35745;&#23545;&#20110;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#32676;&#27169;&#25311;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#20851;&#20110;&#26576;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#30340;&#27934;&#35265;&#65292;&#24182;&#20351;&#29992;&#33021;&#28304;&#25928;&#29575;&#20316;&#20026;&#25351;&#26631;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30452;&#25509;&#26368;&#23567;&#21270;&#33021;&#28304;&#20351;&#29992;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#31574;&#30053;&#65292;&#21482;&#35201;&#23427;&#19982;&#36866;&#24403;&#35843;&#25972;&#30340;&#24341;&#23548;&#21183;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#19981;&#21516;&#22870;&#21169;&#32452;&#25104;&#23545;&#27169;&#25311;&#20154;&#32676;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#20197;&#20026;&#26032;&#30340;&#20154;&#32676;&#27169;&#25311;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowd simulation is important for video-games design, since it enables to populate virtual worlds with autonomous avatars that navigate in a human-like manner. Reinforcement learning has shown great potential in simulating virtual crowds, but the design of the reward function is critical to achieving effective and efficient results. In this work, we explore the design of reward functions for reinforcement learning-based crowd simulation. We provide theoretical insights on the validity of certain reward functions according to their analytical properties, and evaluate them empirically using a range of scenarios, using the energy efficiency as the metric. Our experiments show that directly minimizing the energy usage is a viable strategy as long as it is paired with an appropriately scaled guiding potential, and enable us to study the impact of the different reward components on the behavior of the simulated crowd. Our findings can inform the development of new crowd simulation techniques
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20854;&#20013;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#25104;&#20026;&#20027;&#35201;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#39044;&#27979;&#24615;&#33021;&#21644;&#34892;&#20026;&#24433;&#21709;&#65292;&#32570;&#20047;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12830</link><description>&lt;p&gt;
AxOCS: &#20351;&#29992;&#37197;&#32622;&#36229;&#37319;&#26679;&#26469;&#25193;&#23637;&#22522;&#20110;FPGA&#30340;&#36817;&#20284;&#36816;&#31639;&#31526;
&lt;/p&gt;
&lt;p&gt;
AxOCS: Scaling FPGA-based Approximate Operators using Configuration Supersampling. (arXiv:2309.12830v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12830
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20854;&#20013;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#25104;&#20026;&#20027;&#35201;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#39044;&#27979;&#24615;&#33021;&#21644;&#34892;&#20026;&#24433;&#21709;&#65292;&#32570;&#20047;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22788;&#29702;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#26085;&#30410;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#23545;&#20302;&#25104;&#26412;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#30340;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#32780;&#35328;&#12290;&#20026;&#27492;&#65292;&#36817;&#20284;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#25506;&#32034;&#21151;&#32791;&#12289;&#24615;&#33021;&#12289;&#38754;&#31215;&#65288;PPA&#65289;&#21644;&#34892;&#20026;&#20934;&#30830;&#24615;&#65288;BEHAV&#65289;&#20043;&#38388;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#23454;&#29616;&#30340;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;MAC&#25805;&#20316;&#30340;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#35774;&#35745;&#29305;&#23450;&#20110;&#24179;&#21488;&#30340;&#36817;&#20284;&#31639;&#26415;&#36816;&#31639;&#31526;&#24418;&#25104;&#20102;&#36817;&#20284;&#35745;&#31639;&#20013;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;AI/ML&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#25216;&#26415;&#34987;&#29992;&#20110;&#23454;&#29616;&#36817;&#20284;&#36816;&#31639;&#31526;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#20989;&#25968;&#26469;&#39044;&#27979;&#19968;&#32452;&#30456;&#20851;&#35774;&#35745;&#20915;&#31574;&#30340;PPA&#21644;BEHAV&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22238;&#24402;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26356;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising usage of AI and ML-based processing across application domains has exacerbated the need for low-cost ML implementation, specifically for resource-constrained embedded systems. To this end, approximate computing, an approach that explores the power, performance, area (PPA), and behavioral accuracy (BEHAV) trade-offs, has emerged as a possible solution for implementing embedded machine learning. Due to the predominance of MAC operations in ML, designing platform-specific approximate arithmetic operators forms one of the major research problems in approximate computing. Recently there has been a rising usage of AI/ML-based design space exploration techniques for implementing approximate operators. However, most of these approaches are limited to using ML-based surrogate functions for predicting the PPA and BEHAV impact of a set of related design decisions. While this approach leverages the regression capabilities of ML methods, it does not exploit the more advanced approaches i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12829</link><description>&lt;p&gt;
&#21512;&#25104;&#25552;&#21319;&#65306;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography. (arXiv:2309.12829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#30340;&#25351;&#26631;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20998;&#21106;&#23545;&#20110;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#21644;&#22266;&#26377;&#25361;&#25112;&#38459;&#30861;&#20102;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#35270;&#35273;-&#35821;&#35328;&#20998;&#21106;&#27169;&#22411;&#65288;VLSM&#65289;&#21487;&#20197;&#34701;&#20837;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#36229;&#22768;&#24515;&#21160;&#22270;&#20013;&#32570;&#20047;&#29616;&#25104;&#30340;&#25968;&#25454;&#38459;&#30861;&#20102;VLSM&#30340;&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35821;&#20041;&#25193;&#25955;&#27169;&#22411;&#65288;SDM&#65289;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#30340;VLSM&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#12289;&#20998;&#21106;&#25513;&#27169;&#21644;&#20803;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#30340;&#22810;&#20010;&#23646;&#24615;&#23548;&#20986;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;&#26469;&#35780;&#20272;&#20004;&#20010;&#27969;&#34892;&#30340;VLSM&#27169;&#22411;&#65288;CLIPSeg&#21644;CRIS&#65289;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#39044;&#35757;&#32451;VLSM&#26102;&#65292;&#36716;&#25442;&#21644;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. However, the lack of readily available data in echocardiography hampers the training of VLSMs. In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. Our results show improved metrics and faster convergence when pretraining VLSMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12819</link><description>&lt;p&gt;
&#36830;&#32493;&#27835;&#30103;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Proximal Causal Learning for Continuous Treatments. (arXiv:2309.12819v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#21452;&#37325;&#31283;&#20581;&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#27714;&#35299;&#24178;&#25200;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#22240;&#26524;&#23398;&#20064;&#26159;&#22312;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#30340;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#34917;&#20805;&#31283;&#20581;&#65288;DR&#65289;&#20272;&#35745;&#22120;&#34987;&#25512;&#23548;&#20986;&#26469;&#65292;&#24182;&#22312;&#20272;&#35745;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24418;&#24335;&#30340;DR&#20272;&#35745;&#22120;&#20165;&#38480;&#20110;&#20108;&#36827;&#21046;&#27835;&#30103;&#65292;&#32780;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#27835;&#30103;&#21487;&#20197;&#26159;&#36830;&#32493;&#30340;&#12290;&#36830;&#32493;&#27835;&#30103;&#30340;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#22312;&#21407;&#22987;DR&#20272;&#35745;&#22120;&#20013;&#23384;&#22312;&#30340;delta&#20989;&#25968;&#65292;&#20351;&#24471;&#22312;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20013;&#19981;&#21487;&#34892;&#65292;&#24182;&#22312;&#24178;&#25200;&#20989;&#25968;&#20272;&#35745;&#20013;&#24341;&#20837;&#20102;&#27785;&#37325;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#36830;&#32493;&#27835;&#30103;&#30340;DR&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#36830;&#32493;&#27835;&#30103;&#12290;&#37197;&#22791;&#20854;&#24179;&#28369;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;Oracle&#24418;&#24335;&#26159;&#24433;&#21709;&#20989;&#25968;&#30340;&#19968;&#33268;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#35299;&#20915;&#24178;&#25200;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proximal causal learning is a promising framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatment can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12815</link><description>&lt;p&gt;
&#29992;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Game Agents with Data Augmentation in Imitation Learning. (arXiv:2309.12815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#35757;&#32451;&#28216;&#25103;&#26234;&#33021;&#20307;&#21644;&#39640;&#25928;&#28216;&#25103;&#29983;&#25104;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#33021;&#21147;&#8212;&#8212;&#22312;&#30456;&#20851;&#20294;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#30340;&#33021;&#21147;&#8212;&#8212;&#23545;&#20110;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#22411;&#21270;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31639;&#27861;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#37319;&#21462;&#26377;&#24847;&#20041;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#21463;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#25104;&#21151;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#30340;&#29366;&#24577;-&#34892;&#21160;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20110;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#12289;&#21151;&#33021;&#24615;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#23646;&#24615;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12813</link><description>&lt;p&gt;
&#33258;&#21160;&#27979;&#35797;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#30340;&#21151;&#33021;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Automatically Testing Functional Properties of Code Translation Models. (arXiv:2309.12813v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#12289;&#21151;&#33021;&#24615;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#23646;&#24615;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36328;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;$transpiling$&#65292;&#27491;&#21464;&#24471;&#26085;&#30410;&#23454;&#29992;&#12290;&#23613;&#31649;&#33258;&#21160;&#32763;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#21457;&#32773;&#30340;&#29983;&#20135;&#21147;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#26368;&#21021;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#27979;&#35797;&#22871;&#20214;&#26469;&#27979;&#35797;&#23567;&#35268;&#27169;&#31243;&#24207;&#30340;&#32763;&#35793;&#65292;&#21518;&#26469;&#21448;&#23558;&#36825;&#20123;&#27979;&#35797;&#22871;&#20214;&#33258;&#21160;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#21151;&#33021;&#24615;&#23646;&#24615;&#27979;&#35797;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#20110;&#36716;&#25442;&#21518;&#20195;&#30721;&#30340;&#19968;&#33324;&#29992;&#25143;&#25552;&#20379;&#30340;&#35268;&#33539;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#23646;&#24615;&#65292;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#25152;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#26816;&#27979;&#27969;&#34892;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#36829;&#35268;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#23646;&#24615;&#35780;&#20272;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#29992;&#25143;&#21482;&#38656;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly practical for translating code across programming languages, a process known as $transpiling$. Even though automated transpilation significantly boosts developer productivity, a key concern is whether the generated code is correct. Existing work initially used manually crafted test suites to test the translations of a small corpus of programs; these test suites were later automated. In contrast, we devise the first approach for automated, functional, property-based testing of code translation models. Our general, user-provided specifications about the transpiled code capture a range of properties, from purely syntactic to purely semantic ones. As shown by our experiments, this approach is very effective in detecting property violations in popular code translation models, and therefore, in evaluating model quality with respect to given properties. We also go a step further and explore the usage scenario where a user simply aims to obtain a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#36716;&#25991;&#23383;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#24050;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#21644;&#36716;&#24405;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12802</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#20316;&#20026;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#36716;&#25991;&#23383;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Deepfake audio as a data augmentation technique for training automatic speech to text transcription models. (arXiv:2309.12802v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#21160;&#35821;&#38899;&#36716;&#25991;&#23383;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#24050;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#21644;&#36716;&#24405;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35757;&#32451;&#20135;&#29983;&#40065;&#26834;&#32467;&#26524;&#30340;&#36716;&#24405;&#22120;&#27169;&#22411;&#65292;&#38656;&#35201;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25214;&#21040;&#20855;&#22791;&#25152;&#38656;&#29305;&#24449;&#30340;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19981;&#22914;&#33521;&#35821;&#27969;&#34892;&#30340;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#36825;&#26679;&#30340;&#25968;&#25454;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#21644;&#32463;&#36153;&#12290;&#22240;&#27492;&#65292;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#31574;&#30053;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#20026;&#20102;&#39564;&#35777;&#20135;&#29983;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#21644;&#36716;&#24405;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;&#36873;&#25321;&#20102;&#19968;&#20010;&#35821;&#38899;&#20811;&#38534;&#22120;&#21644;&#30001;&#21360;&#24230;&#20154;&#65288;&#20197;&#33521;&#35821;&#20026;&#20027;&#65289;&#21046;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#38598;&#20013;&#21482;&#26377;&#19968;&#20010;&#21475;&#38899;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#25968;&#25454;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#35757;&#32451;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To train transcriptor models that produce robust results, a large and diverse labeled dataset is required. Finding such data with the necessary characteristics is a challenging task, especially for languages less popular than English. Moreover, producing such data requires significant effort and often money. Therefore, a strategy to mitigate this problem is the use of data augmentation techniques. In this work, we propose a framework that approaches data augmentation based on deepfake audio. To validate the produced framework, experiments were conducted using existing deepfake and transcription models. A voice cloner and a dataset produced by Indians (in English) were selected, ensuring the presence of a single accent in the dataset. Subsequently, the augmented data was used to train speech to text models in various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#26234;&#33021;&#26816;&#27979;&#31163;&#24515;&#27893;&#26032;&#22411;&#25925;&#38556;&#31867;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#30693;&#35782;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#20351;&#29992;t-SNE&#26041;&#27861;&#21644;&#32858;&#31867;&#25216;&#26415;&#26816;&#27979;&#26032;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#26032;&#25968;&#25454;&#22686;&#24378;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#26032;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2309.12765</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#31163;&#24515;&#27893;&#26032;&#22411;&#25925;&#38556;&#31867;&#21035;&#26234;&#33021;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Intelligent Approach to Detecting Novel Fault Classes for Centrifugal Pumps Based on Deep CNNs and Unsupervised Methods. (arXiv:2309.12765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#26234;&#33021;&#26816;&#27979;&#31163;&#24515;&#27893;&#26032;&#22411;&#25925;&#38556;&#31867;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37096;&#20998;&#30693;&#35782;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#20351;&#29992;t-SNE&#26041;&#27861;&#21644;&#32858;&#31867;&#25216;&#26415;&#26816;&#27979;&#26032;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#26032;&#25968;&#25454;&#22686;&#24378;&#32593;&#32476;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#20934;&#30830;&#24615;&#22320;&#26816;&#27979;&#26032;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26059;&#36716;&#26426;&#22120;&#25925;&#38556;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#31995;&#32479;&#21487;&#33021;&#22312;&#29616;&#22330;&#36935;&#21040;&#30340;&#21508;&#31181;&#25925;&#38556;&#32570;&#20047;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;&#23545;&#31995;&#32479;&#25925;&#38556;&#20855;&#26377;&#37096;&#20998;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#30456;&#24212;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#21518;&#20351;&#29992;t-SNE&#26041;&#27861;&#21644;&#32858;&#31867;&#25216;&#26415;&#30340;&#32452;&#21512;&#26469;&#26816;&#27979;&#26032;&#30340;&#25925;&#38556;&#12290;&#22312;&#26816;&#27979;&#21040;&#25925;&#38556;&#21518;&#65292;&#21033;&#29992;&#26032;&#25968;&#25454;&#22686;&#24378;&#32593;&#32476;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#31163;&#24515;&#27893;&#30340;&#27979;&#35797;&#35774;&#32622;&#26469;&#39564;&#35777;&#36825;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#35770;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#26816;&#27979;&#26032;&#25925;&#38556;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success in data-driven fault diagnosis of rotating machines, there are still remaining challenges in this field. Among the issues to be addressed, is the lack of information about variety of faults the system may encounter in the field. In this paper, we assume a partial knowledge of the system faults and use the corresponding data to train a convolutional neural network. A combination of t-SNE method and clustering techniques is then employed to detect novel faults. Upon detection, the network is augmented using the new data. Finally, a test setup is used to validate this two-stage methodology on a centrifugal pump and experimental results show high accuracy in detecting novel faults.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12757</link><description>&lt;p&gt;
&#23545;&#20110;ConvNets&#26469;&#35828;&#65292;&#36974;&#30422;&#65288;masking&#65289;&#33021;&#25913;&#21892;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#32780;&#26174;&#33879;&#24615;&#21578;&#35785;&#20320;&#20309;&#22788;&#12290;&#65288;arXiv:2309.12757v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Masking Improves Contrastive Self-Supervised Learning for ConvNets, and Saliency Tells You Where. (arXiv:2309.12757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#36974;&#30422;&#25805;&#20316;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#36974;&#30422;&#25805;&#20316;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#21103;&#20316;&#29992;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#24320;&#22987;&#21463;&#30410;&#20110;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#24314;&#31435;&#22312;&#36974;&#30422;&#21644;&#33258;&#37325;&#26500;&#30446;&#26631;&#20043;&#19978;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#20196;&#29260;&#21270;&#31243;&#24207;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#22270;&#20687;&#25968;&#25454;&#30340;&#21478;&#19968;&#31181;&#37325;&#35201;&#19988;&#24191;&#27867;&#37319;&#29992;&#30340;&#26550;&#26500;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23613;&#31649;&#20855;&#26377;&#39537;&#21160;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20173;&#28982;&#38754;&#20020;&#23558;&#36825;&#31181;&#30452;&#25509;&#32780;&#36890;&#29992;&#30340;&#36974;&#30422;&#25805;&#20316;&#26174;&#33879;&#22320;&#21033;&#29992;&#20110;&#20854;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#23558;&#36974;&#30422;&#25805;&#20316;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#30340;&#36127;&#25285;&#65292;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;ConvNets&#20013;&#22240;&#36974;&#32617;&#25805;&#20316;&#32780;&#20135;&#29983;&#30340;&#39069;&#22806;&#36793;&#32536;&#65288;&#36974;&#30422;&#21644;&#26410;&#36974;&#30422;&#21306;&#22495;&#20043;&#38388;&#65289;&#20197;&#21450;&#20854;&#20182;&#19981;&#21033;&#24433;&#21709;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#24050;&#32463;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#35752;&#35770;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
While image data starts to enjoy the simple-but-effective self-supervised learning scheme built upon masking and self-reconstruction objective thanks to the introduction of tokenization procedure and vision transformer backbone, convolutional neural networks as another important and widely-adopted architecture for image data, though having contrastive-learning techniques to drive the self-supervised learning, still face the difficulty of leveraging such straightforward and general masking operation to benefit their learning process significantly. In this work, we aim to alleviate the burden of including masking operation into the contrastive-learning framework for convolutional neural networks as an extra augmentation method. In addition to the additive but unwanted edges (between masked and unmasked regions) as well as other adverse effects caused by the masking operations for ConvNets, which have been discussed by prior works, we particularly identify the potential problem where for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#20104;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2309.12742</link><description>&lt;p&gt;
&#35753;UDA&#20013;&#30340;U&#21464;&#24471;&#37325;&#35201;&#65306;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#20104;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;(DA)&#22312;&#22788;&#29702;&#39046;&#22495;&#20869;&#37096;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#22914;&#31867;&#21035;&#36523;&#20221;&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#65288;&#22914;&#29615;&#22659;&#65289;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#26102;&#65292;&#24635;&#26159;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#31181;&#30456;&#20851;&#24615;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20855;&#26377;&#26222;&#36941;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#26080;&#30417;&#30563;&#30446;&#26631;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;DA(UDA)&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#24433;&#21709;&#12290;&#36825;&#26159;&#22240;&#20026;&#28304;&#22495;&#30417;&#30563;&#21482;&#23558;&#30446;&#26631;&#22495;&#26679;&#26412;&#35270;&#20026;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#36890;&#36807;&#20266;&#26631;&#31614;&#65289;&#65292;&#32780;&#30446;&#26631;&#22495;&#20013;&#26377;&#20215;&#20540;&#30340;&#21435;&#30456;&#20851;&#32447;&#32034;&#30340;&#22266;&#26377;&#20998;&#24067;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32473;&#20104;&#20004;&#20010;&#22495;&#30456;&#31561;&#30340;&#22320;&#20301;&#65292;&#26469;&#20351;UDA&#20013;&#30340;U&#21464;&#24471;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#19981;&#21464;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#39044;&#27979;&#21516;&#26102;&#19982;&#28304;&#22495;&#26631;&#31614;&#21644;&#30446;&#26631;&#22495;&#32858;&#31867;&#19968;&#33268;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#30446;&#26631;&#22495;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;"&#19981;&#21464;&#19968;&#33268;&#24615;&#23398;&#20064;"&#65288;ICON&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach "Invariant CONsistency learning" (ICON). Exte
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#20010;&#21306;&#22359;&#38142;&#36164;&#28304;&#30340;&#21160;&#24577;&#36153;&#29992;&#26426;&#21046;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#30830;&#22788;&#29702;&#20102;&#36164;&#28304;&#38656;&#27714;&#20013;&#30340;&#20132;&#21449;&#25928;&#24212;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20132;&#21449;&#25928;&#24212;&#26469;&#25351;&#23548;&#36164;&#28304;&#35774;&#35745;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#23436;&#21892;&#25110;&#25351;&#23548;&#21551;&#21457;&#24335;&#36153;&#29992;&#26356;&#26032;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2309.12735</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36164;&#28304;&#30340;&#26368;&#20248;&#21160;&#24577;&#36153;&#29992;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Dynamic Fees for Blockchain Resources. (arXiv:2309.12735v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12735
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#20010;&#21306;&#22359;&#38142;&#36164;&#28304;&#30340;&#21160;&#24577;&#36153;&#29992;&#26426;&#21046;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#30830;&#22788;&#29702;&#20102;&#36164;&#28304;&#38656;&#27714;&#20013;&#30340;&#20132;&#21449;&#25928;&#24212;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20132;&#21449;&#25928;&#24212;&#26469;&#25351;&#23548;&#36164;&#28304;&#35774;&#35745;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#23436;&#21892;&#25110;&#25351;&#23548;&#21551;&#21457;&#24335;&#36153;&#29992;&#26356;&#26032;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22810;&#20010;&#21306;&#22359;&#38142;&#36164;&#28304;&#30340;&#21160;&#24577;&#36153;&#29992;&#26426;&#21046;&#30340;&#26368;&#20248;&#35774;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#35745;&#31639;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#22312;&#22788;&#29702;&#25345;&#20037;&#38656;&#27714;&#36716;&#21464;&#21644;&#23545;&#35266;&#23519;&#21040;&#30340;&#21306;&#22359;&#38656;&#27714;&#30340;&#23616;&#37096;&#22122;&#22768;&#40065;&#26834;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#35843;&#25972;&#36164;&#28304;&#20215;&#26684;&#12290;&#22312;&#26377;&#22810;&#20010;&#36164;&#28304;&#30340;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26368;&#20248;&#31574;&#30053;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#36164;&#28304;&#38656;&#27714;&#20013;&#30340;&#20132;&#21449;&#25928;&#24212;&#65288;&#20114;&#34917;&#24615;&#21644;&#26367;&#20195;&#24615;&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20132;&#21449;&#25928;&#24212;&#26469;&#25351;&#23548;&#36164;&#28304;&#35774;&#35745;&#65292;&#21363;&#23558;&#36164;&#28304;&#32452;&#21512;&#25104;&#20855;&#26377;&#20302;&#38656;&#27714;&#31471;&#20132;&#21449;&#25928;&#24212;&#30340;&#25414;&#32465;&#21830;&#21697;&#21487;&#20197;&#20135;&#29983;&#26356;&#31616;&#21333;&#21644;&#26356;&#39640;&#25928;&#30340;&#20215;&#26684;&#26356;&#26032;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20063;&#26159;&#23454;&#29992;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#23436;&#21892;&#25110;&#25351;&#23548;&#21551;&#21457;&#24335;&#36153;&#29992;&#26356;&#26032;&#35268;&#21017;&#65292;&#22914;EIP-1559&#25110;EIP-4844&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;&#22826;&#22346;&#30340;&#30495;&#23454;&#24066;&#22330;&#25968;&#25454;&#20272;&#35745;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#21333;&#32500;&#24230;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a general and practical framework to address the problem of the optimal design of dynamic fee mechanisms for multiple blockchain resources. Our framework allows to compute policies that optimally trade-off between adjusting resource prices to handle persistent demand shifts versus being robust to local noise in the observed block demand. In the general case with more than one resource, our optimal policies correctly handle cross-effects (complementarity and substitutability) in resource demands. We also show how these cross-effects can be used to inform resource design, i.e. combining resources into bundles that have low demand-side cross-effects can yield simpler and more efficient price-update rules. Our framework is also practical, we demonstrate how it can be used to refine or inform the design of heuristic fee update rules such as EIP-1559 or EIP-4844 with two case studies. We then estimate a uni-dimensional version of our model using real market data from the Ethereum 
&lt;/p&gt;</description></item><item><title>H2O+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12716</link><description>&lt;p&gt;
H2O+: &#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#21147;&#23398;&#24046;&#36317;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
H2O+: An Improved Framework for Hybrid Offline-and-Online RL with Dynamics Gaps. (arXiv:2309.12716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12716
&lt;/p&gt;
&lt;p&gt;
H2O+&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#32771;&#34385;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#39640;&#31934;&#24230;&#27169;&#25311;&#29615;&#22659;&#25110;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#23454;&#38469;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#38750;&#23436;&#32654;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#22312;&#32447;RL&#20195;&#29702;&#21487;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#30340;&#27169;&#25311;&#19982;&#29616;&#23454;&#38382;&#39064;&#12290;&#34429;&#28982;&#31163;&#32447;RL&#26041;&#27861;&#21487;&#20197;&#32469;&#36807;&#23545;&#27169;&#25311;&#22120;&#30340;&#38656;&#27714;&#65292;&#20294;&#24448;&#24448;&#23545;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#36136;&#37327;&#25552;&#20986;&#20102;&#33499;&#21051;&#30340;&#35201;&#27714;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;RL&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#20351;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#21644;&#19981;&#23436;&#32654;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#21487;&#36716;&#31227;&#31574;&#30053;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2O+&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26725;&#25509;&#19981;&#21516;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#30340;&#21516;&#26102;&#65292;&#20063;&#32771;&#34385;&#20102;&#30495;&#23454;&#21644;&#27169;&#25311;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;H2O+&#22312;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#19978;&#20248;&#20110;&#20808;&#36827;&#30340;&#36328;&#22495;&#22312;&#32447;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving real-world complex tasks using reinforcement learning (RL) without high-fidelity simulation environments or large amounts of offline data can be quite challenging. Online RL agents trained in imperfect simulation environments can suffer from severe sim-to-real issues. Offline RL approaches although bypass the need for simulators, often pose demanding requirements on the size and quality of the offline datasets. The recently emerged hybrid offline-and-online RL provides an attractive framework that enables joint use of limited offline data and imperfect simulator for transferable policy learning. In this paper, we develop a new algorithm, called H2O+, which offers great flexibility to bridge various choices of offline and online learning methods, while also accounting for dynamics gaps between the real and simulation environment. Through extensive simulation and real-world robotics experiments, we demonstrate superior performance and flexibility over advanced cross-domain online
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#21644;&#30417;&#30563;&#20998;&#31867;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#22312;&#23567;&#26679;&#26412;&#38899;&#39057;&#29255;&#27573;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12714</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#34920;&#31034;&#25913;&#36827;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Representations Improve Supervised Learning in Speech Emotion Recognition. (arXiv:2309.12714v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12714
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#21644;&#30417;&#30563;&#20998;&#31867;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#22312;&#23567;&#26679;&#26412;&#38899;&#39057;&#29255;&#27573;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#35782;&#21035;&#19978;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#22312;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23545;&#24773;&#24863;&#29366;&#24577;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20026;&#26356;&#20855;&#20849;&#24773;&#21147;&#21644;&#26377;&#25928;&#27807;&#36890;&#20570;&#20986;&#36129;&#29486;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#19982;&#30417;&#30563;&#20998;&#31867;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20174;&#23567;&#38899;&#39057;&#29255;&#27573;&#20013;&#35782;&#21035;&#24773;&#24863;&#12290;&#22312;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#65292;&#20026;&#28040;&#38500;&#25163;&#24037;&#21046;&#20316;&#38899;&#39057;&#29305;&#24449;&#30340;&#38656;&#35201;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;Wav2Vec&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20174;&#38899;&#39057;&#25968;&#25454;&#20013;&#25429;&#25417;&#22768;&#23398;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#36755;&#20986;&#29305;&#24449;&#22270;&#36755;&#20837;&#21040;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#12290;&#22312;ShEMO&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#20004;&#31181;&#22522;&#20934;&#26041;&#27861;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#21644;&#39044;&#35757;&#32451;CNN&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) plays a pivotal role in enhancing human-computer interaction by enabling a deeper understanding of emotional states across a wide range of applications, contributing to more empathetic and effective communication. This study proposes an innovative approach that integrates self-supervised feature extraction with supervised classification for emotion recognition from small audio segments. In the preprocessing step, to eliminate the need of crafting audio features, we employed a self-supervised feature extractor, based on the Wav2Vec model, to capture acoustic features from audio data. Then, the output featuremaps of the preprocessing step are fed to a custom designed Convolutional Neural Network (CNN)-based model to perform emotion classification. Utilizing the ShEMO dataset as our testing ground, the proposed method surpasses two baseline methods, i.e. support vector machine classifier and transfer learning of a pretrained CNN. comparing the propose meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#26469;&#36873;&#25321;&#26368;&#23567;&#30340;&#36275;&#22815;&#27169;&#22411;&#29992;&#20110;&#38899;&#39057;&#36716;&#24405;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#35745;&#31639;&#33410;&#32422;&#12290;</title><link>http://arxiv.org/abs/2309.12712</link><description>&lt;p&gt;
&#21482;&#38024;&#23545;&#22256;&#38590;&#38899;&#39057;&#30340;&#22823;&#22411;&#27169;&#22411;&#65306;&#22522;&#20110;&#26679;&#26412;&#20381;&#36182;&#30340;Whisper&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#39640;&#25928;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Big model only for hard audios: Sample dependent Whisper model selection for efficient inferences. (arXiv:2309.12712v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#26469;&#36873;&#25321;&#26368;&#23567;&#30340;&#36275;&#22815;&#27169;&#22411;&#29992;&#20110;&#38899;&#39057;&#36716;&#24405;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#35745;&#31639;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#20276;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22823;&#24133;&#22686;&#21152;&#65292;&#29616;&#22312;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#21363;&#20351;&#22312;&#36866;&#24212;&#30340;&#30828;&#20214;&#19978;&#20063;&#20250;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#32531;&#24930;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21516;&#22823;&#23567;&#12289;&#25512;&#26029;&#25104;&#26412;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#27700;&#24179;&#19981;&#21516;&#30340;ASR&#27169;&#22411;&#12290;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#22823;&#37096;&#20998;&#27979;&#35797;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#25105;&#20204;&#25552;&#20986;&#35757;&#32451;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#65292;&#21487;&#20197;&#26681;&#25454;&#38899;&#39057;&#26679;&#26412;&#20351;&#29992;&#26368;&#23567;&#30340;&#36275;&#22815;&#27169;&#22411;&#26469;&#33719;&#24471;&#33391;&#22909;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#12290;&#36890;&#36807;&#20445;&#25345;&#20915;&#31574;&#36807;&#31243;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#20943;&#23567;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#35745;&#31639;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Automatic Speech Recognition (ASR) has been coupled with a substantial increase in the model sizes, which may now contain billions of parameters, leading to slow inferences even with adapted hardware. In this context, several ASR models exist in various sizes, with different inference costs leading to different performance levels. Based on the observation that smaller models perform optimally on large parts of testing corpora, we propose to train a decision module, that would allow, given an audio sample, to use the smallest sufficient model leading to a good transcription. We apply our approach to two Whisper models with different sizes. By keeping the decision process computationally efficient, we build a decision module that allows substantial computational savings with reduced performance drops.
&lt;/p&gt;</description></item><item><title>PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12708</link><description>&lt;p&gt;
PointSSC&#65306;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12708
&lt;/p&gt;
&lt;p&gt;
PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26088;&#22312;&#20026;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#31354;&#38388;&#21344;&#29992;&#21644;&#35821;&#20041;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#27169;&#22411;&#37117;&#38598;&#20013;&#22312;&#20307;&#32032;&#34920;&#31034;&#19978;&#65292;&#23545;&#20110;&#22823;&#22411;&#23460;&#22806;&#31354;&#38388;&#26469;&#35828;&#23384;&#22312;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#28857;&#20113;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#22522;&#20934;&#32570;&#20047;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;&#23460;&#22806;&#28857;&#20113;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;PointSSC&#12290;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#21033;&#29992;Segment Anything&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27880;&#37322;&#27969;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#24863;&#30693;&#21464;&#25442;&#22120;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#21450;&#19968;&#20010;&#34917;&#20840;&#21644;&#20998;&#21106;&#21512;&#20316;&#27169;&#22359;&#29992;&#20110;&#32852;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#12290;PointSSC&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25512;&#21160;&#20102;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#32467;&#21512;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23454;&#29616;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12706</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#19982;&#26631;&#31614;&#30456;&#20851;&#24615;&#65306;&#29702;&#35770;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Noise Transition Matrix Estimation with Label Correlations: Theory and Algorithm. (arXiv:2309.12706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21487;&#36776;&#35782;&#24615;&#65292;&#24182;&#32467;&#21512;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23454;&#29616;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#20934;&#30830;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20351;&#24471;&#22122;&#22768;&#26631;&#31614;&#25104;&#20026;&#26356;&#23454;&#38469;&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#22122;&#22768;&#22810;&#31867;&#21035;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#36716;&#31227;&#30697;&#38453;&#21487;&#20197;&#24110;&#21161;&#24314;&#27169;&#22810;&#26631;&#31614;&#22122;&#22768;&#65292;&#24182;&#23454;&#29616;&#23545;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#20272;&#35745;&#22810;&#26631;&#31614;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#22122;&#22768;&#22810;&#31867;&#21035;&#23398;&#20064;&#20013;&#30340;&#22823;&#22810;&#25968;&#29616;&#26377;&#20272;&#35745;&#22120;&#20381;&#36182;&#20110;&#38170;&#28857;&#21644;&#20934;&#30830;&#25311;&#21512;&#22122;&#22768;&#31867;&#21518;&#39564;&#27010;&#29575;&#65292;&#32780;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#24456;&#38590;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22312;&#22122;&#22768;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#22522;&#20110;&#31867;&#21035;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#22312;&#21487;&#36776;&#35782;&#24615;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#38170;&#28857;&#25110;&#31934;&#30830;&#25311;&#21512;&#22122;&#22768;&#31867;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy multi-label learning has garnered increasing attention due to the challenges posed by collecting large-scale accurate labels, making noisy labels a more practical alternative. Motivated by noisy multi-class learning, the introduction of transition matrices can help model multi-label noise and enable the development of statistically consistent algorithms for noisy multi-label learning. However, estimating multi-label noise transition matrices remains a challenging task, as most existing estimators in noisy multi-class learning rely on anchor points and accurate fitting of noisy class posteriors, which is hard to satisfy in noisy multi-label learning. In this paper, we address this problem by first investigating the identifiability of class-dependent transition matrices in noisy multi-label learning. Building upon the identifiability results, we propose a novel estimator that leverages label correlations without the need for anchor points or precise fitting of noisy class posterior
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;</title><link>http://arxiv.org/abs/2309.12701</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21457;&#29616;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Discovering the Interpretability-Performance Pareto Front of Decision Trees with Dynamic Programming. (arXiv:2309.12701v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20915;&#31574;&#26641;&#30001;&#20110;&#21487;&#20197;&#34987;&#20154;&#31867;&#26816;&#26597;&#21644;&#35299;&#37322;&#32780;&#20855;&#26377;&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30828;&#20214;&#30340;&#36827;&#27493;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#27604;&#36890;&#24120;&#30340;&#36138;&#23146;&#26041;&#27861;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#26641;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20248;&#31639;&#27861;&#36820;&#22238;&#30340;&#26159;&#19968;&#20010;&#20248;&#21270;&#25163;&#21160;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#21333;&#20010;&#26641;&#65292;&#36890;&#36807;&#25351;&#23450;&#26368;&#22823;&#20915;&#31574;&#33410;&#28857;&#25968;&#37327;&#26469;&#33719;&#24471;&#65292;&#23545;&#20110;&#36825;&#20010;&#26435;&#34913;&#30340;&#36136;&#37327;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65288;MDP&#65289;&#24418;&#24335;&#26469;&#25214;&#21040;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#35745;&#31639;&#20986;&#22810;&#20010;&#21487;&#35299;&#37322;&#24615;-&#24615;&#33021;&#26435;&#34913;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65292;&#35753;&#29992;&#25143;&#20107;&#21518;&#36873;&#25321;&#26368;&#36866;&#21512;&#20182;&#20204;&#38656;&#27714;&#30340;&#26641;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are known to be intrinsically interpretable as they can be inspected and interpreted by humans. Furthermore, recent hardware advances have rekindled an interest for optimal decision tree algorithms, that produce more accurate trees than the usual greedy approaches. However, these optimal algorithms return a single tree optimizing a hand defined interpretability-performance trade-off, obtained by specifying a maximum number of decision nodes, giving no further insights about the quality of this trade-off. In this paper, we propose a new Markov Decision Problem (MDP) formulation for finding optimal decision trees. The main interest of this formulation is that we can compute the optimal decision trees for several interpretability-performance trade-offs by solving a single dynamic program, letting the user choose a posteriori the tree that best suits their needs. Empirically, we show that our method is competitive with state-of-the-art algorithms in terms of accuracy and run
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;</title><link>http://arxiv.org/abs/2309.12697</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#20248;&#20110;&#20854;&#20182;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#23376;&#24207;&#21015;&#30340;&#37325;&#21472;&#65288;&#20363;&#22914;BLEU&#65289;&#25110;&#20351;&#29992;&#23884;&#20837;&#65288;&#20363;&#22914;BERTScore&#65292;S-BERT&#65289;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#25105;&#20204;&#20165;&#23545;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#30452;&#25509;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20284;&#24615;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#24494;&#35843;&#30340;STS-B&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;STSScore&#26041;&#27861;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#19982;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#23558;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#30340;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#22312;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12694</link><description>&lt;p&gt;
&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Recurrent Temporal Revision Graph Networks. (arXiv:2309.12694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#26102;&#38388;&#20462;&#35746;&#22270;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#23558;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#30340;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#22312;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#38745;&#24577;&#22270;&#30456;&#27604;&#65292;&#26102;&#38388;&#22270;&#33021;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#37051;&#23621;&#32858;&#21512;&#26159;&#22270;&#32593;&#32476;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23545;&#20110;&#26102;&#38388;&#22270;&#26469;&#35828;&#65292;&#30446;&#21069;&#26159;&#20174;&#38745;&#24577;&#22270;&#30452;&#25509;&#25299;&#23637;&#32780;&#26469;&#30340;&#12290;&#24403;&#22312;&#36825;&#31181;&#32858;&#21512;&#36807;&#31243;&#20013;&#28041;&#21450;&#25152;&#26377;&#21382;&#21490;&#37051;&#23621;&#26102;&#65292;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#38750;&#24120;&#39640;&#26114;&#12290;&#23454;&#38469;&#19978;&#65292;&#36890;&#24120;&#21482;&#28041;&#21450;&#26368;&#36817;&#37051;&#23621;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23376;&#25277;&#26679;&#20250;&#23548;&#33268;&#37051;&#23621;&#20449;&#24687;&#19981;&#23436;&#25972;&#21644;&#26377;&#20559;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#37051;&#23621;&#32858;&#21512;&#65292;&#23427;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#36880;&#33410;&#28857;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#23436;&#25972;&#37051;&#23621;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20855;&#26377;&#20248;&#36234;&#30340;&#29702;&#35770;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;+9.6%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graphs offer more accurate modeling of many real-world scenarios than static graphs. However, neighbor aggregation, a critical building block of graph networks, for temporal graphs, is currently straightforwardly extended from that of static graphs. It can be computationally expensive when involving all historical neighbors during such aggregation. In practice, typically only a subset of the most recent neighbors are involved. However, such subsampling leads to incomplete and biased neighbor information. To address this limitation, we propose a novel framework for temporal neighbor aggregation that uses the recurrent neural network with node-wise hidden states to integrate information from all historical neighbors for each node to acquire the complete neighbor information. We demonstrate the superior theoretical expressiveness of the proposed framework as well as its state-of-the-art performance in real-world applications. Notably, it achieves a significant +9.6% improvement o
&lt;/p&gt;</description></item><item><title>AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12689</link><description>&lt;p&gt;
AMPLIFY: &#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26631;&#31614;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
AMPLIFY:Attention-based Mixup for Performance Improvement and Label Smoothing in Transformer. (arXiv:2309.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12689
&lt;/p&gt;
&lt;p&gt;
AMPLIFY&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Mixup&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#30340;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#20110;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#21407;&#22987;&#26679;&#26412;&#30340;&#32447;&#24615;&#32452;&#21512;&#29983;&#25104;&#26032;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21407;&#22987;&#26679;&#26412;&#20013;&#23384;&#22312;&#22122;&#38899;&#25110;&#24322;&#24120;&#29305;&#24449;&#65292;Mixup&#21487;&#33021;&#23558;&#20854;&#20256;&#25773;&#21040;&#22686;&#24378;&#26679;&#26412;&#20013;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#36807;&#20110;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Mixup&#26041;&#27861;&#31216;&#20026;AMPLIFY&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#33258;&#36523;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20943;&#23569;&#21407;&#22987;&#26679;&#26412;&#20013;&#22122;&#38899;&#21644;&#24322;&#24120;&#20540;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#20302;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24120;&#35265;Mixup&#26041;&#27861;&#65288;&#20363;&#22914;&#35821;&#21477;Mixup&#65289;&#20013;&#36164;&#28304;&#28040;&#32791;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26356;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#19979;&#65292;AMPLIFY&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;Mixup&#26041;&#27861;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an effective data augmentation method that generates new augmented samples by aggregating linear combinations of different original samples. However, if there are noises or aberrant features in the original samples, Mixup may propagate them to the augmented samples, leading to over-sensitivity of the model to these outliers . To solve this problem, this paper proposes a new Mixup method called AMPLIFY. This method uses the Attention mechanism of Transformer itself to reduce the influence of noises and aberrant values in the original samples on the prediction results, without increasing additional trainable parameters, and the computational cost is very low, thereby avoiding the problem of high resource consumption in common Mixup methods such as Sentence Mixup . The experimental results show that, under a smaller computational resource cost, AMPLIFY outperforms other Mixup methods in text classification tasks on 7 benchmark datasets, providing new ideas and new ways to further
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.12673</link><description>&lt;p&gt;
&#20851;&#20110;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On Sparse Modern Hopfield Model. (arXiv:2309.12673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#21644;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#23454;&#29616;&#20102;&#23545;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#30340;&#19968;&#27493;&#36817;&#20284;&#12290;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#65292;&#31232;&#30095;&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#26356;&#32039;&#20945;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#31232;&#30095;&#20248;&#21183;&#26465;&#20214;&#12290;&#21516;&#26102;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#36824;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20316;&#20026;&#29616;&#20195; Hopfield &#27169;&#22411;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#19982;&#20854;&#23494;&#38598;&#30340;&#23545;&#24212;&#29289;&#19968;&#26679;&#65292;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20855;&#22791;&#19968;&#31181;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#20854;&#19968;&#27493;&#36817;&#20284;&#23545;&#24212;&#20110;&#31232;&#30095;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#36890;&#36807;&#31232;&#30095;&#29109;&#27491;&#21017;&#21270;&#22120;&#30340;&#20984;&#20849;&#36717;&#23548;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#30340;&#31232;&#30095; Hopfield &#33021;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20174;&#31232;&#30095;&#33021;&#37327;&#20989;&#25968;&#20013;&#25512;&#23548;&#20986;&#31232;&#30095;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#19968;&#27493;&#36817;&#20284;&#31561;&#20215;&#20110;&#31232;&#30095;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#31232;&#30095;&#24230;&#30340;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#35777;&#26126;&#19978;&#35201;&#27604;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#26356;&#32039;&#20945;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#31232;&#30095;&#20248;&#21183;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#31232;&#30095;&#30340;&#29616;&#20195; Hopfield &#27169;&#22411;&#20445;&#25345;&#20102;&#20854;&#23494;&#38598;&#23545;&#24212;&#29289;&#30340;&#31283;&#20581;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#24555;&#36895;&#30340;&#22266;&#23450;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the sparse modern Hopfield model as a sparse extension of the modern Hopfield model. Like its dense counterpart, the sparse modern Hopfield model equips a memory-retrieval dynamics whose one-step approximation corresponds to the sparse attention mechanism. Theoretically, our key contribution is a principled derivation of a closed-form sparse Hopfield energy using the convex conjugate of the sparse entropic regularizer. Building upon this, we derive the sparse memory retrieval dynamics from the sparse energy function and show its one-step approximation is equivalent to the sparse-structured attention. Importantly, we provide a sparsity-dependent memory retrieval error bound which is provably tighter than its dense analog. The conditions for the benefits of sparsity to arise are therefore identified and discussed. In addition, we show that the sparse modern Hopfield model maintains the robust theoretical properties of its dense counterpart, including rapid fixed point conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.12671</link><description>&lt;p&gt;
&#22914;&#20309;&#24494;&#35843;&#27169;&#22411;&#65306;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization. (arXiv:2309.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#30340;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#25552;&#20379;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#21644;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21644;&#25512;&#23548;&#20986;&#20855;&#26377;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#30340;&#26377;&#25928;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#31639;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#27169;&#22411;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#39640;&#32806;&#21512;&#24615;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#22238;&#25253;&#24046;&#24322;&#26469;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#22810;&#30340;&#27169;&#22411;&#26356;&#26032;&#32780;&#24615;&#33021;&#19979;&#38477;&#12290;&#20854;&#20182;&#26041;&#27861;&#20351;&#29992;&#24615;&#33021;&#24046;&#24322;&#36793;&#30028;&#26469;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#20559;&#31227;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#38408;&#20540;&#26469;&#38480;&#21046;&#27169;&#22411;&#20559;&#31227;&#65292;&#23548;&#33268;&#23545;&#38408;&#20540;&#30340;&#20005;&#37325;&#20381;&#36182;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#20197;&#32479;&#19968;&#27169;&#22411;&#20559;&#31227;&#21644;&#27169;&#22411;&#20559;&#24046;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#28982;&#21518;&#21046;&#23450;&#19968;&#20010;&#24494;&#35843;&#36807;&#31243;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#20445;&#35777;&#65292;&#21516;&#26102;&#36991;&#20813;&#27169;&#22411;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#36825;&#20123;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward 
&lt;/p&gt;</description></item><item><title>OneNet&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32447;&#38598;&#25104;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21478;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;OneNet&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.12659</link><description>&lt;p&gt;
OneNet: &#36890;&#36807;&#22312;&#32447;&#38598;&#25104;&#22686;&#24378;&#27010;&#24565;&#28418;&#31227;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OneNet: Enhancing Time Series Forecasting Models under Concept Drift by Online Ensembling. (arXiv:2309.12659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12659
&lt;/p&gt;
&lt;p&gt;
OneNet&#26159;&#19968;&#20010;&#22312;&#32447;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32447;&#38598;&#25104;&#20004;&#20010;&#27169;&#22411;&#26469;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21478;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;OneNet&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26356;&#26032;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#27969;&#25968;&#25454;&#30340;&#39640;&#25928;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#26469;&#35299;&#20915;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#35768;&#22810;&#31639;&#27861;&#34987;&#35774;&#35745;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#19968;&#20123;&#21033;&#29992;&#20132;&#21449;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#32780;&#20854;&#20182;&#31639;&#27861;&#21017;&#20551;&#35774;&#21464;&#37327;&#20043;&#38388;&#30456;&#20114;&#29420;&#31435;&#12290;&#37492;&#20110;&#27599;&#20010;&#25968;&#25454;&#20551;&#35774;&#22312;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#37117;&#26377;&#20854;&#20248;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OneNet&#65288;&#22312;&#32447;&#38598;&#25104;&#32593;&#32476;&#65289;&#12290;&#23427;&#21160;&#24577;&#26356;&#26032;&#24182;&#32467;&#21512;&#20004;&#20010;&#27169;&#22411;&#65292;&#19968;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#24314;&#27169;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21478;&#19968;&#20010;&#27169;&#22411;&#19987;&#27880;&#20110;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#34701;&#20837;&#20256;&#32479;&#30340;&#22312;&#32447;&#20984;&#35268;&#21010;&#26694;&#26550;&#20013;&#65292;&#20801;&#35768;&#32447;&#24615;&#32452;&#21512;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#24182;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#12290;OneNet&#35299;&#20915;&#20102;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#36866;&#24212;&#27010;&#24565;&#28418;&#31227;&#26041;&#38754;&#36807;&#20110;&#32531;&#24930;&#30340;&#20027;&#35201;&#32570;&#28857;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;OneNet&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online updating of time series forecasting models aims to address the concept drifting problem by efficiently updating forecasting models based on streaming data. Many algorithms are designed for online time series forecasting, with some exploiting cross-variable dependency while others assume independence among variables. Given every data assumption has its own pros and cons in online time series modeling, we propose \textbf{On}line \textbf{e}nsembling \textbf{Net}work (OneNet). It dynamically updates and combines two models, with one focusing on modeling the dependency across the time dimension and the other on cross-variate dependency. Our method incorporates a reinforcement learning-based approach into the traditional online convex programming framework, allowing for the linear combination of the two models with dynamically adjusted weights. OneNet addresses the main shortcoming of classical online learning methods that tend to be slow in adapting to the concept drift. Empirical re
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27491;&#21017;&#21270;Stein&#24046;&#24322;&#30340;&#31070;&#32463;&#31639;&#23376;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#29983;&#25104;&#22120;&#33719;&#24471;&#21462;&#26679;&#22120;&#20197;&#21450;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#21644;&#23376;&#37319;&#26679;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25512;&#26029;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12658</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;Stein&#24046;&#24322;&#30340;&#31070;&#32463;&#31639;&#23376;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Operator Variational Inference based on Regularized Stein Discrepancy for Deep Gaussian Processes. (arXiv:2309.12658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12658
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;Stein&#24046;&#24322;&#30340;&#31070;&#32463;&#31639;&#23376;&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#29983;&#25104;&#22120;&#33719;&#24471;&#21462;&#26679;&#22120;&#20197;&#21450;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#21644;&#23376;&#37319;&#26679;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25512;&#26029;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65288;DGP&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#38750;&#21442;&#25968;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#31934;&#30830;&#25512;&#26029;&#36890;&#24120;&#26159;&#38590;&#20197;&#27714;&#35299;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#22343;&#20540;&#22330;&#39640;&#26031;&#20551;&#35774;&#65292;&#38480;&#21046;&#20102;DGP&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25928;&#26524;&#65292;&#32780;&#38543;&#26426;&#36924;&#36817;&#21487;&#33021;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#65288;NOVI&#65289;&#29992;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#12290;NOVI&#20351;&#29992;&#31070;&#32463;&#29983;&#25104;&#22120;&#33719;&#24471;&#21462;&#26679;&#22120;&#65292;&#24182;&#22312;L2&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#29983;&#25104;&#20998;&#24067;&#21644;&#30495;&#23454;&#21518;&#39564;&#20043;&#38388;&#30340;&#27491;&#21017;&#21270;Stein&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#21644;&#23376;&#37319;&#26679;&#38543;&#26426;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20102;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;Fisher&#25955;&#24230;&#19982;&#24120;&#25968;&#30456;&#20056;&#26469;&#25511;&#21046;&#26041;&#27861;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#35823;&#24046;&#25511;&#21046;&#65292;&#30830;&#20445;&#20102;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Gaussian Process (DGP) models offer a powerful nonparametric approach for Bayesian inference, but exact inference is typically intractable, motivating the use of various approximations. However, existing approaches, such as mean-field Gaussian assumptions, limit the expressiveness and efficacy of DGP models, while stochastic approximation can be computationally expensive. To tackle these challenges, we introduce Neural Operator Variational Inference (NOVI) for Deep Gaussian Processes. NOVI uses a neural generator to obtain a sampler and minimizes the Regularized Stein Discrepancy in L2 space between the generated distribution and true posterior. We solve the minimax problem using Monte Carlo estimation and subsampling stochastic optimization techniques. We demonstrate that the bias introduced by our method can be controlled by multiplying the Fisher divergence with a constant, which leads to robust error control and ensures the stability and precision of the algorithm. Our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FP-PET&#65292;&#20351;&#29992;STUNet-large&#12289;SwinUNETR&#21644;VNet&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;CT&#21644;PET&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;Dice&#20998;&#25968;&#12289;&#20551;&#38451;&#24615;&#20307;&#31215;&#21644;&#20551;&#38452;&#24615;&#20307;&#31215;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#20998;&#21106;&#36755;&#20986;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.12650</link><description>&lt;p&gt;
FP-PET: &#22823;&#27169;&#22411;&#65292;&#22810;&#31181;&#25439;&#22833;&#21644;&#19987;&#27880;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
FP-PET: Large Model, Multiple Loss And Focused Practice. (arXiv:2309.12650v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FP-PET&#65292;&#20351;&#29992;STUNet-large&#12289;SwinUNETR&#21644;VNet&#31561;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;CT&#21644;PET&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#65292;&#32508;&#21512;&#32771;&#34385;&#20102;Dice&#20998;&#25968;&#12289;&#20551;&#38451;&#24615;&#20307;&#31215;&#21644;&#20551;&#38452;&#24615;&#20307;&#31215;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#20998;&#21106;&#36755;&#20986;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FP-PET&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;CT&#21644;PET&#22270;&#20687;&#12290;&#21033;&#29992;AutoPet2023&#25361;&#25112;&#36187;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#37319;&#29992;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;STUNet-large&#65292;SwinUNETR&#21644;VNet&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#21512;&#20102;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;Dice&#20998;&#25968;&#65292;&#20551;&#38451;&#24615;&#20307;&#31215;&#65288;FPV&#65289;&#21644;&#20551;&#38452;&#24615;&#20307;&#31215;&#65288;FNV&#65289;&#65292;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#24230;&#37327;&#12290;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#19982;&#27169;&#22411;&#35757;&#32451;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#22312;&#39640;&#24615;&#33021;GPU&#19978;&#36827;&#34892;&#30340;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#20998;&#21106;&#36755;&#20986;&#65292;&#25506;&#32034;&#20102;&#21253;&#25324;&#39640;&#26031;&#21152;&#26435;&#26041;&#26696;&#21644;&#24418;&#24577;&#23398;&#25805;&#20316;&#22312;&#20869;&#30340;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#20026;&#39640;&#32423;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents FP-PET, a comprehensive approach to medical image segmentation with a focus on CT and PET images. Utilizing a dataset from the AutoPet2023 Challenge, the research employs a variety of machine learning models, including STUNet-large, SwinUNETR, and VNet, to achieve state-of-the-art segmentation performance. The paper introduces an aggregated score that combines multiple evaluation metrics such as Dice score, false positive volume (FPV), and false negative volume (FNV) to provide a holistic measure of model effectiveness. The study also discusses the computational challenges and solutions related to model training, which was conducted on high-performance GPUs. Preprocessing and postprocessing techniques, including gaussian weighting schemes and morphological operations, are explored to further refine the segmentation output. The research offers valuable insights into the challenges and solutions for advanced medical image segmentation.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.12632</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#26159;&#21542;&#20844;&#27491;&#21487;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Deep Learning Classification Results Obtained on CT Scans Fair and Interpretable?. (arXiv:2309.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12632
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;CT&#25195;&#25551;&#20998;&#31867;&#20013;&#30340;&#32467;&#26524;&#24448;&#24448;&#21482;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20844;&#27491;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#20449;&#21644;&#19981;&#36866;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#21644;&#29289;&#20307;&#20998;&#31867;&#20013;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20063;&#38754;&#20020;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#21160;&#35786;&#26029;&#26696;&#20363;&#30340;&#21387;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#23581;&#35797;&#20165;&#20165;&#20851;&#27880;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#21487;&#35299;&#37322;&#24615;&#25110;&#32773;&#24739;&#32773;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#31163;&#12290;&#20363;&#22914;&#65292;&#22823;&#37096;&#20998;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32954;&#32467;&#33410;&#20998;&#31867;&#35770;&#25991;&#20250;&#23545;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#27927;&#29260;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#20154;&#30340;CT&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#26576;&#20123;&#22270;&#20687;&#20301;&#20110;&#35757;&#32451;&#38598;&#20013;&#65292;&#32780;&#20854;&#20182;&#22270;&#20687;&#21017;&#20301;&#20110;&#39564;&#35777;&#25110;&#27979;&#35797;&#22270;&#20687;&#38598;&#20013;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#20934;&#30830;&#29575;&#25253;&#21578;&#21644;&#23398;&#20064;&#21040;&#30340;&#26080;&#20851;&#29305;&#24449;&#65292;&#26368;&#32456;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the great success of various deep learning methods in image and object classification, the biomedical image processing society is also overwhelmed with their applications to various automatic diagnosis cases. Unfortunately, most of the deep learning-based classification attempts in the literature solely focus on the aim of extreme accuracy scores, without considering interpretability, or patient-wise separation of training and test data. For example, most lung nodule classification papers using deep learning randomly shuffle data and split it into training, validation, and test sets, causing certain images from the CT scan of a person to be in the training set, while other images of the exact same person to be in the validation or testing image sets. This can result in reporting misleading accuracy rates and the learning of irrelevant features, ultimately reducing the real-life usability of these models. When the deep neural networks trained on the traditional, unfair data sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sequential Action--Induced Invariant Representation (SAR)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21253;&#21547;&#20219;&#21153;&#20851;&#38190;&#20449;&#21495;&#30340;&#21160;&#20316;&#24207;&#21015;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20174;&#35270;&#35273;&#24178;&#25200;&#30340;&#39640;&#32500;&#35266;&#27979;&#20013;&#20934;&#30830;&#23398;&#20064;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29366;&#24577;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12628</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24207;&#36143;&#21160;&#20316;&#24341;&#21457;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sequential Action-Induced Invariant Representation for Reinforcement Learning. (arXiv:2309.12628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sequential Action--Induced Invariant Representation (SAR)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21253;&#21547;&#20219;&#21153;&#20851;&#38190;&#20449;&#21495;&#30340;&#21160;&#20316;&#24207;&#21015;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20174;&#35270;&#35273;&#24178;&#25200;&#30340;&#39640;&#32500;&#35266;&#27979;&#20013;&#20934;&#30830;&#23398;&#20064;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29366;&#24577;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#20174;&#35270;&#35273;&#24178;&#25200;&#30340;&#39640;&#32500;&#35266;&#27979;&#20013;&#20934;&#30830;&#23398;&#20064;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29366;&#24577;&#34920;&#31034;&#26159;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#29616;&#23454;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23545;&#27604;&#12289;&#39044;&#27979;&#21644;&#37325;&#24314;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#25552;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#39044;&#27979;&#12289;&#23545;&#27604;&#21644;&#37325;&#24314;&#26041;&#27861;&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#20219;&#21153;&#20449;&#24687;&#25552;&#21462;&#26426;&#21046;&#20197;&#21450;&#22312;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#20013;&#30340;&#21452;&#27169;&#25311;&#30456;&#20851;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#22320;&#25193;&#23637;&#21040;&#20855;&#26377;&#24178;&#25200;&#30340;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#23558;&#21253;&#21547;&#20219;&#21153;&#20851;&#38190;&#20449;&#21495;&#30340;&#21160;&#20316;&#24207;&#21015;&#32435;&#20837;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sequential Action--Induced Invariant Representation (SAR)&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#34987;&#20248;&#21270;...
&lt;/p&gt;
&lt;p&gt;
How to accurately learn task-relevant state representations from high-dimensional observations with visual distractions is a realistic and challenging problem in visual reinforcement learning. Recently, unsupervised representation learning methods based on bisimulation metrics, contrast, prediction, and reconstruction have shown the ability for task-relevant information extraction. However, due to the lack of appropriate mechanisms for the extraction of task information in the prediction, contrast, and reconstruction-related approaches and the limitations of bisimulation-related methods in domains with sparse rewards, it is still difficult for these methods to be effectively extended to environments with distractions. To alleviate these problems, in the paper, the action sequences, which contain task-intensive signals, are incorporated into representation learning. Specifically, we propose a Sequential Action--induced invariant Representation (SAR) method, in which the encoder is optim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#27491;&#21017;&#21270;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#20102;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12620</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#27169;&#22411;&#22312;&#26102;&#38388;&#20934;&#21017;&#19979;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-driven Preference Learning Methods for Multiple Criteria Sorting with Temporal Criteria. (arXiv:2309.12620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#27491;&#21017;&#21270;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20445;&#25345;&#20102;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#26041;&#27861;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20559;&#22909;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23384;&#22312;&#26102;&#38388;&#20934;&#21017;&#30340;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#20108;&#27425;&#35268;&#21010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#24182;&#22312;&#27491;&#21017;&#21270;&#26694;&#26550;&#20869;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#21512;&#24182;&#22810;&#20010;&#21487;&#33021;&#36739;&#24369;&#30340;&#20248;&#21270;&#22120;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#24182;&#34892;&#35745;&#31639;&#39640;&#25928;&#25191;&#34892;&#27492;&#36807;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#24182;&#36866;&#24212;&#21487;&#23398;&#20064;&#30340;&#26102;&#38388;&#25240;&#25187;&#22240;&#23376;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#35843;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(mRNN)&#12290;&#23427;&#26088;&#22312;&#25429;&#25417;&#38543;&#26102;&#38388;&#28436;&#21270;&#30340;&#20559;&#22909;&#21160;&#24577;&#65292;&#21516;&#26102;&#20445;&#25345;&#22810;&#20934;&#21017;&#25490;&#24207;&#38382;&#39064;&#22266;&#26377;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#20934;&#21017;&#30340;&#39034;&#24207;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of predictive methodologies has catalyzed the emergence of data-driven decision support across various domains. However, developing models capable of effectively handling input time series data presents an enduring challenge. This study presents novel preference learning approaches to multiple criteria sorting problems in the presence of temporal criteria. We first formulate a convex quadratic programming model characterized by fixed time discount factors, operating within a regularization framework. Additionally, we propose an ensemble learning algorithm designed to consolidate the outputs of multiple, potentially weaker, optimizers, a process executed efficiently through parallel computation. To enhance scalability and accommodate learnable time discount factors, we introduce a novel monotonic Recurrent Neural Network (mRNN). It is designed to capture the evolving dynamics of preferences over time while upholding critical properties inherent to MCS problems, including crit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38646;&#21518;&#24724;&#30340;&#19981;&#24179;&#31561;&#32422;&#26463;&#19979;&#30340;&#28436;&#21270;&#24615;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#36890;&#36807;&#24320;&#21457;&#31283;&#20581;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#28436;&#21270;&#24615;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.12618</link><description>&lt;p&gt;
&#38646;&#21518;&#24724;&#30340;&#19981;&#24179;&#31561;&#32422;&#26463;&#19979;&#30340;&#28436;&#21270;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Regret Performative Prediction Under Inequality Constraints. (arXiv:2309.12618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38646;&#21518;&#24724;&#30340;&#19981;&#24179;&#31561;&#32422;&#26463;&#19979;&#30340;&#28436;&#21270;&#24615;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#36890;&#36807;&#24320;&#21457;&#31283;&#20581;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#28436;&#21270;&#24615;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21270;&#24615;&#39044;&#27979;&#26159;&#19968;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#39044;&#27979;&#25351;&#23548;&#20915;&#31574;&#24182;&#22240;&#27492;&#24433;&#21709;&#26410;&#26469;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#28436;&#21270;&#29616;&#35937;&#22312;&#20132;&#36890;&#12289;&#37329;&#34701;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20851;&#20110;&#28436;&#21270;&#24615;&#39044;&#27979;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#26080;&#32422;&#26463;&#24773;&#20917;&#65292;&#24573;&#35270;&#20102;&#35768;&#22810;&#23454;&#38469;&#23398;&#20064;&#38382;&#39064;&#21463;&#38480;&#21046;&#30340;&#20107;&#23454;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19981;&#24179;&#31561;&#32422;&#26463;&#19979;&#30340;&#28436;&#21270;&#24615;&#39044;&#27979;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#21482;&#25552;&#20379;&#28436;&#21270;&#31283;&#23450;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#39044;&#27979;&#24615;&#26799;&#24230;&#30340;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#28436;&#21270;&#24615;&#30340;&#26080;&#30693;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#65292;&#23427;&#21482;&#38656;&#35201;&#36817;&#20284;&#26799;&#24230;&#36798;&#21040;&#19968;&#23450;&#30340;&#31934;&#24230;&#65292;&#20294;&#25552;&#20379;&#19982;&#38543;&#26426;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#30456;&#21516;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction is a recently proposed framework where predictions guide decision-making and hence influence future data distributions. Such performative phenomena are ubiquitous in various areas, such as transportation, finance, public policy, and recommendation systems. To date, work on performative prediction has only focused on unconstrained scenarios, neglecting the fact that many real-world learning problems are subject to constraints. This paper bridges this gap by studying performative prediction under inequality constraints. Unlike most existing work that provides only performative stable points, we aim to find the optimal solutions. Anticipating performative gradients is a challenging task, due to the agnostic performative effect on data distributions. To address this issue, we first develop a robust primal-dual framework that requires only approximate gradients up to a certain accuracy, yet delivers the same order of performance as the stochastic primal-dual algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#40065;&#26834;&#24615;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#21512;&#29702;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#20010;&#20307;&#21327;&#21464;&#37327;&#20998;&#24067;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12600</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#40065;&#26834;&#24615;&#32852;&#37030;&#20272;&#35745;&#30340;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multiply Robust Federated Estimation of Targeted Average Treatment Effects. (arXiv:2309.12600v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#24615;&#40065;&#26834;&#24615;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#22810;&#20013;&#24515;&#25968;&#25454;&#36827;&#34892;&#21512;&#29702;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#20010;&#20307;&#21327;&#21464;&#37327;&#20998;&#24067;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25110;&#22810;&#20013;&#24515;&#30740;&#31350;&#30456;&#27604;&#21333;&#20013;&#24515;&#30740;&#31350;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#21253;&#25324;&#22686;&#21152;&#26222;&#36866;&#24615;&#12289;&#33021;&#22815;&#30740;&#31350;&#23569;&#25968;&#32676;&#20307;&#21644;&#30740;&#31350;&#31232;&#26377;&#26292;&#38706;&#19982;&#32467;&#26524;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#23384;&#22312;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#20010;&#20307;&#21327;&#21464;&#37327;&#20998;&#24067;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#22810;&#26679;&#24615;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#36741;&#21161;&#20989;&#25968;&#20272;&#35745;&#65292;&#20026;&#30446;&#26631;&#20154;&#32676;&#25552;&#20379;&#26377;&#25928;&#30340;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#24314;&#31435;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28304;&#31449;&#28857;&#30340;&#32452;&#21512;&#26435;&#37325;&#20197;&#32452;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#26159;&#39640;&#25928;&#21644;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26377;&#38480;&#26679;&#26412;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated or multi-site studies have distinct advantages over single-site studies, including increased generalizability, the ability to study underrepresented populations, and the opportunity to study rare exposures and outcomes. However, these studies are challenging due to the need to preserve the privacy of each individual's data and the heterogeneity in their covariate distributions. We propose a novel federated approach to derive valid causal inferences for a target population using multi-site data. We adjust for covariate shift and covariate mismatch between sites by developing multiply-robust and privacy-preserving nuisance function estimation. Our methodology incorporates transfer learning to estimate ensemble weights to combine information from source sites. We show that these learned weights are efficient and optimal under different scenarios. We showcase the finite sample advantages of our approach in terms of efficiency and robustness compared to existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22312;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#29616;&#26377;&#30740;&#31350;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12593</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Machine Learning Robustness via Adversarial Training. (arXiv:2309.12593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22312;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#29616;&#26377;&#30740;&#31350;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#20219;&#20309;&#28508;&#22312;&#30340;&#26368;&#22351;&#24773;&#20917;&#22122;&#22768;&#12289;&#23545;&#25239;&#25915;&#20987;&#21644;&#38750;&#24120;&#19981;&#23547;&#24120;&#30340;&#24773;&#20917;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#23558;&#22312;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#22312;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#12290;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;Fast Gradient Sign Method&#21644;DeepFool&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24471;&#21040;&#20102;65.41%&#21644;83.0%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#30456;&#27604;&#65292;&#36825;&#20123;&#32467;&#26524;&#20998;&#21035;&#25552;&#39640;&#20102;18.41%&#21644;47%&#12290;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#21644;&#38750;IID&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Machine Learning (ML) is increasingly used in solving various tasks in real-world applications, it is crucial to ensure that ML algorithms are robust to any potential worst-case noises, adversarial attacks, and highly unusual situations when they are designed. Studying ML robustness will significantly help in the design of ML algorithms. In this paper, we investigate ML robustness using adversarial training in centralized and decentralized environments, where ML training and testing are conducted in one or multiple computers. In the centralized environment, we achieve a test accuracy of 65.41% and 83.0% when classifying adversarial examples generated by Fast Gradient Sign Method and DeepFool, respectively. Comparing to existing studies, these results demonstrate an improvement of 18.41% for FGSM and 47% for DeepFool. In the decentralized environment, we study Federated learning (FL) robustness by using adversarial training with independent and identically distributed (IID) and non-I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#30340;&#36890;&#29992;&#22768;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#32463;&#35757;&#32451;&#30340;&#37319;&#26679;&#39057;&#29575;&#65292;&#20026;&#23454;&#29616;&#26222;&#36866;&#30340;&#28304;&#20998;&#31163;&#22120;&#25552;&#20379;&#20102;&#20851;&#38190;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.12581</link><description>&lt;p&gt;
&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#30340;&#36890;&#29992;&#22768;&#38899;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sampling-Frequency-Independent Universal Sound Separation. (arXiv:2309.12581v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#30340;&#36890;&#29992;&#22768;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#32463;&#35757;&#32451;&#30340;&#37319;&#26679;&#39057;&#29575;&#65292;&#20026;&#23454;&#29616;&#26222;&#36866;&#30340;&#28304;&#20998;&#31163;&#22120;&#25552;&#20379;&#20102;&#20851;&#38190;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#26410;&#32463;&#35757;&#32451;&#30340;&#37319;&#26679;&#39057;&#29575;&#65288;SF&#65289;&#30340;&#36890;&#29992;&#22768;&#38899;&#20998;&#31163;&#65288;USS&#65289;&#26041;&#27861;&#12290;USS&#26088;&#22312;&#20998;&#31163;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#24847;&#28304;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#22788;&#29702;&#22120;&#32780;&#34987;&#26222;&#36941;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#30340;&#28304;&#20998;&#31163;&#22120;&#65292;&#26377;&#20004;&#20010;&#24517;&#35201;&#23646;&#24615;&#65306;&#23545;&#28304;&#31867;&#22411;&#21644;&#24405;&#21046;&#26465;&#20214;&#30340;&#36890;&#29992;&#24615;&#12290;&#21069;&#32773;&#30340;&#23646;&#24615;&#24050;&#32463;&#22312;USS&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#65292;&#22823;&#22823;&#22686;&#21152;&#20102;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22788;&#29702;&#30340;&#28304;&#31867;&#22411;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#30340;&#23646;&#24615;&#65288;&#20363;&#22914;&#65292;SF&#65289;&#23613;&#31649;&#20854;&#24517;&#35201;&#24615;&#65292;&#20294;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;SF&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#30340;&#19981;&#21516;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#36890;&#29992;&#30340;&#28304;&#20998;&#31163;&#22120;&#24517;&#39035;&#33021;&#22788;&#29702;&#21508;&#31181; SF&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#28085;&#30422;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;USS&#32593;&#32476;SuDoRM-RF&#30340;SF&#26080;&#20851;&#65288;SFI&#65289;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a universal sound separation (USS) method capable of handling untrained sampling frequencies (SFs). The USS aims at separating arbitrary sources of different types and can be the key technique to realize a source separator that can be universally used as a preprocessor for any downstream tasks. To realize a universal source separator, there are two essential properties: universalities with respect to source types and recording conditions. The former property has been studied in the USS literature, which has greatly increased the number of source types that can be handled by a single neural network. However, the latter property (e.g., SF) has received less attention despite its necessity. Since the SF varies widely depending on the downstream tasks, the universal source separator must handle a wide variety of SFs. In this paper, to encompass the two properties, we propose an SF-independent (SFI) extension of a computationally efficient USS network, SuDoRM-RF. The pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27867;&#28389;&#22635;&#20805;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#36880;&#23618;&#31232;&#30095;&#27169;&#24335;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.12578</link><description>&lt;p&gt;
SPION&#65306;&#36890;&#36807;&#21367;&#31215;&#27867;&#28389;&#22635;&#20805;&#23454;&#29616;Transformer&#30340;&#36880;&#23618;&#31232;&#30095;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
SPION: Layer-Wise Sparse Training of Transformer via Convolutional Flood Filling. (arXiv:2309.12578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#38598;&#25104;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27867;&#28389;&#22635;&#20805;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#36880;&#23618;&#31232;&#30095;&#27169;&#24335;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#21270;Transformer&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#35757;&#32451;Transformer&#38656;&#35201;&#24456;&#22823;&#30340;&#35745;&#31639;&#37327;&#12290;&#20808;&#21069;&#31232;&#30095;&#21270;Transformer&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#22266;&#23450;&#30340;&#27169;&#24335;&#65292;&#35201;&#20040;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#28041;&#21450;&#35745;&#31639;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#25805;&#20316;&#25968;&#37327;&#65292;&#36825;&#26159;Transformer&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#22914;&#30001;&#20110;&#22312;&#25152;&#26377;&#23618;&#20013;&#24212;&#29992;&#32479;&#19968;&#30340;&#22266;&#23450;&#27169;&#24335;&#32780;&#23548;&#33268;&#30340;&#28508;&#22312;&#24207;&#21015;&#20851;&#38190;&#29305;&#24449;&#25439;&#22833;&#65292;&#20197;&#21450;&#30001;&#20110;&#20351;&#29992;&#39069;&#22806;&#21442;&#25968;&#23398;&#20064;&#27880;&#24847;&#21147;&#25805;&#20316;&#20013;&#30340;&#31232;&#30095;&#27169;&#24335;&#32780;&#23548;&#33268;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27867;&#28389;&#22635;&#20805;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#39640;&#25928;&#22320;&#25429;&#25417;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#36880;&#23618;&#31232;&#30095;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#31232;&#30095;&#21270;&#26041;&#27861;&#22312;Transformer&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#21344;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparsifying the Transformer has garnered considerable interest, as training the Transformer is very computationally demanding. Prior efforts to sparsify the Transformer have either used a fixed pattern or data-driven approach to reduce the number of operations involving the computation of multi-head attention, which is the main bottleneck of the Transformer. However, existing methods suffer from inevitable problems, such as the potential loss of essential sequence features due to the uniform fixed pattern applied across all layers, and an increase in the model size resulting from the use of additional parameters to learn sparsity patterns in attention operations. In this paper, we propose a novel sparsification scheme for the Transformer that integrates convolution filters and the flood filling method to efficiently capture the layer-wise sparse pattern in attention operations. Our sparsification approach reduces the computational complexity and memory footprint of the Transformer duri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21407;&#22987;&#30524;&#21160;&#25968;&#25454;&#19978;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;VTNet&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;VTNet&#22312;AD&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#20174;&#30524;&#21160;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12574</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#21160;&#25968;&#25454;&#19978;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Alzheimers Disease with Deep Learning on Eye-tracking Data. (arXiv:2309.12574v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21407;&#22987;&#30524;&#21160;&#25968;&#25454;&#19978;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;VTNet&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;VTNet&#22312;AD&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#20174;&#30524;&#21160;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#20998;&#31867;&#22120;&#23545;&#30524;&#21160;&#25968;&#25454;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149; (AD) &#20998;&#31867;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20197;&#21407;&#22987;&#30524;&#21160;&#25968;&#25454;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#26469;&#25913;&#36827;&#29616;&#26377;&#32467;&#26524;&#12290;&#35813;&#20998;&#31867;&#22120; (VTNet) &#21516;&#26102;&#21033;&#29992; GRU &#21644; CNN &#26469;&#21033;&#29992;&#30524;&#21160;&#25968;&#25454;&#30340;&#35270;&#35273; (V) &#21644;&#26102;&#38388; (T) &#34920;&#31034;&#65292;&#24182;&#19988;&#20808;&#21069;&#34987;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#22312;&#22788;&#29702;&#35270;&#35273;&#26174;&#31034;&#26102;&#30340;&#22256;&#24785;&#12290;&#23558;VTNet&#24212;&#29992;&#20110;AD&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#21487;&#29992;&#30340;&#30524;&#21160;&#25968;&#25454;&#24207;&#21015;&#27604;&#20808;&#21069;&#30340;&#22256;&#24785;&#26816;&#27979;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#24207;&#21015;&#35201;&#38271;&#24471;&#22810;&#65292;&#36825;&#23558;LSTM-based&#27169;&#22411;&#30340;&#21487;&#22788;&#29702;&#24615;&#25512;&#21521;&#26497;&#38480;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;VTNet&#22312;AD&#20998;&#31867;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#20010;&#27169;&#22411;&#20174;&#30524;&#21160;&#25968;&#25454;&#20013;&#36827;&#34892;&#39044;&#27979;&#30340;&#26222;&#36866;&#24615;&#25552;&#20379;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research has shown the potential of classifying Alzheimers Disease (AD) from eye-tracking (ET) data with classifiers that rely on task-specific engineered features. In this paper, we investigate whether we can improve on existing results by using a Deep-Learning classifier trained end-to-end on raw ET data. This classifier (VTNet) uses a GRU and a CNN in parallel to leverage both visual (V) and temporal (T) representations of ET data and was previously used to detect user confusion while processing visual displays. A main challenge in applying VTNet to our target AD classification task is that the available ET data sequences are much longer than those used in the previous confusion detection task, pushing the limits of what is manageable by LSTM-based models. We discuss how we address this challenge and show that VTNet outperforms the state-of-the-art approaches in AD classification, providing encouraging evidence on the generality of this model to make predictions from ET dat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;3D&#22810;&#27169;&#24577;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MRCNN&#65289;&#29992;&#20110;&#36731;&#24230;&#39045;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#35786;&#26029;&#65292;&#32467;&#21512;&#36974;&#34109;&#25935;&#24863;&#24230;&#22270;&#65288;OSM&#65289;&#36827;&#34892;&#22686;&#24378;&#12290;&#19982;&#20256;&#32479;&#30340;CT&#22270;&#20687;&#35786;&#26029;&#30456;&#27604;&#65292;MRCNN&#27169;&#22411;&#22312;&#29305;&#24322;&#24230;&#21644;&#20934;&#30830;&#29575;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.12572</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;3D&#22810;&#27169;&#24577;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36731;&#24230;&#39045;&#33041;&#25439;&#20260;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interpretable 3D Multi-Modal Residual Convolutional Neural Network for Mild Traumatic Brain Injury Diagnosis. (arXiv:2309.12572v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12572
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;3D&#22810;&#27169;&#24577;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MRCNN&#65289;&#29992;&#20110;&#36731;&#24230;&#39045;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#35786;&#26029;&#65292;&#32467;&#21512;&#36974;&#34109;&#25935;&#24863;&#24230;&#22270;&#65288;OSM&#65289;&#36827;&#34892;&#22686;&#24378;&#12290;&#19982;&#20256;&#32479;&#30340;CT&#22270;&#20687;&#35786;&#26029;&#30456;&#27604;&#65292;MRCNN&#27169;&#22411;&#22312;&#29305;&#24322;&#24230;&#21644;&#20934;&#30830;&#29575;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#24230;&#39045;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#30001;&#20110;&#20854;&#39640;&#21457;&#30149;&#29575;&#21644;&#28508;&#22312;&#30340;&#38271;&#26399;&#20581;&#24247;&#24433;&#21709;&#32780;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#23613;&#31649;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;mTBI&#30340;&#26631;&#20934;&#35786;&#26029;&#24037;&#20855;&#65292;&#20294;&#22312;mTBI&#24739;&#32773;&#20013;&#24448;&#24448;&#20250;&#20986;&#29616;&#27491;&#24120;&#32467;&#26524;&#65292;&#23613;&#31649;&#23384;&#22312;&#30151;&#29366;&#35777;&#25454;&#12290;&#36825;&#19968;&#20107;&#23454;&#20984;&#26174;&#20102;&#20934;&#30830;&#35786;&#26029;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;3D&#22810;&#27169;&#24577;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;MRCNN&#65289;&#29992;&#20110;mTBI&#35786;&#26029;&#27169;&#22411;&#65292;&#32467;&#21512;&#36974;&#34109;&#25935;&#24863;&#24230;&#22270;&#65288;OSM&#65289;&#36827;&#34892;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;MRCNN&#27169;&#22411;&#22312;mTBI&#35786;&#26029;&#26041;&#38754;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32463;&#30001;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#27969;&#31243;&#39564;&#35777;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;82.4%&#65292;&#28789;&#25935;&#24230;&#20026;82.6%&#65292;&#29305;&#24322;&#24230;&#20026;81.6%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#22522;&#20110;CT&#30340;&#27531;&#24046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RCNN&#65289;&#27169;&#22411;&#30456;&#27604;&#65292;MRCNN&#22312;&#29305;&#24322;&#24230;&#19978;&#25552;&#39640;&#20102;4.4%&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;9.0%&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OSM&#22312;&#27604;&#36739;CT&#22270;&#20687;&#26102;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mild Traumatic Brain Injury (mTBI) is a significant public health challenge due to its high prevalence and potential for long-term health effects. Despite Computed Tomography (CT) being the standard diagnostic tool for mTBI, it often yields normal results in mTBI patients despite symptomatic evidence. This fact underscores the complexity of accurate diagnosis. In this study, we introduce an interpretable 3D Multi-Modal Residual Convolutional Neural Network (MRCNN) for mTBI diagnostic model enhanced with Occlusion Sensitivity Maps (OSM). Our MRCNN model exhibits promising performance in mTBI diagnosis, demonstrating an average accuracy of 82.4%, sensitivity of 82.6%, and specificity of 81.6%, as validated by a five-fold cross-validation process. Notably, in comparison to the CT-based Residual Convolutional Neural Network (RCNN) model, the MRCNN shows an improvement of 4.4% in specificity and 9.0% in accuracy. We show that the OSM offers superior data-driven insights into CT images compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12545</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provably Robust and Plausible Counterfactual Explanations for Neural Networks via Robust Optimisation. (arXiv:2309.12545v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#21644;&#21487;&#20449;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#25345;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#19981;&#21512;&#29702;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;(CEs)&#20316;&#20026;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20027;&#35201;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;CEs&#23545;&#20110;&#36755;&#20837;-&#36755;&#20986;&#23545;&#34987;&#23450;&#20041;&#20026;&#21040;&#36755;&#20837;&#30340;&#26368;&#23567;&#36317;&#31163;&#30340;&#25968;&#25454;&#28857;&#65292;&#20854;&#19982;&#36755;&#20986;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;CEs&#22312;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;(&#27604;&#22914;&#37325;&#26032;&#35757;&#32451;)&#26102;&#24456;&#23481;&#26131;&#34987;&#26080;&#25928;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#21464;&#21270;&#30340;&#33539;&#25968;&#29699;&#30028;&#38480;&#26469;&#35777;&#26126;CEs&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#19981;&#26159;&#23436;&#20840;&#27491;&#30830;&#30340;&#65292;&#25110;&#32773;&#21487;&#33021;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;CEs&#65292;&#21363;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31163;&#32676;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36317;&#31163;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20445;&#25345;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PROPLACE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CEs) have received increasing interest as a major methodology for explaining neural network classifiers. Usually, CEs for an input-output pair are defined as data points with minimum distance to the input that are classified with a different label than the output. To tackle the established problem that CEs are easily invalidated when model parameters are updated (e.g. retrained), studies have proposed ways to certify the robustness of CEs under model parameter changes bounded by a norm ball. However, existing methods targeting this form of robustness are not sound or complete, and they may generate implausible CEs, i.e., outliers wrt the training dataset. In fact, no existing method simultaneously optimises for proximity and plausibility while preserving robustness guarantees. In this work, we propose Provably RObust and PLAusible Counterfactual Explanations (PROPLACE), a method leveraging on robust optimisation techniques to address the aforementioned limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#32447;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#32771;&#34385;&#20102;&#39550;&#39542;&#26102;&#38388;&#21644;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2309.12534</link><description>&lt;p&gt;
&#26080;&#32447;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#31243;&#35268;&#21010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trip Planning for Autonomous Vehicles with Wireless Data Transfer Needs Using Reinforcement Learning. (arXiv:2309.12534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#32447;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#22312;&#22478;&#24066;&#21306;&#22495;&#20013;&#32771;&#34385;&#20102;&#39550;&#39542;&#26102;&#38388;&#21644;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#21644;&#29289;&#32852;&#32593;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36710;&#36742;&#23545;&#20854;&#29615;&#22659;&#30340;&#35748;&#30693;&#36234;&#26469;&#36234;&#22810;&#65292;&#24182;&#21521;&#30528;&#23436;&#20840;&#33258;&#20027;&#39550;&#39542;&#36827;&#21270;&#12290;&#36710;&#36742;&#36890;&#20449;&#25171;&#24320;&#20102;&#36710;&#36742;&#19982;&#22522;&#30784;&#35774;&#26045;&#36827;&#34892;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#65292;&#20854;&#20013;&#36710;&#36742;&#21487;&#20197;&#19982;&#25903;&#25345;&#22269;&#23478;&#36947;&#36335;&#31995;&#32479;&#30340;&#32452;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#12289;&#20132;&#36890;&#28783;&#21644;&#26631;&#35782;&#20998;&#20139;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#36710;&#36742;&#19981;&#20165;&#20165;&#26159;&#20132;&#36890;&#24037;&#20855;&#65292;&#23427;&#20204;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20256;&#36755;&#22823;&#37327;&#25968;&#25454;&#29992;&#20110;&#20351;&#39550;&#39542;&#26356;&#23433;&#20840;&#12289;&#26356;&#20415;&#25463;&#12290;&#38543;&#30528;5G&#34562;&#31389;&#32593;&#32476;&#21450;&#26356;&#39640;&#29256;&#26412;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#36947;&#36335;&#19978;&#23558;&#26377;&#26356;&#22810;&#30340;&#25968;&#25454;&#24102;&#23485;&#21487;&#29992;&#65292;&#20294;&#30001;&#20110;&#35270;&#32447;&#12289;&#22522;&#30784;&#35774;&#26045;&#21644;&#36947;&#36335;&#19978;&#24322;&#26500;&#30340;&#27969;&#37327;&#31561;&#38480;&#21046;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#26159;&#24322;&#26500;&#30340;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22478;&#24066;&#21306;&#22495;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34892;&#31243;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#39550;&#39542;&#26102;&#38388;&#21644;&#25968;&#25454;&#20256;&#36755;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in the field of communications and the Internet of Things, vehicles are becoming more aware of their environment and are evolving towards full autonomy. Vehicular communication opens up the possibility for vehicle-to-infrastructure interaction, where vehicles could share information with components such as cameras, traffic lights, and signage that support a countrys road system. As a result, vehicles are becoming more than just a means of transportation; they are collecting, processing, and transmitting massive amounts of data used to make driving safer and more convenient. With 5G cellular networks and beyond, there is going to be more data bandwidth available on our roads, but it may be heterogeneous because of limitations like line of sight, infrastructure, and heterogeneous traffic on the road. This paper addresses the problem of route planning for autonomous vehicles in urban areas accounting for both driving time and data transfer needs. We propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32423;&#32852;&#39044;&#27979;&#27169;&#22359;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#32423;&#21035;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#22810;&#27169;&#22359;&#31995;&#32479;&#20013;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12510</link><description>&lt;p&gt;
&#32423;&#32852;&#39044;&#27979;&#27169;&#22359;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence Calibration for Systems with Cascaded Predictive Modules. (arXiv:2309.12510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32423;&#32852;&#39044;&#27979;&#27169;&#22359;&#31995;&#32479;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22359;&#32423;&#21035;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#22810;&#27169;&#22359;&#31995;&#32479;&#20013;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21512;&#35268;&#39044;&#27979;&#31639;&#27861;&#22312;&#30446;&#26631;&#32622;&#20449;&#24230;&#27700;&#24179;&#19978;&#20272;&#35745;&#39044;&#27979;&#38388;&#38548;&#65292;&#20197;&#34920;&#24449;&#22238;&#24402;&#27169;&#22411;&#22312;&#26032;&#27979;&#35797;&#26679;&#26412;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#21253;&#21547;&#22810;&#20010;&#27169;&#22359;&#30340;&#33258;&#20027;&#31995;&#32479;&#65292;&#20026;&#21333;&#20010;&#27169;&#22359;&#26500;&#24314;&#30340;&#39044;&#27979;&#38388;&#38548;&#19981;&#36275;&#20197;&#23481;&#32435;&#19981;&#21516;&#27169;&#22359;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#22240;&#27492;&#26080;&#27861;&#20026;&#31995;&#32479;&#34892;&#20026;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#20379;&#38024;&#23545;&#32423;&#32852;&#27169;&#22359;&#32452;&#25104;&#30340;&#39044;&#27979;&#31995;&#32479;&#65288;&#20363;&#22914;&#65292;&#19978;&#28216;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#21644;&#19979;&#28216;&#22238;&#24402;&#27169;&#22359;&#65289;&#26657;&#20934;&#30340;&#39044;&#27979;&#38388;&#38548;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#27169;&#22359;&#32423;&#21035;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#20197;&#22312;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#31471;&#21040;&#31471;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#34920;&#24449;&#31995;&#32479;&#32423;&#35823;&#24046;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#21644;&#32463;&#39564;&#35777;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20197;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing conformal prediction algorithms estimate prediction intervals at target confidence levels to characterize the performance of a regression model on new test samples. However, considering an autonomous system consisting of multiple modules, prediction intervals constructed for individual modules fall short of accommodating uncertainty propagation over different modules and thus cannot provide reliable predictions on system behavior. We address this limitation and present novel solutions based on conformal prediction to provide prediction intervals calibrated for a predictive system consisting of cascaded modules (e.g., an upstream feature extraction module and a downstream regression module). Our key idea is to leverage module-level validation data to characterize the system-level error distribution without direct access to end-to-end validation data. We provide theoretical justification and empirical experimental results to demonstrate the effectiveness of proposed solutions. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861; DJINN&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#12290;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#65292;&#25105;&#20204;&#22312;&#36712;&#36857;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;DJINN&#36824;&#33021;&#28789;&#27963;&#22320;&#20174;&#22810;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2309.12508</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#20132;&#20114;&#24335;&#23548;&#33322;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Diffusion-Model of Joint Interactive Navigation. (arXiv:2309.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861; DJINN&#65292;&#29992;&#20110;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#12290;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#65292;&#25105;&#20204;&#22312;&#36712;&#36857;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;DJINN&#36824;&#33021;&#28789;&#27963;&#22320;&#20174;&#22810;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#27169;&#25311;&#20986;&#23637;&#29616;&#22810;&#26679;&#21644;&#30495;&#23454;&#34892;&#20026;&#30340;&#20132;&#36890;&#21442;&#19982;&#32773;&#12290;&#22312;&#27169;&#25311;&#20013;&#20351;&#29992;&#23454;&#38469;&#19990;&#30028;&#20132;&#36890;&#22330;&#26223;&#30830;&#20445;&#20102;&#30495;&#23454;&#24615;&#65292;&#20294;&#26159;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#32597;&#35265;&#24615;&#20351;&#24471;&#22823;&#35268;&#27169;&#25910;&#38598;&#39550;&#39542;&#22330;&#26223;&#20855;&#26377;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#20132;&#36890;&#22330;&#26223;&#30340;&#26041;&#27861;DJINN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32852;&#21512;&#25193;&#25955;&#25152;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#65292;&#20197;&#36807;&#21435;&#12289;&#29616;&#22312;&#25110;&#26410;&#26469;&#30340;&#19968;&#31995;&#21015;&#28789;&#27963;&#30340;&#29366;&#24577;&#35266;&#23519;&#20026;&#26465;&#20214;&#12290;&#22312;&#27969;&#34892;&#30340;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#22312;&#32852;&#21512;&#36712;&#36857;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DJINN&#22914;&#20309;&#28789;&#27963;&#22320;&#20351;&#24471;&#20174;&#21508;&#31181;&#26377;&#20215;&#20540;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#30452;&#25509;&#36827;&#34892;&#27979;&#35797;&#26102;&#25277;&#26679;&#65292;&#21253;&#25324;&#22522;&#20110;&#30446;&#26631;&#30340;&#25277;&#26679;&#12289;&#34892;&#20026;&#31867;&#21035;&#25277;&#26679;&#21644;&#22330;&#26223;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN - a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12501</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding: An Overview. (arXiv:2309.12501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#30740;&#31350;&#29366;&#24577;&#65292;&#20171;&#32461;&#20102;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;&#22522;&#20110;&#36317;&#31163;&#21644;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;CompoundE&#21644;CompoundE3D&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#23398;&#27169;&#22411;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#23884;&#20837;&#65292;&#20197;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#21644;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#25968;&#23398;&#21551;&#21457;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#22823;&#22411;KG&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#32780;&#19988;&#22312;&#24314;&#27169;&#19981;&#21516;&#20851;&#31995;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#24456;&#22810;&#21487;&#35299;&#37322;&#30340;&#20248;&#21183;&#65292;&#36825;&#20123;&#20248;&#21183;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#32463;&#39564;&#32467;&#26524;&#26469;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;KG&#23436;&#25104;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30528;&#37325;&#20171;&#32461;&#20102;KG&#23884;&#20837;&#65288;KGE&#65289;&#35774;&#35745;&#30340;&#20004;&#20010;&#20027;&#35201;&#20998;&#25903;&#65306;1&#65289;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#21644;2&#65289;&#22522;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#36235;&#21183;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#26126;&#26032;&#39062;&#19988;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;2D&#21644;3D&#20223;&#23556;&#25805;&#20316;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;CompoundE&#21644;CompoundE3D&#12290;&#23427;&#20204;&#28085;&#30422;&#20102;&#21253;&#25324;dis&#22312;&#20869;&#30340;&#24191;&#27867;&#25216;&#26415;&#35859;&#35789;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including dis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#22312;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#23545;&#20110;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36716;&#25442;&#26041;&#27861;&#26469;&#23558;&#39033;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#36716;&#25442;&#20026;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#30456;&#21516;&#25928;&#29992;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#25152;&#38656;&#30340;&#29992;&#25143;&#25968;&#37327;&#12290;&#23545;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#36866;&#24212;&#25351;&#25968;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12500</link><description>&lt;p&gt;
&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#22312;&#23569;&#37327;&#29992;&#25143;&#31034;&#20363;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
User-Level Differential Privacy With Few Examples Per User. (arXiv:2309.12500v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#22312;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#23545;&#20110;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36716;&#25442;&#26041;&#27861;&#26469;&#23558;&#39033;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#36716;&#25442;&#20026;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#30456;&#21516;&#25928;&#29992;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#25152;&#38656;&#30340;&#29992;&#25143;&#25968;&#37327;&#12290;&#23545;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#36866;&#24212;&#25351;&#25968;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#20851;&#20110;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#31034;&#20363;&#20016;&#23500;&#30340;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#36275;&#22815;&#22810;&#30340;&#31034;&#20363;&#65292;&#21487;&#20197;&#33258;&#34892;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#31034;&#20363;&#31232;&#32570;&#30340;&#24773;&#20917;&#65292;&#21363;&#27599;&#20010;&#29992;&#25143;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#65292;&#24182;&#24471;&#21040;&#20197;&#19979;&#32467;&#26524;&#65306;1. &#23545;&#20110;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#23558;&#20219;&#20309;&#39033;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#36716;&#25442;&#20026;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#31895;&#30053;&#22320;&#35828;&#65292;&#21518;&#32773;&#22312;&#36798;&#21040;&#30456;&#21516;&#25928;&#29992;&#26102;&#65292;&#25152;&#38656;&#30340;&#29992;&#25143;&#25968;&#37327;&#30456;&#23545;&#20110;&#31034;&#20363;&#25968;&#37327;$m$&#20197;$O_{\varepsilon,\delta}(\sqrt{m})$&#30340;&#36895;&#24230;&#20943;&#23569;&#65292;&#20854;&#20013;$m$&#26159;&#27599;&#20010;&#29992;&#25143;&#30340;&#31034;&#20363;&#25968;&#37327;&#12290;&#36825;&#20010;&#31639;&#27861;&#22312;&#24674;&#22797;&#22823;&#22810;&#25968;&#24050;&#30693;&#38382;&#39064;&#30340;&#30028;&#38480;&#30340;&#21516;&#26102;&#65292;&#36824;&#32473;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#65292;&#20363;&#22914;&#23545;&#20110;PAC&#23398;&#20064;&#12290;2. &#23545;&#20110;&#32431;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35843;&#25972;&#25351;&#25968;&#26426;&#21046;[McSherry&#65292;Talwar
&lt;/p&gt;
&lt;p&gt;
Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS 2021, Bun et al. STOC 2023] obtained generic algorithms that work for various learning tasks. However, their focus was on the example-rich regime, where the users have so many examples that each user could themselves solve the problem. In this work we consider the example-scarce regime, where each user has only a few examples, and obtain the following results:  1. For approximate-DP, we give a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a (multiplicative) savings of $O_{\varepsilon,\delta}(\sqrt{m})$ in terms of the number of users required for achieving the same utility, where $m$ is the number of examples per user. This algorithm, while recovering most known bounds for specific problems, also gives new bounds, e.g., for PAC learning.  2. For pure-DP, we present a simple technique for adapting the exponential mechanism [McSherry, Talwar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#22312;&#32771;&#34385;&#21040;&#26631;&#31614;&#20013;&#24050;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#35299;&#21644;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.12494</link><description>&lt;p&gt;
&#26377;&#20851;&#23500;&#26631;&#31614;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#35777;&#25454;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evidential uncertainties on rich labels for active learning. (arXiv:2309.12494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#22312;&#32771;&#34385;&#21040;&#26631;&#31614;&#20013;&#24050;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#35299;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#37319;&#26679;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#21487;&#38477;&#20302;&#21644;&#19981;&#21487;&#38477;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#31616;&#21270;&#35745;&#31639;&#38454;&#27573;&#65292;&#24182;&#28040;&#38500;&#23545;&#35266;&#23519;&#32467;&#26524;&#30340;&#20381;&#36182;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#32771;&#34385;&#26631;&#31614;&#20013;&#24050;&#32463;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#31572;&#39064;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;Klir&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#21644;&#35777;&#25454;&#23398;&#27966;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#20449;&#20219;&#20989;&#25968;&#29702;&#35770;&#26469;&#35299;&#20915;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in active learning, and more precisely in uncertainty sampling, has focused on the decomposition of model uncertainty into reducible and irreducible uncertainties. In this paper, we propose to simplify the computational phase and remove the dependence on observations, but more importantly to take into account the uncertainty already present in the labels, \emph{i.e.} the uncertainty of the oracles. Two strategies are proposed, sampling by Klir uncertainty, which addresses the exploration-exploitation problem, and sampling by evidential epistemic uncertainty, which extends the reducible uncertainty to the evidential framework, both using the theory of belief functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.12488</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#21644;&#31283;&#23450;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#65292;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26799;&#24230;&#19979;&#38477;&#21464;&#31181;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#31283;&#23450;&#24615;&#36793;&#30028;&#65292;&#35813;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;(GD)&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30697;&#38453;&#30340;&#25805;&#20316;&#31526;&#33539;&#25968;&#20250;&#22686;&#38271;&#65292;&#30452;&#21040;&#25509;&#36817;$2/\eta$&#65292;&#20043;&#21518;&#20250;&#22312;&#35813;&#20540;&#21608;&#22260;&#27874;&#21160;&#12290;&#26681;&#25454;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#20108;&#27425;&#36924;&#36817;&#65292;$2/\eta$&#34987;&#31216;&#20026;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20026;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;(SAM)&#30830;&#23450;&#20102;&#19968;&#20010;&#8220;&#31283;&#23450;&#24615;&#36793;&#30028;&#8221;&#65292;SAM&#26159;&#19968;&#31181;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;GD&#21464;&#31181;&#12290;&#19982;GD&#19981;&#21516;&#65292;SAM&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#21462;&#20915;&#20110;&#26799;&#24230;&#30340;&#33539;&#25968;&#12290;&#36890;&#36807;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SAM&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#30830;&#23450;&#30340;&#31283;&#23450;&#24615;&#36793;&#30028;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12485</link><description>&lt;p&gt;
&#30740;&#31350;&#21644;&#25913;&#36827;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Studying and improving reasoning in humans and machines. (arXiv:2309.12485v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;LLM&#22312;&#25512;&#29702;&#20013;&#23384;&#22312;&#31867;&#20284;&#20110;&#20154;&#31867;&#21551;&#21457;&#24335;&#25512;&#29702;&#30340;&#38169;&#35823;&#65292;&#20294;&#19982;&#20154;&#31867;&#25512;&#29702;&#26377;&#37325;&#35201;&#24046;&#24322;&#65292;&#26368;&#26032;&#30340;LLM&#29256;&#26412;&#20960;&#20046;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#19981;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#25105;&#20204;&#30340;&#35748;&#35782;&#35770;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#29992;&#20110;&#30740;&#31350;&#65288;&#26377;&#38480;&#65289;&#29702;&#24615;&#30340;&#35748;&#30693;&#24515;&#29702;&#23398;&#24037;&#20855;&#65292;&#30740;&#31350;&#21644;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#21442;&#19982;&#32773;&#21644;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#30340;LLM&#21576;&#29616;&#20102;&#26032;&#30340;&#32463;&#20856;&#35748;&#30693;&#23454;&#39564;&#30340;&#21464;&#20307;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20132;&#21449;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#21576;&#29616;&#20986;&#31867;&#20284;&#20110;&#24120;&#35265;&#30340;&#38169;&#35823;&#20542;&#21521;&#20110;&#21551;&#21457;&#24335;&#20154;&#31867;&#25512;&#29702;&#30340;&#25512;&#29702;&#38169;&#35823;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#30456;&#20284;&#24615;&#65292;&#20154;&#31867;&#21644;LLM&#20043;&#38388;&#30340;&#28145;&#20837;&#27604;&#36739;&#34920;&#26126;&#20102;&#20154;&#31867;&#26679;&#24335;&#25512;&#29702;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38543;&#30528;&#26368;&#36817;LLM&#29256;&#26412;&#30340;&#25512;&#20986;&#65292;&#27169;&#22411;&#30340;&#38480;&#21046;&#20960;&#20046;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20986;&#65292;&#34429;&#28982;&#21487;&#33021;&#21046;&#23450;&#31574;&#30053;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23545;&#30456;&#21516;&#30340;&#25552;&#31034;&#26041;&#26696;&#30340;&#21453;&#24212;&#24182;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#35748;&#35782;&#35770;&#30340;&#24433;&#21709;&#26469;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present study, we investigate and compare reasoning in large language models (LLM) and humans using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. To do so, we presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models limitations disappearing almost entirely in more recent LLMs releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally-responsive to the same prompting schemes. We conclude by discussing the epistemological implicat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.12484</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#22522;&#20110;&#32570;&#22833;&#20540;&#25239;&#24178;&#25200;&#20803;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#33021;&#32791;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Energy Consumption Prediction with a Missing Value-Resilient Metaheuristic-based Neural Network in Mobile App Development. (arXiv:2309.12484v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#32791;&#26159;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#23545;&#24320;&#21457;&#20154;&#21592;&#21644;&#32456;&#31471;&#29992;&#25143;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#22312;&#28040;&#36153;&#32773;&#32771;&#34385;&#26234;&#33021;&#25163;&#26426;&#36141;&#20080;&#26102;&#65292;&#33021;&#32791;&#26159;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20174;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#37492;&#20110;&#25968;&#21313;&#20159;&#37096;&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#20351;&#29992;&#25152;&#24102;&#26469;&#30340;&#37325;&#22823;&#20840;&#29699;&#24433;&#21709;&#65292;&#25506;&#32034;&#26088;&#22312;&#20943;&#23569;&#31227;&#21160;&#35774;&#22791;&#33021;&#32791;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#29615;&#22659;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#23613;&#31649;&#23433;&#21331;&#24179;&#21488;&#65288;&#20027;&#23548;&#30340;&#31227;&#21160;&#29983;&#24577;&#31995;&#32479;&#65289;&#20013;&#23384;&#22312;&#21508;&#31181;&#33410;&#33021;&#32534;&#31243;&#23454;&#36341;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#38376;&#38754;&#21521;&#31227;&#21160;&#24212;&#29992;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;-based&#33021;&#32791;&#39044;&#27979;&#31639;&#27861;&#30340;&#25991;&#26723;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#22686;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#33021;&#32791;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption is a fundamental concern in mobile application development, bearing substantial significance for both developers and end-users. Moreover, it is a critical determinant in the consumer's decision-making process when considering a smartphone purchase. From the sustainability perspective, it becomes imperative to explore approaches aimed at mitigating the energy consumption of mobile devices, given the significant global consequences arising from the extensive utilisation of billions of smartphones, which imparts a profound environmental impact. Despite the existence of various energy-efficient programming practices within the Android platform, the dominant mobile ecosystem, there remains a need for documented machine learning-based energy prediction algorithms tailored explicitly for mobile app development. Hence, the main objective of this research is to propose a novel neural network-based framework, enhanced by a metaheuristic approach, to achieve robust energy predi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.12482</link><description>&lt;p&gt;
State2Explanation:&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65306;&#26377;&#21033;&#20110;Agent&#23398;&#20064;&#21644;&#29992;&#25143;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#38750;AI&#19987;&#23478;&#23545;AI&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#23450;&#20041;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#8220;&#27010;&#24565;&#8221;&#20197;&#21450;&#25506;&#32034;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;AI&#19987;&#23478;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;AI&#31995;&#32479;&#26469;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#21162;&#21147;&#24320;&#21457;&#33021;&#22815;&#20026;&#38750;AI&#19987;&#23478;&#29702;&#35299;&#30340;AI&#20915;&#31574;&#25552;&#20379;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#21033;&#29992;&#39640;&#32423;&#27010;&#24565;&#24182;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#37117;&#26159;&#20026;&#20998;&#31867;&#25216;&#26415;&#32780;&#24320;&#21457;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#20851;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#26041;&#27861;&#36824;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#35774;&#32622;&#20013;&#23450;&#20041;&#8220;&#27010;&#24565;&#8221;&#30340;&#24895;&#26395;&#12290;&#21463;&#21040;&#8220;Protege&#25928;&#24212;&#8221;&#30340;&#21551;&#21457;&#65292;&#35813;&#25928;&#24212;&#35828;&#26126;&#35299;&#37322;&#30693;&#35782;&#36890;&#24120;&#20250;&#22686;&#24378;&#20010;&#20307;&#30340;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;RL agent&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#26368;&#32456;&#29992;&#25143;&#23545;agent&#20915;&#31574;&#29702;&#35299;&#30340;&#21452;&#37325;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;St
&lt;/p&gt;
&lt;p&gt;
With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#20381;&#36182;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.12463</link><description>&lt;p&gt;
&#26550;&#26500;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of architecture on robustness and interpretability of multispectral deep neural networks. (arXiv:2309.12463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#23545;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#20381;&#36182;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#39069;&#22806;&#20809;&#35889;&#27874;&#27573;&#65288;&#20363;&#22914;&#36817;&#32418;&#22806;&#65289;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#26041;&#27861;&#23558;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20294;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#23578;&#26410;&#30830;&#23450;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20013;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#12290;&#22312;&#20854;&#20013;&#19968;&#20010;&#26497;&#31471;&#65292;&#31216;&#20026;"&#26089;&#26399;&#34701;&#21512;"&#65292;&#39069;&#22806;&#27874;&#27573;&#34987;&#22534;&#21472;&#20026;&#39069;&#22806;&#36890;&#36947;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#36229;&#36807;&#19977;&#20010;&#36890;&#36947;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;&#22312;&#21478;&#19968;&#20010;&#26497;&#31471;&#65292;&#31216;&#20026;"&#26202;&#26399;&#34701;&#21512;"&#65292;RGB&#21644;&#38750;RGB&#27874;&#27573;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21333;&#29420;&#20998;&#25903;&#20256;&#36882;&#65292;&#24182;&#22312;&#26368;&#32456;&#20998;&#31867;&#25110;&#20998;&#21106;&#23618;&#20043;&#21069;&#31435;&#21363;&#21512;&#24182;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19968;&#22871;&#19981;&#21516;&#34701;&#21512;&#26041;&#27861;&#30340;&#22810;&#20809;&#35889;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37327;&#21270;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#36755;&#20837;&#27874;&#27573;&#30340;&#30456;&#23545;&#20381;&#36182;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#24433;&#21709;&#19968;&#20010;&#25110;&#22810;&#20010;&#36755;&#20837;&#36890;&#36947;&#30340;&#33258;&#28982;&#22270;&#20687;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Including information from additional spectral bands (e.g., near-infrared) can improve deep learning model performance for many vision-oriented tasks. There are many possible ways to incorporate this additional information into a deep learning model, but the optimal fusion strategy has not yet been determined and can vary between applications. At one extreme, known as "early fusion," additional bands are stacked as extra channels to obtain an input image with more than three channels. At the other extreme, known as "late fusion," RGB and non-RGB bands are passed through separate branches of a deep learning model and merged immediately before a final classification or segmentation layer. In this work, we characterize the performance of a suite of multispectral deep learning models with different fusion approaches, quantify their relative reliance on different input bands and evaluate their robustness to naturalistic image corruptions affecting one or more input channels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.12460</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#31185;&#23398;&#25104;&#20687;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multimodal Deep Learning for Scientific Imaging Interpretation. (arXiv:2309.12460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20132;&#20114;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#33021;&#22815;&#20934;&#30830;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#24120;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#23545;&#20027;&#39064;&#26448;&#26009;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#22797;&#26434;&#32452;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#27169;&#25311;&#24182;&#35780;&#20272;&#19982;&#25195;&#25551;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;SEM&#65289;&#22270;&#20687;&#30340;&#20154;&#31867;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#29627;&#29827;&#26448;&#26009;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#21516;&#34892;&#35780;&#35758;&#30340;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#20511;&#21161; GPT-4 &#30340;&#33021;&#21147;&#36827;&#34892;&#31934;&#32454;&#25968;&#25454;&#21512;&#25104;&#21644;&#35780;&#20272;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#32454;&#24494;&#30340;&#35299;&#37322;&#21644;&#19987;&#19994;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;GlassLLaVA&#65289;&#22312;&#21046;&#23450;&#20934;&#30830;&#30340;&#35299;&#37322;&#12289;&#35782;&#21035;&#20851;&#38190;&#29305;&#24449;&#21644;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#30340;SEM&#22270;&#20687;&#20013;&#30340;&#32570;&#38519;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22810;&#31181;&#31185;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#28789;&#27963;&#35780;&#20272;&#25351;&#26631;&#65292;&#20351;&#24471;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#21363;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12458</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Multimodal Learning. (arXiv:2309.12458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12458
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#21363;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23545;&#32463;&#39564;&#19990;&#30028;&#30340;&#24863;&#30693;&#28041;&#21450;&#21040;&#35782;&#21035;&#22522;&#30784;&#29289;&#20307;&#30340;&#21508;&#31181;&#22806;&#35266;&#25110;&#8220;&#27169;&#24577;&#8221;&#12290;&#23613;&#31649;&#21746;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#32771;&#34385;&#36825;&#19968;&#35266;&#28857;&#65292;&#20294;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#22810;&#27169;&#24577;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#32463;&#39564;&#23454;&#36341;&#65292;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#65292;&#21482;&#26377;&#21551;&#21457;&#24335;&#35770;&#35777;&#12290;&#22810;&#27169;&#24577;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#21457;&#29616;&#26159;&#65292;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#35757;&#32451;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#32463;&#36807;&#31934;&#32454;&#35843;&#33410;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#30456;&#27604;&#20110;&#21333;&#27169;&#24577;&#23398;&#20064;&#20855;&#26377;&#26356;&#20248;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#39640;&#36798;$O(\sqrt{n})$&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#26679;&#26412;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human perception of the empirical world involves recognizing the diverse appearances, or 'modalities', of underlying objects. Despite the longstanding consideration of this perspective in philosophy and cognitive science, the study of multimodality remains relatively under-explored within the field of machine learning. Nevertheless, current studies of multimodal machine learning are limited to empirical practices, lacking theoretical foundations beyond heuristic arguments. An intriguing finding from the practice of multimodal learning is that a model trained on multiple modalities can outperform a finely-tuned unimodal model, even on unimodal tasks. This paper provides a theoretical framework that explains this phenomenon, by studying generalization properties of multimodal learning algorithms. We demonstrate that multimodal learning allows for a superior generalization bound compared to unimodal learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample size. Such adva
&lt;/p&gt;</description></item><item><title>LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12455</link><description>&lt;p&gt;
LongDocFACTScore: &#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#23454;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore: Evaluating the Factuality of Long Document Abstractive Summarisation. (arXiv:2309.12455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12455
&lt;/p&gt;
&lt;p&gt;
LongDocFACTScore&#26159;&#19968;&#31181;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#23454;&#35777;&#24615;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#35780;&#20272;&#38271;&#25991;&#26723;&#25688;&#35201;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#20107;&#23454;&#19968;&#33268;&#24615;&#26159;&#29983;&#25104;&#24615;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#25688;&#35201;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;ROUGE&#24471;&#20998;&#65289;&#26080;&#27861;&#35780;&#20272;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#37327;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26377;&#38480;&#21046;&#24615;&#30340;&#20196;&#29260;&#38480;&#21046;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#26377;&#38480;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#29616;&#26377;&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#22312;&#24212;&#29992;&#20110;&#38271;&#25991;&#26723;&#25968;&#25454;&#38598;&#26102;&#26159;&#21542;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#38271;&#25991;&#26723;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;LongDocFACTScore&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#24230;&#37327;&#26631;&#20934;&#25193;&#23637;&#21040;&#20219;&#24847;&#38271;&#24230;&#30340;&#25991;&#26723;&#12290;&#35813;&#26694;&#26550;&#22312;&#19982;&#20154;&#31867;&#20107;&#23454;&#19968;&#33268;&#24615;&#24230;&#37327;&#30340;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maintaining factual consistency is a critical issue in abstractive text summarisation, however, it cannot be assessed by traditional automatic metrics used for evaluating text summarisation, such as ROUGE scoring. Recent efforts have been devoted to developing improved metrics for measuring factual consistency using pre-trained language models, but these metrics have restrictive token limits, and are therefore not suitable for evaluating long document text summarisation. Moreover, there is limited research evaluating whether existing automatic evaluation metrics are fit for purpose when applied to long document data sets. In this work, we evaluate the efficacy of automatic metrics at assessing factual consistency in long document text summarisation and propose a new evaluation framework LongDocFACTScore. This framework allows metrics to be extended to any length document. This framework outperforms existing state-of-the-art metrics in its ability to correlate with human measures of fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#31574;&#30053;&#20215;&#20540;&#30340;&#31934;&#30830;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.12450</link><description>&lt;p&gt;
&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Convex Framework for Confounding Robust Inference. (arXiv:2309.12450v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#31574;&#30053;&#20215;&#20540;&#30340;&#31934;&#30830;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21463;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#31163;&#32447;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#24120;&#34987;&#29992;&#26469;&#22312;&#32473;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#20272;&#35745;&#22312;&#26368;&#22351;&#28151;&#28102;&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20026;&#20102;&#21487;&#34892;&#24615;&#32780;&#37319;&#29992;&#19968;&#20123;&#31895;&#31961;&#30340;&#26494;&#24347;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#23545;&#31574;&#30053;&#20215;&#20540;&#30340;&#20272;&#35745;&#36807;&#20110;&#20445;&#23432;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#20102;&#31574;&#30053;&#20215;&#20540;&#30340;&#19968;&#20010;&#36739;&#20026;&#31934;&#30830;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#20351;&#24471;&#20854;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#20363;&#22914;&#22522;&#20110;f-&#20998;&#27495;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21644;&#20449;&#24687;&#20934;&#21017;&#30340;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#21033;&#29992;&#19978;&#30028;&#36827;&#34892;&#40065;&#26834;&#31574;&#30053;&#23398;&#20064;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24378;&#23545;&#20598;&#24615;&#37325;&#26032;&#34920;&#36848;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#21033;&#29992;M&#25216;&#26415;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#20272;&#35745;&#22120;&#30340;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value using convex programming. The generality of our estimator enables various extensions such as sensitivity analysis with f-divergence, model selection with cross validation and information criterion, and robust policy learning with the sharp lower bound. Furthermore, our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality, which enables us to provide strong theoretical guarantees of the proposed estimator using techniques of the M-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27010;&#29575;&#24615;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#35299;&#32806;&#20102;&#26469;&#33258;&#31995;&#32479;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2309.12445</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ensemble Neural Networks for Remaining Useful Life (RUL) Prediction. (arXiv:2309.12445v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12445
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#36827;&#34892;&#27010;&#29575;&#24615;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#35299;&#32806;&#20102;&#26469;&#33258;&#31995;&#32479;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25252;&#35745;&#21010;&#30340;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#20010;&#30417;&#27979;&#31995;&#32479;&#65292;&#23427;&#25552;&#20379;&#20581;&#24247;&#21644;&#36864;&#21270;&#30340;&#33391;&#22909;&#39044;&#27979;&#65292;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#21097;&#20313;&#23551;&#21629;(RUL)&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;&#25968;&#25454;&#39537;&#21160;RUL&#39044;&#27979;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#28857;&#39044;&#27979;&#19978;&#12290;&#36825;&#20123;&#28857;&#39044;&#27979;&#26041;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25925;&#38556;&#30340;&#27010;&#29575;&#24615;&#36136;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23569;&#25968;&#30340;&#27010;&#29575;&#24615;&#26041;&#27861;&#35201;&#20040;&#21253;&#25324;&#26469;&#33258;&#31995;&#32479;&#30340;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#21253;&#25324;&#26469;&#33258;&#27169;&#22411;&#21442;&#25968;&#30340;epistemic&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#21516;&#26102;&#21253;&#21547;&#20004;&#32773;&#20316;&#20026;&#24635;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27010;&#29575;&#24615;RUL&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#35299;&#32806;&#12290;&#36825;&#20123;&#35299;&#32806;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#20102;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#30340;&#20449;&#24515;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;NASA&#30340;&#28065;&#21943;&#24335;&#21457;&#21160;&#26426;CMAPSS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#24314;&#27169;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#35299;&#24320;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core part of maintenance planning is a monitoring system that provides a good prognosis on health and degradation, often expressed as remaining useful life (RUL). Most of the current data-driven approaches for RUL prediction focus on single-point prediction. These point prediction approaches do not include the probabilistic nature of the failure. The few probabilistic approaches to date either include the aleatoric uncertainty (which originates from the system), or the epistemic uncertainty (which originates from the model parameters), or both simultaneously as a total uncertainty. Here, we propose ensemble neural networks for probabilistic RUL predictions which considers both uncertainties and decouples these two uncertainties. These decoupled uncertainties are vital in knowing and interpreting the confidence of the predictions. This method is tested on NASA's turbofan jet engine CMAPSS data-set. Our results show how these uncertainties can be modeled and how to disentangle the cont
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24314;&#27169;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#21464;&#26356;&#31649;&#29702;&#65292;&#20026;&#23567;&#22411;&#21644;&#20013;&#22411;&#20225;&#19994;&#25552;&#20379;&#20102;&#23433;&#20840;&#22320;&#31649;&#29702;&#36719;&#20214;&#26356;&#26032;&#21644;&#21464;&#26356;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12421</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24314;&#27169;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#21464;&#26356;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Change Management using Generative Modeling on Digital Twins. (arXiv:2309.12421v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24314;&#27169;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#21464;&#26356;&#31649;&#29702;&#65292;&#20026;&#23567;&#22411;&#21644;&#20013;&#22411;&#20225;&#19994;&#25552;&#20379;&#20102;&#23433;&#20840;&#22320;&#31649;&#29702;&#36719;&#20214;&#26356;&#26032;&#21644;&#21464;&#26356;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#21644;&#20013;&#22411;&#20225;&#19994;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#23433;&#20840;&#22320;&#31649;&#29702;&#36719;&#20214;&#26356;&#26032;&#21644;&#21464;&#26356;&#12290;&#29305;&#21035;&#26159;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#19979;&#65292;&#38656;&#23545;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#24517;&#35201;&#30340;&#21464;&#26356;/&#26356;&#26032;/&#34917;&#19969;&#65292;&#20197;&#20445;&#25345;&#23545;&#26032;&#20852;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#32463;&#24120;&#26159;&#30417;&#31649;&#26426;&#26500;&#35201;&#27714;&#30340;&#12290;&#20294;&#26159;&#65292;&#23433;&#20840;&#34917;&#19969;/&#26356;&#26032;&#38656;&#35201;&#22312;&#29983;&#20135;&#31995;&#32479;&#21457;&#24067;&#20043;&#21069;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#23384;&#22312;&#39118;&#38505;&#24182;&#24102;&#26469;&#23433;&#20840;&#23041;&#32961;&#12290;&#22823;&#22411;&#20225;&#19994;&#36890;&#24120;&#20855;&#26377;&#38750;&#29983;&#20135;&#29615;&#22659;&#65292;&#21487;&#20197;&#22312;&#21457;&#24067;&#21040;&#29983;&#20135;&#29615;&#22659;&#20043;&#21069;&#36827;&#34892;&#21464;&#26356;&#21644;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#23567;&#22411;&#20225;&#19994;&#27809;&#26377;&#36825;&#26679;&#30340;&#35774;&#26045;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20113;&#19978;&#21019;&#24314;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;IT&#21644;&#29289;&#32852;&#32593;&#29615;&#22659;&#30340;&#28151;&#21512;&#29615;&#22659;&#12290;&#36825;&#20123;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#20316;&#20026;&#38750;&#29983;&#20135;&#29615;&#22659;&#65292;&#22312;&#36825;&#37324;&#21487;&#20197;&#36827;&#34892;&#21464;&#26356;&#65292;&#24182;&#21487;&#20197;&#22312;&#21457;&#24067;&#34917;&#19969;&#20043;&#21069;&#36827;&#34892;&#23433;&#20840;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge faced by small and medium-sized business entities is securely managing software updates and changes. Specifically, with rapidly evolving cybersecurity threats, changes/updates/patches to software systems are necessary to stay ahead of emerging threats and are often mandated by regulators or statutory authorities to counter these. However, security patches/updates require stress testing before they can be released in the production system. Stress testing in production environments is risky and poses security threats. Large businesses usually have a non-production environment where such changes can be made and tested before being released into production. Smaller businesses do not have such facilities. In this work, we show how "digital twins", especially for a mix of IT and IoT environments, can be created on the cloud. These digital twins act as a non-production environment where changes can be applied, and the system can be securely tested before patch release. Additio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#32593;&#32476;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21152;&#36895;&#20102;Resnet&#26550;&#26500;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#23545;ResNet50&#30340;&#30740;&#31350;&#26696;&#20363;&#65292;&#35777;&#26126;&#20102;&#30828;&#20214;&#30446;&#26631;&#21387;&#32553;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.12412</link><description>&lt;p&gt;
&#36890;&#36807;&#38024;&#23545;&#32593;&#32476;&#23618;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#21152;&#36895;Resnet&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Speeding up Resnet Architecture with Layers Targeted Low Rank Decomposition. (arXiv:2309.12412v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38024;&#23545;&#32593;&#32476;&#36827;&#34892;&#20302;&#31209;&#20998;&#35299;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21152;&#36895;&#20102;Resnet&#26550;&#26500;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#23545;ResNet50&#30340;&#30740;&#31350;&#26696;&#20363;&#65292;&#35777;&#26126;&#20102;&#30828;&#20214;&#30446;&#26631;&#21387;&#32553;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21387;&#32553;&#21487;&#20197;&#21152;&#36895;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#23618;&#19978;&#24212;&#29992;&#20302;&#31209;&#20998;&#35299;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#33719;&#24471;&#21152;&#36895;&#25928;&#26524;&#65292;&#21387;&#32553;&#26041;&#27861;&#24212;&#35813;&#32771;&#34385;&#24213;&#23618;&#30828;&#20214;&#65292;&#24182;&#36827;&#34892;&#20998;&#26512;&#36873;&#25321;&#38656;&#35201;&#21387;&#32553;&#30340;&#23618;&#27425;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;ResNet50&#36827;&#34892;&#21387;&#32553;&#21644;&#22312;&#23436;&#25972;&#30340;ImageNet-ILSVRC2012&#19978;&#35757;&#32451;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#30828;&#20214;&#31995;&#32479;Nvidia V100&#21644;&#21326;&#20026;Ascend910&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36890;&#36807;&#38024;&#23545;&#30828;&#20214;&#30340;&#21387;&#32553;&#65292;Ascend910&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#30456;&#27604;&#21407;&#22987;&#26410;&#21387;&#32553;&#27169;&#22411;&#65292;&#35757;&#32451;&#21152;&#36895;&#20102;5.36%&#65292;&#22312;Ascend310&#19978;&#25512;&#26029;&#36895;&#24230;&#25552;&#39640;&#20102;15.79%&#65292;&#20165;&#26377;1%&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compression of a neural network can help in speeding up both the training and the inference of the network. In this research, we study applying compression using low rank decomposition on network layers. Our research demonstrates that to acquire a speed up, the compression methodology should be aware of the underlying hardware as analysis should be done to choose which layers to compress. The advantage of our approach is demonstrated via a case study of compressing ResNet50 and training on full ImageNet-ILSVRC2012. We tested on two different hardware systems Nvidia V100 and Huawei Ascend910. With hardware targeted compression, results on Ascend910 showed 5.36% training speedup and 15.79% inference speed on Ascend310 with only 1% drop in accuracy compared to the original uncompressed model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#28151;&#21512;&#31934;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#21442;&#25968;&#30340;&#28014;&#28857;&#21103;&#26412;&#21644;&#21435;&#38500;&#26799;&#24230;&#20540;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12381</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#28151;&#21512;&#31934;&#24230;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Mixed-Precision Optimizers. (arXiv:2309.12381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#28151;&#21512;&#31934;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#21442;&#25968;&#30340;&#28014;&#28857;&#21103;&#26412;&#21644;&#21435;&#38500;&#26799;&#24230;&#20540;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20248;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#21333;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#65292;&#36825;&#22312;&#20869;&#23384;&#22823;&#23567;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#37117;&#26159;&#26114;&#36149;&#30340;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#31934;&#24230;&#20248;&#21270;&#25216;&#26415;&#21033;&#29992;&#21333;&#31934;&#24230;&#21644;&#21322;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30456;&#32467;&#21512;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#36890;&#36807;&#28040;&#38500;&#21442;&#25968;&#30340;&#28014;&#28857;&#21103;&#26412;&#65292;&#23454;&#38469;&#19978;&#21482;&#20445;&#30041;&#21322;&#31934;&#24230;&#25968;&#23383;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22312;&#21518;&#21521;&#20256;&#25773;&#26399;&#38388;&#36890;&#36807;&#25191;&#34892;&#20248;&#21270;&#22120;&#27493;&#39588;&#26469;&#21435;&#38500;&#26799;&#24230;&#20540;&#30340;&#22909;&#22788;&#12290;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36798;25%&#30340;&#23792;&#20540;&#20869;&#23384;&#38477;&#20302;&#21644;15%&#30340;&#26356;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional optimization methods rely on the use of single-precision floating point arithmetic, which can be costly in terms of memory size and computing power. However, mixed precision optimization techniques leverage the use of both single and half-precision floating point arithmetic to reduce memory requirements while maintaining model accuracy. We provide here an algorithm to further reduce memory usage during the training of a model by getting rid of the floating point copy of the parameters, virtually keeping only half-precision numbers. We also explore the benefits of getting rid of the gradient's value by executing the optimizer step during the back-propagation. In practice, we achieve up to 25% lower peak memory use and 15% faster training while maintaining the same level of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12380</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Methods for generating and evaluating synthetic longitudinal patient data: a systematic review. (arXiv:2309.12380v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25968;&#25454;&#30340;&#36805;&#29467;&#22686;&#38271;&#20419;&#36827;&#20102;&#21508;&#31181;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#21152;&#24555;&#20102;&#30740;&#31350;&#21644;&#24320;&#21457;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#34892;&#19994;&#37117;&#33021;&#20174;&#25968;&#25454;&#30340;&#22686;&#21152;&#20013;&#21516;&#31561;&#21463;&#30410;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#35268;&#23450;&#30340;&#27861;&#24459;&#38480;&#21046;&#65292;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#32479;&#35745;&#25259;&#38706;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#22522;&#20110;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#29983;&#25104;&#30340;&#65292;&#30446;&#30340;&#26159;&#23613;&#21487;&#33021;&#22320;&#22797;&#21046;&#23427;&#20204;&#65292;&#24182;&#20805;&#24403;&#30495;&#23454;&#25935;&#24863;&#25968;&#25454;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#35813;&#32508;&#36848;&#36981;&#24490;PRISMA&#25351;&#21335;&#65292;&#24182;&#28085;&#30422;&#20102;&#33258;2022&#24180;&#24213;&#20197;&#26469;&#30340;&#20116;&#20010;&#25968;&#25454;&#24211;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;17&#31181;&#26041;&#27861;&#65292;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of data in recent years has led to the advancement and utilization of various statistical and deep learning techniques, thus expediting research and development activities. However, not all industries have benefited equally from the surge in data availability, partly due to legal restrictions on data usage and privacy regulations, such as in medicine. To address this issue, various statistical disclosure and privacy-preserving methods have been proposed, including the use of synthetic data generation. Synthetic data are generated based on some existing data, with the aim of replicating them as closely as possible and acting as a proxy for real sensitive data. This paper presents a systematic review of methods for generating and evaluating synthetic longitudinal patient data, a prevalent data type in medicine. The review adheres to the PRISMA guidelines and covers literature from five databases until the end of 2022. The paper describes 17 methods, ranging from traditi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33639;&#20809;&#20809;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25506;&#31350;&#20102;&#28201;&#24230;&#23545;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#30340;&#32769;&#21270;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27833;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.12377</link><description>&lt;p&gt;
&#23545;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#30340;&#32769;&#21270;&#36827;&#34892;&#30740;&#31350;&#65306;&#21033;&#29992;&#33639;&#20809;&#20809;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25506;&#31350;&#28201;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Shedding Light on the Ageing of Extra Virgin Olive Oil: Probing the Impact of Temperature with Fluorescence Spectroscopy and Machine Learning Techniques. (arXiv:2309.12377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33639;&#20809;&#20809;&#35889;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25506;&#31350;&#20102;&#28201;&#24230;&#23545;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#30340;&#32769;&#21270;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27833;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22312;&#21152;&#36895;&#20648;&#23384;&#26465;&#20214;&#19979;&#20351;&#29992;UV&#21560;&#25910;&#21644;&#24635;&#33639;&#20809;&#20809;&#35889;&#23545;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#65288;EVOO&#65289;&#36827;&#34892;&#27687;&#21270;&#30740;&#31350;&#12290;&#36890;&#36807;&#25910;&#38598;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#24230;&#32858;&#21512;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#30417;&#27979;&#27833;&#36136;&#37327;&#12290;&#23613;&#31649;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#20197;&#20854;&#20247;&#22810;&#30340;&#20581;&#24247;&#30410;&#22788;&#21644;&#21331;&#36234;&#30340;&#21475;&#21619;&#32780;&#20139;&#35465;&#20840;&#29699;&#65292;&#20294;&#23427;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#21457;&#29983;&#38477;&#35299;&#65292;&#30001;&#20110;&#27687;&#21270;&#20316;&#29992;&#65292;&#20854;&#20581;&#24247;&#21697;&#36136;&#21644;&#39118;&#21619;&#37117;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#27687;&#21270;&#23545;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#30340;&#24433;&#21709;&#24182;&#24320;&#21457;&#26131;&#20110;&#22312;&#29616;&#22330;&#26465;&#20214;&#19979;&#23454;&#26045;&#32780;&#38750;&#19987;&#38376;&#23454;&#39564;&#23460;&#30340;&#35780;&#20272;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#33639;&#20809;&#20809;&#35889;&#20855;&#26377;&#30417;&#27979;&#27687;&#21270;&#25928;&#26524;&#21644;&#35780;&#20272;&#29305;&#32423;&#21021;&#27048;&#27204;&#27012;&#27833;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#25968;&#25454;&#39640;&#24230;&#32858;&#21512;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work systematically investigates the oxidation of extra virgin olive oil (EVOO) under accelerated storage conditions with UV absorption and total fluorescence spectroscopy. With the large amount of data collected, it proposes a method to monitor the oil's quality based on machine learning applied to highly-aggregated data. EVOO is a high-quality vegetable oil that has earned worldwide reputation for its numerous health benefits and excellent taste. Despite its outstanding quality, EVOO degrades over time owing to oxidation, which can affect both its health qualities and flavour. Therefore, it is highly relevant to quantify the effects of oxidation on EVOO and develop methods to assess it that can be easily implemented under field conditions, rather than in specialized laboratories. The following study demonstrates that fluorescence spectroscopy has the capability to monitor the effect of oxidation and assess the quality of EVOO, even when the data are highly aggregated. It shows t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;AUC Gap&#30340;&#25351;&#26631;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;AI/ML&#27169;&#22411;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20026;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#31574;&#30053;&#20998;&#20139;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.12371</link><description>&lt;p&gt;
Fairness Hub&#25216;&#26415;&#31616;&#25253;: AUC Gap
&lt;/p&gt;
&lt;p&gt;
Fairness Hub Technical Briefs: AUC Gap. (arXiv:2309.12371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;AUC Gap&#30340;&#25351;&#26631;&#65292;&#23427;&#21487;&#20197;&#27979;&#37327;AI/ML&#27169;&#22411;&#22312;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20026;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#25552;&#20379;&#20102;&#22522;&#20934;&#21644;&#31574;&#30053;&#20998;&#20139;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27979;&#37327;&#20559;&#35265;&#65292;&#25105;&#20204;&#40723;&#21169;&#22242;&#38431;&#32771;&#34385;&#20351;&#29992;AUC Gap&#65306;&#23376;&#32676;&#20307;&#65288;&#20363;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;SES&#12289;&#20808;&#21069;&#30693;&#35782;&#65289;&#30340;&#26368;&#39640;&#21644;&#26368;&#20302;&#27979;&#35797;AUC&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#23427;&#19981;&#20381;&#36182;&#20110;AI/ML&#31639;&#27861;&#65292;&#24182;&#25429;&#25417;&#27169;&#22411;&#22312;&#20219;&#24847;&#25968;&#37327;&#30340;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38750;&#20108;&#20803;&#30340;&#20844;&#24179;&#35780;&#20272;&#65292;&#20363;&#22914;&#38024;&#23545;&#20132;&#21449;&#36523;&#20221;&#32676;&#20307;&#12290;LEVI&#22242;&#38431;&#22312;&#36861;&#27714;&#22312;&#20302;&#25910;&#20837;&#20013;&#23398;&#20013;&#23558;&#25968;&#23398;&#25104;&#23601;&#32763;&#20493;&#30340;&#20849;&#21516;&#30446;&#26631;&#26102;&#65292;&#20351;&#29992;&#21508;&#31181;AI/ML&#27169;&#22411;&#12290;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#32972;&#26223;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#19981;&#24341;&#20837;&#25110;&#25918;&#22823;&#20559;&#35265;&#65292;&#23545;&#20110;&#23454;&#29616;LEVI&#30446;&#26631;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#27492;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#27169;&#22411;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#20379;&#25152;&#26377;LEVI&#22242;&#38431;&#20351;&#29992;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#22522;&#20934;&#21644;&#20998;&#26512;&#22522;&#30784;&#65292;&#29992;&#20110;&#20849;&#20139;&#19981;&#21516;&#22242;&#38431;&#24050;&#32463;&#37319;&#21462;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
To measure bias, we encourage teams to consider using AUC Gap: the absolute difference between the highest and lowest test AUC for subgroups (e.g., gender, race, SES, prior knowledge). It is agnostic to the AI/ML algorithm used and it captures the disparity in model performance for any number of subgroups, which enables non-binary fairness assessments such as for intersectional identity groups. The LEVI teams use a wide range of AI/ML models in pursuit of a common goal of doubling math achievement in low-income middle schools. Ensuring that the models, which are trained on datasets collected in many different contexts, do not introduce or amplify biases is important for achieving the LEVI goal. We offer here a versatile and easy-to-compute measure of model bias for all LEVI teams in order to create a common benchmark and an analytical basis for sharing what strategies have worked for different teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.12368</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#65306;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rethinking Human-AI Collaboration in Complex Medical Decision Making: A Case Study in Sepsis Diagnosis. (arXiv:2309.12368v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#21307;&#30103;&#20915;&#31574;&#20013;&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#35774;&#35745;&#35201;&#27714;&#65292;&#20197;&#33043;&#27602;&#30151;&#35786;&#26029;&#20026;&#20363;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#21457;&#25381;&#20316;&#29992;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30740;&#31350;&#35770;&#25991;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21364;&#38754;&#20020;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#33043;&#27602;&#30151;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#26089;&#26399;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#35786;&#26029;&#30340;&#24613;&#24615;&#33268;&#21629;&#20840;&#36523;&#24615;&#24863;&#26579;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#33021;&#22815;&#25903;&#25345;&#20020;&#24202;&#19987;&#23478;&#20570;&#20986;&#26356;&#22909;&#33043;&#27602;&#30151;&#26089;&#26399;&#35786;&#26029;&#20915;&#31574;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;&#30740;&#31350;&#20174;&#19968;&#20010;&#24418;&#25104;&#24615;&#30740;&#31350;&#24320;&#22987;&#65292;&#35843;&#26597;&#20026;&#20160;&#20040;&#20020;&#24202;&#19987;&#23478;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#25918;&#24323;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#33043;&#27602;&#30151;&#39044;&#27979;&#27169;&#22359;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#30340;&#20013;&#38388;&#38454;&#27573;&#65288;&#22914;&#29983;&#25104;&#20551;&#35774;&#25110;&#25910;&#38598;&#25968;&#25454;&#65289;&#25903;&#25345;&#20154;&#31867;&#19987;&#23478;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26500;&#24314;&#20102;SepsisLab&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's AI systems for medical decision support often succeed on benchmark datasets in research papers but fail in real-world deployment. This work focuses on the decision making of sepsis, an acute life-threatening systematic infection that requires an early diagnosis with high uncertainty from the clinician. Our aim is to explore the design requirements for AI systems that can support clinical experts in making better decisions for the early diagnosis of sepsis. The study begins with a formative study investigating why clinical experts abandon an existing AI-powered Sepsis predictive module in their electrical health record (EHR) system. We argue that a human-centered AI system needs to support human experts in the intermediate stages of a medical decision-making process (e.g., generating hypotheses or gathering data), instead of focusing only on the final decision. Therefore, we build SepsisLab based on a state-of-the-art AI algorithm and extend it to predict the future projection o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.12367</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30740;&#31350;&#39046;&#22495;&#30693;&#35782;&#24211;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LLM&#23545;&#26597;&#35810;&#30340;&#22238;&#31572;&#32463;&#24120;&#19981;&#20934;&#30830;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#19982;LLM&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#21152;&#22238;&#31572;&#21487;&#38752;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#65292;&#25945;&#32946;&#30417;&#30563;&#21592;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#35838;&#31243;&#65292;&#35813;&#35838;&#31243;&#20250;&#34987;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#23454;&#39564;&#65292;&#23398;&#29983;&#21442;&#19982;&#32773;&#38656;&#35201;&#22238;&#31572;&#26377;&#20851;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#30340;&#38382;&#39064;&#12290; GPT-4&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#23618;&#27425;&#30340;KB&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#30001;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#36825;&#20123;&#22238;&#31572;&#12290;&#26368;&#21518;&#65292;&#23398;&#29983;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#21508;&#31181;&#25945;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12360</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#23454;&#29616;&#39640;&#25928;&#30340;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Social Choice via NLP and Sampling. (arXiv:2309.12360v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25277;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#20272;&#35745;&#20102;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#26469;&#20915;&#23450;&#25552;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#35299;&#20915;&#20102;&#19968;&#20123;&#20195;&#29702;&#31038;&#21306;&#38754;&#20020;&#30340;&#22522;&#26412;&#20914;&#31361;&#65292;&#21363;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#21253;&#25324;&#25152;&#26377;&#25104;&#21592;&#30340;&#28212;&#26395;&#19982;&#31038;&#21306;&#25104;&#21592;&#21487;&#25903;&#37197;&#30340;&#26377;&#38480;&#26102;&#38388;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#24863;&#30693;&#31038;&#20250;&#36873;&#25321;&#30340;&#20004;&#31181;&#25216;&#26415;&#32452;&#21512;&#65292;&#21363;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#25277;&#26679;&#12290;&#22522;&#26412;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;&#27599;&#20010;&#25913;&#21464;&#29616;&#29366;&#30340;&#27835;&#29702;&#25552;&#26696;&#39318;&#20808;&#21457;&#36865;&#21040;&#35757;&#32451;&#26377;&#32032;&#30340;NLP&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20272;&#35745;&#20102;&#22914;&#26524;&#25152;&#26377;&#31038;&#21306;&#25104;&#21592;&#30452;&#25509;&#23545;&#20854;&#36827;&#34892;&#25237;&#31080;&#65292;&#35813;&#25552;&#26696;&#36890;&#36807;&#30340;&#27010;&#29575;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#36825;&#31181;&#20272;&#35745;&#65292;&#36873;&#25321;&#19968;&#20010;&#30830;&#23450;&#22823;&#23567;&#30340;&#20154;&#32676;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22810;&#25968;&#20915;&#23450;&#25552;&#26696;&#12290;&#25105;&#20204;&#26681;&#25454;&#19978;&#36848;&#26041;&#26696;&#24320;&#21457;&#20102;&#20960;&#31181;&#20855;&#20307;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#22810;&#20010;&#20998;&#25955;&#33258;&#27835;&#32452;&#32455;&#65288;DAO&#65289;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-Aware Social Choice tackles the fundamental conflict faced by some agent communities between their desire to include all members in the decision making processes and the limited time and attention that are at the disposal of the community members. Here, we investigate a combination of two techniques for attention-aware social choice, namely Natural Language Processing (NLP) and Sampling. Essentially, we propose a system in which each governance proposal to change the status quo is first sent to a trained NLP model that estimates the probability that the proposal would pass if all community members directly vote on it; then, based on such an estimation, a population sample of a certain size is being selected and the proposal is decided upon by taking the sample majority. We develop several concrete algorithms following the scheme described above and evaluate them using various data, including such from several Decentralized Autonomous Organizations (DAOs).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#24207;&#21644;&#22810;&#27169;&#24577;&#25512;&#29702;&#65292;&#23545;&#25239;&#35299;&#37322;&#24182;&#30452;&#25509;&#25581;&#31034;&#20559;&#35265;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26410;&#26469;&#35745;&#31639;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#30005;&#24433;&#21644;&#35270;&#21548;&#33402;&#26415;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25215;&#35748;&#35760;&#24405;&#30340;&#23384;&#22312;&#65292;&#23558;&#36807;&#21435;&#19982;&#26410;&#26469;&#32039;&#23494;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12345</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#24207;&#21644;&#22810;&#27169;&#24577;&#25512;&#29702;&#23545;&#25239;&#35299;&#37322;&#24182;&#30452;&#25509;&#25581;&#31034;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Antagonising explanation and revealing bias directly through sequencing and multimodal inference. (arXiv:2309.12345v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#24207;&#21644;&#22810;&#27169;&#24577;&#25512;&#29702;&#65292;&#23545;&#25239;&#35299;&#37322;&#24182;&#30452;&#25509;&#25581;&#31034;&#20559;&#35265;&#65292;&#25506;&#35752;&#20102;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26410;&#26469;&#35745;&#31639;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#30005;&#24433;&#21644;&#35270;&#21548;&#33402;&#26415;&#39046;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#25215;&#35748;&#35760;&#24405;&#30340;&#23384;&#22312;&#65292;&#23558;&#36807;&#21435;&#19982;&#26410;&#26469;&#32039;&#23494;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#26469;&#29983;&#25104;&#25968;&#25454;&#65292;&#20363;&#22914;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36817;&#20284;&#35745;&#31639;&#21487;&#33021;&#30340;&#26679;&#26412;&#12290;&#36817;&#20284;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#37325;&#24314;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#21487;&#20197;&#30475;&#20316;&#26159;&#29992;&#26576;&#31181;&#25968;&#25454;&#32467;&#26500;&#65288;&#29031;&#29255;&#12289;&#38899;&#39057;&#35760;&#24405;&#12289;&#25163;&#31295;&#65289;&#26469;&#34920;&#31034;&#29289;&#29702;&#19990;&#30028;&#30340;&#35760;&#24405;&#38598;&#12290;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#65292;&#20363;&#22914;&#65292;&#22270;&#20687;&#24103;&#20250;&#26681;&#25454;&#23398;&#20064;&#30340;&#20559;&#35265;&#36880;&#27493;&#21457;&#23637;&#25104;&#20026;&#25991;&#26412;&#36755;&#20837;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#65292;&#29983;&#25104;&#36807;&#31243;&#26159;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20498;&#36864;&#30340;&#26102;&#38388;&#65307;&#19981;&#26159;&#36890;&#36807;&#23545;&#20498;&#36864;&#25193;&#25955;&#36807;&#31243;&#30340;&#28789;&#24863;&#65292;&#32780;&#26159;&#25215;&#35748;&#25991;&#21270;&#22312;&#35760;&#24405;&#20013;&#29305;&#26377;&#30340;&#26631;&#35760;&#12290;&#29983;&#25104;&#27169;&#22411;&#22312;&#26410;&#26469;&#65292;&#29305;&#21035;&#26159;&#22312;&#30005;&#24433;&#21644;&#35270;&#21548;&#33402;&#26415;&#39046;&#22495;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25193;&#25955;&#31995;&#32479;&#35270;&#20026;&#19968;&#31181;&#23558;&#26410;&#26469;&#35745;&#31639;&#19982;&#36807;&#21435;&#32039;&#23494;&#32852;&#31995;&#30340;&#36807;&#31243;&#26469;&#33719;&#30410;&#65292;&#21482;&#35201;&#25215;&#35748;&#35760;&#24405;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models produce data according to a learned representation, e.g. diffusion models, through a process of approximation computing possible samples. Approximation can be understood as reconstruction and the large datasets used to train models as sets of records in which we represent the physical world with some data structure (photographs, audio recordings, manuscripts). During the process of reconstruction, e.g., image frames develop each timestep towards a textual input description. While moving forward in time, frame sets are shaped according to learned bias and their production, we argue here, can be considered as going back in time; not by inspiration on the backward diffusion process but acknowledging culture is specifically marked in the records. Futures of generative modelling, namely in film and audiovisual arts, can benefit by dealing with diffusion systems as a process to compute the future by inevitably being tied to the past, if acknowledging the records as to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.12342</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#23545;&#40784;&#65306;&#22522;&#20110;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions. (arXiv:2309.12342v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19981;&#21516;&#25991;&#21270;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#30340;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#33021;&#37327;&#21270;&#20854;&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#25991;&#21270;&#23545;&#40784;&#21644;&#23545;&#19981;&#21516;&#25991;&#21270;&#35268;&#33539;&#20010;&#20307;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#25919;&#27835;&#21644;&#31038;&#20250;&#20559;&#35265;&#20197;&#21450;&#20844;&#20247;&#24847;&#35265;&#65292;&#32780;&#26410;&#28041;&#21450;&#25991;&#21270;&#20215;&#20540;&#35266;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#21270;&#23545;&#40784;&#27979;&#35797;&#65288;CAT&#65289;&#65292;&#21033;&#29992;&#38669;&#22827;&#26031;&#27888;&#24503;&#30340;&#25991;&#21270;&#32500;&#24230;&#26694;&#26550;&#37327;&#21270;&#25991;&#21270;&#23545;&#40784;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#20998;&#26512;&#25552;&#20379;&#35299;&#37322;&#24615;&#30340;&#36328;&#25991;&#21270;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;ChatGPT&#21644;Bard&#65289;&#22312;&#19981;&#21516;&#25991;&#21270;&#22269;&#23478;&#65288;&#32654;&#22269;&#12289;&#27801;&#29305;&#38463;&#25289;&#20271;&#12289;&#20013;&#22269;&#21644;&#26031;&#27931;&#20240;&#20811;&#65289;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#20215;&#20540;&#35266;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#39118;&#26684;&#21644;&#36229;&#21442;&#25968;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#37327;&#21270;&#20102;LLMs&#19982;&#29305;&#23450;&#22269;&#23478;&#30340;&#25991;&#21270;&#23545;&#40784;&#31243;&#24230;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;LLMs&#22312;&#35299;&#37322;&#24615;&#25991;&#21270;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;&#23613;&#31649;&#25152;&#26377;&#30340;LLMs&#37117;&#27809;&#26377;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
The deployment of large language models (LLMs) raises concerns regarding their cultural misalignment and potential ramifications on individuals from various cultural norms. Existing work investigated political and social biases and public opinions rather than their cultural values. To address this limitation, the proposed Cultural Alignment Test (CAT) quantifies cultural alignment using Hofstede's cultural dimension framework, which offers an explanatory cross-cultural comparison through the latent variable analysis. We apply our approach to assess the cultural values embedded in state-of-the-art LLMs, such as: ChatGPT and Bard, across diverse cultures of countries: United States (US), Saudi Arabia, China, and Slovakia, using different prompting styles and hyperparameter settings. Our results not only quantify cultural alignment of LLMs with certain countries, but also reveal the difference between LLMs in explanatory cultural dimensions. While all LLMs did not provide satisfactory res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.12334</link><description>&lt;p&gt;
&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#26159;&#19968;&#20010;&#38544;&#21547;&#30340;&#21160;&#24577;&#22810;&#32500;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Knowledge Tracing is an implicit dynamic multidimensional item response theory model. (arXiv:2309.12334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21457;&#29616;&#65292;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#27604;DKT&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26159;&#26681;&#25454;&#23398;&#29983;&#20808;&#21069;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#22312;&#26032;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20248;&#21270;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#20808;&#21069;&#27493;&#39588;&#12290;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#65288;DKT&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#36861;&#36394;&#31454;&#20105;&#27169;&#22411;&#65292;&#21363;&#20351;&#19968;&#20123;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#33021;&#19982;&#20854;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20294;&#23545;&#20110;&#20026;&#20160;&#20040;DKT&#33021;&#22815;&#22914;&#27492;&#25104;&#21151;&#30340;&#20102;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#35270;&#20026;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#12290;&#36825;&#20010;&#35266;&#28857;&#19981;&#20165;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24615;&#33021;&#12289;&#31616;&#21333;&#24615;&#25110;&#34920;&#36798;&#24615;&#26041;&#38754;&#25552;&#20986;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25171;&#24320;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#22411;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#65292;&#21487;&#33021;&#27604;DKT&#20351;&#29992;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#26356;&#23569;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#23398;&#29983;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing consists in predicting the performance of some students on new questions given their performance on previous questions, and can be a prior step to optimizing assessment and learning. Deep knowledge tracing (DKT) is a competitive model for knowledge tracing relying on recurrent neural networks, even if some simpler models may match its performance. However, little is known about why DKT works so well. In this paper, we frame deep knowledge tracing as a encoderdecoder architecture. This viewpoint not only allows us to propose better models in terms of performance, simplicity or expressivity but also opens up promising avenues for future research directions. In particular, we show on several small and large datasets that a simpler decoder, with possibly fewer parameters than the one used by DKT, can predict student performance better.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;UBET&#33258;&#21160;&#24066;&#22330;&#21046;&#36896;&#32773;&#36827;&#34892;&#38142;&#19978;&#20307;&#32946;&#21338;&#24425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;&#24179;&#21488;&#30340;&#32570;&#28857;&#65292;&#30830;&#20445;&#36879;&#26126;&#24230;&#12289;&#23433;&#20840;&#24615;&#21644;&#36739;&#20302;&#30340;&#36153;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#21644;&#31639;&#27861;&#26469;&#23450;&#20215;&#20307;&#32946;&#36180;&#29575;&#65292;&#25552;&#20379;&#27969;&#21160;&#24615;&#24182;&#23454;&#29616;&#20840;&#29699;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12333</link><description>&lt;p&gt;
&#20351;&#29992;UBET&#33258;&#21160;&#24066;&#22330;&#21046;&#36896;&#32773;&#36827;&#34892;&#38142;&#19978;&#20307;&#32946;&#21338;&#24425;
&lt;/p&gt;
&lt;p&gt;
Onchain Sports Betting using UBET Automated Market Maker. (arXiv:2309.12333v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;UBET&#33258;&#21160;&#24066;&#22330;&#21046;&#36896;&#32773;&#36827;&#34892;&#38142;&#19978;&#20307;&#32946;&#21338;&#24425;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27492;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#20013;&#24515;&#21270;&#24179;&#21488;&#30340;&#32570;&#28857;&#65292;&#30830;&#20445;&#36879;&#26126;&#24230;&#12289;&#23433;&#20840;&#24615;&#21644;&#36739;&#20302;&#30340;&#36153;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#21644;&#31639;&#27861;&#26469;&#23450;&#20215;&#20307;&#32946;&#36180;&#29575;&#65292;&#25552;&#20379;&#27969;&#21160;&#24615;&#24182;&#23454;&#29616;&#20840;&#29699;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#20307;&#32946;&#21338;&#24425;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#35299;&#20915;&#20102;&#20256;&#32479;&#20013;&#24515;&#21270;&#24179;&#21488;&#30340;&#32570;&#28857;&#65292;&#30830;&#20445;&#20102;&#36879;&#26126;&#24230;&#12289;&#23433;&#20840;&#24615;&#21644;&#36739;&#20302;&#30340;&#36153;&#29992;&#12290;&#38750;&#30417;&#31649;&#35299;&#20915;&#26041;&#26696;&#20351;&#25237;&#27880;&#32773;&#25317;&#26377;&#36164;&#37329;&#25152;&#26377;&#26435;&#65292;&#32469;&#36807;&#20102;&#22320;&#29702;&#38480;&#21046;&#12290;&#21435;&#20013;&#24515;&#21270;&#24179;&#21488;&#22686;&#24378;&#20102;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#27665;&#20027;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#21435;&#20013;&#24515;&#21270;&#20307;&#32946;&#21338;&#24425;&#38656;&#35201;&#33258;&#21160;&#24066;&#22330;&#21046;&#36896;&#32773;&#65288;AMMs&#65289;&#26469;&#25552;&#20379;&#26377;&#25928;&#30340;&#27969;&#21160;&#24615;&#12290;&#29616;&#26377;&#30340;AMMs&#22914;Uniswap&#32570;&#20047;&#19982;&#20844;&#24179;&#36180;&#29575;&#30340;&#19968;&#33268;&#24615;&#65292;&#32473;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#24102;&#26469;&#20102;&#39118;&#38505;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;UBET AMM&#65288;UAMM&#65289;&#65292;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#21644;&#31639;&#27861;&#26469;&#20844;&#24179;&#23450;&#20215;&#20307;&#32946;&#36180;&#29575;&#12290;&#23427;&#24314;&#31435;&#20102;&#19968;&#20010;&#38142;&#19978;&#21338;&#24425;&#26694;&#26550;&#65292;&#35814;&#32454;&#20171;&#32461;&#24066;&#22330;&#21019;&#24314;&#12289;UAMM&#24212;&#29992;&#12289;&#25269;&#25276;&#27969;&#21160;&#24615;&#27744;&#20197;&#21450;&#23637;&#31034;&#31215;&#26497;&#32467;&#26524;&#30340;&#23454;&#39564;&#12290;UAMM&#36890;&#36807;&#30830;&#20445;&#27969;&#21160;&#24615;&#12289;&#21435;&#20013;&#24515;&#21270;&#23450;&#20215;&#21644;&#20840;&#29699;&#21487;&#35775;&#38382;&#24615;&#26469;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#20307;&#32946;&#21338;&#24425;&#65292;&#20419;&#36827;&#26080;&#38656;&#20449;&#20219;&#30340;
&lt;/p&gt;
&lt;p&gt;
The paper underscores how decentralization in sports betting addresses the drawbacks of traditional centralized platforms, ensuring transparency, security, and lower fees. Non-custodial solutions empower bettors with ownership of funds, bypassing geographical restrictions. Decentralized platforms enhance security, privacy, and democratic decision-making. However, decentralized sports betting necessitates automated market makers (AMMs) for efficient liquidity provision. Existing AMMs like Uniswap lack alignment with fair odds, creating risks for liquidity providers. To mitigate this, the paper introduces UBET AMM (UAMM), utilizing smart contracts and algorithms to price sports odds fairly. It establishes an on-chain betting framework, detailing market creation, UAMM application, collateral liquidity pools, and experiments that exhibit positive outcomes. UAMM enhances decentralized sports betting by ensuring liquidity, decentralized pricing, and global accessibility, promoting trustless 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#21644;&#22810;&#22359;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#36827;&#34892;&#21333;&#19968;/&#22810;&#29289;&#36136;&#30340;&#34920;&#24449;&#65292;&#22312;&#22609;&#26009;&#20998;&#31867;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.12329</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#21644;&#22810;&#22359;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#36827;&#34892;&#21333;&#19968;/&#22810;&#29289;&#36136;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Mono/Multi-material Characterization Using Hyperspectral Images and Multi-Block Non-Negative Matrix Factorization. (arXiv:2309.12329v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12329
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#20809;&#35889;&#22270;&#20687;&#21644;&#22810;&#22359;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#36827;&#34892;&#21333;&#19968;/&#22810;&#29289;&#36136;&#30340;&#34920;&#24449;&#65292;&#22312;&#22609;&#26009;&#20998;&#31867;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22609;&#26009;&#20998;&#31867;&#26159;&#24223;&#29289;&#31649;&#29702;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22810;&#23618;&#22609;&#26009;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#21333;&#19968;&#29289;&#36136;&#21644;&#22810;&#29289;&#36136;&#22609;&#26009;&#24191;&#27867;&#24212;&#29992;&#20110;&#22686;&#24378;&#21253;&#35013;&#30340;&#21151;&#33021;&#24615;&#33021;&#65292;&#32467;&#21512;&#20102;&#21402;&#24230;&#12289;&#26426;&#26800;&#24378;&#24230;&#21644;&#32784;&#28909;&#24615;&#31561;&#26377;&#30410;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#21547;&#26377;&#22810;&#31181;&#32858;&#21512;&#29289;&#29289;&#31181;&#30340;&#26448;&#26009;&#22312;&#22238;&#25910;&#20026;&#21333;&#19968;&#29289;&#36136;&#21069;&#38656;&#35201;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#22240;&#27492;&#19981;&#24212;&#35813;&#20986;&#29616;&#22312;&#21333;&#19968;&#29289;&#36136;&#27969;&#20013;&#12290;&#24037;&#19994;4.0&#22823;&#22823;&#25913;&#36827;&#20102;&#22609;&#26009;&#21253;&#35013;&#26448;&#26009;&#20998;&#31867;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#36817;&#32418;&#22806;&#39640;&#20809;&#35889;&#25104;&#20687;(NIRHSI)&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#29289;&#36136;&#34920;&#24449;&#65292;&#26080;&#38656;&#36827;&#34892;&#26679;&#21697;&#21046;&#22791;&#12290;&#28982;&#32780;&#65292;&#29992;HSI&#35782;&#21035;&#22810;&#29289;&#36136;&#38656;&#35201;&#26032;&#30340;&#19987;&#29992;&#21270;&#23398;&#27169;&#24335;&#35782;&#21035;&#26041;&#27861;&#12290;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21270;&#23398;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Plastic sorting is a very essential step in waste management, especially due to the presence of multilayer plastics. These monomaterial and multimaterial plastics are widely employed to enhance the functional properties of packaging, combining beneficial properties in thickness, mechanical strength, and heat tolerance. However, materials containing multiple polymer species need to be pretreated before they can be recycled as monomaterials and therefore should not end up in monomaterial streams. Industry 4.0 has significantly improved materials sorting of plastic packaging in speed and accuracy compared to manual sorting, specifically through Near Infrared Hyperspectral Imaging (NIRHSI) that provides an automated, fast, and accurate material characterization, without sample preparation. Identification of multimaterials with HSI however requires novel dedicated approaches for chemical pattern recognition. Non negative Matrix Factorization, NMF, is widely used for the chemical resolution 
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#22269;&#27665;&#33322;&#19994;&#20013;&#39134;&#34892;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36229;&#39069;&#20107;&#20214;&#20998;&#26512;&#19981;&#20805;&#20998;&#21407;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#38752;&#24615;&#35780;&#20272;&#12289;&#31070;&#32463;&#32593;&#32476;&#39134;&#34892;&#25511;&#21046;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#39134;&#34892;&#20154;&#21592;&#35780;&#20272;&#21644;&#23454;&#26102;&#35686;&#25253;&#31561;&#25163;&#27573;&#65292;&#26088;&#22312;&#25552;&#21319;&#33322;&#31354;&#23433;&#20840;&#12289;&#20154;&#21592;&#35780;&#20272;&#21644;&#35686;&#25253;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12324</link><description>&lt;p&gt;
&#33322;&#31354;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#21644;&#39134;&#34892;&#25216;&#26415;&#35780;&#20272;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Aviation Safety Risk Analysis and Flight Technology Assessment Issues. (arXiv:2309.12324v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20013;&#22269;&#27665;&#33322;&#19994;&#20013;&#39134;&#34892;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36229;&#39069;&#20107;&#20214;&#20998;&#26512;&#19981;&#20805;&#20998;&#21407;&#22240;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#38752;&#24615;&#35780;&#20272;&#12289;&#31070;&#32463;&#32593;&#32476;&#39134;&#34892;&#25511;&#21046;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#26426;&#22120;&#23398;&#20064;&#39134;&#34892;&#20154;&#21592;&#35780;&#20272;&#21644;&#23454;&#26102;&#35686;&#25253;&#31561;&#25163;&#27573;&#65292;&#26088;&#22312;&#25552;&#21319;&#33322;&#31354;&#23433;&#20840;&#12289;&#20154;&#21592;&#35780;&#20272;&#21644;&#35686;&#25253;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#24378;&#35843;&#20102;&#20013;&#22269;&#27665;&#33322;&#19994;&#20013;&#39134;&#34892;&#23433;&#20840;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20840;&#38754;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#23427;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26041;&#38754;&#65306;&#20998;&#26512;&#36229;&#39069;&#20107;&#20214;&#21644;&#32479;&#35745;&#35780;&#20272;&#38750;&#36229;&#39069;&#25968;&#25454;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#22312;&#20110;&#23545;&#36229;&#39069;&#20107;&#20214;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#20998;&#26512;&#12290;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#21487;&#38752;&#24615;&#35780;&#20272;&#12289;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#39134;&#34892;&#25511;&#21046;&#12289;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12289;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#39134;&#34892;&#20154;&#21592;&#25216;&#33021;&#21644;&#24314;&#31435;&#23454;&#26102;&#33258;&#21160;&#35686;&#25253;&#12290;&#36825;&#20123;&#21162;&#21147;&#26088;&#22312;&#22686;&#24378;&#39134;&#34892;&#23433;&#20840;&#12289;&#20154;&#21592;&#35780;&#20272;&#21644;&#35686;&#25253;&#26426;&#21046;&#65292;&#20026;&#26356;&#23433;&#20840;&#12289;&#26356;&#39640;&#25928;&#30340;&#27665;&#33322;&#19994;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text highlights the significance of flight safety in China's civil aviation industry and emphasizes the need for comprehensive research. It focuses on two main areas: analyzing exceedance events and statistically evaluating non-exceedance data. The challenges of current approaches lie in insufficient cause analysis for exceedances. The proposed solutions involve data preprocessing, reliability assessment, quantifying flight control using neural networks, exploratory data analysis, flight personnel skill evaluation with machine learning, and establishing real-time automated warnings. These endeavors aim to enhance flight safety, personnel assessment, and warning mechanisms, contributing to a safer and more efficient civil aviation sector.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29289;&#29702;&#24341;&#23548;&#30340;&#26230;&#20307;&#29983;&#25104;&#27169;&#22411;&#65288;PGCGM&#65289;&#30340;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#36870;&#21521;&#35774;&#35745;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.12323</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#30340;&#26448;&#26009;&#30340;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the diversity and utility of materials proposed by generative models. (arXiv:2309.12323v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29289;&#29702;&#24341;&#23548;&#30340;&#26230;&#20307;&#29983;&#25104;&#27169;&#22411;&#65288;PGCGM&#65289;&#30340;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#36870;&#21521;&#35774;&#35745;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#31185;&#23398;&#24314;&#27169;&#29983;&#25104;&#30340;&#25968;&#25454;&#21019;&#24314;&#22823;&#37327;&#26032;&#39062;&#30340;&#26448;&#26009;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29289;&#29702;&#24341;&#23548;&#30340;&#26230;&#20307;&#29983;&#25104;&#27169;&#22411;&#65288;PGCGM&#65289;&#65292;&#22312;&#36870;&#21521;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#40664;&#35748;&#30340;PGCGM&#30340;&#36755;&#20837;&#31354;&#38388;&#22312;&#21442;&#25968;&#21464;&#21270;&#26041;&#38754;&#24182;&#19981;&#24179;&#28369;&#65292;&#20351;&#24471;&#26448;&#26009;&#20248;&#21270;&#22256;&#38590;&#19988;&#21463;&#38480;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#29983;&#25104;&#30340;&#32467;&#26500;&#34987;&#19968;&#20010;&#21333;&#29420;&#30340;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#20026;&#28909;&#21147;&#23398;&#19981;&#31283;&#23450;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#22495;&#22806;&#25968;&#25454;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#20309;&#25913;&#36827;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36870;&#21521;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative machine learning models can use data generated by scientific modeling to create large quantities of novel material structures. Here, we assess how one state-of-the-art generative model, the physics-guided crystal generation model (PGCGM), can be used as part of the inverse design process. We show that the default PGCGM's input space is not smooth with respect to parameter variation, making material optimization difficult and limited. We also demonstrate that most generated structures are predicted to be thermodynamically unstable by a separate property-prediction model, partially due to out-of-domain data challenges. Our findings suggest how generative models might be improved to enable better inverse design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.11028</link><description>&lt;p&gt;
&#31070;&#32463;&#34920;&#24449;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#25152;&#20851;&#24515;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#34920;&#24449;&#24863;&#30693;&#21644;&#35748;&#30693;&#20869;&#23481;&#30340;&#22823;&#33041;&#34920;&#24449;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#24449;&#24212;&#35813;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#21644;&#20010;&#20307;&#22823;&#33041;&#30340;&#29305;&#24322;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#19982;&#35745;&#31639;&#24046;&#24322;&#30456;&#23545;&#24212;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#34920;&#24449;&#20960;&#20309;&#32467;&#26500;&#26469;&#34920;&#24449;&#22823;&#33041;&#34920;&#24449;&#65292;&#20960;&#20309;&#32467;&#26500;&#30001;&#34920;&#24449;&#19981;&#30456;&#20284;&#30697;&#38453;&#65288;RDM&#65289;&#23450;&#20041;&#65292;RDM&#26159;&#19968;&#20010;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#25688;&#35201;&#20102;&#20010;&#20307;&#31070;&#32463;&#20803;&#65288;&#25110;&#21709;&#24212;&#36890;&#36947;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#34920;&#24449;&#20102;&#21050;&#28608;&#30340;&#21487;&#36776;&#21035;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#22823;&#33041;&#34920;&#24449;&#25299;&#25169;&#30340;&#25277;&#35937;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;tRSA&#65289;&#65292;&#23427;&#26159;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;RSA&#65289;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#23558;RDM&#36827;&#34892;&#27867;&#21270;&#20197;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#20943;&#24369;&#20960;&#20309;&#32467;&#26500;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
&lt;/p&gt;</description></item><item><title>Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.09976</link><description>&lt;p&gt;
Des-q: &#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#30340;&#26500;&#24314;&#21644;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#30340;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09976
&lt;/p&gt;
&lt;p&gt;
Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30001;&#20110;&#20854;&#31616;&#21333;&#26500;&#36896;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#20915;&#31574;&#26641;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#24930;&#65292;&#19982;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21576;&#22810;&#39033;&#24335;&#35268;&#27169;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#31639;&#27861;Des-q&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#20551;&#35774;&#25968;&#25454;&#27969;&#20135;&#29983;&#36739;&#23567;&#30340;&#26032;&#35757;&#32451;&#26679;&#26412;&#22686;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;Des-q&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#23558;&#26032;&#26679;&#26412;&#21152;&#36733;&#21040;&#37327;&#23376;&#21487;&#35775;&#38382;&#20869;&#23384;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#20063;&#36798;&#21040;&#20102;&#22810;&#23545;&#25968;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#25191;&#34892;k&#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#12290;&#36825;&#20123;&#20998;&#35010;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#36229;&#24179;&#38754;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20302;&#24378;&#24230;&#21644;&#39640;&#35282;&#36895;&#24230;&#19979;&#30340;&#24494;&#24369;&#31354;&#38388;&#30862;&#29255;&#25506;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#23545;&#27604;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#20449;&#22122;&#27604;&#20026;2.0&#30340;&#31354;&#38388;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.08244</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#23616;&#37096;&#23545;&#27604;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#23454;&#26102;&#24494;&#24369;&#31354;&#38388;&#30862;&#29255;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Real-time Faint Space Debris Detector With Learning-based LCM. (arXiv:2309.08244v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20302;&#24378;&#24230;&#21644;&#39640;&#35282;&#36895;&#24230;&#19979;&#30340;&#24494;&#24369;&#31354;&#38388;&#30862;&#29255;&#25506;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23616;&#37096;&#23545;&#27604;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#20449;&#22122;&#27604;&#20026;2.0&#30340;&#31354;&#38388;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33322;&#31354;&#33322;&#22825;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#31354;&#38388;&#30862;&#29255;&#30340;&#19981;&#26029;&#22686;&#21152;&#23545;&#23431;&#23449;&#39134;&#33337;&#30340;&#23433;&#20840;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#30862;&#29255;&#21453;&#23556;&#20809;&#30340;&#20302;&#24378;&#24230;&#21644;&#39640;&#35282;&#36895;&#24230;&#38459;&#30861;&#20102;&#25552;&#21462;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22320;&#38754;&#35266;&#27979;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23567;&#22411;&#31354;&#38388;&#30862;&#29255;&#24456;&#38590;&#34987;&#26816;&#27979;&#21040;&#65292;&#22240;&#27492;&#38656;&#35201;&#21152;&#24378;&#33322;&#22825;&#22120;&#30340;&#31354;&#38388;&#24577;&#21183;&#24863;&#30693;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#20256;&#32479;&#26041;&#27861;&#22312;&#20302;&#20449;&#22122;&#27604;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#22914;&#25928;&#26524;&#20302;&#19979;&#21644;&#26102;&#38388;&#28040;&#32791;&#22823;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#23545;&#27604;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#20302;&#20449;&#22122;&#27604;&#26465;&#32441;&#25552;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#26816;&#27979;&#20449;&#22122;&#27604;&#20026;2.0&#30340;&#31354;&#38388;&#29289;&#20307;&#12290;&#22312;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#65292;&#23558;&#20351;&#29992;&#23616;&#37096;&#23545;&#27604;&#36827;&#34892;&#31895;&#30053;&#20998;&#31867;&#65292;&#36820;&#22238;&#36830;&#25509;&#37096;&#20214;&#20316;&#20026;&#21021;&#27493;&#32467;&#26524;&#65292;&#28982;&#21518;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#37325;&#24314;&#36830;&#25509;&#37096;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of aerospace technology, the increasing population of space debris has posed a great threat to the safety of spacecraft. However, the low intensity of reflected light and high angular velocity of space debris impede the extraction. Besides, due to the limitations of the ground observation methods, small space debris can hardly be detected, making it necessary to enhance the spacecraft's capacity for space situational awareness (SSA). Considering that traditional methods have some defects in low-SNR target detection, such as low effectiveness and large time consumption, this paper proposes a method for low-SNR streak extraction based on local contrast and maximum likelihood estimation (MLE), which can detect space objects with SNR 2.0 efficiently. In the proposed algorithm, local contrast will be applied for crude classifications, which will return connected components as preliminary results, and then MLE will be performed to reconstruct the connected components of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;</title><link>http://arxiv.org/abs/2309.07988</link><description>&lt;p&gt;
&#25240;&#21472;&#27880;&#24847;&#21147;&#65306;&#38754;&#21521;&#35774;&#22791;&#30340;Transformer&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#20869;&#23384;&#21644;&#21151;&#32791;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming Speech Recognition. (arXiv:2309.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25240;&#21472;&#27880;&#24847;&#21147;&#30340;&#25216;&#26415;&#65292;&#22312;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23569;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#25968;&#37327;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;24%&#12289;&#21151;&#32791;&#20943;&#23567;23%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#20248;&#21270;Transformer&#25512;&#26029;&#30340;&#21162;&#21147;&#65292;&#36890;&#24120;&#38024;&#23545;&#38271;&#19978;&#19979;&#25991;&#24212;&#29992;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#31616;&#21270;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#19978;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#36890;&#24120;&#27599;&#27425;&#21482;&#22788;&#29702;&#26377;&#38480;&#25968;&#37327;&#30340;&#20196;&#29260;&#65292;&#22240;&#27492;&#27880;&#24847;&#21147;&#24471;&#20998;&#35745;&#31639;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24182;&#19981;&#26159;&#29942;&#39048;&#25152;&#22312;&#12290;&#30456;&#21453;&#65292;&#29942;&#39048;&#22312;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#32447;&#24615;&#25237;&#24433;&#23618;&#65292;&#23427;&#20204;&#26500;&#25104;&#20102;&#27169;&#22411;&#22823;&#23567;&#30340;&#30456;&#24403;&#37096;&#20998;&#65292;&#24182;&#23545;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#21151;&#32791;&#30340;&#20351;&#29992;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25240;&#21472;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#36825;&#20123;&#32447;&#24615;&#23618;&#30340;&#25216;&#26415;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#20869;&#23384;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;&#35774;&#22791;&#19978;&#30340;&#22522;&#20110;Transformer&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25240;&#21472;&#27880;&#24847;&#21147;&#21487;&#20197;&#23558;&#27169;&#22411;&#22823;&#23567;&#65288;&#21644;&#30456;&#24212;&#30340;&#20869;&#23384;&#28040;&#32791;&#65289;&#20943;&#23567;&#22810;&#36798;24%&#65292;&#24182;&#23558;&#21151;&#32791;&#20943;&#23567;&#22810;&#36798;23%&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in speech recognition. Existing efforts to optimize Transformer inference, typically for long-context applications, center on simplifying attention score calculations. However, streaming speech recognition models usually process a limited number of tokens each time, making attention score calculation less of a bottleneck. Instead, the bottleneck lies in the linear projection layers of multi-head attention and feedforward networks, constituting a substantial portion of the model size and contributing significantly to computation, memory, and power usage.  To address this bottleneck, we propose folding attention, a technique targeting these linear layers, significantly reducing model size and improving memory and power efficiency. Experiments on on-device Transformer-based streaming speech recognition models show that folding attention reduces model size (and corresponding memory consumption) by up to 24% and power consumption by up to 23%, all without comp
&lt;/p&gt;</description></item><item><title>Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07778</link><description>&lt;p&gt;
Virchow: &#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07778
&lt;/p&gt;
&lt;p&gt;
Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30284;&#30151;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23545;&#20110;&#35768;&#22810;&#29305;&#23450;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#65292;&#25968;&#25454;&#37327;&#19981;&#36275;&#20197;&#36827;&#34892;&#24320;&#21457;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Virchow&#65292;&#19968;&#20010;632&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30149;&#29702;&#23398;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;Virchow&#22312;1.5&#30334;&#19975;&#20010;&#19981;&#21516;&#32452;&#32455;&#26679;&#26412;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#25968;&#25454;&#37327;&#22823;&#24471;&#22810;&#12290;&#22312;&#21253;&#25324;&#29926;&#29255;&#32423;&#20840;&#30284;&#26816;&#27979;&#21644;&#20122;&#22411;&#20197;&#21450;&#24187;&#28783;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;Virchow&#22312;&#26469;&#33258;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20154;&#32676;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06548</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#32447;&#24615;&#31639;&#23376;&#30340;&#26080;&#38480;&#32500;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Online Infinite-Dimensional Regression: Learning Linear Operators. (arXiv:2309.06548v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06548
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#26080;&#38480;&#32500;&#32447;&#24615;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#32447;&#24615;&#31639;&#23376;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#20123;&#26465;&#20214;&#19979;&#21017;&#19981;&#21487;&#20197;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#22312;PAC&#35774;&#32622;&#19979;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#35774;&#32622;&#19979;&#23398;&#20064;&#20004;&#20010;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20043;&#38388;&#30340;&#32447;&#24615;&#31639;&#23376;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$p \in [1, \infty)$&#33539;&#22260;&#20869;&#65292;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;$p$-Schatten&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#26377;&#30028;&#31639;&#23376;&#33539;&#25968;&#30340;&#32447;&#24615;&#31639;&#23376;&#31867;\textit{&#19981;}&#26159;&#21487;&#20197;&#22312;&#32447;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#19968;&#31867;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#35777;&#26126;&#20102;&#22312;&#32447;&#22343;&#19968;&#25910;&#25947;&#21644;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#21644;&#22343;&#19968;&#25910;&#25947;&#19982;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20998;&#31163;&#22312;PAC&#35774;&#32622;&#19979;&#21516;&#26679;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning linear operators under squared loss between two infinite-dimensional Hilbert spaces in the online setting. We show that the class of linear operators with uniformly bounded $p$-Schatten norm is online learnable for any $p \in [1, \infty)$. On the other hand, we prove an impossibility result by showing that the class of uniformly bounded linear operators with respect to the operator norm is \textit{not} online learnable. Moreover, we show a separation between online uniform convergence and online learnability by identifying a class of bounded linear operators that is online learnable but uniform convergence does not hold. Finally, we prove that the impossibility result and the separation between uniform convergence and learnability also hold in the agnostic PAC setting.
&lt;/p&gt;</description></item><item><title>&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;</title><link>http://arxiv.org/abs/2309.06375</link><description>&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20132;&#21449;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Modeling Recommender Ecosystems: Research Challenges at the Intersection of Mechanism Design, Reinforcement Learning and Generative Models. (arXiv:2309.06375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06375
&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#25512;&#33616;&#31995;&#32479;&#29983;&#24577;&#31995;&#32479;&#38656;&#35201;&#32771;&#34385;&#21442;&#19982;&#32773;&#28608;&#21169;&#12289;&#34892;&#20026;&#20197;&#21450;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65292;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20301;&#20110;&#28085;&#30422;&#29992;&#25143;&#12289;&#20869;&#23481;&#25552;&#20379;&#21830;&#12289;&#24191;&#21578;&#21830;&#21644;&#20854;&#20182;&#21442;&#19982;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#20197;&#21450;&#22823;&#22810;&#25968;&#37325;&#35201;&#23454;&#29992;&#25512;&#33616;&#31995;&#32479;&#65292;&#20165;&#38480;&#20110;&#20010;&#21035;&#29992;&#25143;&#25512;&#33616;&#30340;&#23616;&#37096;&#12289;&#30701;&#35270;&#20248;&#21270;&#12290;&#36825;&#32473;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20026;&#29992;&#25143;&#24102;&#26469;&#30340;&#38271;&#26399;&#25928;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#25104;&#26412;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#26368;&#22823;&#21270;&#31995;&#32479;&#23545;&#36825;&#20123;&#21442;&#19982;&#32773;&#30340;&#20215;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#29983;&#24577;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#29366;&#20917;&#65292;&#26377;&#24517;&#35201;&#26126;&#30830;&#22320;&#23545;&#31995;&#32479;&#20013;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#28608;&#21169;&#21644;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23545;&#20854;&#31574;&#30053;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#27492;&#38656;&#35201;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31561;&#25216;&#26415;&#36827;&#34892;&#38271;&#26399;&#20248;&#21270;&#65307;&#20351;&#29992;&#31038;&#20250;&#36873;&#25321;&#26041;&#27861;&#20026;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25928;&#29992;&#36827;&#34892;&#19981;&#21487;&#36991;&#20813;&#30340;&#26435;&#34913;&#65307;&#20943;&#23569;&#20449;&#24687;&#19981;&#23545;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems lie at the heart of complex ecosystems that couple the behavior of users, content providers, advertisers, and other actors. Despite this, the focus of the majority of recommender research -- and most practical recommenders of any import -- is on the local, myopic optimization of the recommendations made to individual users. This comes at a significant cost to the long-term utility that recommenders could generate for its users. We argue that explicitly modeling the incentives and behaviors of all actors in the system -- and the interactions among them induced by the recommender's policy -- is strictly necessary if one is to maximize the value the system brings to these actors and improve overall ecosystem "health". Doing so requires: optimization over long horizons using techniques such as reinforcement learning; making inevitable tradeoffs in the utility that can be generated for different actors using the methods of social choice; reducing information asymm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.01816</link><description>&lt;p&gt;
&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Computation and Communication Efficient Federated Learning over Wireless Networks. (arXiv:2309.01816v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#26080;&#32447;&#32593;&#32476;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#33021;&#22815;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#22791;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#23398;&#20064;&#20934;&#30830;&#24230;&#19979;&#38477;&#65292;&#22312;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#21644;&#26080;&#32447;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#26356;&#26032;&#22823;&#35268;&#27169;&#23398;&#20064;&#27169;&#22411;&#20250;&#22686;&#21152;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#21098;&#26525;&#21644;&#20010;&#24615;&#21270;&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;&#23398;&#20064;&#27169;&#22411;&#20998;&#20026;&#20840;&#23616;&#37096;&#20998;&#21644;&#20010;&#24615;&#21270;&#37096;&#20998;&#65292;&#20840;&#23616;&#37096;&#20998;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#19982;&#25152;&#26377;&#35774;&#22791;&#20849;&#20139;&#20197;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20010;&#24615;&#21270;&#37096;&#20998;&#38024;&#23545;&#29305;&#23450;&#35774;&#22791;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;FL&#36807;&#31243;&#20013;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#20197;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#35774;&#22791;&#30340;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;&#28982;&#21518;&#65292;&#23545;&#25152;&#25552;&#20986;&#30340;FL&#26694;&#26550;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24310;&#36831;&#20197;&#21450;&#25910;&#25947;&#20998;&#26512;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables distributed learning across edge devices while protecting data privacy. However, the learning accuracy decreases due to the heterogeneity of devices' data, and the computation and communication latency increase when updating large-scale learning models on devices with limited computational capability and wireless resources. We consider a novel FL framework with partial model pruning and personalization to overcome these challenges. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine-tuned for a specific device, which adapts the model size during FL to reduce both computation and communication latency and increases the learning accuracy for the device with non-independent and identically distributed (non-IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically ana
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00723</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#19978;&#19979;&#25991;&#20559;&#20506;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contextual Biasing of Named-Entities with Large Language Models. (arXiv:2309.00723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;boosting&#65292;&#24182;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#20559;&#20506;&#65292;&#21363;&#22312;&#31532;&#20108;&#27425;&#25171;&#20998;&#26102;&#20026;LLM&#25552;&#20379;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#25171;&#20998;&#26399;&#38388;&#21033;&#29992;&#25552;&#31034;&#20449;&#24687;&#23545;LLM&#36827;&#34892;boosting&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#25552;&#31034;&#20449;&#24687;&#21253;&#25324;&#20559;&#20506;&#21015;&#34920;&#21644;&#23569;&#26679;&#26412;&#31034;&#20363;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#20551;&#35774;&#24471;&#20998;&#26102;&#20316;&#20026;&#38468;&#21152;&#20449;&#24687;&#12290;&#38500;&#20102;&#23569;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LLM&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#21644;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#20026;&#20102;&#25552;&#39640;&#19978;&#19979;&#25991;&#20559;&#20506;&#30340;&#25928;&#29575;&#24182;&#36991;&#20813;&#36229;&#36807;LLMs&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#21363;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#39044;&#27979;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20307;&#20316;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#12290;&#23545;&#20869;&#37096;&#30340;&#21628;&#21483;&#12289;&#28040;&#24687;&#21644;&#21475;&#36848;&#25968;&#25454;&#38598;&#20197;&#21450;SLUE-Voxpopuli&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35789;&#38169;&#35823;&#29575;(WER)&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.15316</link><description>&lt;p&gt;
3D-MuPPET: 3D&#22810;&#40509;&#23039;&#24577;&#20272;&#35745;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15316
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#21160;&#29289;&#23039;&#21183;&#36319;&#36394;&#30340;&#26080;&#26631;&#35760;&#26041;&#27861;&#24050;&#26377;&#25152;&#21457;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#29992;&#20110;&#36861;&#36394;&#22823;&#35268;&#27169;&#21160;&#29289;&#32676;&#20307;&#30340;&#19977;&#32500;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D-MuPPET&#65292;&#19968;&#20010;&#20351;&#29992;&#22810;&#35270;&#35282;&#23454;&#26102;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#36798;10&#21482;&#40509;&#23376;&#30340;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#25512;&#27979;&#22810;&#21482;&#40509;&#23376;&#30340;2D&#20851;&#38190;&#28857;&#21644;&#36793;&#30028;&#26694;&#65292;&#28982;&#21518;&#23558;&#20851;&#38190;&#28857;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#12290;&#23545;&#20110;&#21305;&#37197;&#23545;&#24212;&#20851;&#31995;&#65292;&#25105;&#20204;&#39318;&#20808;&#21160;&#24577;&#22320;&#23558;2D&#26816;&#27979;&#32467;&#26524;&#19982;&#31532;&#19968;&#24103;&#20013;&#30340;&#20840;&#23616;&#36523;&#20221;&#36827;&#34892;&#21305;&#37197;&#65292;&#28982;&#21518;&#20351;&#29992;2D&#36319;&#36394;&#22120;&#22312;&#21518;&#32493;&#24103;&#20013;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#27491;&#30830;&#20851;&#38190;&#28857;&#30334;&#20998;&#27604;&#65288;PCK&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20363;&#65292;&#21363;&#25105;&#20204;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21253;&#21547;&#22810;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#19978;&#24471;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#23545;&#26032;&#22330;&#26223;&#30340;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markerless methods for animal posture tracking have been developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple-views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For correspondence matching, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain correspondences accross views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator for Root Mean Square Error (RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel use case where our model trained with data of single pigeons provides comparable results on data containing multiple pigeons. This can simplify the domain shift to new sp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren</title><link>http://arxiv.org/abs/2308.13985</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#65306;&#19968;&#20010;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26631;&#37327;&#21270;&#65292;&#21363;&#36890;&#36807;&#21152;&#26435;&#24635;&#21644;&#26469;&#32452;&#21512;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#33258;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21019;&#31435;&#20197;&#26469;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#30340;&#40664;&#35748;&#36873;&#25321;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19987;&#38376;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#22120;&#65288;SMTOs&#65289;&#26469;&#22788;&#29702;MTL&#20316;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;SMTOs&#26159;&#21542;&#27604;&#26631;&#37327;&#21270;&#26377;&#26681;&#26412;&#19978;&#30340;&#20248;&#21183;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#20013;&#23384;&#22312;&#23545;&#27604;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#28608;&#28872;&#35752;&#35770;&#65292;&#20027;&#35201;&#26159;&#20174;&#32463;&#39564;&#35282;&#24230;&#20986;&#21457;&#12290;&#20026;&#20102;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#37327;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;MTL&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#37027;&#20123;&#22768;&#31216;&#26631;&#37327;&#21270;&#20855;&#26377;&#32463;&#39564;&#20248;&#21183;&#30340;&#26368;&#36817;&#24037;&#20316;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;
Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
&lt;/p&gt;</description></item><item><title>BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12952</link><description>&lt;p&gt;
BridgeData V2:&#19968;&#20010;&#29992;&#20110;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BridgeData V2: A Dataset for Robot Learning at Scale. (arXiv:2308.12952v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12952
&lt;/p&gt;
&lt;p&gt;
BridgeData V2&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#22791;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#24182;&#19988;&#20860;&#23481;&#22810;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BridgeData V2&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#20419;&#36827;&#35268;&#27169;&#21270;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;BridgeData V2&#21253;&#21547;&#20102;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#19988;&#25104;&#26412;&#36739;&#20302;&#30340;&#26426;&#22120;&#20154;&#19978;&#25910;&#38598;&#30340;60,096&#20010;&#36712;&#36857;&#65292;&#35206;&#30422;&#20102;24&#20010;&#29615;&#22659;&#12290;BridgeData V2&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#21464;&#24322;&#24615;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#12289;&#39046;&#22495;&#21644;&#26426;&#26500;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#30340;&#25216;&#33021;&#65292;&#20351;&#24471;&#35813;&#25968;&#25454;&#38598;&#25104;&#20026;&#24191;&#22823;&#30740;&#31350;&#20154;&#21592;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#22810;&#31181;&#24320;&#25918;&#35789;&#27719;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20197;&#30446;&#26631;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20026;&#26465;&#20214;&#26159;&#20860;&#23481;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;6&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#38656;&#35201;&#19981;&#21516;&#27867;&#21270;&#31243;&#24230;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#38543;&#30528;&#26356;&#22810;&#30340;&#25968;&#25454;&#21644;&#26356;&#39640;&#23481;&#37327;&#30340;&#27169;&#22411;&#32780;&#25913;&#21892;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#22686;&#21152;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12874</link><description>&lt;p&gt;
&#31616;&#26131;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#31616;&#21333;&#33258;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;Transformer&#31070;&#32463;&#32593;&#32476;&#22312;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#65292;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#27604;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#21644;&#31616;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#29992;&#20110;&#28151;&#27788;&#31995;&#32479;&#26102;&#38388;&#21160;&#24577;&#39044;&#27979;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31216;&#20026;&#31616;&#26131;&#27880;&#24847;&#21147;&#12290;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#20165;&#20351;&#29992;&#26597;&#35810;&#21644;&#38190;&#30340;&#20869;&#31215;&#65292;&#22240;&#27492;&#35777;&#26126;&#20102;&#20026;&#20102;&#33719;&#21462;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#25152;&#38656;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#65292;&#24182;&#19981;&#38656;&#35201;&#38190;&#12289;&#26597;&#35810;&#21644;softmax&#12290;&#36890;&#36807;&#22312;softmax&#27880;&#24847;&#21147;&#24471;&#20998;&#19978;&#23454;&#26045;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#33258;&#27880;&#24847;&#21147;&#22312;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#24352;&#25104;&#31354;&#38388;&#20013;&#21387;&#32553;&#20102;&#26469;&#33258;&#26597;&#35810;&#21644;&#38190;&#30340;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31616;&#26131;&#27880;&#24847;&#21147;&#26041;&#27861;&#30452;&#25509;&#23558;&#27880;&#24847;&#21147;&#24471;&#20998;&#20316;&#20026;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#37325;&#26500;&#21644;&#39044;&#27979;&#23637;&#29616;&#26356;&#24378;&#40065;&#26834;&#24615;&#21644;&#26356;&#23569;&#22797;&#26434;&#24615;&#30340;&#28151;&#27788;&#31995;&#32479;&#30340;&#26102;&#38388;&#21160;&#24577;&#26102;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#27604;&#33258;&#27880;&#24847;&#26426;&#21046;&#25110;&#24191;&#27867;&#20351;&#29992;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30340;&#23637;&#24320;&#26041;&#27861;SBUnfold&#65292;&#23427;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12351</link><description>&lt;p&gt;
&#29992;Schr&#246;dinger&#26725;&#25913;&#36827;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Improving Generative Model-based Unfolding with Schr\"{o}dinger Bridges. (arXiv:2308.12351v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#30340;&#23637;&#24320;&#26041;&#27861;SBUnfold&#65292;&#23427;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23637;&#24320;&#24050;&#32463;&#23454;&#29616;&#20102;&#26080;bin&#21644;&#39640;&#32500;&#24494;&#20998;&#25130;&#38754;&#27979;&#37327;&#12290;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#21028;&#21035;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#65292;&#23427;&#20204;&#23398;&#20064;&#20102;&#23545;&#36215;&#22987;&#27169;&#25311;&#30340;&#23567;&#20462;&#27491;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#30456;&#31354;&#38388;&#21306;&#22495;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Schr&#246;dinger&#26725;&#21644;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;SBUnfold&#65292;&#19968;&#31181;&#23558;&#21028;&#21035;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#30340;&#23637;&#24320;&#26041;&#27861;&#12290;SBUnfold&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#23427;&#30340;&#29983;&#25104;&#27169;&#22411;&#23558;&#19968;&#32452;&#20107;&#20214;&#26144;&#23556;&#21040;&#21478;&#19968;&#32452;&#20107;&#20214;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#24050;&#30693;&#30340;&#27010;&#29575;&#23494;&#24230;&#65288;&#19982;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#21644;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SBUnfold&#22312;&#21512;&#25104;&#30340;Z+jets&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
&lt;/p&gt;</description></item><item><title>&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12066</link><description>&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#65306;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#25512;&#29702;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12066
&lt;/p&gt;
&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#28304;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;LLMs&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#25104;&#27604;&#20363;&#22320;&#25193;&#22823;&#35745;&#31639;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#39640;&#23384;&#20648;&#38656;&#27714;&#21644;&#31232;&#30095;&#19987;&#23478;&#30340;&#21160;&#24577;&#28608;&#27963;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;MoE&#30340;&#20869;&#23384;&#21344;&#29992;&#39640;&#30340;&#19987;&#23478;&#21442;&#25968;&#36716;&#31227;&#21040;CPU&#20869;&#23384;&#19978;&#65292;&#20294;&#26159;&#20174;CPU&#36801;&#31227;&#24050;&#28608;&#27963;&#30340;&#19987;&#23478;&#21040;GPU&#30340;&#24310;&#36831;&#23548;&#33268;&#20102;&#39640;&#24615;&#33021;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#65292;&#24182;&#22312;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2308.11631</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27700;&#30005;&#31449;&#31649;&#29702;&#27969;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based flow disaggregation for hydropower plant management. (arXiv:2308.11631v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#65292;&#24182;&#22312;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#25968;&#25454;&#23545;&#20110;&#27700;&#30005;&#31449;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#25386;&#23041;&#27700;&#30005;&#31449;&#21482;&#26377;&#27599;&#26085;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#31649;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;&#20122;&#26085;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#20122;&#26085;&#25968;&#25454;&#30340;&#26222;&#36941;&#32570;&#22833;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25386;&#23041;&#26576;&#27969;&#37327;&#27979;&#31449;&#30340;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#23558;&#27599;&#26085;&#27969;&#37327;&#20998;&#35299;&#20026;&#27599;&#23567;&#26102;&#27969;&#37327;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
&lt;/p&gt;</description></item><item><title>ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.10457</link><description>&lt;p&gt;
ALI-DPFL: &#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL: Differentially Private Federated Learning with Adaptive Local Iterations. (arXiv:2308.10457v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10457
&lt;/p&gt;
&lt;p&gt;
ALI-DPFL&#26159;&#19968;&#31181;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#26469;&#20248;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20849;&#20139;&#35757;&#32451;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#65292;&#20801;&#35768;&#22810;&#20010;&#35774;&#22791;&#25110;&#32452;&#32455;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#23545;&#36825;&#20123;&#35757;&#32451;&#21442;&#25968;&#30340;&#25512;&#29702;&#25915;&#20987;&#65288;&#20363;&#22914;&#24046;&#20998;&#25915;&#20987;&#65289;&#26469;&#25512;&#26029;&#20010;&#20307;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20197;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#32771;&#34385;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65292;&#20854;&#20013;&#26082;&#26377;&#38544;&#31169;&#39044;&#31639;&#21463;&#38480;&#65292;&#21448;&#26377;&#36890;&#20449;&#36718;&#27425;&#21463;&#38480;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#25214;&#21040;&#22312;&#20219;&#24847;&#20004;&#20010;&#39034;&#24207;&#20840;&#23616;&#26356;&#26032;&#20043;&#38388;&#30340;&#23458;&#25143;&#26426;&#20043;&#38388;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#26412;&#22320;&#36845;&#20195;&#27425;&#25968;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#26412;&#22320;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65288;ALI-DPFL&#65289;&#12290;&#25105;&#20204;&#22312;FashionMNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning technique that allows model training among multiple devices or organizations by sharing training parameters instead of raw data. However, adversaries can still infer individual information through inference attacks (e.g. differential attacks) on these training parameters. As a result, Differential Privacy (DP) has been widely used in FL to prevent such attacks. We consider differentially private federated learning in a resource-constrained scenario, where both privacy budget and communication round are constrained. By theoretically analyzing the convergence, we can find the optimal number of differentially private local iterations for clients between any two sequential global updates. Based on this, we design an algorithm of differentially private federated learning with adaptive local iterations (ALI-DPFL). We experiment our algorithm on the FashionMNIST and CIFAR10 datasets, and demonstrate significantly better performances th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#30340;&#36827;&#23637;&#36712;&#36857;&#24182;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.09735</link><description>&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#30340;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#36827;&#23637;&#36712;&#36857;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Interpretable Progression Trajectory Analysis of Chronic Disease. (arXiv:2308.09735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#30340;&#36827;&#23637;&#36712;&#36857;&#24182;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#24378;&#35843;&#20102;&#20934;&#30830;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#36712;&#36857;&#21644;&#30693;&#24773;&#20020;&#24202;&#20915;&#31574;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#25429;&#25417;&#24739;&#32773;&#29305;&#24449;&#20013;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#32570;&#20047;&#25552;&#20379;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;&#21644;&#35780;&#20272;&#27835;&#30103;&#25928;&#26524;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#20915;&#31574;&#36741;&#21161;&#30340;&#35282;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#36712;&#36857;&#39044;&#27979;&#65288;CTP&#65289;&#30340;&#26032;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;CTP&#27169;&#22411;&#23558;&#36712;&#36857;&#39044;&#27979;&#21644;&#22240;&#26524;&#21457;&#29616;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#36712;&#36857;&#21644;&#25581;&#31034;&#29305;&#24449;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;&#23558;&#22240;&#26524;&#22270;&#32467;&#21512;&#21040;&#39044;&#27979;&#36807;&#31243;&#20013;&#65292;CTP&#30830;&#20445;&#31062;&#20808;&#29305;&#24449;&#19981;&#21463;&#23545;&#21518;&#20195;&#29305;&#24449;&#30340;&#27835;&#30103;&#24433;&#21709;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15299</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#36127;&#33655;&#39044;&#27979;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24046;&#20998;&#36827;&#21270;&#31639;&#27861;&#36873;&#25321;Transformer&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20248;&#21270;&#36229;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#36127;&#33655;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#22312;&#20247;&#22810;&#39046;&#22495;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20934;&#30830;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#22797;&#26434;&#21160;&#24577;&#20173;&#28982;&#26159;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;ARIMA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;ANN&#65292;LSTM&#65292;GRU&#31561;&#65289;&#32463;&#24120;&#34987;&#20351;&#29992;&#65292;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;Transformer-based&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#25928;&#26524;&#12290;Transformer&#27169;&#22411;&#26377;&#26395;&#25913;&#36827;&#36127;&#33655;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20854;Attention&#26426;&#21046;&#23398;&#20064;&#21040;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#20960;&#31181;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#36827;&#21270;&#65292;&#20197;&#23547;&#25214;Transformer-based&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#12290;&#24046;&#20998;&#36827;&#21270;&#20026;&#38750;&#21487;&#24494;&#20998;&#12289;&#22810;&#30446;&#26631;&#25110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#12289;&#24378;&#20581;&#21644;&#20840;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#27169;&#22411;&#22312;&#36127;&#33655;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MiVOLO&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#38754;&#37096;&#20449;&#24687;&#21644;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04616</link><description>&lt;p&gt;
MiVOLO: &#22810;&#36755;&#20837;&#21464;&#25442;&#22120;&#29992;&#20110;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MiVOLO: Multi-input Transformer for Age and Gender Estimation. (arXiv:2307.04616v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MiVOLO&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#23558;&#38754;&#37096;&#20449;&#24687;&#21644;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#35782;&#21035;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#38500;&#20102;&#26465;&#20214;&#30340;&#21487;&#21464;&#24615;&#12289;&#23039;&#21183;&#30340;&#22797;&#26434;&#24615;&#21644;&#22270;&#20687;&#36136;&#37327;&#30340;&#21464;&#21270;&#20043;&#22806;&#65292;&#36824;&#23384;&#22312;&#20154;&#33080;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MiVOLO&#65288;&#22810;&#36755;&#20837;VOLO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26368;&#26032;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20004;&#20010;&#20219;&#21153;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#36755;&#20837;/&#36755;&#20986;&#27169;&#22411;&#20013;&#65292;&#19981;&#20165;&#21033;&#29992;&#20102;&#38754;&#37096;&#20449;&#24687;&#65292;&#36824;&#21033;&#29992;&#20102;&#20154;&#29289;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#25552;&#39640;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#21363;&#20351;&#22312;&#22270;&#20687;&#20013;&#20154;&#33080;&#19981;&#21487;&#35265;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;Open Images&#25968;&#25454;&#38598;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#30495;&#23454;&#27880;&#37322;&#30001;&#35748;&#30495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age and gender recognition in the wild is a highly challenging task: apart from the variability of conditions, pose complexities, and varying image quality, there are cases where the face is partially or completely occluded. We present MiVOLO (Multi Input VOLO), a straightforward approach for age and gender estimation using the latest vision transformer. Our method integrates both tasks into a unified dual input/output model, leveraging not only facial information but also person image data. This improves the generalization ability of our model and enables it to deliver satisfactory results even when the face is not visible in the image. To evaluate our proposed model, we conduct experiments on four popular benchmarks and achieve state-of-the-art performance, while demonstrating real-time processing capabilities. Additionally, we introduce a novel benchmark based on images from the Open Images Dataset. The ground truth annotations for this benchmark have been meticulously generated by 
&lt;/p&gt;</description></item><item><title>FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2307.03756</link><description>&lt;p&gt;
FITS&#65306;&#27169;&#25311;&#20855;&#26377;10k&#20010;&#21442;&#25968;&#30340;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
FITS: Modeling Time Series with $10k$ Parameters. (arXiv:2307.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03756
&lt;/p&gt;
&lt;p&gt;
FITS&#26159;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#25805;&#20316;&#65292;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FITS&#65292;&#19968;&#31181;&#36731;&#37327;&#32780;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#12290;&#19982;&#30452;&#25509;&#22788;&#29702;&#21407;&#22987;&#26102;&#38388;&#22495;&#25968;&#25454;&#30340;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FITS&#22522;&#20110;&#22312;&#22797;&#26434;&#39057;&#29575;&#22495;&#20013;&#36827;&#34892;&#25554;&#20540;&#30340;&#21407;&#29702;&#25805;&#20316;&#26102;&#38388;&#24207;&#21015;&#12290;&#36890;&#36807;&#20002;&#24323;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24433;&#21709;&#24494;&#23567;&#30340;&#39640;&#39057;&#20998;&#37327;&#65292;FITS&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#36817;&#20284;10k&#20010;&#21442;&#25968;&#30340;&#26174;&#33879;&#32039;&#20945;&#22823;&#23567;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#20026;&#21508;&#31181;&#24212;&#29992;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications. The anonymous code repo is available in: \url{https://anonymous.4open.science/r/FITS}
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01849</link><description>&lt;p&gt;
&#20132;&#21449;&#25193;&#25955;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25913;&#36827;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning. (arXiv:2307.01849v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#22686;&#24378;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#37319;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#20013;&#65292;&#24182;&#20174;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#29305;&#24322;&#33021;&#21147;&#20013;&#33719;&#30410;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crossway Diffusion&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30446;&#26631;&#26469;&#22686;&#24378;&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#35273;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#12290;&#26631;&#20934;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#29983;&#25104;&#21160;&#20316;&#24207;&#21015;&#65292;&#26465;&#20214;&#26159;&#35270;&#35273;&#35266;&#27979;&#21644;&#20854;&#20182;&#20302;&#32500;&#29366;&#24577;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#30721;&#22120;&#65292;&#20174;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#37325;&#26500;&#21407;&#22987;&#22270;&#20687;&#20687;&#32032;&#65288;&#21644;&#20854;&#20182;&#29366;&#24577;&#20449;&#24687;&#65289;&#65292;&#24182;&#20351;&#29992;SSL&#25439;&#22833;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Crossway Diffusion&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#39564;&#35777;&#20102;&#20854;&#30456;&#23545;&#20110;&#26631;&#20934;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence modeling approaches have shown promising results in robot imitation learning. Recently, diffusion models have been adopted for behavioral cloning, benefiting from their exceptional capabilities in modeling complex data distribution. In this work, we propose Crossway Diffusion, a method to enhance diffusion-based visuomotor policy learning by using an extra self-supervised learning (SSL) objective. The standard diffusion-based policy generates action sequences from random noise conditioned on visual observations and other low-dimensional states. We further extend this by introducing a new decoder that reconstructs raw image pixels (and other state information) from the intermediate representations of the reverse diffusion process, and train the model jointly using the SSL loss. Our experiments demonstrate the effectiveness of Crossway Diffusion in various simulated and real-world robot tasks, confirming its advantages over the standard diffusion-based policy. We demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#31890;&#23376;&#30340;&#21160;&#21147;&#23398;&#27169;&#25311;&#30740;&#31350;&#20102;&#24494;&#35266;&#29615;&#22659;&#20013;&#28201;&#24230;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#31574;&#30053;&#24418;&#25104;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36739;&#39640;&#28201;&#24230;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#20986;&#26032;&#30340;&#31574;&#30053;&#65292;&#20026;&#35299;&#20915;&#24494;&#35266;&#23610;&#24230;&#25511;&#21046;&#38382;&#39064;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.00994</link><description>&lt;p&gt;
&#24494;&#35266;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29615;&#22659;&#23545;&#26032;&#20852;&#31574;&#30053;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Environmental effects on emergent strategy in micro-scale multi-agent reinforcement learning. (arXiv:2307.00994v2 [physics.bio-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#31890;&#23376;&#30340;&#21160;&#21147;&#23398;&#27169;&#25311;&#30740;&#31350;&#20102;&#24494;&#35266;&#29615;&#22659;&#20013;&#28201;&#24230;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#31574;&#30053;&#24418;&#25104;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36739;&#39640;&#28201;&#24230;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#23637;&#20986;&#26032;&#30340;&#31574;&#30053;&#65292;&#20026;&#35299;&#20915;&#24494;&#35266;&#23610;&#24230;&#25511;&#21046;&#38382;&#39064;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#23454;&#29616;&#24494;&#35266;&#31890;&#23376;&#65288;&#22914;&#24494;&#22411;&#26426;&#22120;&#20154;&#65289;&#39640;&#25928;&#25511;&#21046;&#30340;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24494;&#35266;&#31890;&#23376;&#30340;&#29615;&#22659;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22312;&#36275;&#22815;&#23567;&#30340;&#23610;&#24230;&#19978;&#30340;&#24067;&#26391;&#36816;&#21160;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#31890;&#23376;&#30340;Langevin&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20316;&#20026;&#24494;&#35266;&#29615;&#22659;&#30340;&#36924;&#30495;&#34920;&#31034;&#65292;&#25506;&#35752;&#20102;&#28201;&#24230;&#22312;MARL&#31995;&#32479;&#20013;&#31574;&#30053;&#24418;&#25104;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#28201;&#24230;&#19979;&#23545;&#24494;&#35266;&#29615;&#22659;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#36825;&#21253;&#25324;&#26816;&#27979;&#27987;&#24230;&#26799;&#24230;&#30340;&#26469;&#28304;&#21644;&#26438;&#30340;&#26059;&#36716;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36739;&#39640;&#28201;&#24230;&#19979;&#65292;RL&#26234;&#33021;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#23454;&#29616;&#36825;&#20123;&#20219;&#21153;&#30340;&#26032;&#31574;&#30053;&#65292;&#31361;&#26174;&#20102;&#29702;&#35299;&#35813;&#28201;&#24230;&#33539;&#22260;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24357;&#21512;&#27169;&#25311;&#19982;&#23454;&#38469;&#29615;&#22659;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#25552;&#20379;&#20102;&#35757;&#32451;&#31574;&#30053;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) is a promising candidate for realizing efficient control of microscopic particles, of which micro-robots are a subset. However, the microscopic particles' environment presents unique challenges, such as Brownian motion at sufficiently small length-scales. In this work, we explore the role of temperature in the emergence and efficacy of strategies in MARL systems using particle-based Langevin molecular dynamics simulations as a realistic representation of micro-scale environments. To this end, we perform experiments on two different multi-agent tasks in microscopic environments at different temperatures, detecting the source of a concentration gradient and rotation of a rod. We find that at higher temperatures, the RL agents identify new strategies for achieving these tasks, highlighting the importance of understanding this regime and providing insight into optimal training strategies for bridging the generalization gap between simulation and re
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18378</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#36827;&#34892;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement via Latent Quantization. (arXiv:2305.18378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28508;&#22312;&#37327;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#25104;&#21151;&#23558;&#25968;&#25454;&#36827;&#34892;&#20102;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#26368;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#29420;&#31435;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#32780;&#27169;&#22411;&#24182;&#27809;&#26377;&#25552;&#20379;&#26377;&#20851;&#36825;&#20123;&#22240;&#32032;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#24402;&#32435;&#20559;&#35265;&#22312;&#23454;&#29616;&#35299;&#32544;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#20132;&#27969;&#29942;&#39048;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#35268;&#33539;&#21270;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#26397;&#30528;&#32452;&#21512;&#32534;&#30721;&#21644;&#35299;&#30721;&#25968;&#25454;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#28508;&#22312;&#32500;&#24230;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#31163;&#25955;&#32534;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#24212;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#26631;&#37327;&#30721;&#20070;&#12290;&#28508;&#22312;&#37327;&#21270;&#36843;&#20351;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#25968;&#25454;&#28857;&#19978;&#20351;&#29992;&#23569;&#37327;&#28508;&#22312;&#20540;&#65292;&#20174;&#32780;&#20351;&#35299;&#30721;&#22120;&#33021;&#22815;&#20026;&#27599;&#20010;&#20540;&#20998;&#37197;&#19968;&#33268;&#30340;&#21547;&#20041;&#12290;&#35268;&#33539;&#21270;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#24341;&#21521;&#36825;&#31181;&#31616;&#26126;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#24212;&#29992;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26631;&#20934;VAE&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#24449;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In disentangled representation learning, a model is asked to tease apart a dataset's underlying sources of variation and represent them independently of one another. Since the model is provided with no ground truth information about these sources, inductive biases take a paramount role in enabling disentanglement. In this work, we construct an inductive bias towards compositionally encoding and decoding data by enforcing a harsh communication bottleneck. Concretely, we do this by (i) quantizing the latent space into learnable discrete codes with a separate scalar codebook per dimension and (ii) applying strong model regularization via an unusually high weight decay. Intuitively, the quantization forces the encoder to use a small number of latent values across many datapoints, which in turn enables the decoder to assign a consistent meaning to each value. Regularization then serves to drive the model towards this parsimonious strategy. We demonstrate the broad applicability of this appr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17558</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Fast Finite Particle Variants of SVGD via Virtual Particle Stochastic Approximation. (arXiv:2305.17558v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34394;&#25311;&#31890;&#23376;&#38543;&#26426;&#36924;&#36817;&#30340;&#21487;&#35777;&#36895;&#38480;&#21046;&#21464;&#31181;&#30340;SVGD&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#65292;&#23427;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#20197;&#36817;&#20284;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#20855;&#26377;&#21508;&#31181;&#39046;&#22495;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#30340;&#32676;&#20307;&#65288;&#21363;&#65292;&#26080;&#38480;&#31890;&#23376;&#65289;&#26497;&#38480;&#21160;&#21147;&#23398;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;SVGD&#22312;&#26377;&#38480;&#31890;&#23376;&#20307;&#21046;&#19979;&#30340;&#34892;&#20026;&#21017;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;SVGD&#21464;&#20307;&#65292;&#21363;VP-SVGD&#65288;&#20174;&#27010;&#24565;&#19978;&#35762;&#24456;&#20248;&#38597;&#65289;&#21644;GB-SVGD&#65288;&#20174;&#32463;&#39564;&#19978;&#30475;&#24456;&#26377;&#25928;&#65289;&#65292;&#20855;&#26377;&#21487;&#35777;&#36895;&#30340;&#26377;&#38480;&#31890;&#23376;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#34394;&#25311;&#31890;&#23376;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#24320;&#21457;&#20102;&#20154;&#21475;&#26497;&#38480;SVGD&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#31890;&#23376;&#31934;&#30830;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;SVGD&#30340;&#29305;&#23450;&#38543;&#26426;&#25209;&#22788;&#29702;&#36924;&#36817;&#65292;&#27604;&#26222;&#36890;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a popular variational inference algorithm which simulates an interacting particle system to approximately sample from a target distribution, with impressive empirical performance across various domains. Theoretically, its population (i.e, infinite-particle) limit dynamics is well studied but the behavior of SVGD in the finite-particle regime is much less understood. In this work, we design two computationally efficient variants of SVGD, namely VP-SVGD (which is conceptually elegant) and GB-SVGD (which is empirically effective), with provably fast finite-particle convergence rates. We introduce the notion of \emph{virtual particles} and develop novel stochastic approximations of population-limit SVGD dynamics in the space of probability measures, which are exactly implementable using a finite number of particles. Our algorithms can be viewed as specific random-batch approximations of SVGD, which are computationally more efficient than ordinar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.16272</link><description>&lt;p&gt;
&#22312;&#21327;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#28608;&#21169;&#31454;&#20105;&#23545;&#25163;&#35802;&#23454;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#27604;&#20165;&#21033;&#29992;&#21333;&#19968;&#25968;&#25454;&#28304;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#30340;&#21442;&#19982;&#32773;&#26159;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#22914;&#27599;&#20010;&#37117;&#24076;&#26395;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#25512;&#33616;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20844;&#21496;&#12290;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#19981;&#35802;&#23454;&#30340;&#26356;&#26032;&#65292;&#25439;&#23475;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#33021;&#30772;&#22351;&#21327;&#20316;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#31181;&#20132;&#20114;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20869;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#65306;&#21333;&#36718;&#22343;&#20540;&#20272;&#35745;&#21644;&#24378;&#20984;&#30446;&#26631;&#30340;&#22810;&#36718; SGD&#12290;&#23545;&#20110;&#19968;&#31867;&#33258;&#28982;&#30340;&#21442;&#19982;&#32773;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#29702;&#24615;&#30340;&#23458;&#25143;&#20250;&#34987;&#28608;&#21169;&#24378;&#28872;&#22320;&#25805;&#32437;&#20182;&#20204;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16052</link><description>&lt;p&gt;
&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#30340;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Strategic Data Sharing between Competitors. (arXiv:2305.16052v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#31454;&#20105;&#23545;&#25163;&#20043;&#38388;&#25112;&#30053;&#25968;&#25454;&#20849;&#20139;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#32452;&#32455;&#20043;&#38388;&#30340;&#31169;&#23494;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#22312;&#32771;&#34385;&#19982;&#31454;&#20105;&#23545;&#25163;&#20849;&#20139;&#25968;&#25454;&#26102;&#38754;&#20020;&#30528;&#22256;&#22659;&#8212;&#8212;&#34429;&#28982;&#21327;&#20316;&#21487;&#20197;&#25913;&#21892;&#20844;&#21496;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#20063;&#21487;&#33021;&#20351;&#31454;&#20105;&#23545;&#25163;&#21463;&#30410;&#65292;&#20174;&#32780;&#38477;&#20302;&#21033;&#28070;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#36825;&#31181;&#25968;&#25454;&#20849;&#20139;&#26435;&#34913;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20998;&#21035;&#20195;&#34920;&#20225;&#19994;&#30340;&#29983;&#20135;&#20915;&#31574;&#12289;&#39069;&#22806;&#25968;&#25454;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#20197;&#21450;&#25968;&#25454;&#20849;&#20139;&#21327;&#21830;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#32463;&#27982;&#29702;&#35770;&#20013;&#30340;&#20256;&#32479;&#24066;&#22330;&#27169;&#22411;&#30340;&#26694;&#26550;&#23454;&#20363;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#21327;&#20316;&#28608;&#21169;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#24066;&#22330;&#26465;&#20214;&#23545;&#25968;&#25454;&#20849;&#20139;&#28608;&#21169;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24066;&#22330;&#31454;&#20105;&#30340;&#20943;&#23569;&#65292;&#21363;&#20225;&#19994;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have significantly advanced in recent years, enabling private model training across multiple organizations. Despite this opportunity, firms face a dilemma when considering data sharing with competitors -- while collaboration can improve a company's machine learning model, it may also benefit competitors and hence reduce profits. In this work, we introduce a general framework for analyzing this data-sharing trade-off. The framework consists of three components, representing the firms' production decisions, the effect of additional data on model quality, and the data-sharing negotiation process, respectively. We then study an instantiation of the framework, based on a conventional market model from economic theory, to identify key factors that affect collaboration incentives. Our findings indicate a profound impact of market conditions on the data-sharing incentives. In particular, we find that reduced competition, in terms of the similarities between th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#32463;&#39564;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#24341;&#20837;&#24378;&#24402;&#32435;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#23398;&#20064;&#20102;&#24191;&#27867;&#30340;&#35821;&#20041;&#31867;&#21035;&#21644;&#23545;&#35937;&#23450;&#20301;&#33021;&#21147;&#12290;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#20026;ImageNet&#27169;&#22411;&#30340;70%&#27700;&#24179;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15372</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#23398;&#20064;&#39640;&#32423;&#35270;&#35273;&#34920;&#31034;&#32780;&#19981;&#24341;&#20837;&#24378;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Learning high-level visual representations from a child's perspective without strong inductive biases. (arXiv:2305.15372v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15372
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#32463;&#39564;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#24341;&#20837;&#24378;&#24402;&#32435;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#23398;&#20064;&#20102;&#24191;&#27867;&#30340;&#35821;&#20041;&#31867;&#21035;&#21644;&#23545;&#35937;&#23450;&#20301;&#33021;&#21147;&#12290;&#23884;&#20837;&#27169;&#22411;&#34920;&#29616;&#20026;ImageNet&#27169;&#22411;&#30340;70%&#27700;&#24179;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#24188;&#30340;&#20799;&#31461;&#36890;&#36807;&#20182;&#20204;&#30340;&#35270;&#35273;&#32463;&#39564;&#21457;&#23637;&#20986;&#22797;&#26434;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#21487;&#20197;&#20174;&#20799;&#31461;&#30340;&#35270;&#35273;&#32463;&#39564;&#20013;&#23398;&#20064;&#36825;&#26679;&#30340;&#27169;&#22411;&#32780;&#19981;&#24341;&#20837;&#24378;&#24402;&#32435;&#20559;&#24046;&#21527;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#30340;&#30495;&#23454;&#35270;&#35273;&#32463;&#39564;&#30340;&#20195;&#29702;&#19978;&#35757;&#32451;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#30340;&#30417;&#30563;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#20799;&#31461;&#30340;200&#23567;&#26102;&#22836;&#25140;&#25668;&#20687;&#26426;&#35270;&#39057;&#35757;&#32451;&#20102;&#23884;&#20837;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#21442;&#32771;&#27169;&#22411;&#20316;&#20026;&#34913;&#37327;&#26631;&#20934;&#20840;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#26368;&#20339;&#30340;&#23884;&#20837;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36798;&#21040;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;ImageNet&#35757;&#32451;&#27169;&#22411;&#30340;70%&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#24322;&#12290;&#23427;&#20204;&#36824;&#23398;&#20064;&#20102;&#24191;&#27867;&#30340;&#35821;&#20041;&#31867;&#21035;&#21644;&#23545;&#35937;&#23450;&#20301;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#30417;&#30563;&#65292;&#20294;&#23427;&#20204;&#27604;&#22312;&#20840;&#37096;ImageNet&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23569;&#20851;&#27880;&#23545;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#25928;&#26524;&#36739;&#24046;&#8230;
&lt;/p&gt;
&lt;p&gt;
Young children develop sophisticated internal models of the world based on their visual experience. Can such models be learned from a child's visual experience without strong inductive biases? To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child's visual experience without any explicit supervision or domain-specific inductive biases. Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks. On average, the best embedding models perform at a respectable 70% of a high-performance ImageNet-trained model, despite substantial differences in training data. They also learn broad semantic categories and object localization capabilities without explicit supervision, but they are less object-centric than models trained on all of ImageNet. Generative models trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25552;&#39640;&#29289;&#20307;&#20013;&#24515;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11281</link><description>&lt;p&gt;
SlotDiffusion: &#22522;&#20110;Diffusion&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models. (arXiv:2305.11281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#33021;&#22815;&#25552;&#39640;&#29289;&#20307;&#20013;&#24515;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#29992;&#23545;&#35937;&#23454;&#20307;&#65288;&#20063;&#31216;&#20026;&#27133;&#65289;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#25552;&#20379;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#31995;&#32479;&#21270;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SlotDiffusion&#30340;&#23545;&#35937;&#20013;&#24515;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411; (LDM)&#65292;&#26088;&#22312;&#25552;&#39640;&#27133;&#21040;&#22270;&#20687;&#35299;&#30721;&#30340;&#36136;&#37327;&#65292;&#26159;&#19968;&#31181;&#26082;&#21487;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#21448;&#21487;&#29992;&#20110;&#35270;&#39057;&#25968;&#25454;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#30001;&#20110;LDM&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#65292;SlotDiffusion&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#20998;&#21106;&#21644;&#35270;&#35273;&#29983;&#25104;&#26041;&#38754;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#27133;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric learning aims to represent visual data with a set of object entities (a.k.a. slots), providing structured representations that enable systematic generalization. Leveraging advanced architectures like Transformers, recent approaches have made significant progress in unsupervised object discovery. In addition, slot-based representations hold great potential for generative modeling, such as controllable image generation and object manipulation in image editing. However, current slot-based methods often produce blurry images and distorted objects, exhibiting poor generative modeling capabilities. In this paper, we focus on improving slot-to-image decoding, a crucial aspect for high-quality visual generation. We introduce SlotDiffusion -- an object-centric Latent Diffusion Model (LDM) designed for both image and video data. Thanks to the powerful modeling capacity of LDMs, SlotDiffusion surpasses previous slot models in unsupervised object segmentation and visual generation a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#25552;&#39640;&#21046;&#36896;&#19994;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#26410;&#26631;&#35760;&#30340;&#21644;&#26377;&#38480;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#39046;&#22495;&#31227;&#20301;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14398</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21046;&#36896;&#19994;&#27169;&#22411;&#27867;&#21270;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Maximizing Model Generalization for Manufacturing with Self-Supervised Learning and Federated Learning. (arXiv:2304.14398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#25552;&#39640;&#21046;&#36896;&#19994;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#26410;&#26631;&#35760;&#30340;&#21644;&#26377;&#38480;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#39046;&#22495;&#31227;&#20301;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20445;&#25252;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#22312;&#27809;&#26377;&#25163;&#21160;&#35774;&#35745;&#30340;&#32479;&#35745;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21407;&#22987;&#30340;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20013;&#35786;&#26029;&#25925;&#38556;&#21644;&#35780;&#20272;&#26426;&#22120;&#20581;&#24247;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26497;&#20854;&#22256;&#38590;&#36866;&#29992;&#20110;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#12290;&#26426;&#22120;&#25968;&#25454;&#36890;&#24120;&#26159;&#26410;&#26631;&#35760;&#30340;&#65292;&#21482;&#26377;&#24456;&#23569;&#30340;&#20581;&#24247;&#26465;&#20214;&#65288;&#20363;&#22914;&#65292;&#20165;&#26377;&#27491;&#24120;&#25805;&#20316;&#25968;&#25454;&#65289;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24037;&#33402;&#21442;&#25968;&#30340;&#21464;&#21270;&#21644;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#22495;&#30340;&#31227;&#20301;&#12290;&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#21487;&#33021;&#38590;&#20197;&#23398;&#20064;&#32039;&#20945;&#12289;&#26377;&#21306;&#21035;&#21147;&#30340;&#34920;&#31034;&#65292;&#24182;&#19988;&#19981;&#33021;&#25512;&#24191;&#21040;&#36825;&#20123;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22495;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#25317;&#26377;&#20016;&#23500;&#30340;&#31867;&#26469;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#30340;&#36801;&#31227;&#23398;&#20064;&#23581;&#35797;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#65292;&#20294;&#20551;&#23450;&#20102;&#31867;&#20284;&#30340;&#23376;&#32467;&#26500;&#65292;&#22312;&#26032;&#30340;&#25925;&#38556;&#20986;&#29616;&#26102;&#21487;&#33021;&#19981;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#28304;&#22495;&#29305;&#24449;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#26469;&#25913;&#21892;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#30340;&#25925;&#38556;&#35786;&#26029;&#21644;&#26426;&#22120;&#20581;&#24247;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#26410;&#26631;&#35760;&#21644;&#26377;&#38480;&#29366;&#20917;&#30417;&#27979;&#25968;&#25454;&#20197;&#21450;&#22495;&#31227;&#20301;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) can diagnose faults and assess machine health from raw condition monitoring data without manually designed statistical features. However, practical manufacturing applications remain extremely difficult for existing DL methods. Machine data is often unlabeled and from very few health conditions (e.g., only normal operating data). Furthermore, models often encounter shifts in domain as process parameters change and new categories of faults emerge. Traditional supervised learning may struggle to learn compact, discriminative representations that generalize to these unseen target domains since it depends on having plentiful classes to partition the feature space with decision boundaries. Transfer Learning (TL) with domain adaptation attempts to adapt these models to unlabeled target domains but assumes similar underlying structure that may not be present if new faults emerge. This study proposes focusing on maximizing the feature generality on the source domain and apply
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.05422</link><description>&lt;p&gt;
&#21487;&#21306;&#20998;&#30340;&#22270;&#32467;&#26500;&#27169;&#22411;&#29992;&#20110;&#26230;&#26684;&#26448;&#26009;&#21453;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Differentiable graph-structured models for inverse design of lattice materials. (arXiv:2304.05422v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#26426;&#26800;&#23646;&#24615;&#20197;&#23454;&#29616;&#21453;&#21521;&#35774;&#35745;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#20110;&#28145;&#31354;&#24694;&#21155;&#29615;&#22659;&#20013;&#33021;&#22815;&#26681;&#25454;&#38656;&#35201;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#26448;&#26009;&#23558;&#22312;&#23450;&#20041;&#26410;&#26469;&#30340;&#31354;&#38388;&#25506;&#32034;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#33258;&#28982;&#30028;&#20013;&#65292;&#24494;&#22937;&#30340;&#24494;&#35266;&#32467;&#26500;&#21644;&#26684;&#23376;&#20960;&#20309;&#24418;&#29366;&#26159;&#35774;&#35745;&#36866;&#24212;&#20110;&#29305;&#23450;&#29615;&#22659;&#26448;&#26009;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#19981;&#35268;&#21017;&#25299;&#25169;&#35206;&#30422;&#30340;&#24040;&#22823;&#35774;&#35745;&#31354;&#38388;&#65292;&#22312;&#20998;&#26512;&#19978;&#36827;&#34892;&#25506;&#32034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21512;&#25104;&#26230;&#26684;&#26448;&#26009;&#37117;&#26159;&#22522;&#20110;&#21608;&#26399;&#24615;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#24418;&#34920;&#31034;&#23545;&#35268;&#21017;&#21644;&#19981;&#35268;&#21017;&#26230;&#26684;&#26448;&#26009;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#24494;&#20998;&#20256;&#36882;&#31639;&#27861;&#35745;&#31639;&#21147;&#23398;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#26469;&#35843;&#25972;&#21333;&#20010;&#26230;&#26684;&#20803;&#32032;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#65292;&#20174;&#32780;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26448;&#26009;&#12290;&#24341;&#20837;&#23545;&#26230;&#26684;&#32467;&#26500;&#21644;&#26448;&#26009;&#23646;&#24615;&#30340;&#38544;&#24335;&#21487;&#23398;&#20064;&#20960;&#20309;&#34920;&#31034;&#65292;&#32467;&#21512;&#21453;&#35774;&#35745;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#22810;&#26679;&#24615;&#30340;&#26230;&#26684;&#26448;&#26009;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials possessing flexible physico-chemical properties that adapt on-demand to the hostile environmental conditions of deep space will become essential in defining the future of space exploration. A promising venue for inspiration towards the design of environment-specific materials is in the intricate micro-architectures and lattice geometry found throughout nature. However, the immense design space covered by such irregular topologies is challenging to probe analytically. For this reason, most synthetic lattice materials have to date been based on periodic architectures instead. Here, we propose a computational approach using a graph representation for both regular and irregular lattice materials. Our method uses differentiable message passing algorithms to calculate mechanical properties, and therefore allows using automatic differentiation to adjust both the geometric structure and attributes of individual lattice elements to design materials with desired properties. The introdu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.15585</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#22238;&#39038;&#65306;&#36229;&#36234;&#20934;&#30830;&#24615;&#22312;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#21450;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#38761;&#21629;&#24615;&#25972;&#21512;&#12290;&#35774;&#22791;&#29616;&#22312;&#21487;&#20197;&#35786;&#26029;&#30142;&#30149;&#12289;&#39044;&#27979;&#24515;&#33039;&#19981;&#35268;&#21017;&#21160;&#65292;&#21457;&#25496;&#20154;&#31867;&#35748;&#30693;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#31639;&#27861;&#22312;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#65289;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#27495;&#35270;&#24615;&#32467;&#26524;&#12290;&#36817;&#26399;&#65292;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;&#65288;AI-Ethics&#65289;&#30740;&#31350;&#31038;&#21306;&#24320;&#22987;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#25581;&#31034;&#24182;&#26368;&#32456;&#23545;&#25239;&#36825;&#20123;&#20559;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#36825;&#20123;&#25253;&#21578;&#26041;&#38754;UbiComp&#31038;&#21306;&#25152;&#37319;&#32435;&#30340;&#31243;&#24230;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#36807;&#21435;&#20116;&#24180;&#65288;2018-2022&#65289;&#22312;ACM&#20132;&#20114;&#12289;&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#36866;&#25216;&#26415;&#65288;IMWUT&#65289;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.13458</link><description>&lt;p&gt;
&#31561;&#21464;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38480;&#21046;&#26550;&#26500;&#31561;&#21464;&#21644;&#20351;&#29992;&#22686;&#24378;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#25439;&#22833;&#21644;&#38750;&#32447;&#24615;&#24615;&#36827;&#34892;&#33258;&#28982;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#23545;&#20110;&#36825;&#20004;&#31181;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#31561;&#21464;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#26159;&#31283;&#23450;&#30340;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.08983</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#22686;&#24378;&#65306;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement. (arXiv:2303.08983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08983
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#25454;&#38598;&#22686;&#24378;&#30340;&#31574;&#30053;&#65292;&#19968;&#27425;&#24615;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#20219;&#20309;&#32463;&#36807;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#23545;&#29992;&#25143;&#27809;&#26377;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#31574;&#30053;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;CNN&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#30340;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#24102;&#26377;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#33976;&#39311;&#30740;&#31350;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;ImageDataNet+&#30340;&#22686;&#24378;&#29256;&#26412;&#65292;&#20197;&#21450;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;CIFAR-100+&#65292;Flowers-102+&#21644;Food-101+&#12290;&#20351;&#29992;ImageDataNet+&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12289;&#26356;&#26377;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#24615;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#21106;&#21644;&#26816;&#27979;&#65289;&#20855;&#26377;&#24456;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;ResNet-50&#22312;ImageNet&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.7&#65285;&#65292;&#22312;ImageNetV2&#19978;&#25552;&#39640;&#20102;3.5&#65285;&#65292;&#22312;ImageNet-R&#19978;&#25552;&#39640;&#20102;10.0&#65285;&#12290;&#22312;ImageDataNet+&#19978;&#27979;&#37327;&#30340;Expected Calibration Error&#65288;ECE&#65289;&#20063;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the Ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.06614</link><description>&lt;p&gt;
&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65306;&#26088;&#22312;&#29992;&#25193;&#20805;&#25968;&#25454;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Synthetic Experience Replay. (arXiv:2303.06614v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#21294;&#20047;&#38382;&#39064;&#65292;&#36890;&#36807;&#24039;&#22937;&#24212;&#29992;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#26469;&#25193;&#20805;&#25968;&#25454;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#30340;&#19968;&#20010;&#20851;&#38190;&#20027;&#39064;&#26159;&#65292;&#24403;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#24778;&#24322;&#30340;&#32467;&#26524;&#12290;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#36890;&#36807;&#32463;&#39564;&#22238;&#25918;&#23454;&#29616;&#65292;&#20854;&#20013;&#36807;&#21435;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#31574;&#30053;&#25110;&#20540;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#25110;&#33258;&#30417;&#30563;&#23398;&#20064;&#19981;&#21516;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#65292;&#36825;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21363;&#20351;&#26159;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#24040;&#22823;&#36827;&#27493;&#65292;&#24182;&#25552;&#20986;&#20102;&#21512;&#25104;&#32463;&#39564;&#22238;&#25918;&#65288;SynthER&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#19978;&#37319;&#26679;&#20195;&#29702;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SynthER&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#26080;&#35770;&#26159;&#22312;&#24863;&#30693;&#29615;&#22659;&#36824;&#26159;&#22312;&#20687;&#32032;&#29615;&#22659;&#20013;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements 
&lt;/p&gt;</description></item><item><title>Neural-BO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#32553;&#25918;&#21644;&#32500;&#25968;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01682</link><description>&lt;p&gt;
Neural-BO: &#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks. (arXiv:2303.01682v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01682
&lt;/p&gt;
&lt;p&gt;
Neural-BO&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#32553;&#25918;&#21644;&#32500;&#25968;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#20989;&#25968;&#36827;&#34892;&#20840;&#23616;&#20248;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24403;&#20989;&#25968;&#35780;&#20272;&#20195;&#20215;&#39640;&#26102;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#27169;&#25311;&#40657;&#30418;&#20989;&#25968;&#65292;&#28982;&#32780;&#65292;&#39640;&#26031;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#23548;&#33268;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#28857;&#25968;&#37327;&#36739;&#22823;&#26102;&#32553;&#25918;&#22256;&#38590;&#65292;&#20108;&#26159;&#26680;&#26041;&#27861;&#22312;&#22797;&#26434;&#32467;&#26500;&#39640;&#32500;&#25968;&#25454;&#19978;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#22240;&#20026;&#32500;&#25968;&#28798;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20013;&#40657;&#30418;&#20989;&#25968;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#35745;&#31639;&#19978;&#26356;&#21152;&#26377;&#21033;&#12290;&#25105;&#20204;&#20351;&#29992;NTK&#29702;&#35770;&#30340;&#36827;&#23637;&#20998;&#26512;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#23637;&#31034;&#20102;&#20854;&#25910;&#25947;&#30340;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is an effective approach for global optimization of black-box functions when function evaluations are expensive. Most prior works use Gaussian processes to model the black-box function, however, the use of kernels in Gaussian processes leads to two problems: first, the kernel-based methods scale poorly with the number of data points and second, kernel methods are usually not effective on complex structured high dimensional data due to curse of dimensionality. Therefore, we propose a novel black-box optimization algorithm where the black-box function is modeled using a neural network. Our algorithm does not need a Bayesian neural network to estimate predictive uncertainty and is therefore computationally favorable. We analyze the theoretical behavior of our algorithm in terms of regret bound using advances in NTK theory showing its efficient convergence. We perform experiments with both synthetic and real-world optimization tasks and show that our algorithm is
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28145;&#24230;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2302.13563</link><description>&lt;p&gt;
&#28145;&#24230;&#19981;&#24179;&#34913;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Imbalanced Time-series Forecasting via Local Discrepancy Density. (arXiv:2302.13563v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#24322;&#23494;&#24230;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28145;&#24230;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#36890;&#36807;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#20250;&#36935;&#21040;&#22312;&#26576;&#20010;&#26102;&#38388;&#27573;&#20869;&#30340;&#31361;&#21457;&#21464;&#21270;&#65292;&#36825;&#20123;&#21464;&#21270;&#36890;&#24120;&#26159;&#30001;&#24847;&#22806;&#20107;&#20214;&#25110;&#26410;&#30693;&#20107;&#20214;&#23548;&#33268;&#30340;&#12290;&#23613;&#31649;&#22312;&#35757;&#32451;&#38598;&#20013;&#21457;&#29983;&#30340;&#27425;&#25968;&#24456;&#23569;&#65292;&#20294;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#20250;&#26174;&#33879;&#24433;&#21709;&#24635;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#20316;&#20026;&#22024;&#26434;&#30340;&#35757;&#32451;&#26679;&#26412;&#38459;&#27490;&#20102;&#27169;&#22411;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#27169;&#24335;&#65292;&#21363;&#27491;&#24120;&#29366;&#24577;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#26032;&#21152;&#26435;&#30340;&#26694;&#26550;&#65292;&#38477;&#20302;&#31361;&#21457;&#21464;&#21270;&#24341;&#36215;&#30340;&#25439;&#22833;&#65292;&#22686;&#21152;&#27491;&#24120;&#29366;&#24577;&#24341;&#36215;&#30340;&#25439;&#22833;&#12290;&#23545;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#23616;&#37096;&#24046;&#24322;&#24230;&#65288;LD&#65289;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#21464;&#21270;&#30340;&#31361;&#28982;&#31243;&#24230;&#12290;&#30001;&#20110;&#35757;&#32451;&#38598;&#20027;&#35201;&#30001;&#27491;&#24120;&#29366;&#24577;&#32452;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#22522;&#20110;LD&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#37325;&#26032;&#21152;&#26435;&#26694;&#26550;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26080;&#35770;&#20854;&#26550;&#26500;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting models often encounter abrupt changes in a given period of time which generally occur due to unexpected or unknown events. Despite their scarce occurrences in the training set, abrupt changes incur loss that significantly contributes to the total loss. Therefore, they act as noisy training samples and prevent the model from learning generalizable patterns, namely the normal states. Based on our findings, we propose a reweighting framework that down-weights the losses incurred by abrupt changes and up-weights those by normal states. For the reweighting framework, we first define a measurement termed Local Discrepancy (LD) which measures the degree of abruptness of a change in a given period of time. Since a training set is mostly composed of normal states, we then consider how frequently the temporal changes appear in the training set based on LD. Our reweighting framework is applicable to existing time-series forecasting models regardless of the architectures. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.10894</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21512;&#25104;&#24037;&#20855;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32418;&#38431;&#28436;&#32451;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26080;&#27861;&#20998;&#26512;&#26410;&#30693;&#29305;&#24449;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36890;&#24120;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#24456;&#23569;&#26377;&#33021;&#22815;&#21457;&#29616;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#38169;&#35823;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#20849;&#21516;&#29305;&#28857;&#65306;&#23427;&#20204;&#20351;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#34429;&#28982;&#36825;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24037;&#20855;&#21482;&#33021;&#20998;&#26512;&#29992;&#25143;&#21487;&#20197;&#20107;&#20808;&#37319;&#26679;&#25110;&#35782;&#21035;&#30340;&#29305;&#24449;&#25152;&#24341;&#21457;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#28041;&#21450;&#20351;&#29992;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29305;&#23450;&#35302;&#21457;&#22120;&#65288;&#20363;&#22914;&#65292;&#25554;&#20837;&#22270;&#20687;&#30340;&#29305;&#23450;&#34917;&#19969;&#65289;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#65288;&#21363;&#26631;&#31614;&#65289;&#65292;&#28982;&#21518;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#24182;&#28155;&#21152;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#30340;&#20559;&#24046;&#26657;&#27491;&#12290;</title><link>http://arxiv.org/abs/2302.00662</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;Fitted-Q&#35780;&#20272;&#21644;&#36845;&#20195;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;
&lt;/p&gt;
&lt;p&gt;
Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders. (arXiv:2302.00662v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#24182;&#28155;&#21152;&#20102;&#20998;&#20301;&#25968;&#20272;&#35745;&#30340;&#20559;&#24046;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#12289;&#32463;&#27982;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#32447;&#23454;&#39564;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12289;&#21361;&#38505;&#25110;&#19981;&#36947;&#24503;&#65292;&#24182;&#19988;&#30495;&#23454;&#27169;&#22411;&#26410;&#30693;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#34892;&#20026;&#31574;&#30053;&#30340;&#25152;&#26377;&#21327;&#21464;&#37327;&#37117;&#26159;&#24050;&#35266;&#23519;&#21040;&#30340;&#12290;&#23613;&#31649;&#36825;&#20010;&#20551;&#35774;"&#39034;&#24207;&#21487;&#24573;&#30053;&#24615;"&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#19981;&#22826;&#21487;&#33021;&#25104;&#31435;&#65292;&#20294;&#22823;&#37096;&#20998;&#32771;&#34385;&#36827;&#20837;&#27835;&#30103;&#22240;&#32032;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#35266;&#23519;&#21040;&#30340;&#65292;&#36825;&#28608;&#21169;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25935;&#24863;&#24615;&#27169;&#22411;&#19979;&#39034;&#24207;&#22806;&#28304;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#19979;&#30340;&#24378;&#20581;&#31574;&#30053;&#35780;&#20272;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#27491;&#20132;&#21270;&#30340;&#24378;&#20581;Fitted-Q&#36845;&#20195;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24378;&#20581;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#26469;&#23548;&#20986;&#24378;&#20581;Q&#20989;&#25968;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23545;&#20998;&#20301;&#25968;&#20272;&#35745;&#21152;&#20837;&#20559;&#24046;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20860;&#20855;Fitted-Q&#36845;&#20195;&#30340;&#35745;&#31639;&#31616;&#20415;&#24615;&#21644;&#32479;&#35745;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning is important in domains such as medicine, economics, and e-commerce where online experimentation is costly, dangerous or unethical, and where the true model is unknown. However, most methods assume all covariates used in the behavior policy's action decisions are observed. Though this assumption, sequential ignorability/unconfoundedness, likely does not hold in observational data, most of the data that accounts for selection into treatment may be observed, motivating sensitivity analysis. We study robust policy evaluation and policy optimization in the presence of sequentially-exogenous unobserved confounders under a sensitivity model. We propose and analyze orthogonalized robust fitted-Q-iteration that uses closed-form solutions of the robust Bellman operator to derive a loss minimization problem for the robust Q function, and adds a bias-correction to quantile estimation. Our algorithm enjoys the computational ease of fitted-Q-iteration and statistical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29983;&#29289;&#36807;&#31243;&#30340;&#31934;&#30830;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00600</link><description>&lt;p&gt;
&#19968;&#25307;&#20004;&#24471;&#65306;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics. (arXiv:2302.00600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29983;&#29289;&#36807;&#31243;&#30340;&#31934;&#30830;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31895;&#31890;&#21270;&#65288;CG&#65289;&#20998;&#23376;&#21160;&#21147;&#23398;&#20351;&#24471;&#22312;&#21407;&#23376;&#20998;&#36776;&#29575;&#19979;&#26080;&#27861;&#35299;&#20915;&#30340;&#29983;&#29289;&#36807;&#31243;&#21487;&#20197;&#24471;&#20197;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#23398;&#20064;CG&#21147;&#22330;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#12289;&#21147;&#22330;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;CG&#21147;&#22330;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#21147;&#22330;&#36755;&#20837;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#20102;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#35780;&#20998;&#20989;&#25968;&#36817;&#20284;&#19968;&#20010;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#27169;&#25311;CG&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#21147;&#22330;&#12290;&#23613;&#31649;&#30456;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22823;&#22823;&#31616;&#21270;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#23567;&#22411;&#21040;&#20013;&#22411;&#34507;&#30333;&#36136;&#27169;&#25311;&#20013;&#20855;&#26377;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#33021;&#22815;&#37325;&#29616;CG&#24179;&#34913;&#20998;&#24067;&#65292;&#24182;&#20445;&#25345;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#20840;&#21407;&#23376;&#27169;&#25311;&#30340;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coarse-grained (CG) molecular dynamics enables the study of biological processes at temporal and spatial scales that would be intractable at an atomistic resolution. However, accurately learning a CG force field remains a challenge. In this work, we leverage connections between score-based generative models, force fields and molecular dynamics to learn a CG force field without requiring any force inputs during training. Specifically, we train a diffusion generative model on protein structures from molecular dynamics simulations, and we show that its score function approximates a force field that can directly be used to simulate CG molecular dynamics. While having a vastly simplified training setup compared to previous work, we demonstrate that our approach leads to improved performance across several small- to medium-sized protein simulations, reproducing the CG equilibrium distribution, and preserving dynamics of all-atom simulations such as protein folding events.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11476</link><description>&lt;p&gt;
&#20351;&#29992;Tsallis KL&#25955;&#24230;&#30340;&#24191;&#20041;Munchausen&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence. (arXiv:2301.11476v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11476
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#30340;Tsallis KL&#25955;&#24230;&#65292;&#25193;&#23637;&#20102;Munchausen&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;Tsallis KL&#65292;&#24403;$q &gt; 1$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26032;&#30340;&#31574;&#30053;&#20248;&#21270;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#37117;&#37319;&#29992;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#21040;&#19978;&#19968;&#20010;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#31574;&#30053;&#21464;&#21270;&#36807;&#24555;&#12290;&#36825;&#20010;&#24819;&#27861;&#26368;&#21021;&#26159;&#22312;Conservative Policy Iteration&#30340;&#19968;&#31687;&#37325;&#35201;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#65292;&#36817;&#20284;&#31639;&#27861;&#22914;TRPO&#21644;Munchausen Value Iteration&#65288;MVI&#65289;&#32473;&#20986;&#20102;&#26377;&#38480;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19968;&#31181;&#24191;&#20041;&#30340;KL&#25955;&#24230; - &#31216;&#20026;Tsallis KL&#25955;&#24230; - &#26469;&#32487;&#32493;&#36825;&#19968;&#24037;&#20316;&#65292;&#23427;&#22312;&#23450;&#20041;&#20013;&#20351;&#29992;&#20102;$q$-&#23545;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#25512;&#24191;&#65292;&#22240;&#20026;$q = 1$&#23545;&#24212;&#20110;&#26631;&#20934;&#30340;KL&#25955;&#24230;&#65307;$q &gt; 1$&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#23545;&#22312;Tsallis KL&#19979;&#23398;&#20064;&#30340;&#31574;&#30053;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#36848;&#20102;&#20309;&#26102;$ q &gt; 1 $&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#23558;Tsallis KL&#27491;&#21017;&#21270;&#32435;&#20837;&#23454;&#38469;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;MVI&#65292;&#23427;&#26159;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#21253;&#21547;KL&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24191;&#20041;MVI&#65288;$q$&#65289;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many policy optimization approaches in reinforcement learning incorporate a Kullback-Leilbler (KL) divergence to the previous policy, to prevent the policy from changing too quickly. This idea was initially proposed in a seminal paper on Conservative Policy Iteration, with approximations given by algorithms like TRPO and Munchausen Value Iteration (MVI). We continue this line of work by investigating a generalized KL divergence -- called the Tsallis KL divergence -- which use the $q$-logarithm in the definition. The approach is a strict generalization, as $q = 1$ corresponds to the standard KL divergence; $q &gt; 1$ provides a range of new options. We characterize the types of policies learned under the Tsallis KL, and motivate when $q &gt;1$ could be beneficial. To obtain a practical algorithm that incorporates Tsallis KL regularization, we extend MVI, which is one of the simplest approaches to incorporate KL regularization. We show that this generalized MVI($q$) obtains significant improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;</title><link>http://arxiv.org/abs/2212.12393</link><description>&lt;p&gt;
A-NeSI: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65288;PNL&#65289;&#26694;&#26550;&#65292;&#22914;DeepProbLog&#65292;&#25191;&#34892;&#25351;&#25968;&#26102;&#38388;&#30340;&#31934;&#30830;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;PNL&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36817;&#20284;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#65288;A-NeSI&#65289;&#65306;&#19968;&#31181;&#26032;&#30340;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#25512;&#29702;&#12290;A-NeSI 1) &#22312;&#19981;&#25913;&#21464;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#25191;&#34892;&#36817;&#20284;&#25512;&#29702;&#65307;2) &#20351;&#29992;&#30001;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;3) &#21487;&#20197;&#29983;&#25104;&#26377;&#20851;&#39044;&#27979;&#30340;&#31526;&#21495;&#35299;&#37322;&#65307;4) &#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#25351;&#25968;&#32452;&#21512;&#25193;&#23637;&#30340;&#19977;&#31181;&#31070;&#32463;&#31526;&#21495;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#32780;&#27809;&#26377;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.11679</link><description>&lt;p&gt;
&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23454;&#20363;&#30340;&#20998;&#21106;&#26159;&#26426;&#22120;&#20154;&#38656;&#35201;&#25484;&#25569;&#30340;&#20851;&#38190;&#24863;&#30693;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#26377;&#21161;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#12290;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#19981;&#21487;&#24494;&#20998;&#65292;&#20351;&#20854;&#38590;&#20197;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65288;MSMFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#27169;&#25311; von Mises-Fisher&#65288;vMF&#65289;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#65292;&#20801;&#35768;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#36229;&#29699;&#38754;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#22312;&#36229;&#29699;&#38754;&#19978;&#26356;&#26032;&#29289;&#20307;&#26597;&#35810;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;MSMFormer&#24212;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MSMFormer&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
&lt;/p&gt;</description></item><item><title>"&#20219;&#21153;&#30456;&#20851;&#30340;&#33258;&#32534;&#30721;"&#65288;TRACE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20154;&#31867;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#21463;&#35797;&#32773;&#34892;&#20026;&#30456;&#20851;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#21457;&#29616;&#33021;&#21147;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#24120;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.08478</link><description>&lt;p&gt;
"&#20219;&#21153;&#30456;&#20851;&#30340;&#33258;&#32534;&#30721;"&#22686;&#24378;&#20102;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
"Task-relevant autoencoding" enhances machine learning for human neuroscience. (arXiv:2208.08478v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08478
&lt;/p&gt;
&lt;p&gt;
"&#20219;&#21153;&#30456;&#20851;&#30340;&#33258;&#32534;&#30721;"&#65288;TRACE&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20154;&#31867;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#21463;&#35797;&#32773;&#34892;&#20026;&#30456;&#20851;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#21457;&#29616;&#33021;&#21147;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#24120;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#25581;&#31034;&#19982;&#21463;&#35797;&#32773;&#34892;&#20026;&#30456;&#20851;&#30340;&#20302;&#32500;&#31070;&#32463;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#23481;&#26131;&#22312;&#20154;&#31867;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#36739;&#23569;&#30340;&#26679;&#26412;&#20294;&#24456;&#22810;&#36755;&#20837;&#32500;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#25105;&#20204;&#22312;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#20013;&#23547;&#27714;&#30340;&#29305;&#24449;&#24688;&#22909;&#19982;&#21463;&#35797;&#32773;&#34892;&#20026;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#31867;&#22120;&#22686;&#24378;&#30340;&#20219;&#21153;&#30456;&#20851;&#33258;&#32534;&#30721;&#22120;&#65288;TRACE&#65289;&#65292;&#24182;&#23558;&#20854;&#19982;&#26631;&#20934;&#33258;&#32534;&#30721;&#22120;&#12289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23545;&#20004;&#20010;&#20005;&#37325;&#32553;&#20943;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#25552;&#21462;&#20102;&#34892;&#20026;&#30456;&#20851;&#30340;&#12289;&#21487;&#20998;&#31163;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;59&#21517;&#35266;&#23519;&#21160;&#29289;&#21644;&#29289;&#20307;&#30340;&#21463;&#35797;&#32773;&#30340;fMRI&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#25152;&#26377;&#27169;&#22411;&#12290;TRACE&#20960;&#20046;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#31034;&#20986;&#39640;&#36798;12%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#21319;&#21644;&#39640;&#36798;56%&#30340;&#21457;&#29616;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human neuroscience, machine learning can help reveal lower-dimensional neural representations relevant to subjects' behavior. However, state-of-the-art models typically require large datasets to train, so are prone to overfitting on human neuroimaging data that often possess few samples but many input dimensions. Here, we capitalized on the fact that the features we seek in human neuroscience are precisely those relevant to subjects' behavior. We thus developed a Task-Relevant Autoencoder via Classifier Enhancement (TRACE), and tested its ability to extract behaviorally-relevant, separable representations compared to a standard autoencoder, a variational autoencoder, and principal component analysis for two severely truncated machine learning datasets. We then evaluated all models on fMRI data from 59 subjects who observed animals and objects. TRACE outperformed all models nearly unilaterally, showing up to 12% increased classification accuracy and up to 56% improvement in discoveri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11723</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#24322;&#24120;&#30340;&#26679;&#26412;&#20248;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#26222;&#36941;&#35748;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#24212;&#22312;&#24212;&#29992;&#38454;&#27573;&#26410;&#33021;&#20934;&#30830;&#37325;&#26500;&#24322;&#24120;&#21306;&#22495;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#36890;&#36807;&#25511;&#21046;&#32593;&#32476;&#30340;&#23481;&#37327;&#26469;&#35299;&#20915;&#65292;&#35201;&#20040;&#36890;&#36807;&#20943;&#23569;&#29942;&#39048;&#23618;&#30340;&#22823;&#23567;&#65292;&#35201;&#20040;&#36890;&#36807;&#23545;&#20854;&#28608;&#27963;&#26045;&#21152;&#31232;&#30095;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#27809;&#26377;&#26126;&#30830;&#24809;&#32602;&#24322;&#24120;&#20449;&#21495;&#30340;&#37325;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21028;&#21035;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#37325;&#26500;&#35823;&#24046;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#23616;&#37096;&#19968;&#33268;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#37327;&#21270;&#39640;&#26031;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#20108;&#27425;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#23618;&#22823;&#23567;&#23545;&#32593;&#32476;&#39640;&#26031;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#23450;&#37327;&#22320;&#24674;&#22797;&#20102;&#22312;&#23485;&#38480;&#21046;&#19979;&#30340;&#20998;&#24067;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.07379</link><description>&lt;p&gt;
&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#21270;&#39640;&#26031;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Quantitative Gaussian Approximation of Randomly Initialized Deep Neural Networks. (arXiv:2203.07379v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#37327;&#21270;&#39640;&#26031;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20998;&#24067;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#20108;&#27425;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#23618;&#22823;&#23567;&#23545;&#32593;&#32476;&#39640;&#26031;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#23450;&#37327;&#22320;&#24674;&#22797;&#20102;&#22312;&#23485;&#38480;&#21046;&#19979;&#30340;&#20998;&#24067;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#24847;&#19968;&#20010;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23545;&#20854;&#36755;&#20986;&#20998;&#24067;&#19982;&#36866;&#24403;&#30340;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20108;&#27425;Wasserstein&#36317;&#31163;&#36827;&#34892;&#20102;&#19978;&#30028;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26126;&#30830;&#19981;&#31561;&#24335;&#34920;&#26126;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#23618;&#30340;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#30340;&#39640;&#26031;&#34892;&#20026;&#65292;&#24182;&#19988;&#23450;&#37327;&#22320;&#24674;&#22797;&#20102;&#23485;&#38480;&#21046;&#19979;&#30340;&#20998;&#24067;&#25910;&#25947;&#32467;&#26524;&#65292;&#21363;&#22914;&#26524;&#25152;&#26377;&#38544;&#34255;&#23618;&#30340;&#22823;&#23567;&#21464;&#24471;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given any deep fully connected neural network, initialized with random Gaussian parameters, we bound from above the quadratic Wasserstein distance between its output distribution and a suitable Gaussian process. Our explicit inequalities indicate how the hidden and output layers sizes affect the Gaussian behaviour of the network and quantitatively recover the distributional convergence results in the wide limit, i.e., if all the hidden layers sizes become large.
&lt;/p&gt;</description></item><item><title>Pixyz&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;Python&#24211;&#65292;&#36890;&#36807;&#23553;&#35013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#27010;&#29575;&#20998;&#24067;&#20197;&#21450;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#31616;&#27905;&#30452;&#35266;&#22320;&#23454;&#29616;&#21508;&#31181;DGMs&#12290;&#27492;&#22806;&#65292;&#35813;&#24211;&#36824;&#24341;&#20837;&#20102;&#35760;&#24518;&#21270;&#25216;&#26415;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#35757;&#32451;DGMs&#26102;&#27604;&#29616;&#26377;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2107.13109</link><description>&lt;p&gt;
Pixyz:&#29992;&#20110;&#24320;&#21457;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Pixyz: a Python library for developing deep generative models. (arXiv:2107.13109v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.13109
&lt;/p&gt;
&lt;p&gt;
Pixyz&#26159;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;Python&#24211;&#65292;&#36890;&#36807;&#23553;&#35013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#27010;&#29575;&#20998;&#24067;&#20197;&#21450;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#23398;&#20064;&#65292;&#21487;&#20197;&#26356;&#31616;&#27905;&#30452;&#35266;&#22320;&#23454;&#29616;&#21508;&#31181;DGMs&#12290;&#27492;&#22806;&#65292;&#35813;&#24211;&#36824;&#24341;&#20837;&#20102;&#35760;&#24518;&#21270;&#25216;&#26415;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#35757;&#32451;DGMs&#26102;&#27604;&#29616;&#26377;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30740;&#31350;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#20197;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#26041;&#24335;&#23454;&#29616;&#23427;&#20204;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;DGMs&#30340;&#20004;&#20010;&#29305;&#28857;&#65306;&#65288;1&#65289;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#27010;&#29575;&#20998;&#24067;&#25152;&#23553;&#35013;&#65292;&#65288;2&#65289;&#27169;&#22411;&#26159;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#35774;&#35745;&#21644;&#23398;&#20064;&#30340;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#29305;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pixyz&#30340;&#26032;&#30340;Python&#24211;&#26469;&#23454;&#29616;DGMs&#12290;&#35813;&#24211;&#37319;&#29992;&#36880;&#27493;&#23454;&#29616;&#26041;&#27861;&#65292;&#20855;&#26377;&#19977;&#20010;API&#65292;&#21487;&#20197;&#26356;&#31616;&#27905;&#30452;&#35266;&#22320;&#23454;&#29616;&#21508;&#31181;DGMs&#12290;&#27492;&#22806;&#65292;&#35813;&#24211;&#24341;&#20837;&#20102;&#35760;&#24518;&#21270;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;DGMs&#20013;&#37325;&#22797;&#35745;&#31639;&#30340;&#25104;&#26412;&#65292;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#30456;&#27604;&#65292;&#35813;&#24211;&#22312;&#35757;&#32451;DGMs&#26102;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent rapid progress in the study of deep generative models (DGMs), there is a need for a framework that can implement them in a simple and generic way. In this research, we focus on two features of DGMs: (1) deep neural networks are encapsulated by probability distributions, and (2) models are designed and learned based on an objective function. Taking these features into account, we propose a new Python library to implement DGMs called Pixyz. This library adopts a step-by-step implementation method with three APIs, which allows us to implement various DGMs more concisely and intuitively. In addition, the library introduces memoization to reduce the cost of duplicate computations in DGMs to speed up the computation. We demonstrate experimentally that this library is faster than existing probabilistic programming languages in training DGMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#19982;&#20843;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2012.02420</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#30340;&#22522;&#20934;&#65306;&#25968;&#25454;&#38598;&#12289;&#31639;&#27861;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Automated Clinical Language Simplification: Dataset, Algorithm, and Evaluation. (arXiv:2012.02420v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.02420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#19982;&#20843;&#31181;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20581;&#24247;&#32032;&#20859;&#30340;&#24739;&#32773;&#36890;&#24120;&#24456;&#38590;&#29702;&#35299;&#21307;&#23398;&#26415;&#35821;&#21644;&#19987;&#19994;&#21307;&#23398;&#35821;&#35328;&#30340;&#22797;&#26434;&#32467;&#26500;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#23558;&#19987;&#19994;&#35821;&#35328;&#32763;&#35793;&#25104;&#26222;&#36890;&#20154;&#21487;&#20197;&#29702;&#35299;&#30340;&#35821;&#35328;&#65292;&#20294;&#20854;&#20013;&#21482;&#26377;&#23569;&#25968;&#20851;&#27880;&#20102;&#20020;&#24202;&#39046;&#22495;&#20013;&#20934;&#30830;&#24615;&#21644;&#21487;&#35835;&#24615;&#30340;&#20004;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#20020;&#24202;&#35821;&#35328;&#30340;&#31616;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20294;&#21487;&#24796;&#30340;&#26159;&#65292;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MedLane&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#33258;&#21160;&#20020;&#24202;&#35821;&#35328;&#31616;&#21270;&#26041;&#27861;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DECLARE&#30340;&#26032;&#27169;&#22411;&#65292;&#36981;&#24490;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#65292;&#19982;&#20843;&#31181;&#24378;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;MedLane&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients with low health literacy usually have difficulty understanding medical jargon and the complex structure of professional medical language. Although some studies are proposed to automatically translate expert language into layperson-understandable language, only a few of them focus on both accuracy and readability aspects simultaneously in the clinical domain. Thus, simplification of the clinical language is still a challenging task, but unfortunately, it is not yet fully addressed in previous work. To benchmark this task, we construct a new dataset named MedLane to support the development and evaluation of automated clinical language simplification approaches. Besides, we propose a new model called DECLARE that follows the human annotation procedure and achieves state-of-the-art performance compared with eight strong baselines. To fairly evaluate the performance, we also propose three specific evaluation metrics. Experimental results demonstrate the utility of the annotated Med
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#21253;&#21547;&#22823;&#36136;&#37327;&#20013;&#24494;&#23376;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#22810;&#20010;&#30456;&#20851;&#32479;&#35745;&#37327;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/1910.04255</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23431;&#23449;&#23398;&#20013;&#30340;&#20013;&#24494;&#23376;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning neutrino effects in Cosmology with Convolutional Neural Networks. (arXiv:1910.04255v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.04255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#24555;&#36895;&#29983;&#25104;&#21253;&#21547;&#22823;&#36136;&#37327;&#20013;&#24494;&#23376;&#30340;&#27169;&#25311;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#22810;&#20010;&#30456;&#20851;&#32479;&#35745;&#37327;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#19977;&#31181;&#27963;&#21160;&#20013;&#24494;&#23376;&#36136;&#37327;&#20043;&#21644;Mv&#26159;&#29616;&#20195;&#23431;&#23449;&#23398;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#22823;&#36136;&#37327;&#20013;&#24494;&#23376;&#22312;&#23431;&#23449;&#30340;&#22823;&#23610;&#24230;&#32467;&#26500;&#19978;&#30041;&#19979;&#20102;&#29305;&#24449;&#24615;&#30340;&#31614;&#21517;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#21487;&#20197;&#20174;&#26143;&#31995;&#35843;&#26597;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#38750;&#32447;&#24615;&#21306;&#22495;&#25552;&#20379;&#20934;&#30830;&#30340;&#29702;&#35770;&#39044;&#27979;&#12290;&#30446;&#21069;&#65292;&#23454;&#29616;&#36825;&#20123;&#39044;&#27979;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#36816;&#34892;&#23431;&#23449;&#25968;&#20540;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#36825;&#20123;&#27169;&#25311;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#20110;&#27599;&#20010;&#20013;&#24494;&#23376;&#36136;&#37327;&#24773;&#20917;&#65292;&#38656;&#35201;&#25968;&#30334;&#21040;&#25968;&#21315;&#23567;&#26102;&#30340;&#26680;&#24515;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#24555;&#36895;&#29983;&#25104;&#27809;&#26377;&#20013;&#24494;&#23376;&#30340;&#26631;&#20934;&#923;CDM&#27169;&#25311;&#20013;&#21547;&#26377;&#22823;&#36136;&#37327;&#20013;&#24494;&#23376;&#30340;&#27169;&#25311;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#25311;&#30340;&#22810;&#20010;&#30456;&#20851;&#32479;&#35745;&#37327;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#20934;&#30830;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring the sum of the three active neutrino masses, $M_\nu$, is one of the most important challenges in modern cosmology. Massive neutrinos imprint characteristic signatures on several cosmological observables in particular on the large-scale structure of the Universe. In order to maximize the information that can be retrieved from galaxy surveys, accurate theoretical predictions in the non-linear regime are needed. Currently, one way to achieve those predictions is by running cosmological numerical simulations. Unfortunately, producing those simulations requires high computational resources -- several hundred to thousand core-hours for each neutrino mass case. In this work, we propose a new method, based on a deep learning network, to quickly generate simulations with massive neutrinos from standard $\Lambda$CDM simulations without neutrinos. We computed multiple relevant statistical measures of deep-learning generated simulations, and conclude that our approach is an accurate alte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#25910;&#32553;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35777;&#26126;&#30001;&#36845;&#20195;&#38543;&#26426;&#31639;&#23376;&#20135;&#29983;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#29702;&#35299;&#21508;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/1804.01195</link><description>&lt;p&gt;
&#36845;&#20195;&#38543;&#26426;&#31639;&#23376;&#30340;&#27010;&#29575;&#25910;&#32553;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contraction Analysis of Iterated Random Operators. (arXiv:1804.01195v6 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1804.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#25910;&#32553;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35777;&#26126;&#30001;&#36845;&#20195;&#38543;&#26426;&#31639;&#23376;&#20135;&#29983;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#24615;&#65292;&#20026;&#29702;&#35299;&#21508;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#31243;&#23398;&#30340;&#39046;&#22495;&#20013;&#65292;Banach&#25910;&#32553;&#26144;&#23556;&#23450;&#29702;&#34987;&#29992;&#26469;&#35777;&#26126;&#29305;&#23450;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#38543;&#26426;&#29256;&#26412;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#22312;&#19968;&#31867;&#38543;&#26426;&#21270;&#31639;&#27861;&#20013;&#65292;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25910;&#32553;&#26144;&#23556;&#34987;&#36817;&#20284;&#20026;&#20351;&#29992;&#26576;&#20123;&#38543;&#26426;&#21464;&#37327;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#30340;&#31639;&#23376;&#12290;&#36825;&#23548;&#33268;&#20102;&#20316;&#29992;&#22312;&#23436;&#22791;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#21021;&#22987;&#28857;&#19978;&#30340;&#36845;&#20195;&#38543;&#26426;&#31639;&#23376;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38543;&#26426;&#25903;&#37197;&#30340;&#35777;&#26126;&#25216;&#26415;&#65292;&#31216;&#20026;&#27010;&#29575;&#25910;&#32553;&#20998;&#26512;&#65292;&#29992;&#20110;&#22312;&#29305;&#23450;&#26497;&#38480;&#35268;&#21017;&#19979;&#35777;&#26126;&#30001;&#36825;&#20123;&#36845;&#20195;&#38543;&#26426;&#31639;&#23376;&#20135;&#29983;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#26041;&#27861;&#20026;&#29702;&#35299;&#21508;&#31181;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many branches of engineering, Banach contraction mapping theorem is employed to establish the convergence of certain deterministic algorithms. Randomized versions of these algorithms have been developed that have proved useful in data-driven problems. In a class of randomized algorithms, in each iteration, the contraction map is approximated with an operator that uses independent and identically distributed samples of certain random variables. This leads to iterated random operators acting on an initial point in a complete metric space, and it generates a Markov chain. In this paper, we develop a new stochastic dominance based proof technique, called probabilistic contraction analysis, for establishing the convergence in probability of Markov chains generated by such iterated random operators in certain limiting regime. The methods developed in this paper provides a general framework for understanding convergence of a wide variety of Monte Carlo methods in which contractive property
&lt;/p&gt;</description></item></channel></rss>