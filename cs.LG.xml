<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27700;&#19979;&#23545;&#25509;&#30340;&#23454;&#26102;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.01522</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65306;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Detection and Control System for Underwater Docking using Machine Learning and Realistic Simulation: A Comprehensive Approach. (arXiv:2311.01522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#36924;&#30495;&#20223;&#30495;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27700;&#19979;&#23545;&#25509;&#30340;&#23454;&#26102;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#23545;&#25509;&#23545;&#20110;&#20351;&#33258;&#20027;&#27700;&#19979;&#36710;&#36742;(AUVs)&#33021;&#25345;&#20037;&#36816;&#34892;&#38750;&#24120;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;AUV&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#23450;&#20301;&#23545;&#25509;&#31449;&#65292;&#30001;&#20110;&#28023;&#24213;&#29615;&#22659;&#30340;&#39640;&#24230;&#21160;&#24577;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#22797;&#26434;&#12290;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#39640;&#37319;&#38598;&#36895;&#29575;&#21644;&#36866;&#24212;&#29615;&#22659;&#30340;&#28789;&#27963;&#24615;; &#28982;&#32780;&#65292;&#27700;&#19979;&#29615;&#22659;&#23384;&#22312;&#20302;&#33021;&#35265;&#24230;&#12289;&#39640;&#27985;&#27978;&#24230;&#21644;&#22833;&#30495;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#39564;&#35777;&#27700;&#19979;&#23545;&#25509;&#33021;&#21147;&#65292;&#29616;&#22330;&#23454;&#39564;&#21487;&#33021;&#20250;&#30001;&#20110;&#38656;&#35201;&#19987;&#29992;&#35774;&#22791;&#21644;&#23433;&#20840;&#32771;&#34385;&#32780;&#26114;&#36149;&#21644;&#21361;&#38505;&#12290;&#26412;&#24037;&#20316;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36827;&#34892;&#27700;&#19979;&#23545;&#25509;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#28982;&#21518;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#22312;&#24072;&#29983;&#33539;&#24335;&#19979;&#36827;&#34892;&#21387;&#32553;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#23454;&#29616;&#23454;&#26102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater docking is critical to enable the persistent operation of Autonomous Underwater Vehicles (AUVs). For this, the AUV must be capable of detecting and localizing the docking station, which is complex due to the highly dynamic undersea environment. Image-based solutions offer a high acquisition rate and versatile alternative to adapt to this environment; however, the underwater environment presents challenges such as low visibility, high turbidity, and distortion. In addition to this, field experiments to validate underwater docking capabilities can be costly and dangerous due to the specialized equipment and safety considerations required to conduct the experiments. This work compares different deep-learning architectures to perform underwater docking detection and classification. The architecture with the best performance is then compressed using knowledge distillation under the teacher-student paradigm to reduce the network's memory footprint, allowing real-time implementatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01479</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#30340;&#35270;&#35282;&#26816;&#27979;&#21040;&#32676;&#22806;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;OOD&#26816;&#27979;&#22120;&#24212;&#35813;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;OOD&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20542;&#21521;&#20110;&#24418;&#25104;&#31751;&#65292;&#32780;&#32676;&#22806;&#29305;&#24449;&#21017;&#36828;&#31163;&#30340;&#35266;&#23519;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#31070;&#32463;&#22349;&#22604;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ID&#29305;&#24449;&#20542;&#21521;&#20110;&#22312;&#25509;&#36817;&#26435;&#37325;&#21521;&#37327;&#30340;&#20301;&#32622;&#32858;&#38598;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25193;&#23637;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#26816;&#27979;OOD&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25490;&#38500;OOD&#26679;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OOD&#29305;&#24449;&#20542;&#21521;&#20110;&#27604;ID&#29305;&#24449;&#26356;&#25509;&#36817;&#21407;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;OOD&#26816;&#27979;&#26041;&#38754;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01442</link><description>&lt;p&gt;
&#28145;&#24230;&#21452;&#19979;&#38477;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#36991;&#20813;&#27169;&#22411;&#27424;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Double Descent for Time Series Forecasting: Avoiding Undertrained Models. (arXiv:2311.01442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21457;&#29616;&#20102;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#22312;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#27169;&#22411;&#26550;&#26500;&#20462;&#25913;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20294;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35757;&#32451;&#27169;&#24335;&#65292;&#21363;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#20854;&#26550;&#26500;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#22312;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20960;&#20010;Transformer&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;epoch-wise&#30340;&#28145;&#24230;&#21452;&#19979;&#38477;&#20197;&#21450;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#36718;&#27425;&#21487;&#20197;&#28040;&#38500;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#22312;72&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#36817;70%&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#25991;&#29486;&#20013;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#35757;&#32451;&#27169;&#24335;&#20462;&#25913;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models, particularly Transformers, have achieved impressive results in various domains, including time series forecasting. While existing time series literature primarily focuses on model architecture modifications and data augmentation techniques, this paper explores the training schema of deep learning models for time series; how models are trained regardless of their architecture. We perform extensive experiments to investigate the occurrence of deep double descent in several Transformer models trained on public time series data sets. We demonstrate epoch-wise deep double descent and that overfitting can be reverted using more epochs. Leveraging these findings, we achieve state-of-the-art results for long sequence time series forecasting in nearly 70% of the 72 benchmarks tested. This suggests that many models in the literature may possess untapped potential. Additionally, we introduce a taxonomy for classifying training schema modifications, covering data augmentation
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#33021;&#22815;&#20248;&#21270;&#20219;&#20309;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#23398;&#20064;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.00975</link><description>&lt;p&gt;
&#29992;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#38598;&#21512;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autonomous Learning of Generative Models with Chemical Reaction Network Ensembles. (arXiv:2311.00975v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#33021;&#22815;&#20248;&#21270;&#20219;&#20309;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#21516;&#26102;&#25105;&#20204;&#36824;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#23398;&#20064;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#39063;&#24494;&#31859;&#22823;&#23567;&#30340;&#30456;&#20114;&#20316;&#29992;&#20998;&#23376;&#22218;&#26159;&#21542;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#19988;&#27874;&#21160;&#30340;&#29615;&#22659;&#30340;&#20869;&#37096;&#27169;&#22411;&#65311;&#25105;&#20204;&#20174;&#25511;&#21046;&#29702;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#12289;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#29702;&#35770;&#21644;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#27762;&#21462;&#20102;&#32463;&#39564;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#65292;&#20351;&#24471;&#24191;&#27867;&#31867;&#21035;&#30340;&#21270;&#23398;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20248;&#21270;&#30340;&#26680;&#24515;&#26041;&#27861;&#65306;&#30456;&#23545;&#29109;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#21270;&#23398;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#19968;&#20010;&#35814;&#32454;&#24179;&#34913;&#30340;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#24182;&#19988;&#36825;&#20010;&#26500;&#36896;&#33021;&#22815;&#20351;&#29992;&#38544;&#34255;&#21333;&#20803;&#26469;&#23398;&#20064;&#22797;&#26434;&#20998;&#24067;&#12290;&#36825;&#20010;&#32467;&#26524;&#21448;&#34987;&#37325;&#26032;&#35299;&#37322;&#20026;&#31215;&#20998;&#21453;&#39304;&#25511;&#21046;&#30340;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#25105;&#20204;&#20351;&#29992;&#20102;&#26174;&#24335;&#30340;&#29289;&#29702;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19982;&#36825;&#20010;&#36807;&#31243;&#30456;&#20851;&#30340;&#28909;&#21147;&#23398;&#25104;&#26412;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a micron sized sack of interacting molecules autonomously learn an internal model of a complex and fluctuating environment? We draw insights from control theory, machine learning theory, chemical reaction network theory, and statistical physics to develop a general architecture whereby a broad class of chemical systems can autonomously learn complex distributions. Our construction takes the form of a chemical implementation of machine learning's optimization workhorse: gradient descent on the relative entropy cost function. We show how this method can be applied to optimize any detailed balanced chemical reaction network and that the construction is capable of using hidden units to learn complex distributions. This result is then recast as a form of integral feedback control. Finally, due to our use of an explicit physical model of learning, we are able to derive thermodynamic costs and trade-offs associated to this process.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00855</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S. Ending the HIV Epidemic Plan. (arXiv:2311.00855v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#32654;&#22269;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#24182;&#32771;&#34385;&#21040;&#22320;&#21306;&#20043;&#38388;&#30340;&#27969;&#34892;&#30149;&#23398;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#65288;HIV&#65289;&#26159;&#32654;&#22269;&#30340;&#20027;&#35201;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#27599;&#24180;&#26377;&#32422;1.2&#19975;&#20154;&#24863;&#26579;HIV&#65292;&#20854;&#20013;&#26377;3.5&#19975;&#20154;&#26159;&#26032;&#24863;&#26579;&#32773;&#12290;&#32654;&#22269;&#30340;HIV&#36127;&#25285;&#21644;&#25252;&#29702;&#25509;&#35302;&#23384;&#22312;&#30528;&#22320;&#29702;&#24046;&#24322;&#12290;2019&#24180;&#30340;&#32456;&#32467;HIV&#27969;&#34892;&#35745;&#21010;&#26088;&#22312;&#21040;2030&#24180;&#23558;&#26032;&#24863;&#26579;&#20154;&#25968;&#20943;&#23569;90%&#65292;&#36890;&#36807;&#25552;&#39640;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#38450;&#24178;&#39044;&#25514;&#26045;&#30340;&#35206;&#30422;&#29575;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;HIV&#39640;&#27969;&#34892;&#22320;&#21306;&#12290;&#30830;&#23450;&#26368;&#20339;&#24178;&#39044;&#25514;&#26045;&#30340;&#35268;&#27169;&#25193;&#22823;&#23558;&#26377;&#21161;&#20110;&#36164;&#28304;&#20998;&#37197;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;HIV&#20915;&#31574;&#27169;&#22411;&#35201;&#20040;&#21482;&#35780;&#20272;&#29305;&#23450;&#22478;&#24066;&#65292;&#35201;&#20040;&#35780;&#20272;&#25972;&#20010;&#22269;&#23478;&#20154;&#21475;&#65292;&#24573;&#35270;&#22320;&#26041;&#30340;&#30456;&#20114;&#20316;&#29992;&#25110;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#29305;&#23450;&#22320;&#21306;&#30340;&#20915;&#31574;&#20998;&#26512;&#65292;&#21516;&#26102;&#32771;&#34385;&#36328;&#22320;&#21306;&#30340;&#27969;&#34892;&#30149;&#20114;&#21160;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Human immunodeficiency virus (HIV) is a major public health concern in the United States, with about 1.2 million people living with HIV and 35,000 newly infected each year. There are considerable geographical disparities in HIV burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE) initiative aims to reduce new infections by 90% by 2030, by improving coverage of diagnoses, treatment, and prevention interventions and prioritizing jurisdictions with high HIV prevalence. Identifying optimal scale-up of intervention combinations will help inform resource allocation. Existing HIV decision analytic models either evaluate specific cities or the overall national population, thus overlooking jurisdictional interactions or differences. In this paper, we propose a multi-agent reinforcement learning (MARL) model, that enables jurisdiction-specific decision analyses but in an environment with cross-jurisdictional epidemiological interactions. In experimental analyses, conduct
&lt;/p&gt;</description></item><item><title>Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00136</link><description>&lt;p&gt;
Neuroformer&#65306;&#29992;&#20110;&#33041;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00136
&lt;/p&gt;
&lt;p&gt;
Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38656;&#35201;&#26032;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#21463;&#21040;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22823;&#35268;&#27169;&#30340;&#32454;&#32990;&#20998;&#36776;&#29575;&#31070;&#32463;&#20803;&#23574;&#23792;&#25968;&#25454;&#30340;&#20998;&#26512;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#26102;&#31354;&#29983;&#25104;&#38382;&#39064;&#12290;Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#19987;&#20026;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#12290;&#23427;&#19982;&#29305;&#24449;&#22823;&#23567;&#21576;&#32447;&#24615;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#65292;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#39044;&#27979;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;Neuroformer&#65292;&#24182;&#21457;&#29616;&#23427;&#26082;&#33021;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#65292;&#20063;&#33021;&#20869;&#22312;&#22320;&#25512;&#26029;&#20986;&#24213;&#23618;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21253;&#25324;&#26041;&#21521;&#12290;&#24403;&#39044;&#35757;&#32451;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#21709;&#24212;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#39044;&#27979;&#23567;&#40736;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
&lt;/p&gt;</description></item><item><title>AdaSub&#26159;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;AdaSub&#22312;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.20060</link><description>&lt;p&gt;
AdaSub&#65306;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
AdaSub: Stochastic Optimization Using Second-Order Information in Low-Dimensional Subspaces. (arXiv:2310.20060v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20060
&lt;/p&gt;
&lt;p&gt;
AdaSub&#26159;&#19968;&#31181;&#20351;&#29992;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#20108;&#38454;&#20449;&#24687;&#36827;&#34892;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;AdaSub&#22312;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AdaSub&#65292;&#19968;&#31181;&#22522;&#20110;&#20302;&#32500;&#33258;&#36866;&#24212;&#23450;&#20041;&#30340;&#20108;&#38454;&#20449;&#24687;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#20108;&#38454;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#20294;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35745;&#31639;Hessian&#30697;&#38453;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20351;&#20854;&#19981;&#23454;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#25628;&#32034;&#30340;&#23376;&#31354;&#38388;&#32500;&#24230;&#26469;&#31649;&#29702;&#35745;&#31639;&#24320;&#38144;&#21644;&#31639;&#27861;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#22312;GitHub&#19978;&#20813;&#36153;&#25552;&#20379;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;AdaSub&#22312;&#36798;&#21040;&#32473;&#23450;&#31934;&#24230;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AdaSub, a stochastic optimization algorithm that computes a search direction based on second-order information in a low-dimensional subspace that is defined adaptively based on available current and past information. Compared to first-order methods, second-order methods exhibit better convergence characteristics, but the need to compute the Hessian matrix at each iteration results in excessive computational expenses, making them impractical. To address this issue, our approach enables the management of computational expenses and algorithm efficiency by enabling the selection of the subspace dimension for the search. Our code is freely available on GitHub, and our preliminary numerical results demonstrate that AdaSub surpasses popular stochastic optimizers in terms of time and number of iterations required to reach a given accuracy.
&lt;/p&gt;</description></item><item><title>&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#20013;&#25506;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#21644;&#37319;&#26679;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.19685</link><description>&lt;p&gt;
DGFN: &#21452;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DGFN: Double Generative Flow Networks. (arXiv:2310.19685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19685
&lt;/p&gt;
&lt;p&gt;
&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#20013;&#25506;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#21644;&#37319;&#26679;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#33647;&#29289;&#21457;&#29616;&#24037;&#20855;&#27491;&#22312;&#23853;&#38706;&#22836;&#35282;&#65292;&#20855;&#26377;&#22312;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets / GFNs&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#23567;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#29289;&#32780;&#21463;&#21040;&#35748;&#21487;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#12290;&#21463;&#24378;&#21270;&#23398;&#20064;&#21644;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30446;&#26631;&#32593;&#32476;&#29992;&#20110;&#37319;&#26679;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#37319;&#26679;&#36335;&#24452;&#26469;&#26356;&#26032;&#20027;&#32593;&#32476;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#23454;&#65292;DGFNs&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36825;&#37117;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#20840;&#26032;&#35774;&#35745;&#30340;&#25361;&#25112;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19253</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flow-based Distributionally Robust Optimization. (arXiv:2310.19253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#27969;&#27169;&#22411;&#21644;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FlowDRO&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#27969;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#35201;&#27714;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65288;&#20063;&#31216;&#20026;&#26368;&#19981;&#21033;&#20998;&#24067;&#65292;LFD&#65289;&#26159;&#36830;&#32493;&#30340;&#65292;&#20174;&#32780;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#26679;&#26412;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#23545;&#35825;&#23548;&#30340;&#40065;&#26834;&#31639;&#27861;&#30340;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;Wasserstein&#36817;&#31471;&#26799;&#24230;&#27969;&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36880;&#27493;&#35757;&#32451;&#22359;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#24207;&#21015;&#26469;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#26694;&#26550;&#36890;&#29992;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a computationally efficient framework, called \texttt{FlowDRO}, for solving flow-based distributionally robust optimization (DRO) problems with Wasserstein uncertainty sets, when requiring the worst-case distribution (also called the Least Favorable Distribution, LFD) to be continuous so that the algorithm can be scalable to problems with larger sample sizes and achieve better generalization capability for the induced robust algorithms. To tackle the computationally challenging infinitely dimensional optimization problem, we leverage flow-based models, continuous-time invertible transport maps between the data distribution and the target distribution, and develop a Wasserstein proximal gradient flow type of algorithm. In practice, we parameterize the transport maps by a sequence of neural networks progressively trained in blocks by gradient descent. Our computational framework is general, can handle high-dimensional data with large sample sizes, and can be useful for various
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#32852;&#21512;&#22810;&#22270;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#22270;&#23398;&#20064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.19005</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#22810;&#22270;&#20449;&#21495;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#30340;&#32852;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kernel-based Joint Multiple Graph Learning and Clustering of Graph Signals. (arXiv:2310.19005v1 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19005
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#32852;&#21512;&#22810;&#22270;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#22270;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#22270;&#23398;&#20064;&#65288;GL&#65289;&#30740;&#31350;&#20174;&#33410;&#28857;&#35266;&#27979;&#65288;&#21363;&#22270;&#20449;&#21495;&#65289;&#20013;&#25512;&#26029;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#28151;&#21512;&#24418;&#24335;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;&#36825;&#31181;&#24322;&#36136;&#24615;&#38656;&#35201;&#32852;&#21512;&#32858;&#31867;&#21644;&#23398;&#20064;&#22810;&#20010;&#22270;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26377;&#21487;&#29992;&#30340;&#33410;&#28857;&#20391;&#21327;&#21464;&#37327;&#65288;&#21363;&#26680;&#20989;&#25968;&#65289;&#65292;&#24517;&#39035;&#21152;&#20197;&#21512;&#24182;&#65292;&#32780;&#36825;&#22312;&#29616;&#26377;&#30340;&#22270;&#20449;&#21495;&#32858;&#31867;&#26041;&#27861;&#20013;&#23578;&#26410;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#21463;&#20016;&#23500;&#30340;K-means&#26694;&#26550;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26680;&#30340;&#31639;&#27861;&#65292;&#23558;&#33410;&#28857;&#20391;&#20449;&#24687;&#25972;&#21512;&#21040;&#20449;&#21495;&#30340;&#20998;&#21306;&#21644;&#27599;&#20010;&#38598;&#32676;&#30340;&#22270;&#23398;&#20064;&#20013;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#20043;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is concerned with the inference of a graph's topology from nodal observations, i.e., graph signals. However, data is often in mixed form, relating to different underlying structures. This heterogeneity necessitates the joint clustering and learning of multiple graphs. In many real-life applications, there are available node-side covariates (i.e., kernels) that imperatively should be incorporated, which has not been addressed by the rare graph signal clustering approaches. To this end and inspired by the rich K-means framework, we propose a novel kernel-based algorithm to incorporate this node-side information as we jointly partition the signals and learn a graph for each cluster. Numerical experiments demonstrate its effectiveness over the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17273</link><description>&lt;p&gt;
&#23558;&#24490;&#29615;&#24341;&#20837;&#20154;&#31867;&#65306;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Looping in the Human: Collaborative and Explainable Bayesian Optimization. (arXiv:2310.17273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17273
&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;(CoExBO)&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24341;&#20837;&#20102;&#24490;&#29615;&#65292;&#24179;&#34913;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#23558;&#29992;&#25143;&#35265;&#35299;&#34701;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#35299;&#37322;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#35768;&#22810;&#20248;&#21270;&#22120;&#19968;&#26679;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#33719;&#24471;&#29992;&#25143;&#20449;&#20219;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#20854;&#19981;&#36879;&#26126;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#24320;&#21457;&#38754;&#21521;&#20154;&#31867;&#30340;&#20248;&#21270;&#22120;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20551;&#35774;&#29992;&#25143;&#30693;&#35782;&#26159;&#26126;&#30830;&#19988;&#26080;&#35823;&#30340;&#65292;&#24182;&#20027;&#35201;&#23558;&#29992;&#25143;&#20316;&#20026;&#20248;&#21270;&#36807;&#31243;&#30340;&#30417;&#30563;&#32773;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#34913;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#20851;&#31995;&#65292;&#21363;&#25105;&#20204;&#30340;&#21327;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CoExBO&#65289;&#26694;&#26550;&#12290;CoExBO&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#26080;&#32541;&#22320;&#23558;&#20154;&#31867;&#35265;&#35299;&#25972;&#21512;&#21040;&#20248;&#21270;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#29992;&#25143;&#20351;&#29992;&#20559;&#22909;&#19968;&#33268;&#30340;&#31639;&#27861;&#24314;&#35758;&#12290;CoExBO&#35299;&#37322;&#20854;&#27599;&#27425;&#36845;&#20195;&#30340;&#20505;&#36873;&#36873;&#25321;&#65292;&#20197;&#22521;&#20859;&#20449;&#20219;&#65292;&#20351;&#29992;&#25143;&#26356;&#28165;&#26970;&#22320;&#25484;&#25569;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;CoExBO&#25552;&#20379;&#26080;&#23475;&#20445;&#35777;&#65292;&#20801;&#35768;&#29992;&#25143;&#29359;&#38169;&#35823;&#65307;&#21363;&#20351;&#22312;&#26497;&#31471;&#23545;&#25239;&#24615;&#24178;&#25200;&#19979;&#65292;&#31639;&#27861;&#20063;&#20250;&#28176;&#36827;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.12425</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#33258;&#21160;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12425
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#35821;&#35328;&#30340;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#22256;&#38590;&#24615;&#65292;&#20984;&#26174;&#20102;&#23545;&#36866;&#29992;&#20110;&#27492;&#31867;&#35821;&#35328;&#30340;&#26377;&#25928;&#33258;&#21160;&#21270;&#20462;&#22797;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#65292;&#22914;&#22522;&#20110;&#27169;&#26495;&#30340;&#20462;&#22797;&#12289;&#21453;&#39304;&#39537;&#21160;&#30340;&#36845;&#20195;&#20462;&#22797;&#21644;&#26377;&#30028;&#31351;&#20030;&#26041;&#27861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#35268;&#33539;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;OpenAI&#30340;ChatGPT&#20462;&#22797;&#29992;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#32534;&#20889;&#30340;&#36719;&#20214;&#35268;&#33539;&#30340;&#25928;&#26524;&#12290;&#19982;&#21629;&#20196;&#24335;&#35821;&#35328;&#19981;&#21516;&#65292;Alloy&#20013;&#30340;&#35268;&#33539;&#19981;&#20250;&#34987;&#25191;&#34892;&#65292;&#32780;&#26159;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#21518;&#31471;&#32422;&#26463;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;&#35268;&#33539;&#23454;&#20363;&#21644;&#26029;&#35328;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;ChatGPT&#22312;&#25913;&#36827;&#22768;&#26126;&#24335;&#35268;&#33539;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08812</link><description>&lt;p&gt;
&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#24577;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20869;&#22312;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#26263;&#21547;&#27874;&#21160;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#12289;&#24555;&#36895;&#28436;&#21464;&#21644;&#27874;&#21160;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;-&#38598;&#25104;&#33539;&#24335;&#65292;&#21363;VMD-LSTM-GARCH&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#31639;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#20026;K&#20010;&#23376;&#27169;&#24577;&#12290;&#38543;&#21518;&#65292;GARCH&#27169;&#22411;&#20174;&#36825;&#20123;&#23376;&#27169;&#24577;&#20013;&#25552;&#21462;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;LSTM&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#23376;&#27169;&#24577;&#30340;&#25968;&#20540;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#39044;&#27979;&#23376;&#27169;&#24577;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#25152;&#26377;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2310.06827</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#26356;&#23569;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Teaching Language Models to Hallucinate Less with Synthetic Tasks. (arXiv:2310.06827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#21512;&#25104;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65288;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#38382;&#31572;&#12289;&#20250;&#35758;&#27010;&#36848;&#21644;&#20020;&#24202;&#25253;&#21578;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#20351;&#25152;&#26377;&#24517;&#35201;&#20449;&#24687;&#37117;&#22312;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#21270;LLMs&#20197;&#20943;&#23569;&#24187;&#35273;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26377;&#25928;&#35780;&#20272;&#24187;&#35273;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20943;&#23569;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#20063;&#21487;&#20197;&#20943;&#23569;&#29616;&#23454;&#19990;&#30028;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;SynTra&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#26131;&#20110;&#35825;&#21457;&#21644;&#34913;&#37327;&#24187;&#35273;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#20219;&#21153;&#36827;&#34892;&#21069;&#32512;&#35843;&#20248;&#26469;&#20248;&#21270;LLM&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#24182;&#26368;&#32456;&#23558;&#31995;&#32479;&#28040;&#24687;&#36716;&#31227;&#21040;&#29616;&#23454;&#20013;&#38590;&#20197;&#20248;&#21270;&#30340;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#29616;&#23454;&#30340;&#25277;&#35937;&#27010;&#25324;&#20219;&#21153;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#26816;&#32034;&#20219;&#21153;&#36827;&#34892;&#30417;&#30563;&#65292;SynTra&#20943;&#23569;&#20102;&#20004;&#20010;&#20855;&#26377;13B&#21442;&#25968;&#30340;LLMs&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36890;&#36807;&#20248;&#21270;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#28040;&#24687;&#65292;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05309</link><description>&lt;p&gt;
&#20248;&#21270;&#32452;&#21512;&#38382;&#39064;&#30340;&#35299;&#37319;&#26679;&#22120;&#65306;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#26799;&#24230;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods. (arXiv:2310.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#20316;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#31561;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#36880;&#27493;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#31215;&#26497;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21253;&#25324;&#26368;&#22823;&#21106;&#21644;&#26368;&#23567;&#21106;&#12289;&#26368;&#22823;$k$&#32422;&#26463;&#38382;&#39064;&#12289;&#26368;&#22823;&#26435;&#37325;&#20108;&#20998;&#22270;&#21305;&#37197;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Reinforcement Learning methods have empirically shown great promise in tackling challenging combinatorial problems. In those methods a deep neural network is used as a solution generator which is then trained by gradient-based methods (e.g., policy gradient) to successively obtain better solution distributions. In this work we introduce a novel theoretical framework for analyzing the effectiveness of such methods. We ask whether there exist generative models that (i) are expressive enough to generate approximately optimal solutions; (ii) have a tractable, i.e, polynomial in the size of the input, number of parameters; (iii) their optimization landscape is benign in the sense that it does not contain sub-optimal stationary points. Our main contribution is a positive answer to this question. Our result holds for a broad class of combinatorial problems including Max- and Min-Cut, Max-$k$-CSP, Maximum-Weight-Bipartite-Matching, and the Traveling Salesman Problem. A
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05166</link><description>&lt;p&gt;
&#19968;&#20010;&#22312;&#26377;&#22122;&#22768;&#35266;&#27979;&#19979;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20462;&#27491;&#30340;&#26399;&#26395;&#25913;&#21892;&#37319;&#38598;&#20989;&#25968;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#35299;&#20915;&#20102;&#23545;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#24573;&#30053;&#20505;&#36873;&#35299;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26368;&#22823;&#21270;&#26399;&#26395;&#25913;&#21892;(EI)&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#24120;&#29992;&#30340;&#31574;&#30053;&#20043;&#19968;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#22788;&#29702;&#22122;&#22768;&#35266;&#27979;&#30340;&#33021;&#21147;&#32780;&#24191;&#27867;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#25913;&#21892;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#26368;&#20339;&#21518;&#39564;&#22343;&#20540;&#20316;&#20026;&#26368;&#20339;&#20505;&#36873;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#35299;&#26512;&#30340;EI&#31867;&#22411;&#26041;&#27861;&#20013;&#65292;&#24120;&#24120;&#24573;&#30053;&#19982;&#20505;&#36873;&#35299;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#22312;&#26080;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#37319;&#38598;&#20989;&#25968;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#26377;&#22122;&#22768;&#35266;&#27979;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;EI&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;(GP)&#27169;&#22411;&#25552;&#20379;&#30340;&#21327;&#26041;&#24046;&#20449;&#24687;&#32435;&#20837;&#20854;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#20013;&#12290;&#36825;&#20010;&#37319;&#38598;&#20989;&#25968;&#19982;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#32467;&#26524;&#30456;&#21563;&#21512;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#24212;&#35813;&#21462;&#20195;&#36125;&#21494;&#26031;&#20248;&#21270;&#36719;&#20214;&#21253;&#12289;&#25945;&#31243;&#21644;&#25945;&#26448;&#20013;&#30340;&#37027;&#20010;&#20844;&#24335;&#12290;&#36825;&#20010;&#25913;&#36827;&#30340;&#37319;&#38598;&#20989;&#25968;&#20026;&#26377;&#22122;&#22768;&#21644;&#26080;&#22122;&#22768;&#30340;&#35299;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#29702;&#35299;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#21028;&#26029;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#20250;&#20986;&#29616;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2310.04411</link><description>&lt;p&gt;
&#29702;&#35299;&#12289;&#39044;&#27979;&#21644;&#26356;&#22909;&#22320;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q&#20540;&#20998;&#27495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL. (arXiv:2310.04411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#29702;&#35299;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#21028;&#26029;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#20250;&#20986;&#29616;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&#20540;&#20272;&#35745;&#30340;&#20998;&#27495;&#19968;&#30452;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#26080;&#27861;&#33719;&#24471;&#30495;&#23454;&#21160;&#24577;&#20449;&#24687;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#24403;&#24341;&#23548;&#20540;&#30446;&#26631;&#26102;&#26597;&#35810;&#20998;&#24067;&#20043;&#22806;&#30340;&#21160;&#20316;&#26159;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#31574;&#30053;&#32422;&#26463;&#25110;&#20445;&#23432;&#30340;Q&#20540;&#20272;&#35745;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#23548;&#33268;&#20998;&#27495;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#19968;&#30452;&#32570;&#20047;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20010;&#26426;&#21046;&#24182;&#33719;&#24471;&#25913;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#35782;&#21035;&#20102;&#33258;&#28608;&#21169;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;Q&#20540;&#20272;&#35745;&#20998;&#27495;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;(NTK)&#30340;&#33258;&#28608;&#21169;&#29305;&#24449;&#20540;&#27979;&#37327;(SEEM)&#25351;&#26631;&#65292;&#29992;&#20110;&#27979;&#37327;&#35757;&#32451;&#36807;&#31243;&#20013;Q&#32593;&#32476;&#30340;&#28436;&#21270;&#24615;&#36136;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#20998;&#27495;&#20986;&#29616;&#30340;&#26377;&#36259;&#35299;&#37322;&#12290;&#39318;&#27425;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#33021;&#22815;&#21487;&#38752;&#22320;&#20915;&#23450;&#35757;&#32451;&#26159;&#21542;&#22312;&#26089;&#26399;&#38454;&#27573;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07289</link><description>&lt;p&gt;
&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#27979;&#35797;&#20102;&#19968;&#20010;&#23454;&#26102;&#25511;&#21046;&#29992;&#25143;&#30028;&#38754;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#33109;&#24102;&#37197;&#32622;&#30340;&#20843;&#20010;&#30005;&#26497;&#20013;&#25552;&#21462;&#34920;&#38754;&#32908;&#30005;&#27963;&#21160;&#65288;sEMG&#65289;&#12290;sEMG&#25968;&#25454;&#34987;&#23454;&#26102;&#27969;&#20837;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25163;&#21183;&#30340;&#23454;&#26102;&#20998;&#31867;&#12290;&#22312;&#21021;&#22987;&#27169;&#22411;&#26657;&#20934;&#21518;&#65292;&#21442;&#19982;&#32773;&#22312;&#20154;&#31867;&#23398;&#20064;&#38454;&#27573;&#20013;&#34987;&#25552;&#20379;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#21453;&#39304;&#65306;&#30495;&#23454;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#39044;&#27979;&#30340;&#25163;&#21183;&#20998;&#31867;&#31639;&#27861;&#30340;&#27010;&#29575;&#34987;&#26174;&#31034;&#32780;&#19981;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#65307;&#20462;&#25913;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#36825;&#20123;&#27010;&#29575;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#38544;&#34255;&#22686;&#24378;&#22788;&#29702;&#65307;&#21644;&#26080;&#21453;&#39304;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36855;&#20320;&#28216;&#25103;&#35780;&#20272;&#20102;&#29992;&#25143;&#30340;&#34920;&#29616;&#65292;&#35201;&#27714;&#34987;&#35797;&#20351;&#29992;&#20843;&#20010;&#25163;&#21183;&#26469;&#25805;&#20316;&#28216;&#25103;&#35282;&#33394;&#23436;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#20462;&#25913;&#21453;&#39304;&#26465;&#20214;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25163;&#21183;&#31867;&#21035;&#21306;&#20998;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06782</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21315;&#20806;&#32423;&#25968;&#25454;&#38598;&#29992;&#20110;&#31890;&#23376;&#27969;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable neural network models and terascale datasets for particle-flow reconstruction. (arXiv:2309.06782v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#30828;&#20214;&#22788;&#29702;&#22120;&#30340;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#21462;&#24471;&#20102;&#30495;&#23454;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#39640;&#33021;&#30005;&#23376;-&#27491;&#30005;&#23376;&#30896;&#25758;&#20013;&#22522;&#20110;&#39640;&#24230;&#31890;&#24230;&#25506;&#27979;&#22120;&#27169;&#25311;&#30340;&#23436;&#25972;&#20107;&#20214;&#37325;&#24314;&#65292;&#30740;&#31350;&#20102;&#21487;&#25193;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#31890;&#23376;&#27969;&#65288;PF&#65289;&#37325;&#24314;&#21487;&#36890;&#36807;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#22242;&#31751;&#25110;&#20987;&#20013;&#26469;&#26500;&#24314;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#20869;&#26680;&#30340;&#21464;&#25442;&#22120;&#65292;&#24182;&#35777;&#26126;&#20004;&#32773;&#37117;&#36991;&#20813;&#20102;&#20108;&#27425;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#30495;&#23454;&#30340;&#31890;&#23376;&#27969;&#37325;&#24314;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#32423;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#24471;&#27169;&#22411;&#22312;&#30828;&#20214;&#22788;&#29702;&#22120;&#19978;&#20855;&#26377;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#65292;&#25903;&#25345;NVIDIA, AMD&#21644;&#33521;&#29305;&#23572; Habana&#21345;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#36319;&#36394;&#21644;&#37327;&#33021;&#22120;&#20987;&#20013;&#32452;&#25104;&#30340;&#39640;&#31890;&#24230;&#36755;&#20837;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33719;&#24471;&#19982;&#22522;&#20934;&#30456;&#31454;&#20105;&#30340;&#29289;&#29702;&#24615;&#33021;&#12290;&#26377;&#20851;&#22797;&#29616;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#36719;&#20214;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study scalable machine learning models for full event reconstruction in high-energy electron-positron collisions based on a highly granular detector simulation. Particle-flow (PF) reconstruction can be formulated as a supervised learning task using tracks and calorimeter clusters or hits. We compare a graph neural network and kernel-based transformer and demonstrate that both avoid quadratic memory allocation and computational cost while achieving realistic PF reconstruction. We show that hyperparameter tuning on a supercomputer significantly improves the physics performance of the models. We also demonstrate that the resulting model is highly portable across hardware processors, supporting Nvidia, AMD, and Intel Habana cards. Finally, we demonstrate that the model can be trained on highly granular inputs consisting of tracks and calorimeter hits, resulting in a competitive physics performance with the baseline. Datasets and software to reproduce the studies are published following 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.04037</link><description>&lt;p&gt;
SRN-SZ: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#19982;&#36229;&#20998;&#36776;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SRN-SZ: Deep Leaning-Based Scientific Error-bounded Lossy Compression with Super-resolution Neural Networks. (arXiv:2309.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;SRN-SZ&#65292;&#29992;&#20110;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#35268;&#27169;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#36229;&#32423;&#35745;&#31639;&#31995;&#32479;&#30340;&#31649;&#29702;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25345;&#31185;&#23398;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#25552;&#20986;&#24182;&#21457;&#23637;&#20102;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#20316;&#20026;&#31185;&#23398;&#25968;&#25454;&#23610;&#23544;&#32553;&#20943;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#20197;&#38480;&#21046;&#25968;&#25454;&#22833;&#30495;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#20013;&#65292;&#26576;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#20256;&#32479;&#25216;&#26415;&#30340;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#26377;&#25928;&#21387;&#32553;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#20102;&#22810;&#20301;&#30740;&#31350;&#20154;&#21592;&#23558;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#21387;&#32553;&#27604;&#21644;/&#25110;&#26497;&#20302;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25913;&#21892;&#38590;&#20197;&#21387;&#32553;&#30340;&#25968;&#25454;&#38598;&#30340;&#21387;&#32553;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SRN-SZ&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31185;&#23398;&#25968;&#25454;&#38169;&#35823;&#26377;&#30028;&#25439;&#22833;&#21387;&#32553;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast growth of computational power and scales of modern super-computing systems have raised great challenges for the management of exascale scientific data. To maintain the usability of scientific data, error-bound lossy compression is proposed and developed as an essential technique for the size reduction of scientific data with constrained data distortion. Among the diverse datasets generated by various scientific simulations, certain datasets cannot be effectively compressed by existing error-bounded lossy compressors with traditional techniques. The recent success of Artificial Intelligence has inspired several researchers to integrate neural networks into error-bounded lossy compressors. However, those works still suffer from limited compression ratios and/or extremely low efficiencies. To address those issues and improve the compression on the hard-to-compress datasets, in this paper, we propose SRN-SZ, which is a deep learning-based scientific error-bounded lossy compressor 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.02422</link><description>&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#36935;&#19978;&#31070;&#32463;&#32593;&#32476;&#65306;Radon-Kolmogorov-Smirnov&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#65288;MMD&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#26368;&#22823;&#21270;&#20004;&#20010;&#20998;&#24067;$P$&#21644;$Q$&#20043;&#38388;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#25152;&#26377;&#22312;&#26576;&#20010;&#20989;&#25968;&#31354;&#38388;$\mathcal{F}$&#20013;&#30340;&#25968;&#25454;&#21464;&#25442;$f$&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#26368;&#36817;&#23558;&#25152;&#35859;&#30340;Radon&#26377;&#30028;&#21464;&#24046;&#20989;&#25968;&#65288;RBV&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#32852;&#31995;&#36215;&#26469;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65288;Parhi&#21644;Nowak, 2021, 2023&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;$\mathcal{F}$&#21462;&#20026;&#32473;&#23450;&#24179;&#28369;&#24230;&#39034;&#24207;$k \geq 0$&#19979;&#30340;RBV&#31354;&#38388;&#20013;&#30340;&#21333;&#20301;&#29699;&#30340;MMD&#12290;&#36825;&#20010;&#26816;&#39564;&#34987;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#30340;&#32463;&#20856;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#30340;&#19968;&#33324;&#21270;&#12290;&#23427;&#36824;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#65306;&#25105;&#20204;&#35777;&#26126;RKS&#26816;&#39564;&#20013;&#30340;&#35777;&#25454;&#20989;&#25968;$f$&#65292;&#21363;&#36798;&#21040;&#26368;&#22823;&#22343;&#24046;&#30340;&#20989;&#25968;&#65292;&#24635;&#26159;&#19968;&#20010;&#20108;&#27425;&#26679;&#26465;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00543</link><description>&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#31579;&#36873;&#22825;&#28982;&#23545;&#31435;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00543
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#26159;&#36890;&#36807;&#21521;&#28165;&#27905;&#36755;&#20837;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#32780;&#21046;&#20316;&#20986;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#24182;&#19981;&#33021;&#20934;&#30830;&#21453;&#26144;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#26410;&#24517;&#33021;&#22815;&#36716;&#21270;&#20026;&#23545;&#33258;&#28982;&#20135;&#29983;&#30340;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;AI&#32780;&#35328;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31579;&#36873;&#30001;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#65292;&#36825;&#31181;&#26631;&#31614;&#32467;&#21512;&#20102;&#22024;&#26434;&#19988;&#26131;&#33719;&#24471;&#30340;&#26631;&#27880;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#35777;&#26126;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.15873</link><description>&lt;p&gt;
&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26368;&#23567;&#23485;&#24230;&#38382;&#39064;&#65306;&#19968;&#31181;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Width for Deep, Narrow MLP: A Diffeomorphism and the Whitney Embedding Theorem Approach. (arXiv:2308.15873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#35777;&#26126;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20110;&#30830;&#23450;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#38382;&#39064;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#65292;&#20197;&#19968;&#33268;&#33539;&#25968;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#19979;&#30028;&#21644;&#19978;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#24456;&#38590;&#32553;&#23567;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#21363;$\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$&#65292;&#20197;&#23454;&#29616;&#22312;&#28145;&#24230;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#20013;&#30340;&#19968;&#33268;&#36924;&#36817;&#65292;&#20854;&#20013;$0\leq \alpha(\sigma)\leq 2$&#20195;&#34920;&#20102;&#19982;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#35777;&#26126;&#26469;&#35777;&#26126;&#36825;&#20010;&#19978;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#36739;&#23567;&#30340;&#39069;&#22806;&#23485;&#24230;&#30340;&#28145;&#24230;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#21487;&#20197;&#36924;&#36817;&#24494;&#20998;&#21516;&#32986;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#34920;&#26126;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;&#23884;&#20837;&#36924;&#36817;&#65292;&#36827;&#19968;&#27493;&#20998;&#35299;&#20026;&#32447;&#24615;&#21464;&#25442;&#21644;&#24494;&#20998;&#21516;&#32986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant attention on determining the minimum width for the universal approximation property of deep, narrow MLPs. Among these challenges, approximating a continuous function under the uniform norm is important and challenging, with the gap between its lower and upper bound being hard to narrow. In this regard, we propose a novel upper bound for the minimum width, given by $\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$, to achieve uniform approximation in deep narrow MLPs, where $0\leq \alpha(\sigma)\leq 2$ represents the constant depending on the activation function. We demonstrate this bound through two key proofs. First, we establish that deep, narrow MLPs with little additional width can approximate diffeomorphisms. Secondly, we utilize the Whitney embedding theorem to show that any continuous function can be approximated by embeddings, further decomposed into linear transformations and diffeomorphisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;</title><link>http://arxiv.org/abs/2308.11849</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#26159;&#19968;&#31181;&#21450;&#26102;&#28789;&#27963;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#26102;&#21464;&#26465;&#20214;&#33258;&#21160;&#25913;&#21464;&#36816;&#33829;&#35745;&#21010;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#38081;&#36335;&#20013;&#20056;&#23458;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#27969;&#21160;&#24615;&#65292;&#20027;&#35201;&#20381;&#36182;&#22522;&#20110;OD&#30340;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21015;&#36710;&#30340;&#38656;&#27714;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#38271;&#26399;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#35843;&#24230;&#26356;&#26032;&#21407;&#21017;&#24573;&#35270;&#20102;&#38656;&#27714;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38656;&#27714;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#20419;&#36827;&#23454;&#26102;&#35843;&#24230;&#12290;&#19982;&#32593;&#32476;&#23618;&#38754;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#30528;&#37325;&#20851;&#27880;&#32039;&#24613;&#21306;&#22495;&#19978;&#28216;&#30340;&#39640;&#38656;&#27714;&#36710;&#31449;&#12290;&#30446;&#26631;&#26159;&#37325;&#26032;&#23433;&#25490;&#36890;&#36807;&#35813;&#30446;&#26631;&#31449;&#30340;&#22810;&#26465;&#32447;&#36335;&#19978;&#21463;&#21040;&#20005;&#37325;&#31361;&#21457;&#20107;&#20214;&#65288;&#22914;&#33258;&#28982;&#28798;&#23475;&#65289;&#24433;&#21709;&#30340;&#25152;&#26377;&#21015;&#36710;&#12290;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#36991;&#20813;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#36873;&#25321;&#21644;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#30340;&#21033;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21160;&#24577;&#36873;&#21462;&#22320;&#22312;&#32447;&#25209;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#20943;&#23569;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10544</link><description>&lt;p&gt;
&#36739;&#24555;&#30340;&#27169;&#22411;&#35757;&#32451;&#20043;&#36335;: &#22522;&#20110;&#36125;&#21494;&#26031;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerated Model Training via Bayesian Data Selection. (arXiv:2308.10544v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10544
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#36125;&#21494;&#26031;&#36873;&#25321;&#21644;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#30340;&#21033;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21160;&#24577;&#36873;&#21462;&#22320;&#22312;&#32447;&#25209;&#37327;&#35757;&#32451;&#26679;&#26412;&#65292;&#20943;&#23569;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#12289;&#37325;&#22797;&#25110;&#26377;&#20559;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#29978;&#33267;&#38459;&#30861;&#27169;&#22411;&#25910;&#25947;&#12290;&#20256;&#32479;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#31616;&#21333;&#25110;&#22256;&#38590;&#26679;&#26412;&#65292;&#32570;&#20047;&#21516;&#26102;&#22788;&#29702;&#22810;&#26679;&#24773;&#20917;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21512;&#29702;&#30340;&#25968;&#25454;&#36873;&#25321;&#21407;&#21017;&#65292;&#36890;&#36807;&#26816;&#26597;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#38469;&#24212;&#29992;&#20381;&#36182;&#20110;&#19981;&#22826;&#21487;&#38752;&#30340;&#36817;&#20284;&#26041;&#27861;&#21644;&#39069;&#22806;&#30340;holdout&#25968;&#25454;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#22823;&#37327;&#25968;&#25454;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#24615;&#30340;&#22312;&#32447;&#25209;&#37327;&#36873;&#25321;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WebVision&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#36739;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mislabeled, duplicated, or biased data in real-world scenarios can lead to prolonged training and even hinder model convergence. Traditional solutions prioritizing easy or hard samples lack the flexibility to handle such a variety simultaneously. Recent work has proposed a more reasonable data selection principle by examining the data's impact on the model's generalization loss. However, its practical adoption relies on less principled approximations and additional holdout data. This work solves these problems by leveraging a lightweight Bayesian treatment and incorporating off-the-shelf zero-shot predictors built on large-scale pre-trained models. The resulting algorithm is efficient and easy to implement. We perform extensive empirical studies on challenging benchmarks with considerable data noise and imbalance in the online batch selection scenario, and observe superior training efficiency over competitive baselines. Notably, on the challenging WebVision benchmark, our method can ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10364</link><description>&lt;p&gt;
SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;
&lt;/p&gt;
&lt;p&gt;
SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SE(3)&#31561;&#21464;&#22686;&#24378;&#32806;&#21512;&#27969;&#65292;&#21487;&#20197;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#22352;&#26631;&#20998;&#21106;&#20013;&#20445;&#25345;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#33021;&#22815;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#20351;&#20854;&#25104;&#20026;&#29289;&#29702;&#31995;&#32479;&#27010;&#29575;&#24314;&#27169;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#32806;&#21512;&#26550;&#26500;&#26080;&#27861;&#36171;&#20104;&#25805;&#20316;&#21407;&#23376;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#27969;SE(3)&#21644;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27839;&#38468;&#21152;&#22686;&#24378;&#32500;&#24230;&#36827;&#34892;&#22352;&#26631;&#20998;&#21106;&#30340;&#32806;&#21512;&#27969;&#65292;&#20197;&#20445;&#25345;SE(3)&#21644;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;&#22312;&#27599;&#19968;&#23618;&#20013;&#65292;&#27969;&#23558;&#21407;&#23376;&#30340;&#20301;&#32622;&#26144;&#23556;&#21040;&#23398;&#20064;&#24471;&#21040;&#30340;SE(3)&#19981;&#21464;&#22522;&#19978;&#65292;&#25105;&#20204;&#22312;&#36820;&#22238;&#21040;&#21407;&#22987;&#22522;&#20043;&#21069;&#24212;&#29992;&#26631;&#20934;&#27969;&#21464;&#25442;&#65292;&#22914;&#21333;&#35843;&#20998;&#23376;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#27969;&#20445;&#25345;&#20102;&#24555;&#36895;&#37319;&#26679;&#21644;&#23494;&#24230;&#35780;&#20272;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#20135;&#29983;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#26399;&#26395;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#22312;DW4&#12289;LJ13&#21644;QM9&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#27969;&#19982;&#31561;&#21464;&#27969;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09000</link><description>&lt;p&gt;
DealMVC: &#38754;&#21521;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
DealMVC: Dual Contrastive Calibration for Multi-view Clustering. (arXiv:2308.09000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DealMVC&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#22270;&#32858;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#26469;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23545;&#27604;&#32858;&#31867;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#35270;&#35282;&#19968;&#33268;&#24615;&#20449;&#24687;&#25366;&#25496;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#38480;&#21046;&#32858;&#31867;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#19981;&#21516;&#35270;&#22270;&#20013;&#30456;&#21516;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20132;&#21449;&#35270;&#22270;&#22330;&#26223;&#20013;&#31867;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#21452;&#21521;&#23545;&#27604;&#26657;&#20934;&#32593;&#32476;&#65288;DealMVC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#34701;&#21512;&#26426;&#21046;&#65292;&#33719;&#24471;&#20840;&#23616;&#30340;&#36328;&#35270;&#22270;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#40784;&#35270;&#22270;&#29305;&#24449;&#30456;&#20284;&#24615;&#22270;&#21644;&#39640;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23616;&#37096;&#23545;&#27604;&#26657;&#20934;&#25439;&#22833;&#65292;&#29992;&#20110;&#32422;&#26463;&#25104;&#23545;&#35270;&#22270;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#12290;&#29305;&#24449;&#32467;&#26500;&#26159; ...
&lt;/p&gt;
&lt;p&gt;
Benefiting from the strong view-consistent information mining capacity, multi-view contrastive clustering has attracted plenty of attention in recent years. However, we observe the following drawback, which limits the clustering performance from further improvement. The existing multi-view models mainly focus on the consistency of the same samples in different views while ignoring the circumstance of similar but different samples in cross-view scenarios. To solve this problem, we propose a novel Dual contrastive calibration network for Multi-View Clustering (DealMVC). Specifically, we first design a fusion mechanism to obtain a global cross-view feature. Then, a global contrastive calibration loss is proposed by aligning the view feature similarity graph and the high-confidence pseudo-label graph. Moreover, to utilize the diversity of multi-view information, we propose a local contrastive calibration loss to constrain the consistency of pair-wise view features. The feature structure is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.13813</link><description>&lt;p&gt;
&#22914;&#20309;&#25193;&#23637;&#24744;&#30340;EMA&#65288;arXiv:2307.13813v1 [stat.ML]&#65289;
&lt;/p&gt;
&lt;p&gt;
How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#20197;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#23545;&#20110;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26435;&#34913;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#27169;&#22411;EMA&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#20197;&#21450;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20445;&#25345;&#35757;&#32451;&#21160;&#24577;&#22312;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#25209;&#37327;&#22823;&#23567;&#21644;&#22681;&#38047;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#32553;&#25918;&#35268;&#21017;&#26469;&#23454;&#29616;&#65292;&#20363;&#22914;&#65292;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#65292;&#24212;&#35813;&#23558;&#23398;&#20064;&#29575;&#19982;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#21478;&#19968;&#20010;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24037;&#20855;&#26159;&#27169;&#22411;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#19981;&#25509;&#25910;&#26799;&#24230;&#20449;&#24687;&#30340;&#27169;&#22411;&#21103;&#26412;&#65292;&#32780;&#26159;&#20197;&#19968;&#23450;&#30340;&#21160;&#37327;&#36319;&#38543;&#20854;&#30446;&#26631;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;EMA&#21487;&#20197;&#25552;&#39640;&#30417;&#30563;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#65292;&#31283;&#23450;&#20266;&#26631;&#35760;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#23398;&#20064;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;EMA&#19982;&#20248;&#21270;&#20998;&#24320;&#22788;&#29702;&#65292;&#23548;&#33268;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#36739;&#20302;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#23384;&#22312;&#27169;&#22411;EMA&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#30340;&#32553;&#25918;&#35268;&#21017;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
&lt;/p&gt;</description></item><item><title>FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2307.13214</link><description>&lt;p&gt;
FedMEKT: &#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning. (arXiv:2307.13214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13214
&lt;/p&gt;
&lt;p&gt;
FedMEKT&#26159;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#24191;&#20041;&#20840;&#23616;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#21482;&#26159;&#38024;&#23545;&#21333;&#27169;&#24577;&#25968;&#25454;&#25552;&#20986;&#20102;&#20856;&#22411;&#30340;FL&#31995;&#32479;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#23545;&#20110;&#21033;&#29992;&#23453;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26410;&#26469;&#20010;&#24615;&#21270;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#30001;&#20110;&#29992;&#25143;&#26080;&#27861;&#36827;&#34892;&#33258;&#27880;&#37322;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#26377;&#38480;&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;FL&#26694;&#26550;&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#12290;&#23558;&#36825;&#20010;&#27010;&#24565;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#30693;&#35782;&#20256;&#36755;&#26426;&#21046;&#65292;&#31216;&#20026;FedMEKT&#65292;&#23427;&#20801;&#35768;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20132;&#25442;&#20174;&#23567;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#32852;&#21512;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMIGA&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.11620</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#19982;&#38544;&#24335;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization. (arXiv:2307.11620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OMIGA&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38544;&#24335;&#30340;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#20854;&#22312;&#26080;&#38656;&#29615;&#22659;&#20132;&#20114;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#31574;&#30053;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24222;&#22823;&#30340;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#32039;&#23494;&#32806;&#21512;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#20026;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31163;&#32447;MARL&#30740;&#31350;&#20165;&#22312;&#21333;&#20010;&#26234;&#33021;&#20307;&#19978;&#24212;&#29992;&#19982;&#31163;&#32447;&#25968;&#25454;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#65292;&#32780;&#26410;&#23436;&#20840;&#32771;&#34385;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OMIGA&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#38544;&#24335;&#20840;&#23616;&#21040;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#12290;OMIGA&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#23616;&#23618;&#38754;&#30340;&#20215;&#20540;&#27491;&#21017;&#21270;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#38544;&#24335;&#23616;&#37096;&#20215;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#21516;&#26102;&#23454;&#29616;&#26679;&#26412;&#20869;&#23398;&#20064;&#65292;&#20174;&#32780;&#20248;&#38597;&#22320;&#26725;&#25509;&#20102;&#22810;&#26234;&#33021;&#20307;&#20215;&#20540;&#20998;&#35299;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has received considerable attention in recent years due to its attractive capability of learning policies from offline datasets without environmental interactions. Despite some success in the single-agent setting, offline multi-agent RL (MARL) remains to be a challenge. The large joint state-action space and the coupled multi-agent behaviors pose extra complexities for offline policy optimization. Most existing offline MARL studies simply apply offline data-related regularizations on individual agents, without fully considering the multi-agent system at the global level. In this work, we present OMIGA, a new offline m ulti-agent RL algorithm with implicit global-to-local v alue regularization. OMIGA provides a principled framework to convert global-level value regularization into equivalent implicit local value regularizations and simultaneously enables in-sample learning, thus elegantly bridging multi-agent value decomposition and policy learning wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35745;&#31639;&#36830;&#32493;&#20998;&#24067;&#19979;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#32593;&#32476;&#22823;&#23567;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#30028;&#38480;&#21644;&#20998;&#31163;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#26469;&#25913;&#21892;&#28145;&#24230;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09212</link><description>&lt;p&gt;
&#38656;&#35201;&#22810;&#23569;&#20010;&#31070;&#32463;&#20803;&#25165;&#33021;&#36817;&#20284;&#35745;&#31639;&#26368;&#22823;&#20540;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Many Neurons Does it Take to Approximate the Maximum?. (arXiv:2307.09212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35745;&#31639;&#36830;&#32493;&#20998;&#24067;&#19979;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#32593;&#32476;&#22823;&#23567;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#30028;&#38480;&#21644;&#20998;&#31163;&#32467;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#26469;&#25913;&#21892;&#28145;&#24230;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32593;&#32476;&#20013;&#65292;&#36817;&#20284;&#35745;&#31639;$L_2$&#33539;&#25968;&#19979;&#36830;&#32493;&#20998;&#24067;$d$&#20010;&#36755;&#20837;&#30340;&#26368;&#22823;&#20540;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36817;&#20284;&#25152;&#38656;&#23485;&#24230;&#30340;&#26032;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#65292;&#20197;&#21450;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#30340;&#28145;&#24230;&#20998;&#31163;&#65292;&#21253;&#25324;&#28145;&#24230;2&#21644;3&#20197;&#21450;&#28145;&#24230;3&#21644;5&#32593;&#32476;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#20026;$\mathcal{O}(\log(\log(d)))$&#65292;&#23485;&#24230;&#20026;$\mathcal{O}(d)$&#30340;&#26500;&#36896;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#26368;&#22823;&#20540;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32447;&#24615;&#23485;&#24230;&#38480;&#21046;&#26465;&#20214;&#19979;&#24050;&#30693;&#26368;&#20248;&#30340;&#28145;&#24230;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#20998;&#31163;&#32467;&#26524;&#36890;&#36807;&#23545;&#22343;&#21248;&#20998;&#24067;&#19979;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#36817;&#20284;&#26368;&#22823;&#20540;&#20989;&#25968;&#30340;&#26032;&#30340;&#19979;&#30028;&#24471;&#21040;&#65292;&#24182;&#20551;&#35774;&#26435;&#37325;&#22823;&#23567;&#20855;&#26377;&#25351;&#25968;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#36825;&#20010;&#28145;&#24230;&#20026;2&#30340;&#19979;&#30028;&#25552;&#20379;&#32039;&#33268;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the size of a neural network needed to approximate the maximum function over $d$ inputs, in the most basic setting of approximating with respect to the $L_2$ norm, for continuous distributions, for a network that uses ReLU activations. We provide new lower and upper bounds on the width required for approximation across various depths. Our results establish new depth separations between depth 2 and 3, and depth 3 and 5 networks, as well as providing a depth $\mathcal{O}(\log(\log(d)))$ and width $\mathcal{O}(d)$ construction which approximates the maximum function, significantly improving upon the depth requirements of the best previously known bounds for networks with linearly-bounded width. Our depth separation results are facilitated by a new lower bound for depth 2 networks approximating the maximum function over the uniform distribution, assuming an exponential upper bound on the size of the weights. Furthermore, we are able to use this depth 2 lower bound to provide tight
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2307.07870</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#21270;&#35282;&#24230;&#30340;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07870
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#30340;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#21472;&#21152;&#12290;&#36890;&#36807;&#35282;&#24230;&#21487;&#25511;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35282;&#24230;&#19979;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24120;&#24120;&#34987;&#38169;&#35823;&#22320;&#35748;&#20026;&#20855;&#26377;&#20010;&#24615;&#25110;&#19968;&#22871;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#35748;&#20026;LLMs&#21487;&#20197;&#30475;&#20316;&#26159;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#21472;&#21152;&#12290;LLMs&#34920;&#29616;&#20986;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#22522;&#20110;&#20135;&#29983;&#30340;&#35282;&#24230;&#32780;&#25913;&#21464;&#65288;&#19982;&#20154;&#31867;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#36890;&#24120;&#20855;&#26377;&#26356;&#19968;&#33268;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#35282;&#24230;&#21487;&#25511;&#24615;&#8221;&#30340;&#27010;&#24565;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#24230;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#12289;VSM&#12289;IPIP&#65289;&#26469;&#30740;&#31350;&#23637;&#31034;&#30340;&#20215;&#20540;&#35266;&#21644;&#20010;&#24615;&#29305;&#24449;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#35282;&#24230;&#32780;&#25913;&#21464;&#12290;&#36890;&#36807;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25552;&#31034;&#20013;&#65288;&#38544;&#24335;&#25110;&#26174;&#24335;&#65289;&#26263;&#31034;&#20102;&#26576;&#20123;&#20215;&#20540;&#35266;&#26102;&#65292;LLMs&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#26174;&#26263;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#20063;&#20250;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05857</link><description>&lt;p&gt;
FAIRO: &#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#20844;&#24179;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRO&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#20844;&#24179;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#35813;&#31639;&#27861;&#23558;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20559;&#22909;&#21464;&#21270;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23558;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#29615;&#22659;&#20013;&#23454;&#29616;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#21644;&#26399;&#26395;&#30340;&#20154;&#21463;&#21040;&#31995;&#32479;&#20013;&#30456;&#21516;&#36866;&#24212;&#20915;&#31574;&#30340;&#24433;&#21709;&#26102;&#12290;&#20154;&#30340;&#21487;&#21464;&#24615;&#22240;&#32032;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#22312;&#26576;&#19968;&#26102;&#38388;&#28857;&#34987;&#35748;&#20026;&#26159;&#20844;&#24179;&#30340;&#25919;&#31574;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#30001;&#20110;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#32780;&#25104;&#20026;&#27495;&#35270;&#24615;&#25919;&#31574;&#12290;&#26412;&#25991;&#20174;&#20844;&#24179;&#24615;&#35270;&#35282;&#32771;&#34385;&#20154;&#30340;&#34892;&#20026;&#21487;&#21464;&#24615;&#21644;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;FAIRO&#65292;&#29992;&#20110;&#38754;&#21521;&#20154;&#26426;&#20132;&#20114;&#36866;&#24212;&#30340;&#20844;&#24179;&#39034;&#24207;&#20915;&#31574;&#65292;&#23427;&#23558;&#36825;&#20123;&#27010;&#24565;&#32435;&#20837;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FAIRO&#23558;&#36825;&#20010;&#22797;&#26434;&#30340;&#20844;&#24179;&#20219;&#21153;&#22522;&#20110;&#20010;&#20307;&#20154;&#30340;&#20559;&#22909;&#36890;&#36807;&#21033;&#29992;&#36873;&#39033;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20998;&#35299;&#20026;&#33258;&#36866;&#24212;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving fairness in sequential-decision making systems within Human-in-the-Loop (HITL) environments is a critical concern, especially when multiple humans with different behavior and expectations are affected by the same adaptation decisions in the system. This human variability factor adds more complexity since policies deemed fair at one point in time may become discriminatory over time due to variations in human preferences resulting from inter- and intra-human variability. This paper addresses the fairness problem from an equity lens, considering human behavior variability, and the changes in human preferences over time. We propose FAIRO, a novel algorithm for fairness-aware sequential-decision making in HITL adaptation, which incorporates these notions into the decision-making process. In particular, FAIRO decomposes this complex fairness task into adaptive sub-tasks based on individual human preferences through leveraging the Options reinforcement learning framework. We design 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2307.04841</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04841
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#23376;&#37319;&#26679;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#20250;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#38656;&#35201;&#23398;&#20064;&#22312;&#21453;&#39304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#34892;&#21160;&#30340;&#22810;&#20010;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36825;&#31181;&#32463;&#39564;&#19978;&#30340;&#25104;&#21151;&#65292;&#20173;&#28982;&#27809;&#26377;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#29992;&#20110;&#34920;&#31034;&#29366;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25511;&#21046;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#19979;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#30340;&#20856;&#22411;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#26159;&#22312;&#19968;&#20010;&#39640;&#26031;&#31561;&#25928;&#20551;&#35774;&#19979;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#20854;&#20013;&#23545;&#38543;&#26426;&#36712;&#36857;&#30340;&#24179;&#22343;&#20540;&#34987;&#26367;&#25442;&#20026;&#26102;&#24577;&#30456;&#20851;&#30340;&#39640;&#26031;&#29305;&#24449;&#24179;&#22343;&#20540;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#23545;&#21487;&#33021;&#30340;&#36712;&#36857;&#31354;&#38388;&#36827;&#34892;&#23376;&#37319;&#26679;&#32780;&#20135;&#29983;&#30340;&#38543;&#26426;&#21322;&#26799;&#24230;&#22122;&#22768;&#23548;&#33268;&#20540;&#35823;&#24046;&#20986;&#29616;&#26174;&#33879;&#30340;&#24179;&#21488;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been successful across several applications in which agents have to learn to act in environments with sparse feedback. However, despite this empirical success there is still a lack of theoretical understanding of how the parameters of reinforcement learning models and the features used to represent states interact to control the dynamics of learning. In this work, we use concepts from statistical physics, to study the typical case learning curves for temporal difference learning of a value function with linear function approximators. Our theory is derived under a Gaussian equivalence hypothesis where averages over the random trajectories are replaced with temporally correlated Gaussian feature averages and we validate our assumptions on small scale Markov Decision Processes. We find that the stochastic semi-gradient noise due to subsampling the space of possible episodes leads to significant plateaus in the value error, unlike in traditional gradient descent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.01050</link><description>&lt;p&gt;
&#36816;&#36755;&#12289;&#21464;&#20998;&#25512;&#26029;&#21644;&#25193;&#25955;&#65306;&#24212;&#29992;&#20110;&#22238;&#28779;&#27969;&#21644;&#34203;&#23450;&#35860;&#26725;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#36816;&#36755;&#21644;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#21644;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#30446;&#26631;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36816;&#36755;&#19982;&#21464;&#20998;&#25512;&#26029;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27491;&#21521;&#21644;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#21450;Girsanov&#21464;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36335;&#24452;&#31354;&#38388;&#25955;&#24230;&#30340;&#37319;&#26679;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26368;&#32456;&#21457;&#23637;&#20986;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#22238;&#28779;&#27969;&#25216;&#26415;&#65288;&#19982;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;Jarzynski&#21644;Crooks&#24658;&#31561;&#24335;&#26377;&#20851;&#65289;&#21644;&#19968;&#20010;&#27491;&#21017;&#21270;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#22411;&#30446;&#26631;&#65292;&#19981;&#21516;&#20110;&#26631;&#20934;IPF&#30340;&#39034;&#24207;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#29983;&#25104;&#24314;&#27169;&#31034;&#20363;&#21644;&#22522;&#20110;&#21452;&#20117;&#30340;&#31232;&#26377;&#20107;&#20214;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22240;&#26524;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;(RCM)&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#65292;&#24182;&#38416;&#26126;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#27599;&#20010;RCM&#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#12290;&#20316;&#32773;&#20171;&#32461;&#20102;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.14351</link><description>&lt;p&gt;
&#27604;&#36739;&#22240;&#26524;&#26694;&#26550;&#65306;&#28508;&#22312;&#32467;&#26524;&#12289;&#32467;&#26500;&#27169;&#22411;&#12289;&#22270;&#21644;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Comparing Causal Frameworks: Potential Outcomes, Structural Models, Graphs, and Abstractions. (arXiv:2306.14351v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22240;&#26524;&#26694;&#26550;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;(RCM)&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;(SCM)&#65292;&#24182;&#38416;&#26126;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#65292;&#20197;&#21450;&#27599;&#20010;RCM&#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#12290;&#20316;&#32773;&#20171;&#32461;&#20102;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#28508;&#22312;&#32467;&#26524;&#27169;&#22411;&#65288;RCM&#65289;&#19982;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26694;&#26550;&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20851;&#31995;&#12290;&#37319;&#29992;&#20013;&#31435;&#30340;&#36923;&#36753;&#35270;&#35282;&#65292;&#20511;&#37492;&#20197;&#21069;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RCM&#25104;&#20026;SCM&#21487;&#34920;&#36798;&#30340;&#26465;&#20214;&#12290;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#26174;&#31034;&#65292;&#27599;&#20010;RCM -- &#21253;&#25324;&#37027;&#20123;&#36829;&#21453;SCM&#26694;&#26550;&#20013;&#26263;&#31034;&#30340;&#20195;&#25968;&#21407;&#21017;&#30340;RCM -- &#20316;&#20026;&#26576;&#20123;&#21487;&#34920;&#36798;&#30340;RCM&#30340;&#25277;&#35937;&#32780;&#20986;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;SCM&#21407;&#21017;&#22312;RCM&#32463;&#20856;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#36825;&#31181;&#25913;&#36827;&#24615;&#35270;&#35282;&#30340;&#20248;&#21183;&#65307;&#21453;&#20043;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30001;&#22270;&#34920;&#31034;&#30340;&#20195;&#25968;&#32422;&#26463;&#30340;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#27604;&#36739;&#20004;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to make clear and precise the relationship between the Rubin causal model (RCM) and structural causal model (SCM) frameworks for causal inference. Adopting a neutral logical perspective, and drawing on previous work, we show what is required for an RCM to be representable by an SCM. A key result then shows that every RCM -- including those that violate algebraic principles implied by the SCM framework -- emerges as an abstraction of some representable RCM. Finally, we illustrate the power of this ameliorative perspective by pinpointing an important role for SCM principles in classic applications of RCMs; conversely, we offer a characterization of the algebraic constraints implied by a graph, helping to substantiate further comparisons between the two frameworks.
&lt;/p&gt;</description></item><item><title>OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10280</link><description>&lt;p&gt;
OpenGSL: &#19968;&#39033;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10280
&lt;/p&gt;
&lt;p&gt;
OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#22797;&#26434;&#21644;&#20381;&#36182;&#24418;&#25104;&#36807;&#31243;&#23548;&#33268;&#30340;&#33410;&#28857;&#36830;&#25509;&#30340;&#22266;&#26377;&#27425;&#20248;&#24615;&#36136;&#65292;&#22312;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22270;&#32467;&#26500;&#23398;&#20064;(GSL)&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GSL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#21516;&#26102;&#20248;&#21270;&#22270;&#32467;&#26500;&#21644;&#23545;&#24212;&#30340;GNN&#27169;&#22411;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;GSL&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20998;&#21106;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenGSL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;GSL&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;OpenGSL&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#27604;&#36739;&#24179;&#21488;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#19981;&#21516;&#30340;GSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65307;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65307;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04186</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#38899;&#39057;&#24072;&#29983;Transformer&#29992;&#20110;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Audio Teacher-Student Transformer for Both Clip-level and Frame-level Tasks. (arXiv:2306.04186v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65307;&#36825;&#20004;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65307;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#23398;&#20064;&#38899;&#39057;&#34920;&#31034;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#38899;&#39057;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#19979;&#28216;&#38899;&#39057;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#21253;&#25324;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20004;&#31181;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65306;ATST-Clip&#21644;ATST-Frame&#65292;&#20998;&#21035;&#29992;&#20110;&#23398;&#20064;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#34920;&#31034;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#24072;&#29983;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38899;&#39057;&#23454;&#20363;&#36776;&#21035;&#21644;&#38899;&#39057;&#24207;&#21015;&#37325;&#26500;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29255;&#27573;&#32423;&#21644;&#24103;&#32423;&#19979;&#28216;&#20219;&#21153;&#19978;&#22343;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) has emerged as a popular approach for learning audio representations. The ultimate goal of audio self-supervised pre-training is to transfer knowledge to downstream audio tasks, generally including clip-level and frame-level tasks. Clip-level tasks classify the scene or sound of an entire audio clip, e.g. audio tagging, instrument recognition, etc. While frame-level tasks detect event-level timestamps from an audio clip, e.g. sound event detection, speaker diarization, etc. Prior studies primarily evaluate on clip-level downstream tasks. Frame-level tasks are important for fine-grained acoustic scene/event understanding, and are generally more challenging than clip-level tasks. In order to tackle both clip-level and frame-level tasks, this paper proposes two self-supervised audio representation learning methods: ATST-Clip and ATST-Frame, responsible for learning clip-level and frame-level representations, respectively. ATST stands for Aud
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;DeepONets&#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;X-DeepONet&#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#24182;&#36890;&#36807;&#26681;&#32593;&#32476;&#35299;&#20915;&#20102;&#26631;&#20934;DeepONet&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04096</link><description>&lt;p&gt;
&#19968;&#31181;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#30340;&#26032;&#22411;Deeponet&#27169;&#22411;&#21450;&#20854;&#22312;&#22320;&#38663;&#38663;&#28304;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A novel deeponet model for learning moving-solution operators with applications to earthquake hypocenter localization. (arXiv:2306.04096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;DeepONets&#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;X-DeepONet&#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#24182;&#36890;&#36807;&#26681;&#32593;&#32476;&#35299;&#20915;&#20102;&#26631;&#20934;DeepONet&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#31867;&#27963;&#21160;&#24341;&#36215;&#30340;&#22320;&#38663;&#27963;&#21160;&#23545;&#20844;&#20849;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#21450;&#26102;&#30340;&#22320;&#38663;&#38663;&#28304;&#23450;&#20301;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;X-DeepONet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476; (DeepONets) &#21464;&#20307;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#21442;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243; (PDEs) &#30340;&#31227;&#21160;&#35299;&#31639;&#22120;&#65292;&#24182;&#24212;&#29992;&#20110;&#23454;&#26102;&#22320;&#38663;&#23450;&#20301;&#12290;&#21033;&#29992;&#31070;&#32463;&#31639;&#23376;&#30340;&#21147;&#37327;&#65292;X-DeepONet &#23398;&#20064;&#20272;&#35745;&#19982;&#22320;&#38663;&#28304;&#30456;&#20851;&#30340;&#36208;&#26102;&#22330;&#65292;&#36890;&#36807;&#23558;&#22320;&#38663;&#21040;&#26102;&#21644;&#36895;&#24230;&#27169;&#22411;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#31867;&#20284;&#20110; DeepONet&#65292;X-DeepONet &#21253;&#25324;&#20027;&#24178;&#32593;&#32476;&#21644;&#20998;&#25903;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26681;&#32593;&#32476;&#65292;&#23427;&#19981;&#20165;&#23558;&#26631;&#20934;&#30340; DeepONet &#20056;&#27861;&#31639;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#36824;&#23558;&#21152;&#27861;&#21644;&#20943;&#27861;&#31639;&#23376;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#31227;&#21160;&#22330;&#30340;&#38382;&#39064;&#20013;&#65292;DeepONet &#30340;&#26631;&#20934;&#20056;&#27861;&#25805;&#20316;&#26080;&#27861;&#25429;&#33719;&#22330;&#30340;&#37325;&#23450;&#20301;&#65292;&#32780;X-DeepONet &#30340;&#26681;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismicity induced by human activities poses a significant threat to public safety, emphasizing the need for accurate and timely earthquake hypocenter localization. In this study, we introduce X-DeepONet, a novel variant of deep operator networks (DeepONets), for learning moving-solution operators of parametric partial differential equations (PDEs), with application to real-time earthquake localization. Leveraging the power of neural operators, X-DeepONet learns to estimate traveltime fields associated with earthquake sources by incorporating information from seismic arrival times and velocity models. Similar to the DeepONet, X-DeepONet includes a trunk net and a branch net. Additionally, we introduce a root network that not only takes the standard DeepONet's multiplication operator as input, it also takes addition and subtraction operators. We show that for problems with moving fields, the standard multiplication operation of DeepONet is insufficient to capture field relocation, while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#19982;&#26684;&#29702;&#35770;&#24314;&#31435;&#20102;&#25968;&#23398;&#32852;&#31995;&#65292;&#20026;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00904</link><description>&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#27979;&#37327;&#65292;&#20998;&#21306;&#26684;&#21644;&#26680;&#27979;&#35797;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interaction Measures, Partition Lattices and Kernel Tests for High-Order Interactions. (arXiv:2306.00904v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27979;&#37327;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#24182;&#19982;&#26684;&#29702;&#35770;&#24314;&#31435;&#20102;&#25968;&#23398;&#32852;&#31995;&#65292;&#20026;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20381;&#36182;&#20110;&#25104;&#23545;&#20851;&#31995;&#30340;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#31038;&#20250;&#32463;&#27982;&#12289;&#29983;&#24577;&#25110;&#29983;&#29289;&#21307;&#23398;&#31995;&#32479;&#65289;&#20013;&#25214;&#21040;&#30340;&#22797;&#26434;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#23436;&#25972;&#32479;&#35745;&#32467;&#26500;&#12290;&#20004;&#20010;&#20197;&#19978;&#21464;&#37327;&#32452;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20381;&#36182;&#20851;&#31995;&#22312;&#36825;&#20123;&#31995;&#32479;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#36825;&#26679;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;$d$-order ($d \geq 2$)&#30456;&#20114;&#20316;&#29992;&#27979;&#37327;&#65292;&#20381;&#27425;&#21253;&#25324;&#21487;&#33021;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#20998;&#35299;&#65292;&#24182;&#23450;&#20041;&#20102;&#38750;&#21442;&#25968;&#12289;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#65292;&#20197;&#31995;&#32479;&#22320;&#30830;&#23450;$d$-order&#30456;&#20114;&#20316;&#29992;&#30340;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#26684;&#29702;&#35770;&#30340;&#25968;&#23398;&#32852;&#31995;&#65292;&#38416;&#26126;&#20102;&#30456;&#20114;&#20316;&#29992;&#24230;&#37327;&#30340;&#23548;&#20986;&#21450;&#20854;&#22797;&#21512;&#25490;&#21015;&#27979;&#35797;&#30340;&#28085;&#20041;&#65307;&#28548;&#28165;&#20102;&#21333;&#32431;&#22797;&#21512;&#20307;&#19982;&#26680;&#30697;&#38453;&#20013;&#24515;&#21270;&#30340;&#32852;&#31995;&#65307;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;&#30456;&#20114;&#20316;&#29992;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models that rely solely on pairwise relationships often fail to capture the complete statistical structure of the complex multivariate data found in diverse domains, such as socio-economic, ecological, or biomedical systems. Non-trivial dependencies between groups of more than two variables can play a significant role in the analysis and modelling of such systems, yet extracting such high-order interactions from data remains challenging. Here, we introduce a hierarchy of $d$-order ($d \geq 2$) interaction measures, increasingly inclusive of possible factorisations of the joint probability distribution, and define non-parametric, kernel-based tests to establish systematically the statistical significance of $d$-order interactions. We also establish mathematical links with lattice theory, which elucidate the derivation of the interaction measures and their composite permutation tests; clarify the connection of simplicial complexes with kernel matrix centring; and provide a means to enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#24179;&#34913;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#20998;&#24067;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#21450;&#26799;&#24230;&#22914;&#20309;&#23454;&#29616;&#26435;&#37325;&#23398;&#20064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.00802</link><description>&lt;p&gt;
&#19968;&#31181;&#35760;&#24518;&#35270;&#35282;&#19979;&#30340;Transformer&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Birth of a Transformer: A Memory Viewpoint. (arXiv:2306.00802v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#24179;&#34913;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#20998;&#24067;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#21450;&#26799;&#24230;&#22914;&#20309;&#23454;&#29616;&#26435;&#37325;&#23398;&#20064;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#34987;&#24191;&#27867;&#37096;&#32626;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#20351;&#23427;&#20204;&#26356;&#21152;&#21487;&#38752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;transformers&#22914;&#20309;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#21512;&#25104;&#30340;&#35774;&#32622;&#26469;&#24179;&#34913;&#23384;&#20648;&#20110;&#23427;&#20204;&#20043;&#20013;&#30340;&#20004;&#31181;&#30693;&#35782;&#31867;&#22411;&#8212;&#8212;&#20840;&#23616;&#20998;&#24067;&#21644;&#19978;&#19979;&#25991;&#29305;&#23450;&#30340;&#20108;&#20803;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;Transformer&#30340;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20180;&#32454;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23545;&#20840;&#23616;&#20108;&#20803;&#20998;&#24067;&#30340;&#24555;&#36895;&#23398;&#20064;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#20013;&#30340;&#20108;&#20803;&#20998;&#24067;&#30340;"&#24402;&#32435;&#22836;"&#26426;&#21046;&#30340;&#36739;&#24930;&#21457;&#23637;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26435;&#20540;&#30697;&#38453;&#20316;&#20026;&#32852;&#24819;&#35760;&#24518;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#65292;&#35828;&#26126;&#20102;&#26799;&#24230;&#22914;&#20309;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#26435;&#37325;&#30340;&#23398;&#20064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>MAGNet&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19303</link><description>&lt;p&gt;
MAGNet&#65306;&#22522;&#20110;&#24418;&#29366;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MAGNet: Motif-Agnostic Generation of Molecules from Shapes. (arXiv:2305.19303v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19303
&lt;/p&gt;
&lt;p&gt;
MAGNet&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#20998;&#23376;&#29983;&#25104;&#25216;&#26415;&#65292;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20998;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#26497;&#26377;&#28508;&#21147;&#65292;&#21487;&#20026;&#33647;&#29289;&#21457;&#29616;&#25552;&#20379;&#20415;&#21033;&#12290;&#29616;&#26377;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#23558;&#20998;&#23376;&#20998;&#35299;&#20026;&#24120;&#35265;&#30340;&#23376;&#32467;&#26500;&#65288;motifs&#65289;&#65292;&#20043;&#21518;&#29983;&#25104;&#26032;&#30340;&#21270;&#21512;&#29289;&#12290;&#23613;&#31649;motif&#30340;&#34920;&#31034;&#27861;&#26497;&#22823;&#22320;&#24110;&#21161;&#23398;&#20064;&#20998;&#23376;&#20998;&#24067;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#20195;&#34920;&#36229;&#20986;&#24050;&#30693;motif&#38598;&#20043;&#22806;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;MAGNet&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#37197;&#21407;&#23376;&#21644;&#38190;&#31867;&#22411;&#20043;&#21069;&#65292;&#29983;&#25104;&#25277;&#35937;&#30340;&#24418;&#29366;&#65292;&#20197;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#24182;&#22686;&#21152;&#23545;&#25968;&#25454;&#38598;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#20998;&#24067;&#30340;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#20998;&#23376;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#24182;&#20419;&#36827;&#20102;&#21407;&#23376;&#21644;&#38190;&#30340;&#36866;&#24403;&#20998;&#37197;&#12290;&#34429;&#28982;&#23558;&#25277;&#35937;&#24418;&#29366;&#24341;&#20837;&#20998;&#24067;&#23398;&#20064;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;MAGNet&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning for molecules exhibit great potential for facilitating drug discovery from in silico predictions. Most models for molecule generation rely on the decomposition of molecules into frequently occurring substructures (motifs), from which they generate novel compounds. While motif representations greatly aid in learning molecular distributions, such methods struggle to represent substructures beyond their known motif set. To alleviate this issue and increase flexibility across datasets, we propose MAGNet, a graph-based model that generates abstract shapes before allocating atom and bond types. To this end, we introduce a novel factorisation of the molecules' data distribution that accounts for the molecules' global context and facilitates learning adequate assignments of atoms and bonds onto shapes. While the abstraction to shapes introduces greater complexity for distribution learning, we show the competitive performance of MAGNet on standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21338;&#24328;&#29702;&#35770;&#20013;&#26799;&#24230;&#26041;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#22312;&#37096;&#20998;&#26354;&#29575;&#26465;&#20214;&#19979;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#65292;&#21516;&#26102;&#24179;&#22343;&#29305;&#24449;&#20540;&#27604;&#26368;&#23567;&#29305;&#24449;&#20540;&#26356;&#33021;&#20307;&#29616;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17275</link><description>&lt;p&gt;
&#27839;&#30528;&#37096;&#20998;&#26354;&#29575;&#65292;&#26799;&#24230;&#26041;&#27861;&#22312;&#38646;&#21644;&#21338;&#24328;&#20013;&#23616;&#37096;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Local Convergence of Gradient Methods for Min-Max Games under Partial Curvature. (arXiv:2305.17275v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21338;&#24328;&#29702;&#35770;&#20013;&#26799;&#24230;&#26041;&#27861;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#22312;&#37096;&#20998;&#26354;&#29575;&#26465;&#20214;&#19979;&#20445;&#35777;&#20102;&#23616;&#37096;&#25910;&#25947;&#65292;&#21516;&#26102;&#24179;&#22343;&#29305;&#24449;&#20540;&#27604;&#26368;&#23567;&#29305;&#24449;&#20540;&#26356;&#33021;&#20307;&#29616;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38646;&#21644;&#21487;&#24494;&#20998;&#21338;&#24328;&#30340;&#26799;&#24230;&#26041;&#27861;&#23545;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#30340;&#25910;&#25947;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403; $S \succ 0$ &#26102;&#65292;&#36825;&#31181;&#21160;&#24577;&#20250;&#22312;&#23616;&#37096;&#25910;&#25947;&#65292;&#32780;&#24403; $S = 0$ &#26102;&#21487;&#33021;&#20250;&#21457;&#25955;&#65292;&#20854;&#20013; $S \succeq 0$ &#26159;&#22343;&#34913;&#26102;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#23545;&#31216;&#37096;&#20998;&#65292;&#23427;&#21344;&#25454;&#20102;&#28216;&#25103;&#30340; &#8220;&#21183;&#33021;&#8221; &#32452;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21482;&#35201; $S$ &#19981;&#20026;&#38646;&#65288;&#37096;&#20998;&#26354;&#29575;&#65289;&#65292;&#24182;&#19988;&#21453;&#23545;&#31216;&#37096;&#20998; $A$ &#30340;&#29305;&#24449;&#21521;&#37327;&#19982; $S$ &#30340;&#26680;&#30456;&#23545;&#20301;&#32622;&#33391;&#22909;&#65292;&#36825;&#20123;&#21160;&#24577;&#20063;&#20250;&#25910;&#25947;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403; $S \ll A$ &#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#36890;&#24120;&#21462;&#20915;&#20110; $S$ &#30340;&#29305;&#24449;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#20540;&#65292;&#36825;&#19982;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31867;&#27604;&#25152;&#24314;&#35758;&#30340;&#30456;&#21453;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#36830;&#32493;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30001;&#20110;&#37096;&#20998;&#26354;&#29575;&#65292;&#38181;&#24418;&#31890;&#23376;&#26041;&#27861;&#8212;&#8212;&#23427;&#22312;&#28151;&#21512;&#31574;&#30053;&#30340;&#26435;&#37325;&#21644;&#25903;&#25345;&#19978;&#36827;&#34892;&#20248;&#21270;&#8212;&#8212;&#21576;&#29616;&#20986;&#19968;&#33268;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#65292;&#32780;&#26631;&#20934;&#30340;&#23545;&#20598;&#26799;&#24230;&#26041;&#27861;&#21017;&#21487;&#33021;&#20250;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convergence to local Nash equilibria of gradient methods for two-player zero-sum differentiable games. It is well-known that such dynamics converge locally when $S \succ 0$ and may diverge when $S=0$, where $S\succeq 0$ is the symmetric part of the Jacobian at equilibrium that accounts for the "potential" component of the game. We show that these dynamics also converge as soon as $S$ is nonzero (partial curvature) and the eigenvectors of the antisymmetric part $A$ are in general position with respect to the kernel of $S$. We then study the convergence rates when $S \ll A$ and prove that they typically depend on the average of the eigenvalues of $S$, instead of the minimum as an analogy with minimization problems would suggest. To illustrate our results, we consider the problem of computing mixed Nash equilibria of continuous games. We show that, thanks to partial curvature, conic particle methods -- which optimize over both weights and supports of the mixed strategies -- g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31934;&#30830;&#25512;&#29702;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#31163;&#25955;&#37319;&#26679;&#12289;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#12290;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#23454;&#29616;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#20110;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#24182;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.17058</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#31163;&#25955;&#27169;&#22411;&#31934;&#30830;&#25512;&#29702;&#65306;&#27010;&#29575;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exact Bayesian Inference on Discrete Models via Probability Generating Functions: A Probabilistic Programming Approach. (arXiv:2305.17058v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17058
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31934;&#30830;&#25512;&#29702;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25903;&#25345;&#31163;&#25955;&#37319;&#26679;&#12289;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#12290;&#36890;&#36807;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#23454;&#29616;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#24615;&#33021;&#20248;&#20110;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#24182;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#32479;&#35745;&#27169;&#22411;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#26080;&#38480;&#25903;&#25345;&#21644;&#36830;&#32493;&#20808;&#39564;&#20063;&#21487;&#20197;&#25214;&#21040;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#34920;&#36798;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25903;&#25345;&#31163;&#25955;&#21644;&#36830;&#32493;&#37319;&#26679;&#12289;&#31163;&#25955;&#35266;&#27979;&#12289;&#20223;&#23556;&#20989;&#25968;&#12289;&#65288;&#38543;&#26426;&#65289;&#20998;&#25903;&#21644;&#20107;&#20214;&#26465;&#20214;&#30340;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;&#27010;&#29575;&#29983;&#25104;&#20989;&#25968;&#65306;&#23427;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#31243;&#24207;&#30340;&#20998;&#24067;&#30340;&#32039;&#20945;&#38381;&#21512;&#24418;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21518;&#39564;&#27010;&#29575;&#12289;&#26399;&#26395;&#12289;&#26041;&#24046;&#21644;&#39640;&#38454;&#30697;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26041;&#27861;&#26159;&#21487;&#35777;&#26126;&#27491;&#30830;&#30340;&#12289;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#65288;&#29305;&#21035;&#26159;&#27888;&#21202;&#22810;&#39033;&#24335;&#65289;&#65292;&#20294;&#19981;&#38656;&#35201;&#35745;&#31639;&#26426;&#20195;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#20013;&#30340;&#24615;&#33021;&#19982;&#36817;&#20284;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#31454;&#20105;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an exact Bayesian inference method for discrete statistical models, which can find exact solutions to many discrete inference problems, even with infinite support and continuous priors. To express such models, we introduce a probabilistic programming language that supports discrete and continuous sampling, discrete observations, affine functions, (stochastic) branching, and conditioning on events. Our key tool is probability generating functions: they provide a compact closed-form representation of distributions that are definable by programs, thus enabling the exact computation of posterior probabilities, expectation, variance, and higher moments. Our inference method is provably correct, fully automated and uses automatic differentiation (specifically, Taylor polynomials), but does not require computer algebra. Our experiments show that its performance on a range of real-world examples is competitive with approximate Monte Carlo methods, while avoiding approximation errors
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#31454;&#20105;&#21147;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.15947</link><description>&lt;p&gt;
&#38271;&#20381;&#36182;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online learning of long range dependencies. (arXiv:2305.15947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#31454;&#20105;&#21147;&#65292;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#26377;&#26395;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#38271;&#26399;&#20449;&#29992;&#20998;&#37197;&#65292;&#32780;&#24403;&#21069;&#31639;&#27861;&#35201;&#20040;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;&#65292;&#35201;&#20040;&#26080;&#27861;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20165;&#23558;&#21333;&#27425;&#25512;&#26029;&#25152;&#38656;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#32763;&#20493;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#29420;&#31435;&#24490;&#29615;&#27169;&#22359;&#21462;&#24471;&#20102;&#36825;&#20010;&#25104;&#26524;&#65292;&#36825;&#31181;&#32467;&#26500;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#38024;&#23545;&#21512;&#25104;&#35760;&#24518;&#38382;&#39064;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#31243;&#31454;&#25216;&#22330;&#22522;&#20934;&#22871;&#20214;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#65292;&#26641;&#31435;&#20102;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#26631;&#20934;&#12290;&#36825;&#31181;&#23398;&#20064;&#38271;&#31243;&#20381;&#36182;&#30340;&#33021;&#21147;&#20026;&#20102;&#35299;&#22823;&#33041;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#24320;&#36767;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online learning holds the promise of enabling efficient long-term credit assignment in recurrent neural networks. However, current algorithms fall short of offline backpropagation by either not being scalable or failing to learn long-range dependencies. Here we present a high-performance online learning algorithm that merely doubles the memory and computational requirements of a single inference pass. We achieve this by leveraging independent recurrent modules in multi-layer networks, an architectural motif that has recently been shown to be particularly powerful. Experiments on synthetic memory problems and on the challenging long-range arena benchmark suite reveal that our algorithm performs competitively, establishing a new standard for what can be achieved through online learning. This ability to learn long-range dependencies offers a new perspective on learning in the brain and opens a promising avenue in neuromorphic computing.
&lt;/p&gt;</description></item><item><title>CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14517</link><description>&lt;p&gt;
CongFu: &#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
CongFu: Conditional Graph Fusion for Drug Synergy Prediction. (arXiv:2305.14517v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14517
&lt;/p&gt;
&lt;p&gt;
CongFu&#26159;&#19968;&#31181;&#29992;&#20110;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#30340;&#26032;&#22411;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#65292;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#26159;&#25351;&#22810;&#31181;&#33647;&#29289;&#32852;&#21512;&#20316;&#29992;&#25152;&#20135;&#29983;&#30340;&#21512;&#25104;&#25928;&#24212;&#65292;&#23545;&#20110;&#20248;&#21270;&#27835;&#30103;&#32467;&#26524;&#38750;&#24120;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#25968;&#37327;&#24040;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23548;&#33268;&#33647;&#29289;&#21327;&#21516;&#25968;&#25454;&#26377;&#38480;&#65292;&#38656;&#35201;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#22270;&#34701;&#21512;&#23618;&#8212;&#8212;CongFu&#65292;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#12290;CongFu&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#29942;&#39048;&#25216;&#26415;&#26469;&#25552;&#21462;&#26412;&#22320;&#22270;&#19978;&#19979;&#25991;&#24182;&#22312;&#20840;&#23616;&#19978;&#19979;&#25991;&#20013;&#26377;&#26465;&#20214;&#22320;&#34701;&#21512;&#22270;&#25968;&#25454;&#12290;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#20351;&#24471;&#21487;&#20197;&#26356;&#25442;&#22270;&#23618;&#27169;&#22359;&#65292;&#21253;&#25324;&#35835;&#20986;&#21644;&#22270;&#32534;&#30721;&#22120;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;CongFu&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#33647;&#29289;&#21327;&#21516;&#39044;&#27979;&#35774;&#32622;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CongFu&#22312;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug synergy, characterized by the amplified combined effect of multiple drugs, presents a critical phenomenon for optimizing therapeutic outcomes. However, limited data on drug synergy, arising from the vast number of possible drug combinations and computational costs, motivate the need for predictive methods. In this work, we introduce CongFu, a novel Conditional Graph Fusion Layer, designed to predict drug synergy. CongFu employs an attention mechanism and a bottleneck to extract local graph contexts and conditionally fuse graph data within a global context. Its modular architecture enables flexible replacement of layer modules, including readouts and graph encoders, facilitating customization for diverse applications. To evaluate the performance of CongFu, we conduct comprehensive experiments on four datasets, encompassing three distinct setups for drug synergy prediction. Remarkably, CongFu achieves state-of-the-art results on 11 out of 12 benchmark datasets, demonstrating its abi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11915</link><description>&lt;p&gt;
&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#65292;PINNs&#35823;&#24046;&#20272;&#35745;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces. (arXiv:2305.11915v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#31639;&#23376;&#24418;&#24335;&#25551;&#36848;&#20102;&#19968;&#31867;&#25903;&#25345;PINN&#35823;&#24046;&#20272;&#35745;&#30340;PDE&#65292;&#24182;&#19988;&#23545;&#20110;$L^p$&#31354;&#38388;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;Bramble-Hilbert&#24341;&#29702;&#65292;&#20316;&#20026;&#19982;PINN&#27531;&#24046;&#36793;&#30028;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we describe in operator form classes of PDEs that admit PINN's error estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert type lemma that is a tool for PINN's residuals bounding.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.11531</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#26032;&#22411;&#30005;&#30913;&#37327;&#35745;&#20960;&#20309;&#27169;&#25311;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalizing to new calorimeter geometries with Geometry-Aware Autoregressive Models (GAAMs) for fast calorimeter simulation. (arXiv:2305.11531v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11531
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#24555;&#36895;&#26377;&#25928;&#22320;&#27169;&#25311;&#38750;&#29615;&#24418;&#30340;&#30005;&#30913;&#37327;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#29983;&#25104;&#23545;&#25758;&#20135;&#29289;&#30340;&#27169;&#25311;&#25506;&#27979;&#22120;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#12290;&#20854;&#20013;&#19968;&#20010;&#23376;&#25506;&#27979;&#22120;&#65292;&#30005;&#30913;&#37327;&#35745;&#30001;&#20110;&#20854;&#21333;&#20803;&#26684;&#30340;&#39640;&#31890;&#24230;&#21644;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#32780;&#21344;&#25454;&#20102;&#35745;&#31639;&#26102;&#38388;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#24555;&#30340;&#26679;&#26412;&#29983;&#25104;&#65292;&#20294;&#30446;&#21069;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#26469;&#20248;&#21270;&#29305;&#23450;&#25506;&#27979;&#22120;&#20960;&#20309;&#24418;&#29366;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#32593;&#32476;&#26469;&#25551;&#36848;&#19981;&#21516;&#30340;&#21333;&#20803;&#26684;&#22823;&#23567;&#21644;&#25490;&#21015;&#26041;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#33021;&#25512;&#24191;&#21040;&#20854;&#20182;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#23398;&#20064;&#30005;&#30913;&#37327;&#35745;&#21709;&#24212;&#22914;&#20309;&#38543;&#20960;&#20309;&#24418;&#29366;&#21464;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#20960;&#20309;&#24418;&#29366;&#30340;&#27169;&#25311;&#21709;&#24212;&#32780;&#26080;&#38656;&#20854;&#20182;&#35757;&#32451;&#12290;&#35813;&#20960;&#20309;&#24863;&#30693;&#27169;&#22411;&#22312;&#28041;&#21450;&#20851;&#38190;&#21709;&#24212;&#30340;&#29983;&#25104;&#21644;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#31561;&#25351;&#26631;&#19978;&#27604;&#22522;&#32447;&#27169;&#22411;&#20248;&#36234;50&#65285;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#27169;&#25311;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#25193;&#23637;&#21040;&#38750;&#24179;&#38754;&#20960;&#20309;&#24418;&#29366;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of simulated detector response to collision products is crucial to data analysis in particle physics, but computationally very expensive. One subdetector, the calorimeter, dominates the computational time due to the high granularity of its cells and complexity of the interaction. Generative models can provide more rapid sample production, but currently require significant effort to optimize performance for specific detector geometries, often requiring many networks to describe the varying cell sizes and arrangements, which do not generalize to other geometries. We develop a {\it geometry-aware} autoregressive model, which learns how the calorimeter response varies with geometry, and is capable of generating simulated responses to unseen geometries without additional training. The geometry-aware model outperforms a baseline, unaware model by 50\% in metrics such as the Wasserstein distance between generated and true distributions of key quantities which summarize the simulate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;</title><link>http://arxiv.org/abs/2305.08463</link><description>&lt;p&gt;
&#22343;&#20540;&#28418;&#31227;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Mean Shift. (arXiv:2305.08463v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#20540;&#28418;&#31227;&#65288;MS&#65289;&#31639;&#27861;&#23547;&#25214;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#30340;&#27169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;MS&#31639;&#27861;&#20135;&#29983;&#30340;&#27169;&#20272;&#35745;&#24207;&#21015;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22312;&#30456;&#24403;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#20511;&#21161;&#20110;&#20851;&#20110;{\L}ojasiewicz&#19981;&#31561;&#24335;&#30340;&#35770;&#35777;&#65292;&#35780;&#20272;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#28085;&#30422;&#35299;&#26512;&#26680;&#21644;Epanechnikov&#26680;&#30340;&#21457;&#29616;&#65292;&#24847;&#20041;&#22312;&#20110;&#28085;&#30422;&#20102;&#22312;&#22522;&#20110;KDE&#30340;&#27169;&#20272;&#35745;&#30340;&#28176;&#36817;&#32479;&#35745;&#25928;&#29575;&#26041;&#38754;&#26368;&#20248;&#30340;&#38750;&#36127;&#26680;&#8212;&#8212;&#21452;&#37325;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mean shift (MS) algorithm seeks a mode of the kernel density estimate (KDE). This study presents a convergence guarantee of the mode estimate sequence generated by the MS algorithm and an evaluation of the convergence rate, under fairly mild conditions, with the help of the argument concerning the {\L}ojasiewicz inequality. Our findings, which extend existing ones covering analytic kernels and the Epanechnikov kernel, are significant in that they cover the biweight kernel that is optimal among non-negative kernels in terms of the asymptotic statistical efficiency for the KDE-based mode estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.13972</link><description>&lt;p&gt;
&#26494;&#24347;&#20551;&#35774;&#19979;Adam&#25910;&#25947;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Convergence of Adam Under Relaxed Assumptions. (arXiv:2304.13972v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#31639;&#27861;&#20570;&#20102;&#26032;&#30340;&#20551;&#35774;&#24182;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#21152;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#22815;&#20197;&#36739;&#23567;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#36798;&#21040;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31867;&#24191;&#27867;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#23545;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;Adam&#65289;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20005;&#26684;&#35777;&#26126;&#12290;&#34429;&#28982;Adam&#31639;&#27861;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27969;&#34892;&#24230;&#21644;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#20854;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#38656;&#35201;&#36807;&#20110;&#24378;&#30340;&#20551;&#35774;&#65292;&#22914;&#20840;&#23616;&#26799;&#24230;&#26377;&#30028;&#65292;&#20197;&#35777;&#26126;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26356;&#20026;&#29616;&#23454;&#30340;&#26465;&#20214;&#19979;&#65292;Adam&#33021;&#20197;$\mathcal{O}(\epsilon^{-4})$&#26799;&#24230;&#22797;&#26434;&#24230;&#25910;&#25947;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#26159;&#26681;&#25454;&#19968;&#31181;&#24191;&#20041;&#20809;&#28369;&#24615;&#20551;&#35774;&#32473;&#20986;&#30340;&#65292;&#27839;&#30528;&#20248;&#21270;&#36712;&#36857;&#30340;&#26799;&#24230;&#26377;&#30028;&#30340;&#26032;&#35777;&#26126;&#12290;&#26681;&#25454;&#35813;&#20551;&#35774;&#65292;&#23616;&#37096;&#20809;&#28369;&#24615;(&#21363;&#23384;&#22312;&#26102;&#30340;Hessian norm)&#21463;&#26799;&#24230;&#33539;&#25968;&#30340;&#27425;&#24179;&#26041;&#20989;&#25968;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#32422;&#20943;&#29256;&#26412;&#30340;Adam&#19982;&#21152;&#36895;Gradient&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a rigorous proof of convergence of the Adaptive Moment Estimate (Adam) algorithm for a wide class of optimization objectives. Despite the popularity and efficiency of the Adam algorithm in training deep neural networks, its theoretical properties are not yet fully understood, and existing convergence proofs require unrealistically strong assumptions, such as globally bounded gradients, to show the convergence to stationary points. In this paper, we show that Adam provably converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more realistic conditions. The key to our analysis is a new proof of boundedness of gradients along the optimization trajectory, under a generalized smoothness assumption according to which the local smoothness (i.e., Hessian norm when it exists) is bounded by a sub-quadratic function of the gradient norm. Moreover, we propose a variance-reduced version of Adam with an accelerated gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13105</link><description>&lt;p&gt;
&#21033;&#29992;&#23460;&#20869;WiFi&#31995;&#32479;&#36827;&#34892;&#26080;&#35774;&#22791;&#31359;&#22681;&#23384;&#22312;&#26816;&#27979;&#30340;&#27880;&#24847;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;&#20851;&#27880;&#26426;&#21046;&#21644;&#21452;&#21521;LSTM&#32593;&#32476;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#26816;&#27979;&#20154;&#21592;&#23384;&#22312;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#33021;&#28304;&#31649;&#29702;&#21644;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;WiFi&#20449;&#21495;&#30340;&#36890;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#36827;&#34892;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#26032;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21517;&#20026;&#27880;&#24847;&#21147;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#65288;ALPD&#65289;&#65292;&#37319;&#29992;&#20851;&#27880;&#26426;&#21046;&#20174;CSI&#25968;&#25454;&#20013;&#33258;&#21160;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#23376;&#36733;&#27874;&#65292;&#24182;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25429;&#25417;CSI&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#38745;&#24577;&#29305;&#24449;&#26469;&#25552;&#39640;&#38745;&#24577;&#29366;&#24577;&#19979;&#20154;&#21592;&#23384;&#22312;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#19968;&#23545;WiFi&#25509;&#20837;&#28857;&#65288;AP&#65289;&#26469;&#25910;&#38598;CSI&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;ALPD&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36827;&#19968;&#27493;&#19982;&#20960;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ALPD&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#21452;&#21521;&#20256;&#36755;&#25968;&#25454;&#19981;&#20250;&#24433;&#21709;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate detection of human presence in indoor environments is important for various applications, such as energy management and security. In this paper, we propose a novel system for human presence detection using the channel state information (CSI) of WiFi signals. Our system named attention-enhanced deep learning for presence detection (ALPD) employs an attention mechanism to automatically select informative subcarriers from the CSI data and a bidirectional long short-term memory (LSTM) network to capture temporal dependencies in CSI. Additionally, we utilize a static feature to improve the accuracy of human presence detection in static states. We evaluate the proposed ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI dataset, which is further compared with several benchmarks. The results demonstrate that our ALPD system outperforms the benchmarks in terms of accuracy, especially in the presence of interference. Moreover, bidirectional transmission data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.03408</link><description>&lt;p&gt;
&#26377;&#38480;&#23485;&#24230;&#26680;&#21644;&#24179;&#22343;&#22330;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#27874;&#21160;&#21160;&#21147;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks. (arXiv:2304.03408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;&#27874;&#21160;&#30340;&#34920;&#24449;&#20197;&#21450;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#23485;&#20294;&#26377;&#38480;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#38480;&#23485;&#24230;&#25928;&#24212;&#30340;&#21160;&#21147;&#23398;&#12290;&#19982;&#35768;&#22810;&#20808;&#21069;&#30340;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#38024;&#23545;&#29305;&#24449;&#23398;&#20064;&#24378;&#24230;&#30340;&#38750;&#24494;&#25200;&#26377;&#38480;&#23485;&#24230;&#30340;&#32467;&#26524;&#12290;&#20174;&#26080;&#38480;&#23485;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26680;&#21644;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#21160;&#21147;&#23398;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#26435;&#37325;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;DMFT&#24207;&#21442;&#25968;$\mathcal{O}(1/\sqrt{\text{width}})$&#27874;&#21160;&#30340;&#34920;&#24449;&#12290;&#22312;&#32593;&#32476;&#35757;&#32451;&#30340;&#25042;&#24816;&#26497;&#38480;&#20013;&#65292;&#25152;&#26377;&#26680;&#37117;&#26159;&#38543;&#26426;&#30340;&#20294;&#22312;&#26102;&#38388;&#19978;&#38745;&#27490;&#30340;&#65292;&#39044;&#27979;&#26041;&#24046;&#20855;&#26377;&#36890;&#29992;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#23500;&#26377;&#29305;&#24449;&#23398;&#20064;&#30340;&#21306;&#22495;&#65292;&#26680;&#21644;&#39044;&#27979;&#30340;&#27874;&#21160;&#26159;&#21160;&#24577;&#32806;&#21512;&#19988;&#26041;&#24046;&#21487;&#20197;&#34987;&#33258;&#27965;&#35745;&#31639;&#12290;&#22312;&#20004;&#23618;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#22914;&#20309;&#21160;&#24577;&#22320;&#20943;&#23569;&#26368;&#32456;NTK&#21644;&#26368;&#32456;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Unlike many prior analyses, our results, while perturbative in width, are non-perturbative in the strength of feature learning. Starting from a dynamical mean field theory (DMFT) description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the $\mathcal{O}(1/\sqrt{\text{width}})$ fluctuations of the DMFT order parameters over random initialization of the network weights. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final NTK and final network predictions. We also show how initialization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.09354</link><description>&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65306;&#35745;&#31639;&#30149;&#29702;&#23398;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09354
&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488; (IDC) &#26088;&#22312;&#20419;&#36827;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65292;&#25552;&#20379;&#20844;&#20849;&#24211;&#21644;&#20113;&#31471;&#25216;&#26415;&#25903;&#25345;&#65292;&#26041;&#20415;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30284;&#30151;&#32452;&#32455;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#23558;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CompPath&#65289;&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#36716;&#21270;&#20026;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25253;&#21578;&#38590;&#20197;&#37325;&#22797; ML &#32467;&#26524;&#30340;&#22256;&#38590;&#12290;&#22269;&#23478;&#30284;&#30151;&#30740;&#31350;&#25152;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#65288;IDC&#65289;&#26159;&#19968;&#20010;&#20844;&#20849;&#24211;&#65292;&#21253;&#21547; &gt;120 &#20010;&#30284;&#30151;&#22270;&#20687;&#25910;&#38598;&#65292;&#21253;&#25324; &gt;38,000 &#24352;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#26088;&#22312;&#19982;&#20113;&#31471; ML &#26381;&#21153;&#19968;&#36215;&#20351;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102; IDC &#20419;&#36827; CompPath &#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#28508;&#21147;&#12290; &#26448;&#26009;&#21644;&#26041;&#27861;&#65306;IDC &#23454;&#29616;&#20102; FAIR &#21407;&#21017;&#65306;&#25152;&#26377;&#22270;&#20687;&#37117;&#26681;&#25454; DICOM &#26631;&#20934;&#36827;&#34892;&#32534;&#30721;&#65292;&#20855;&#26377;&#25345;&#20037;&#21270;&#26631;&#35782;&#31526;&#12289;&#21487;&#36890;&#36807;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#36827;&#34892;&#21457;&#29616;&#65292;&#24182;&#21487;&#36890;&#36807;&#24320;&#25918;&#24335;&#24037;&#20855;&#35775;&#38382;&#12290;&#20511;&#27492;&#20248;&#21183;&#65292;&#25105;&#20204;&#22312; IDC &#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#38024;&#23545;&#32954;&#30284;&#32452;&#32455;&#20998;&#31867;&#30340;&#19968;&#31181;&#20195;&#34920;&#24615;&#22522;&#20110; ML &#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#12290;&#20026;&#35780;&#20272;&#21487;&#37325;&#22797;&#24615;&#65292;&#23454;&#39564;&#34987;&#22810;&#27425;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of &gt;120 cancer image collections, including &gt;38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09702</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#21512;&#25104;&#36827;&#34892;&#20809;&#29031;&#21464;&#21270;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#20154;&#29289;&#20877;&#35782;&#21035;&#26088;&#22312;&#20174;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#22270;&#20687;&#20013;&#23398;&#20064;&#36523;&#20221;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#22270;&#20687;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#20877;&#35782;&#21035;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#23427;&#20204;&#22312;&#22823;&#30340;&#39046;&#22495;&#21464;&#21270;&#65288;&#22914;&#20809;&#29031;&#12289;&#35270;&#35282;&#21644;&#36974;&#25377;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#27169;&#22411;&#24211;&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;SMB&#21253;&#25324;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#30340;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#29992;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#12290;&#23427;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;SMB&#23545;&#20809;&#29031;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#37327;&#21270;&#20809;&#29031;&#24378;&#24230;&#24182;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#26032;&#22411;&#19977;&#32500;&#34394;&#25311;&#20154;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.01346</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#30340;&#33258;&#28982;&#32422;&#26463;&#25285;&#20445;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Conformance of Neurosymbolic Models to Natural Constraints. (arXiv:2212.01346v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01346
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25285;&#20445;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31526;&#21512;&#33258;&#28982;&#31185;&#23398;&#24050;&#26377;&#30693;&#35782;&#24182;&#26368;&#20248;&#36924;&#36817;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#21644;&#25511;&#21046;&#24212;&#29992;&#30340;&#20027;&#35201;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#31867;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21448;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21644;&#39564;&#35777;&#33258;&#20027;&#31995;&#32479;&#12290;&#22312;&#21307;&#30103;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#23588;&#20854;&#26377;&#29992;&#65292;&#22240;&#20026;&#25968;&#25454;&#33021;&#22815;&#34987;&#29992;&#20110;&#20010;&#20307;&#21270;&#27835;&#30103;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23545;&#26469;&#33258;&#33258;&#28982;&#31185;&#23398;&#30340;&#24050;&#26377;&#30693;&#35782;&#30340;&#31526;&#21512;&#24615;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#65292;&#25110;&#32773;&#21487;&#20197;&#34987;&#27987;&#32553;&#25104;&#65288;&#21487;&#33021;&#26159;&#40657;&#30418;&#30340;&#65289;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;F1&#36187;&#36710;&#24212;&#31526;&#21512;&#29275;&#39039;&#23450;&#24459;&#65288;&#36825;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#21333;&#36718;&#27169;&#22411;&#20013;&#65289;&#12290;&#37492;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#8212;&#8212;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;M&#21644;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#36317;&#31163;M&#30340;&#33539;&#22260;&#20869;&#26368;&#22909;&#22320;&#36817;&#20284;&#31995;&#32479;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#25285;&#20445;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#23558;&#25968;&#25454;&#38598;&#27987;&#32553;&#25104;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#26679;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have emerged as the workhorse for a large section of robotics and control applications, especially as models for dynamical systems. Such data-driven models are in turn used for designing and verifying autonomous systems. They are particularly useful in modeling medical systems where data can be leveraged to individualize treatment. In safety-critical applications, it is important that the data-driven model is conformant to established knowledge from the natural sciences. Such knowledge is often available or can often be distilled into a (possibly black-box) model. For instance, an F1 racing car should conform to Newton's laws (which are encoded within a unicycle model). In this light, we consider the following problem - given a model $M$ and a state transition dataset, we wish to best approximate the system model while being a bounded distance away from $M$. We propose a method to guarantee this conformance. Our first step is to distill the dataset into a few repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#23384;&#22312;&#25361;&#25112;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#31639;&#27861;&#21644;&#23616;&#37096;&#25910;&#25947;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16583</link><description>&lt;p&gt;
&#22312;&#28151;&#28102;&#19979;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#28151;&#28102;&#21464;&#37327;&#23548;&#33268;&#31574;&#30053;&#35780;&#20272;&#21644;&#20248;&#21270;&#23384;&#22312;&#25361;&#25112;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#31639;&#27861;&#21644;&#23616;&#37096;&#25910;&#25947;&#30340;&#25913;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35780;&#20272;&#21644;&#20248;&#21270;&#31574;&#30053;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#26102;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#31967;&#31957;&#30340;&#20915;&#31574;&#21644;&#31574;&#30053;&#65292;&#32780;&#19988;&#22312;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#25945;&#32946;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21246;&#21202;&#20102;&#28151;&#28102;&#30340; MDP &#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#30340;&#38754;&#35980;&#65292;&#24182;&#26681;&#25454;&#28151;&#28102;&#23545;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#26102;&#38388;&#28436;&#21464;&#21644;&#24433;&#21709;&#26469;&#21306;&#20998;&#28151;&#28102;&#30340;&#20551;&#35774;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26080;&#27861;&#33719;&#24471;&#19968;&#33268;&#20215;&#20540;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#21644;&#35752;&#35770;&#20102;&#35745;&#31639;&#20855;&#26377;&#20445;&#35777;&#30340;&#19979;&#38480;&#30340;&#31639;&#27861;&#12290;&#24403;&#19968;&#33268;&#30340;&#20272;&#35745;&#21487;&#34892;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23616;&#37096;&#25910;&#25947;&#30340;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26684;&#23376;&#19990;&#30028;&#21644;&#27169;&#25311;&#21307;&#30103;&#22330;&#26223;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#26657;&#20934;&#26041;&#27861;RCAL&#26469;&#35299;&#20915;&#22312;&#22024;&#26434;&#26631;&#31614;&#21644;&#38271;&#23614;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;RCAL&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#30340;&#34920;&#31034;&#65292;&#24182;&#20551;&#35774;&#23454;&#20363;&#30340;&#34920;&#31034;&#31526;&#21512;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.10955</link><description>&lt;p&gt;
&#24403;&#22024;&#26434;&#26631;&#31614;&#36935;&#19978;&#38271;&#23614;&#22256;&#22659;&#65306;&#19968;&#31181;&#34920;&#31034;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method. (arXiv:2211.10955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#26657;&#20934;&#26041;&#27861;RCAL&#26469;&#35299;&#20915;&#22312;&#22024;&#26434;&#26631;&#31614;&#21644;&#38271;&#23614;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;RCAL&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#30340;&#34920;&#31034;&#65292;&#24182;&#20551;&#35774;&#23454;&#20363;&#30340;&#34920;&#31034;&#31526;&#21512;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26082;&#23384;&#22312;&#22024;&#26434;&#30340;&#26631;&#31614;&#65292;&#21448;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#20102;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#21516;&#26102;&#23384;&#22312;&#38169;&#35823;&#26631;&#35760;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#21363;&#22122;&#22768;&#26631;&#31614;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30340;&#23398;&#20064;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24635;&#26159;&#20381;&#36182;&#20110;&#22312;&#23454;&#36341;&#20013;&#26080;&#25928;&#25110;&#38590;&#20197;&#26816;&#26597;&#30340;&#24378;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#35299;&#20915;&#20808;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#26657;&#20934;&#26041;&#27861;RCAL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RCAL&#20351;&#29992;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#30340;&#34920;&#31034;&#36827;&#34892;&#24037;&#20316;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#27809;&#26377;&#38169;&#35823;&#26631;&#35760;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#31867;&#21035;&#20013;&#23454;&#20363;&#30340;&#34920;&#31034;&#31526;&#21512;&#22810;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#65292;&#36825;&#31181;&#20998;&#24067;&#26356;&#21152;&#28201;&#21644;&#19988;&#26356;&#23481;&#26131;&#26816;&#26597;&#12290;&#22522;&#20110;&#35813;&#20551;&#35774;&#65292;&#25105;&#20204;&#20174;&#34987;&#27745;&#26579;&#30340;&#34920;&#31034;&#20013;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#34920;&#31034;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.02671</link><description>&lt;p&gt;
&#34920;&#36798;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#30340;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A Logic for Expressing Log-Precision Transformers. (arXiv:2210.02671v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#25551;&#36848;&#23427;&#20204;&#21487;&#20197;&#22312;&#26576;&#20123;&#36755;&#20837;&#25991;&#26412;&#19978;&#35299;&#20915;&#30340;&#36923;&#36753;&#35268;&#21017;&#31867;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#22312;&#38271;&#24230;&#20026;n&#30340;&#35821;&#22659;&#19978;&#35745;&#31639;&#21069;&#21521;&#20256;&#36882;&#30340;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26631;&#20934;&#30340;&#20840;&#31216;&#21644;&#23384;&#22312;&#37327;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to interpret the reasoning power of transformer-based language models is to describe the types of logical rules they can resolve over some input text. Recently, Chiang et al. (2023) showed that finite-precision transformers can be equivalently expressed in a generalization of first-order logic. However, finite-precision transformers are a weak transformer variant because, as we show, a single head can only attend to a constant number of tokens and, in particular, cannot represent uniform attention. Since attending broadly is a core capability for transformers, we ask whether a minimally more expressive model that can attend universally can also be characterized in logic. To this end, we analyze transformers whose forward pass is computed in $\log n$ precision on contexts of length $n$. We prove that any log-precision transformer can be equivalently expressed as a first-order logic sentence that, in addition to standard universal and existential quantifiers, may also contain maj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.10540</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31185;&#23398;&#21457;&#29616;&#20013;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#24182;&#37325;&#26032;&#21019;&#24314;&#20102;&#31526;&#21495;&#22238;&#24402;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#21644;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#36827;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#22522;&#20110;&#36153;&#26364;&#29289;&#29702;&#35762;&#20041;&#30340;&#19968;&#32452;&#20844;&#24335;&#65292;&#25105;&#20204;&#37325;&#26032;&#21019;&#24314;&#20102;120&#20010;&#25968;&#25454;&#38598;&#65292;&#35752;&#35770;&#31526;&#21495;&#22238;&#24402;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24615;&#33021;&#65288;SRSD&#65289;&#12290;&#23545;&#20110;&#36825;120&#20010;SRSD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#20844;&#24335;&#21450;&#20854;&#21464;&#37327;&#30340;&#23646;&#24615;&#65292;&#35774;&#35745;&#20102;&#21512;&#29702;&#30340;&#23454;&#20540;&#33539;&#22260;&#26469;&#37319;&#26679;&#20540;&#65292;&#20197;&#20415;&#25105;&#20204;&#30340;&#26032;SRSD&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;SRSD&#30340;&#28508;&#21147;&#65292;&#22914;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20013;&#65288;&#37325;&#26032;&#65289;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#22806;120&#20010;&#21253;&#21547;&#34394;&#25311;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#26816;&#39564;SR&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20165;&#36873;&#25321;&#24517;&#35201;&#21464;&#37327;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#26041;&#31243;&#19982;&#30495;&#23454;&#26041;&#31243;&#26641;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#32534;&#36753;&#36317;&#31163;&#65288;NED&#65289;&#26469;&#35299;&#20915;&#29616;&#26377;SR&#24230;&#37327;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#20108;&#20803;&#24230;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
&lt;/p&gt;</description></item><item><title>FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.10581</link><description>&lt;p&gt;
FAIR4Cov&#65306;&#29992;&#20110; COVID-19 &#26816;&#27979;&#30340;&#34701;&#21512;&#38899;&#39057;&#23454;&#20363;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov: Fused Audio Instance and Representation for COVID-19 Detection. (arXiv:2204.10581v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10581
&lt;/p&gt;
&lt;p&gt;
FAIR4Cov&#26159;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;COVID-19&#24739;&#32773;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#22768;&#38899;&#30340;&#20998;&#31867;&#25216;&#26415;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#30740;&#31350;&#29992;&#20110;&#25903;&#25345;&#35786;&#26029;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#22312;&#32954;&#37096;&#30142;&#30149;&#26041;&#38754;&#12290;&#38024;&#23545; COVID-19 &#30123;&#24773;&#30340;&#32039;&#36843;&#24615;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27169;&#22411;&#34987;&#24320;&#21457;&#26469;&#22522;&#20110;&#22768;&#23398;&#36755;&#20837;&#35782;&#21035; COVID-19 &#24739;&#32773;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#20391;&#37325;&#20110;&#21683;&#22013;&#65292;&#22240;&#20026;&#24178;&#21683;&#26159; COVID-19 &#26368;&#20026;&#20154;&#25152;&#30693;&#30340;&#30151;&#29366;&#12290;&#28982;&#32780;&#65292;&#21628;&#21560;&#21644;&#35328;&#35821;&#31561;&#20854;&#20182;&#36523;&#20307;&#22768;&#38899;&#20063;&#34987;&#21457;&#29616;&#19982; COVID-19 &#30456;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FAIR4Cov&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36523;&#20307;&#22768;&#38899;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#36523;&#20307;&#22768;&#38899;&#30340;&#27874;&#24418;&#21644;&#35889;&#22270;&#34920;&#31034;&#30340;&#20851;&#33410;&#29305;&#24449;&#21521;&#37327;&#12290;FAIR4Cov &#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#34701;&#21512;&#21333;&#20803;&#65292;&#23427;&#30340;&#35757;&#32451;&#30446;&#30340;&#26159;&#24314;&#31435;&#22810;&#20010;&#36523;&#20307;&#22768;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#30340;&#20851;&#31995;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35774;&#32622;&#20102;&#23454;&#39564;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#65292;&#21253;&#25324;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#26089;&#26399;&#26816;&#27979;&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FAIR4Cov &#32988;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#21508;&#31181;&#36523;&#20307;&#22768;&#38899;&#26816;&#27979; COVID-19 &#24739;&#32773;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-based classification techniques on body sounds have long been studied to support diagnostic decisions, particularly in pulmonary diseases. In response to the urgency of the COVID-19 pandemic, a growing number of models are developed to identify COVID-19 patients based on acoustic input. Most models focus on cough because the dry cough is the best-known symptom of COVID-19. However, other body sounds, such as breath and speech, have also been revealed to correlate with COVID-19 as well. In this work, rather than relying on a specific body sound, we propose Fused Audio Instance and Representation for COVID-19 Detection (FAIR4Cov). It relies on constructing a joint feature vector obtained from a plurality of body sounds in waveform and spectrogram representation. The core component of FAIR4Cov is a self-attention fusion unit that is trained to establish the relation of multiple body sounds and audio representations and integrate it into a compact feature vector. We set up our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2203.03897</link><description>&lt;p&gt;
&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29992;&#20110;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Geodesic Multi-Modal Mixup for Robust Fine-Tuning. (arXiv:2203.03897v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#20854;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#23884;&#20837;&#65292;&#24182;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#23398;&#20064;&#21040;&#30340;&#22810;&#27169;&#22411;&#23884;&#20837;&#30340;&#20998;&#26512;&#30456;&#23545;&#36739;&#23569;&#65292;&#23884;&#20837;&#30340;&#21487;&#36716;&#31227;&#24615;&#26377;&#24453;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;CLIP&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20445;&#30041;&#20102;&#20998;&#31163;&#30340;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#23545;&#40784;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;CLIP&#20173;&#28982;&#20445;&#25345;&#30528;&#36739;&#24046;&#30340;&#32479;&#19968;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#21487;&#33021;&#38480;&#21046;&#20102;&#23884;&#20837;&#30340;&#20256;&#36882;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#40065;&#26834;&#34920;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23545;&#40784;&#21644;&#32479;&#19968;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20960;&#20309;&#22810;&#27169;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#23884;&#20837;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#22312;&#36229;&#29699;&#38754;&#19978;&#29983;&#25104;&#38590;&#36127;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained multi-modal models, such as CLIP, provide transferable embeddings and show promising results in diverse applications. However, the analysis of learned multi-modal embeddings is relatively unexplored, and the embedding transferability can be improved. In this work, we observe that CLIP holds separated embedding subspaces for two different modalities, and then we investigate it through the lens of uniformity-alignment to measure the quality of learned representation. Both theoretically and empirically, we show that CLIP retains poor uniformity and alignment even after fine-tuning. Such a lack of alignment and uniformity might restrict the transferability and robustness of embeddings. To this end, we devise a new fine-tuning method for robust representation equipping better alignment and uniformity. First, we propose a Geodesic Multi-Modal Mixup that mixes the embeddings of image and text to generate hard negative samples on the hypersphere. Then, we fine-tune the model on har
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.08440</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65306;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#22312;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#27668;&#20505;&#20013;&#23545;&#26368;&#36817;&#30340;&#36807;&#21435;&#36827;&#34892;&#22806;&#25512;&#12290;&#30446;&#21069;&#30340;&#27668;&#20505;&#27169;&#22411;&#38656;&#35201;&#23545;&#23567;&#20110;&#27169;&#22411;&#32593;&#26684;&#22823;&#23567;&#30340;&#23610;&#24230;&#19978;&#21457;&#29983;&#30340;&#36807;&#31243;&#36827;&#34892;&#34920;&#31034;&#65292;&#36825;&#20123;&#36807;&#31243;&#26159;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26377;&#26395;&#25913;&#21892;&#36825;&#31181;&#36807;&#31243;&#34920;&#31034;&#65292;&#20294;&#24448;&#24448;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#27668;&#20505;&#29615;&#22659;&#20013;&#22806;&#25512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#29289;&#29702;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#8212;&#8212;&#31216;&#20026;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;ML&#31639;&#27861;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22823;&#27668;&#27169;&#22411;&#20013;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#22320;&#29699;&#31995;&#32479;&#36807;&#31243;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
&lt;/p&gt;</description></item></channel></rss>