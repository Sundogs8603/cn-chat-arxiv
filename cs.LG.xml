<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.</title><link>https://arxiv.org/abs/2403.04764</link><description>&lt;p&gt;
&#23558;Thompson&#25277;&#26679;&#36951;&#25022;&#19982;Sigma&#27604;&#29575;&#65288;TS-RSR&#65289;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32463;&#36807;&#35777;&#26126;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#20854;&#20013;&#25277;&#26679;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#26041;&#27861;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33021;&#22815;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#20013;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#20197;&#26368;&#23567;&#21270;&#28857;&#20043;&#38388;&#30340;&#20887;&#20313;&#65292;&#21516;&#26102;&#20851;&#27880;&#20855;&#26377;&#39640;&#39044;&#27979;&#22343;&#20540;&#25110;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#36951;&#25022;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#20174;&#25968;&#23383;&#19978;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24179;&#22343;&#20540;&#19978;&#27604;&#20960;&#20010;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#25209;&#37327;BO&#31639;&#27861;&#34920;&#29616;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21508;&#31181;&#22270;&#23398;&#20064;&#25216;&#26415;&#37325;&#26032;&#26500;&#24314;&#20026;&#21452;&#23618;&#20248;&#21270;&#30340;&#29305;&#20363;&#25110;&#31616;&#21270;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#28789;&#27963;&#30340;&#33021;&#37327;&#20989;&#25968;&#20197;&#24418;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#27531;&#20313;&#36817;&#20284;&#35823;&#24046;&#30340;&#26469;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.04763</link><description>&lt;p&gt;
BloomGML: &#36879;&#36807;&#21452;&#23618;&#20248;&#21270;&#38236;&#22836;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BloomGML: Graph Machine Learning through the Lens of Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21508;&#31181;&#22270;&#23398;&#20064;&#25216;&#26415;&#37325;&#26032;&#26500;&#24314;&#20026;&#21452;&#23618;&#20248;&#21270;&#30340;&#29305;&#20363;&#25110;&#31616;&#21270;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#28789;&#27963;&#30340;&#33021;&#37327;&#20989;&#25968;&#20197;&#24418;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#27531;&#20313;&#36817;&#20284;&#35823;&#24046;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#26159;&#25351;&#22312;&#19968;&#20010;&#20302;&#23618;&#27425;&#33021;&#37327;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#29992;&#20316;&#24863;&#20852;&#36259;&#30340;&#19978;&#23618;&#30446;&#26631;&#30340;&#36755;&#20837;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#26368;&#20248;&#29305;&#24449;&#36890;&#24120;&#20381;&#36182;&#20110;&#20302;&#23618;&#27425;&#33021;&#37327;&#30340;&#21487;&#35843;&#21442;&#25968;&#65292;&#20351;&#24471;&#25972;&#20010;&#21452;&#23618;&#31649;&#36947;&#21487;&#20197;&#31471;&#23545;&#31471;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#24120;&#26410;&#34987;&#25552;&#20986;&#65292;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#21508;&#31181;&#22270;&#23398;&#20064;&#25216;&#26415;&#37325;&#26032;&#26500;&#24314;&#20026;&#21452;&#23618;&#20248;&#21270;&#30340;&#29305;&#20363;&#25110;&#31616;&#21270;&#24418;&#24335;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22522;&#20110;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#19968;&#31867;&#26356;&#28789;&#27963;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#24403;&#19982;&#21508;&#31181;&#19979;&#38477;&#27493;&#39588;&#37197;&#23545;&#65288;&#20363;&#22914;&#26799;&#24230;&#19979;&#38477;&#12289;&#36817;&#31471;&#26041;&#27861;&#12289;&#21160;&#37327;&#31561;&#65289;&#65292;&#24418;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#28040;&#24687;&#20256;&#36882;&#23618;&#65307;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#36824;&#20180;&#32454;&#35299;&#37322;&#20102;&#19982;&#24213;&#23618;&#20256;&#36882;&#20989;&#25968;&#26377;&#20851;&#30340;&#20219;&#20309;&#21097;&#20313;&#36817;&#20284;&#35823;&#24046;&#25152;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20960;&#20010;&#30456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04763v1 Announce Type: new  Abstract: Bilevel optimization refers to scenarios whereby the optimal solution of a lower-level energy function serves as input features to an upper-level objective of interest. These optimal features typically depend on tunable parameters of the lower-level energy in such a way that the entire bilevel pipeline can be trained end-to-end. Although not generally presented as such, this paper demonstrates how a variety of graph learning techniques can be recast as special cases of bilevel optimization or simplifications thereof. In brief, building on prior work we first derive a more flexible class of energy functions that, when paired with various descent steps (e.g., gradient descent, proximal methods, momentum, etc.), form graph neural network (GNN) message-passing layers; critically, we also carefully unpack where any residual approximation error lies with respect to the underlying constituent message-passing functions. We then probe several sim
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;iScore&#24037;&#20855;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04760</link><description>&lt;p&gt;
iScore: &#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04760
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;iScore&#24037;&#20855;&#65292;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24613;&#21095;&#22686;&#38271;&#65292;&#28608;&#21457;&#20102;&#23398;&#20064;&#24037;&#31243;&#24072;&#23558;&#23427;&#20204;&#32435;&#20837;&#33258;&#36866;&#24212;&#25945;&#32946;&#24037;&#20855;&#20013;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#25688;&#35201;&#20889;&#20316;&#12290;&#22312;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#20851;&#38190;&#23398;&#20064;&#29615;&#22659;&#20043;&#21069;&#65292;&#20102;&#35299;&#21644;&#35780;&#20272;LLMs&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#23427;&#20204;&#25968;&#37327;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#26085;&#30410;&#22686;&#21152;&#30340;&#35268;&#27169;&#38459;&#30861;&#20102;&#36879;&#26126;&#24230;&#65292;&#24403;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#26102;&#36824;&#20250;&#24433;&#21709;&#20449;&#20219;&#12290;&#36890;&#36807;&#19982;&#22810;&#20301;&#26500;&#24314;&#21644;&#37096;&#32626;&#25688;&#35201;&#35780;&#20998;LLMs&#30340;&#23398;&#20064;&#24037;&#31243;&#24072;&#23637;&#24320;&#21327;&#20316;&#30340;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#30028;&#23450;&#20102;&#35299;&#37322;&#20854;&#27169;&#22411;&#30340;&#22522;&#26412;&#35774;&#35745;&#25361;&#25112;&#21644;&#30446;&#26631;&#65292;&#21253;&#25324;&#25972;&#21512;&#22823;&#37327;&#25991;&#26412;&#36755;&#20837;&#12289;&#36319;&#36394;&#24471;&#20998;&#26469;&#28304;&#20197;&#21450;&#25193;&#23637;LLM&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#20182;&#20204;&#30340;&#20851;&#20999;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;iScore&#65292;&#19968;&#27454;&#20114;&#21160;&#24335;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#20379;&#23398;&#20064;&#24037;&#31243;&#24072;&#21516;&#26102;&#19978;&#20256;&#12289;&#35780;&#20998;&#21644;&#27604;&#36739;&#22810;&#20010;&#25688;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#32039;&#23494;&#38598;&#25104;&#30340;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04760v1 Announce Type: cross  Abstract: The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#37096;&#32626;&#20102;&#21517;&#20026;LifeHD&#30340;&#39318;&#20010;&#38754;&#21521;&#36890;&#29992;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#32456;&#29983;&#25104;&#38271;&#23398;&#20064;&#31995;&#32479;&#65292;&#22522;&#20110;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#23398;&#20064;&#33539;&#24335;&#36229;&#39640;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#20004;&#23618;&#32852;&#24819;&#35760;&#24518;&#32452;&#32455;&#26234;&#33021;&#23384;&#20648;&#21644;&#31649;&#29702;&#39640;&#32500;&#12289;&#20302;&#31934;&#24230;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#23545;&#21382;&#21490;&#27169;&#24335;&#36827;&#34892;&#26080;&#38480;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04759</link><description>&lt;p&gt;
&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#23454;&#29616;&#36793;&#32536;&#20197;&#22806;&#30340;&#32456;&#29983;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#37096;&#32626;&#20102;&#21517;&#20026;LifeHD&#30340;&#39318;&#20010;&#38754;&#21521;&#36890;&#29992;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#32456;&#29983;&#25104;&#38271;&#23398;&#20064;&#31995;&#32479;&#65292;&#22522;&#20110;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#23398;&#20064;&#33539;&#24335;&#36229;&#39640;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#20004;&#23618;&#32852;&#24819;&#35760;&#24518;&#32452;&#32455;&#26234;&#33021;&#23384;&#20648;&#21644;&#31649;&#29702;&#39640;&#32500;&#12289;&#20302;&#31934;&#24230;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#23545;&#21382;&#21490;&#27169;&#24335;&#36827;&#34892;&#26080;&#38480;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
On-device learning&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#30427;&#34892;&#36235;&#21183;&#65292;&#23427;&#36991;&#20813;&#20102;&#20113;&#31471;&#23398;&#20064;&#30340;&#21709;&#24212;&#26102;&#38388;&#32531;&#24930;&#21644;&#27807;&#36890;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#36830;&#32493;&#12289;&#26080;&#38480;&#22320;&#23398;&#20064;&#65292;&#24182;&#19988;&#21463;&#21040;&#36164;&#28304;&#32422;&#26463;&#65292;&#23545;&#20110;&#30495;&#23454;&#20256;&#24863;&#22120;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35774;&#35745;&#23545;&#20110;&#20855;&#26377;(i)&#25968;&#25454;&#27969;&#36755;&#20837;&#12289;(ii)&#32570;&#20047;&#30417;&#30563;&#21644;(iii)&#26377;&#38480;&#26495;&#36733;&#36164;&#28304;&#30340;&#23454;&#38469;&#22330;&#26223;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#37096;&#32626;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#20855;&#26377;&#26377;&#38480;&#30417;&#29702;&#30340;&#19968;&#33324;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#36793;&#32536;&#35774;&#22791;&#32456;&#29983;&#25104;&#38271;&#23398;&#20064;&#31995;&#32479;LifeHD&#12290;LifeHD &#22522;&#20110;&#19968;&#31181;&#21517;&#20026;&#36229;&#39640;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#30340;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#23398;&#20064;&#33539;&#24335;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#23618;&#32852;&#24819;&#35760;&#24518;&#32452;&#32455;&#26234;&#33021;&#23384;&#20648;&#21644;&#31649;&#29702;&#39640;&#32500;&#12289;&#20302;&#31934;&#24230;&#21521;&#37327;&#65292;&#36825;&#20123;&#21521;&#37327;&#20195;&#34920;&#21382;&#21490;&#27169;&#24335;&#20316;&#20026;&#31751;&#36136;&#24515;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04759v1 Announce Type: new  Abstract: On-device learning has emerged as a prevailing trend that avoids the slow response time and costly communication of cloud-based learning. The ability to learn continuously and indefinitely in a changing environment, and with resource constraints, is critical for real sensor deployments. However, existing designs are inadequate for practical scenarios with (i) streaming data input, (ii) lack of supervision and (iii) limited on-board resources. In this paper, we design and deploy the first on-device lifelong learning system called LifeHD for general IoT applications with limited supervision. LifeHD is designed based on a novel neurally-inspired and lightweight learning paradigm called Hyperdimensional Computing (HDC). We utilize a two-tier associative memory organization to intelligently store and manage high-dimensional, low-precision vectors, which represent the historical patterns as cluster centroids. We additionally propose two varian
&lt;/p&gt;</description></item><item><title>KnowledgeVIS&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.04758</link><description>&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;KnowledgeVIS
&lt;/p&gt;
&lt;p&gt;
KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04758
&lt;/p&gt;
&lt;p&gt;
KnowledgeVIS&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#22635;&#31354;&#25552;&#31034;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#22686;&#38271;&#23548;&#33268;&#23427;&#20204;&#22312;&#24635;&#32467;&#12289;&#39044;&#27979;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#22240;&#27492;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#29702;&#35299;&#20854;&#24037;&#20316;&#26041;&#24335;&#21450;&#21407;&#22240;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KnowledgeVis&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#21487;&#35270;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#20351;&#29992;&#22635;&#31354;&#21477;&#20316;&#20026;&#25552;&#31034;&#26469;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;&#21477;&#23376;&#20043;&#38388;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;KnowledgeVis&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32852;&#31995;&#65292;&#36825;&#30452;&#35266;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#23398;&#21040;&#30340;&#20869;&#23481;&#19982;&#33258;&#28982;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#24110;&#21161;&#29992;&#25143;&#21019;&#24314;&#21644;&#27979;&#35797;&#22810;&#20010;&#25552;&#31034;&#21464;&#20307;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#35821;&#20041;&#32858;&#31867;&#25216;&#26415;&#20998;&#26512;&#39044;&#27979;&#30340;&#21333;&#35789;&#65292;&#24182;&#20351;&#29992;&#20114;&#21160;&#21487;&#35270;&#21270;&#21457;&#29616;&#35265;&#35299;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#21487;&#35270;&#21270;&#24110;&#21161;&#29992;&#25143;&#30830;&#23450;&#21333;&#20010;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#29420;&#29305;&#24615;&#65292;&#27604;&#36739;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#19968;&#32452;&#39044;&#27979;&#65292;&#24182;&#24635;&#32467;&#27169;&#24335;&#21644;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04758v1 Announce Type: cross  Abstract: Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships betw
&lt;/p&gt;</description></item><item><title>JAX-SPH&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;LagrangeBench&#39033;&#30446;&#30340;&#20195;&#30721;&#65292;&#38598;&#25104;&#20102;&#20851;&#38190;&#30340;SPH&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#26799;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04750</link><description>&lt;p&gt;
JAX-SPH&#65306;&#19968;&#31181;&#21487;&#24494;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04750
&lt;/p&gt;
&lt;p&gt;
JAX-SPH&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;LagrangeBench&#39033;&#30446;&#30340;&#20195;&#30721;&#65292;&#38598;&#25104;&#20102;&#20851;&#38190;&#30340;SPH&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#26799;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#20307;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29289;&#29702;&#21644;&#33258;&#30001;&#34920;&#38754;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#28155;&#21152;&#21040;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#24037;&#20855;&#31665;&#20013;&#27491;&#22312;&#25512;&#21160;&#36825;&#20123;&#25968;&#20540;&#27169;&#25311;&#30340;&#36136;&#37327;&#19982;&#36895;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#36793;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#39046;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#30340;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#26032;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;JAX-SPH&#8212;&#8212;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#26694;&#26550;&#12290;JAX-SPH&#22522;&#20110;&#20174;LagrangeBench&#39033;&#30446;&#65288;Toshev&#31561;&#20154;&#65292;2023&#24180;&#65289;&#20013;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25193;&#23637;&#20102;&#27492;&#20195;&#30721;&#65306;(a)&#38598;&#25104;&#20102;&#36827;&#19968;&#27493;&#30340;&#20851;&#38190;SPH&#31639;&#27861;&#65292;(b)&#23558;&#20195;&#30721;&#37325;&#32452;&#20026;Python&#24211;&#65292;(c)&#36890;&#36807;&#27714;&#35299;&#22120;&#39564;&#35777;&#26799;&#24230;&#65292;&#20197;&#21450;(d)&#28436;&#31034;&#20102;&#36825;&#20123;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04750v1 Announce Type: cross  Abstract: Particle-based fluid simulations have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces. The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical simulations. In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX. JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#22312;&#32500;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#65292;&#36827;&#32780;&#23548;&#33268;&#20102;&#22686;&#24378;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;</title><link>https://arxiv.org/abs/2403.04747</link><description>&lt;p&gt;
GNN-VPA: &#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24046;&#20445;&#25345;&#32858;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04747
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#22312;&#32500;&#25345;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#65292;&#36827;&#32780;&#23548;&#33268;&#20102;&#22686;&#24378;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#25913;&#21892;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#29305;&#21035;&#26159;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#29289;&#29702;&#23398;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#24314;&#27169;&#31561;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#30340;&#33021;&#21147;&#65292;&#20851;&#38190;&#21462;&#20915;&#20110;&#29992;&#20110;&#28040;&#24687;&#32858;&#21512;&#21644;&#22270;&#32423;&#35835;&#20986;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#26041;&#24046;&#30340;&#32858;&#21512;&#20989;&#25968;&#65288;VPA&#65289;&#65292;&#35813;&#20989;&#25968;&#20445;&#25345;&#20102;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#21160;&#21147;&#23398;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;VPA&#23548;&#33268;&#20102;&#27969;&#34892;&#30340;GNN&#26550;&#26500;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#39640;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#20026;&#26080;&#24402;&#19968;&#21270;&#25110;&#33258;&#24402;&#19968;&#21270;&#30340;GNNs&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;</title><link>https://arxiv.org/abs/2403.04746</link><description>&lt;p&gt;
&#22312;&#24187;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#27169;&#25311;&#35797;&#38169;&#23398;&#20064;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#26368;&#26032;&#20449;&#24687;&#24182;&#22312;&#22806;&#37096;&#29615;&#22659;&#20013;&#37319;&#21462;&#37325;&#35201;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#20851;&#20110;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#24037;&#20855;&#30340;&#24191;&#27867;&#35206;&#30422;&#33539;&#22260;&#21644;&#28789;&#27963;&#24615;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#20154;&#24847;&#22806;&#24573;&#35270;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;LLM&#22312;&#32463;&#36807;&#35757;&#32451;&#21518;&#22914;&#20309;&#20934;&#30830;&#20351;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21253;&#25324;GPT-4&#21644;&#19987;&#38376;&#20026;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;LLMs&#22312;&#27491;&#30830;&#29575;&#26041;&#38754;&#20165;&#36798;&#21040;30%&#21040;60%&#30340;&#33539;&#22260;&#65292;&#36828;&#19981;&#36275;&#20197;&#22312;&#23454;&#36341;&#20013;&#21487;&#38752;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#25311;&#35797;&#38169;&#65288;STE&#65289;&#65292;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;LLMs&#32534;&#25490;&#20102;&#19977;&#20010;&#20851;&#38190;&#26426;&#21046;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#24037;&#20855;&#20351;&#29992;&#34892;&#20026;&#65306;&#35797;&#38169;&#12289;&#24819;&#35937;&#21644;&#35760;&#24518;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;STE&#21033;&#29992;LLM&#30340;&#8220;&#24819;&#35937;&#21147;&#8221;&#26469;&#27169;&#25311;&#20351;&#29992;&#24037;&#20855;&#30340;&#21487;&#33021;&#22330;&#26223;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#22312;&#38750;&#39640;&#26031;&#25104;&#20998;&#20998;&#26512;&#20013;&#65292;&#23545;&#20110;&#21306;&#20998;&#26631;&#20934;&#22810;&#20803;&#39640;&#26031;&#21644;&#22312;&#38543;&#26426;&#38544;&#34255;&#26041;&#21521;&#19978;&#34892;&#20026;&#31867;&#20284;&#26576;&#19968;&#32500;&#20998;&#24067;&#32780;&#22312;&#27491;&#20132;&#34917;&#19978;&#34892;&#20026;&#31867;&#20284;&#26631;&#20934;&#39640;&#26031;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#34987;&#35201;&#27714;&#30340;&#21345;&#26041;&#26465;&#20214;&#23454;&#38469;&#19978;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.04744</link><description>&lt;p&gt;
SQ&#22312;&#36739;&#24369;&#20551;&#35774;&#19979;&#29992;&#20110;&#38750;&#39640;&#26031;&#25104;&#20998;&#20998;&#26512;&#30340;&#19979;&#38480;
&lt;/p&gt;
&lt;p&gt;
SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#22312;&#38750;&#39640;&#26031;&#25104;&#20998;&#20998;&#26512;&#20013;&#65292;&#23545;&#20110;&#21306;&#20998;&#26631;&#20934;&#22810;&#20803;&#39640;&#26031;&#21644;&#22312;&#38543;&#26426;&#38544;&#34255;&#26041;&#21521;&#19978;&#34892;&#20026;&#31867;&#20284;&#26576;&#19968;&#32500;&#20998;&#24067;&#32780;&#22312;&#27491;&#20132;&#34917;&#19978;&#34892;&#20026;&#31867;&#20284;&#26631;&#20934;&#39640;&#26031;&#30340;&#38382;&#39064;&#65292;&#20808;&#21069;&#34987;&#35201;&#27714;&#30340;&#21345;&#26041;&#26465;&#20214;&#23454;&#38469;&#19978;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#27169;&#22411;&#20013;&#38750;&#39640;&#26031;&#25104;&#20998;&#20998;&#26512;&#65288;NGCA&#65289;&#30340;&#22797;&#26434;&#24615;&#12290; &#20808;&#21069;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#33324;&#26041;&#27861;&#65292;&#29992;&#20110;&#35777;&#26126;&#36825;&#19968;&#20219;&#21153;&#30340;SQ&#19979;&#30028;&#65292;&#24182;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#12290; &#29305;&#21035;&#22320;&#65292;&#24050;&#30693;&#23545;&#20110;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#30340;&#20219;&#20309;&#19968;&#32500;&#20998;&#24067;$A$&#65292;&#21306;&#20998;&#26631;&#20934;&#22810;&#20803;&#39640;&#26031;&#21644;&#22312;&#38543;&#26426;&#38544;&#34255;&#26041;&#21521;&#19978;&#30340;&#34892;&#20026;&#31867;&#20284;$A$&#32780;&#22312;&#27491;&#20132;&#34917;&#19978;&#34892;&#20026;&#31867;&#20284;&#26631;&#20934;&#39640;&#26031;&#30340;&#20998;&#24067;&#26159;SQ-hard&#30340;&#12290; &#25152;&#38656;&#26465;&#20214;&#26159;&#65288;1&#65289;$A$&#19982;&#26631;&#20934;&#19968;&#32500;&#39640;&#26031;&#21305;&#37197;&#35768;&#22810;&#20302;&#38454;&#30697;&#65292;&#21644;&#65288;2&#65289;$A$&#30456;&#23545;&#20110;&#26631;&#20934;&#39640;&#26031;&#30340;&#21345;&#26041;&#33539;&#25968;&#26159;&#26377;&#38480;&#30340;&#12290; &#34429;&#28982;&#28385;&#36275;&#30697;&#21305;&#37197;&#26465;&#20214;&#23545;&#20110;&#38590;&#24230;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#21345;&#26041;&#26465;&#20214;&#20165;&#20986;&#20110;&#25216;&#26415;&#21407;&#22240;&#32780;&#34987;&#35201;&#27714;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#19968;&#20010;&#26465;&#20214;&#23454;&#38469;&#19978;&#24182;&#38750;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04744v1 Announce Type: new  Abstract: We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In partic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27425;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#24694;&#24847;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#40065;&#26834;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;</title><link>https://arxiv.org/abs/2403.04726</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;&#30340;&#27425;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27425;&#20108;&#27425;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#24694;&#24847;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#40065;&#26834;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31639;&#27861;&#35266;&#27979;&#21040;&#20174;$\mathcal{N}(\mu,\mathbf{I}_d)$&#20013;&#30340;\emph{&#21463;&#25439;&#23475;}&#26679;&#26412;&#38598;&#65292;&#20854;&#20013;&#26410;&#30693;&#22343;&#20540;$\mu \in \mathbb{R}^d$&#34987;&#38480;&#21046;&#20026;$k$-&#31232;&#30095;&#12290;&#19968;&#31995;&#21015;&#20808;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;$\mathrm{poly}(k,\log d, 1/\epsilon)$&#21644;&#36816;&#34892;&#26102;&#38388;$d^2 \mathrm{poly}(k,\log d,1/\epsilon)$&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;$\epsilon$&#26159;&#27745;&#26579;&#20998;&#25968;&#12290;&#29305;&#21035;&#26159;&#65292;&#29616;&#26377;&#31639;&#27861;&#30340;&#26368;&#24555;&#36816;&#34892;&#26102;&#38388;&#26159;&#20108;&#27425;&#30340;($\Omega(d^2)$)&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#31105;&#27490;&#24615;&#30340;&#12290;&#29616;&#26377;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#30340;&#20108;&#27425;&#38556;&#30861;&#28304;&#20110;&#36825;&#20123;&#31639;&#27861;&#23545;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20381;&#36182;&#65292;&#20854;&#22823;&#23567;&#20026;$d^2$&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#31232;&#30095;&#22343;&#20540;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;\emph{&#27425;&#20108;&#27425;}&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#20351;&#29992;$\mathrm{poly}
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04726v1 Announce Type: cross  Abstract: We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean $\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.04720</link><description>&lt;p&gt;
&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#28909;&#21551;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking of Encoder-based Warm-start Methods in Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04720
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#24322;&#36136;&#24615;&#34920;&#26684;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#20803;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20803;&#29305;&#24449;&#65292;&#20363;&#22914;&#65292;&#32479;&#35745;&#37327;&#25110;&#26631;&#24535;&#28857;&#12290;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#22914;Dataset2Vec&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#25552;&#21462;&#37325;&#35201;&#30340;&#20803;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;liltab&#21253;&#20013;&#65292;&#35813;&#21253;&#21487;&#22312;GitHub&#19978;&#25214;&#21040;https://github.com/azoz01/liltab&#12290;&#25105;&#20204;&#30340;&#21253;&#22522;&#20110;[Iwata and Kumagai, 2020]&#25552;&#20986;&#30340;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#34920;&#26684;&#25968;&#25454;&#30340;&#24050;&#24314;&#31435;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#22914;Dataset2Vec &#30340;&#32534;&#30721;&#29305;&#24449;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26367;&#20195;&#34920;&#31034;&#12290;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;&#25968;&#25454;&#38598;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20803;&#20219;&#21153;&#19978;&#35780;&#20215;&#20102;Dataset2Vec&#21644;liltab
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04720v1 Announce Type: new  Abstract: Effectively representing heterogeneous tabular datasets for meta-learning remains an open problem. Previous approaches rely on predefined meta-features, for example, statistical measures or landmarkers. Encoder-based models, such as Dataset2Vec, allow us to extract significant meta-features automatically without human intervention. This research introduces a novel encoder-based representation of tabular datasets implemented within the liltab package available on GitHub https://github.com/azoz01/liltab. Our package is based on an established model for heterogeneous tabular data proposed in [Iwata and Kumagai, 2020]. The proposed approach employs a different model for encoding feature relationships, generating alternative representations compared to existing methods like Dataset2Vec. Both of them leverage the fundamental assumption of dataset similarity learning. In this work, we evaluate Dataset2Vec and liltab on two common meta-tasks - r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04696</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#19981;&#21487;&#38752;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20135;&#29983;&#38169;&#35823;&#30340;&#22768;&#26126;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#31181;&#24187;&#35273;&#21487;&#33021;&#24456;&#21361;&#38505;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20598;&#23572;&#20986;&#29616;&#30340;&#20107;&#23454;&#19981;&#20934;&#30830;&#21487;&#33021;&#20250;&#34987;&#25972;&#20307;&#19978;&#26159;&#20107;&#23454;&#30340;&#25991;&#26412;&#25513;&#30422;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#26497;&#20854;&#38590;&#20197;&#21457;&#29616;&#12290;&#21033;&#29992;LLMs&#30340;&#24403;&#21069;&#26381;&#21153;&#36890;&#24120;&#19981;&#25552;&#20379;&#26816;&#27979;&#19981;&#21487;&#38752;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26032;&#22411;&#20107;&#23454;&#26680;&#26597;&#21644;&#24187;&#35273;&#26816;&#27979;&#27969;&#31243;&#12290;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#25110;&#20854;&#23618;&#36755;&#20986;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26469;&#26816;&#27979;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#24182;&#25105;&#20204;&#23637;&#31034;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26680;&#26597;LLM&#36755;&#20986;&#20013;&#30340;&#21508;&#31181;&#22768;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26631;&#35760;&#32423;&#21035;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#20107;&#23454;&#25552;&#20986;&#24576;&#30097;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20843;&#20010;&#31454;&#36187;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04693</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#20013;&#31995;&#32479;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Systems' Performance in Natural Language Processing Competitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20843;&#20010;&#31454;&#36187;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#31454;&#36187;&#22312;&#31185;&#23398;&#25216;&#26415;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#31454;&#36187;&#28041;&#21450;&#23450;&#20041;&#20219;&#21153;&#65292;&#36873;&#25321;&#35780;&#20272;&#20998;&#25968;&#21644;&#35774;&#35745;&#32467;&#26524;&#39564;&#35777;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#36890;&#24120;&#20250;&#25910;&#21040;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#20027;&#21150;&#26041;&#20445;&#30041;&#30340;&#19968;&#20010;&#26410;&#20844;&#24320;&#25968;&#25454;&#38598;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#31454;&#36187;&#32467;&#26524;&#21644;&#31454;&#20105;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#35774;&#35745;&#25104;&#36890;&#29992;&#30340;&#65292;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#20102;&#20843;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04693v1 Announce Type: new  Abstract: Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regressio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#24494;&#20998;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;CRO&#27169;&#22411;&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;&#25152;&#35268;&#23450;&#20915;&#31574;&#30340;&#32463;&#39564;&#39118;&#38505;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#35821;&#22659;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#35206;&#30422;&#36136;&#37327;</title><link>https://arxiv.org/abs/2403.04670</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#26465;&#20214;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-end Conditional Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04670
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#24494;&#20998;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;CRO&#27169;&#22411;&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;&#25152;&#35268;&#23450;&#20915;&#31574;&#30340;&#32463;&#39564;&#39118;&#38505;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#35821;&#22659;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#35206;&#30422;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#22659;&#20248;&#21270;&#65288;CO&#65289;&#39046;&#22495;&#25972;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;CO&#21464;&#20307;&#65292;&#31216;&#20026;&#26465;&#20214;&#40065;&#26834;&#20248;&#21270;&#65288;CRO&#65289;&#65292;&#32467;&#21512;&#20102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#40065;&#26834;&#20248;&#21270;&#65292;&#20197;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#20419;&#36827;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21033;&#29992;&#29616;&#20195;&#21487;&#24494;&#20998;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#26469;&#35757;&#32451;CRO&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#25152;&#35268;&#23450;&#20915;&#31574;&#30340;&#32463;&#39564;&#39118;&#38505;&#21644;&#25903;&#25345;&#23427;&#20204;&#30340;&#35821;&#22659;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#35206;&#30422;&#36136;&#37327;&#12290;&#23613;&#31649;&#20174;&#21512;&#35268;&#39044;&#27979;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26080;&#27861;&#33719;&#24471;&#23545;&#21518;&#32773;&#30446;&#26631;&#25104;&#21151;&#30340;&#20445;&#35777;&#65292;&#20294;&#36890;&#36807;&#24039;&#22937;&#22320;&#22312;&#35206;&#30422;&#36136;&#37327;&#30340;&#35745;&#31639;&#20013;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#21487;&#24494;&#20998;&#23618;&#65292;&#21487;&#20197;&#22312;&#32463;&#39564;&#19978;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26465;&#20214;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04670v1 Announce Type: new  Abstract: The field of Contextual Optimization (CO) integrates machine learning and optimization to solve decision making problems under uncertainty. Recently, a risk sensitive variant of CO, known as Conditional Robust Optimization (CRO), combines uncertainty quantification with robust optimization in order to promote safety and reliability in high stake applications. Exploiting modern differentiable optimization methods, we propose a novel end-to-end approach to train a CRO model in a way that accounts for both the empirical risk of the prescribed decisions and the quality of conditional coverage of the contextual uncertainty set that supports them. While guarantees of success for the latter objective are impossible to obtain from the point of view of conformal prediction theory, high quality conditional coverage is achieved empirically by ingeniously employing a logistic regression differentiable layer within the calculation of coverage quality
&lt;/p&gt;</description></item><item><title>&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04666</link><description>&lt;p&gt;
&#30005;&#20449;&#35821;&#35328;&#27169;&#22411;&#65306;&#23427;&#20204;&#24517;&#39035;&#24222;&#22823;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Telecom Language Models: Must They Be Large?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04666
&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#37096;&#38376;&#23545;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#25913;&#21464;&#36816;&#33829;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#20854;&#24040;&#22823;&#20307;&#31215;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#20986;&#29616;&#20102;&#19968;&#25209;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#20854;&#36739;&#22823;&#23545;&#24212;&#29289;&#30456;&#24403;&#65292;&#27604;&#22914;&#32534;&#30721;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;Phi-2&#26159;&#19968;&#31181;&#32039;&#20945;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20307;&#29616;&#20102;&#36825;&#19968;&#31995;&#21015;&#39640;&#25928;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#28010;&#28526;&#12290;&#26412;&#25991;&#23545;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#20869;&#22312;&#26412;&#36136;&#19978;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#37492;&#20110;&#35268;&#27169;&#30456;&#20851;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#31934;&#24515;&#22686;&#24378;&#20102;Phi-2&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.04661</link><description>&lt;p&gt;
&#38754;&#21521;&#38899;&#39057;-&#35270;&#39057;&#20154;&#21592;&#39564;&#35777;&#30340;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Cross Attention for Audio-Visual Person Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#21592;&#25110;&#36523;&#20221;&#39564;&#35777;&#36890;&#24120;&#20351;&#29992;&#20010;&#20307;&#27169;&#24577;&#65288;&#22914;&#38754;&#37096;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;&#26368;&#36817;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#36890;&#24120;&#34987;&#26399;&#26395;&#20855;&#26377;&#24378;&#28872;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#22312;&#26377;&#25928;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#24635;&#26159;&#24378;&#28872;&#30456;&#20114;&#34917;&#20805;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23637;&#29616;&#20986;&#24369;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#23548;&#33268;&#38899;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#36328;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26465;&#20214;&#38376;&#25511;&#23618;&#26469;&#35780;&#20272;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36129;&#29486;&#65292;&#24182;&#20165;&#36873;&#25321;&#36328;&#27169;&#24577;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04661v1 Announce Type: cross  Abstract: Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24050;&#25104;&#20026;&#23558;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#31639;&#27861;&#22312;&#25552;&#21319;LLM&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19987;&#23478;&#36845;&#20195;&#25928;&#26524;&#26368;&#20339;&#65292;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;PPO&#31867;&#20284;&#12290;</title><link>https://arxiv.org/abs/2403.04642</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Teaching Large Language Models to Reason with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04642
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24050;&#25104;&#20026;&#23558;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#22810;&#31181;&#31639;&#27861;&#22312;&#25552;&#21319;LLM&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19987;&#23478;&#36845;&#20195;&#25928;&#26524;&#26368;&#20339;&#65292;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;PPO&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;\textbf{RLHF}&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#21463;RLHF&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#65288;&#19987;&#23478;&#36845;&#20195;&#12289;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;\textbf{PPO}&#65289;&#12289;&#26377;&#26465;&#20214;&#22238;&#25253;&#30340;RL&#65289;&#22312;&#25552;&#21319;LLM&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21551;&#21457;&#24335;&#21644;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#32473;LLM&#30340;&#31232;&#30095;&#21644;&#31264;&#23494;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#27169;&#22411;&#22823;&#23567;&#21644;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#21253;&#25324;&#26377;&#21644;&#27809;&#26377;&#30417;&#30563;&#24494;&#35843;&#65288;\textbf{SFT}&#65289;&#25968;&#25454;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#31639;&#27861;&#30340;&#34920;&#29616;&#22522;&#26412;&#30456;&#24403;&#65292;&#19987;&#23478;&#36845;&#20195;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#23478;&#36845;&#20195;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;PPO&#30456;&#20284;&#65292;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#25910;&#25947;&#38656;&#35201;&#26368;&#22810;&#22823;&#32422;$10^6$&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04642v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#29109;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#39033;&#26469;&#32531;&#35299;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04636</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29109;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Entropy Aware Message Passing in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#29109;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#39033;&#26469;&#32531;&#35299;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#24230;&#24179;&#28369;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#27169;&#22411;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;GNN&#26550;&#26500;&#38598;&#25104;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#29109;&#24863;&#30693;&#28040;&#24687;&#20256;&#36882;&#39033;&#12290;&#35813;&#39033;&#22312;&#33410;&#28857;&#32858;&#21512;&#36807;&#31243;&#20013;&#23545;&#29109;&#36827;&#34892;&#26799;&#24230;&#19978;&#21319;&#65292;&#20174;&#32780;&#22312;&#23884;&#20837;&#20013;&#20445;&#30041;&#19968;&#23450;&#31243;&#24230;&#30340;&#29109;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#36827;&#34892;&#20102;&#23545;&#27604;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04636v1 Announce Type: new  Abstract: Deep Graph Neural Networks struggle with oversmoothing. This paper introduces a novel, physics-inspired GNN model designed to mitigate this issue. Our approach integrates with existing GNN architectures, introducing an entropy-aware message passing term. This term performs gradient ascent on the entropy during node aggregation, thereby preserving a certain degree of entropy in the embeddings. We conduct a comparative analysis of our model against state-of-the-art GNNs across various common datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.04629</link><description>&lt;p&gt;
&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ShapleyBO&#26694;&#26550;&#65292;&#29992;Shapley&#20540;&#35299;&#37322;&#36125;&#21494;&#26031;&#20248;&#21270;&#25552;&#35758;&#65292;&#37327;&#21270;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25506;&#32034;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#19982;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#40657;&#21283;&#23376;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;BO&#26412;&#36523;&#20063;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#21283;&#23376;&#65292;&#32570;&#20047;&#25552;&#20379;&#20026;&#20309;&#25552;&#35758;&#35780;&#20272;&#26576;&#20123;&#21442;&#25968;&#30340;&#29702;&#30001;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ShapleyBO&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#21338;&#24328;&#35770;Shapley&#20540;&#35299;&#37322;BO&#25552;&#35758;&#30340;&#26694;&#26550;&#12290;&#23427;&#37327;&#21270;&#20102;&#27599;&#20010;&#21442;&#25968;&#23545;BO&#30340;&#25910;&#33719;&#20989;&#25968;&#30340;&#36129;&#29486;&#12290;&#21033;&#29992;Shapley&#20540;&#30340;&#32447;&#24615;&#24615;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#30830;&#23450;&#27599;&#20010;&#21442;&#25968;&#23545;&#20110;&#20687;&#32622;&#20449;&#36793;&#30028;&#36825;&#26679;&#30340;&#21152;&#27861;&#25910;&#33719;&#20989;&#25968;&#25512;&#21160;BO&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24378;&#24230;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ShapleyBO&#33021;&#22815;&#35299;&#20915;&#25506;&#32034;&#23545;&#20110;&#21208;&#25506;aleatoric&#21644;&#35748;&#35782;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method 
&lt;/p&gt;</description></item><item><title>MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04626</link><description>&lt;p&gt;
MedFLIP&#65306;&#21307;&#23398;&#35270;&#35273;&#19982;&#35821;&#35328;&#33258;&#30417;&#30563;&#24555;&#36895;&#39044;&#35757;&#32451;&#19982;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04626
&lt;/p&gt;
&lt;p&gt;
MedFLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;SVD&#25439;&#22833;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#65292;&#39564;&#35777;&#20102;&#29992;&#35821;&#35328;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#20998;&#26512;&#39046;&#22495;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#20114;&#30456;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;MAEs&#23545;&#36328;&#27169;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MedFLIP&#65292;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#20998;&#26512;&#30340;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;MAEs&#36827;&#34892;&#36328;&#39046;&#22495;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#24120;&#35265;&#30340;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#22270;&#20687;&#36827;&#34892;&#25513;&#34109;&#19981;&#20250;&#24433;&#21709;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD&#25439;&#22833;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#36825;&#31867;&#25968;&#25454;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#35821;&#35328;&#23558;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;MedFLIP&#23545;&#25513;&#34109;&#36807;&#31243;&#30340;&#25193;&#23637;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04626v1 Announce Type: cross  Abstract: Within the domain of medical analysis, extensive research has explored the potential of mutual learning between Masked Autoencoders(MAEs) and multimodal data. However, the impact of MAEs on intermodality remains a key challenge. We introduce MedFLIP, a Fast Language-Image Pre-training method for Medical analysis. We explore MAEs for zero-shot learning with crossed domains, which enhances the model ability to learn from limited data, a common scenario in medical diagnostics. We verify that masking an image does not affect intermodal learning. Furthermore, we propose the SVD loss to enhance the representation learning for characteristics of medical images, aiming to improve classification accuracy by leveraging the structural intricacies of such data. Lastly, we validate using language will improve the zero-shot performance for the medical image analysis. MedFLIP scaling of the masking process marks an advancement in the field, offering 
&lt;/p&gt;</description></item><item><title>IN-N-OUT&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;GNN&#26657;&#20934;&#20559;&#24046;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#31616;&#21333;&#30452;&#35273;&#23454;&#29616;&#26657;&#20934;</title><link>https://arxiv.org/abs/2403.04605</link><description>&lt;p&gt;
In-n-Out: &#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
In-n-Out: Calibrating Graph Neural Networks for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04605
&lt;/p&gt;
&lt;p&gt;
IN-N-OUT&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#26041;&#27861;&#65292;&#22522;&#20110;&#23545;GNN&#26657;&#20934;&#20559;&#24046;&#30340;&#35266;&#23519;&#65292;&#36890;&#36807;&#31616;&#21333;&#30452;&#35273;&#23454;&#29616;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#26657;&#20934;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#30340;&#36755;&#20986;&#19981;&#33021;&#21453;&#26144;&#25105;&#20204;&#25171;&#31639;&#39044;&#27979;&#30340;&#20107;&#20214;&#30340;&#30495;&#23454;&#27010;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#28151;&#21512;&#30340;&#34892;&#20026;&#65292;&#21363;&#22312;&#36127;&#39044;&#27979;&#19978;&#21487;&#33021;&#36807;&#20110;&#33258;&#20449;&#65292;&#22312;&#27491;&#39044;&#27979;&#19978;&#21487;&#33021;&#19981;&#22815;&#33258;&#20449;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IN-N-OUT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;IN-N-OUT&#22522;&#20110;&#20004;&#20010;&#31616;&#21333;&#30340;&#30452;&#35273;&#65306;i) &#32473;&#36793;&#26631;&#27880;&#30495;/&#20551;&#26631;&#31614;&#65292;&#21516;&#26102;&#36981;&#24490;GNN&#30340;&#39044;&#27979;&#24212;&#23548;&#33268;&#35813;&#36793;&#23884;&#20837;&#30340;&#24494;&#23567;&#27874;&#21160;&#65307;ii) &#30456;&#21453;&#22320;&#65292;&#22914;&#26524;&#25105;&#20204;&#26631;&#35760;&#30456;&#21516;&#30340;&#36793;&#19982;&#25105;&#20204;&#30340;GNN&#39044;&#27979;&#30456;&#24726;&#65292;&#37027;&#20040;&#23884;&#20837;&#24212;&#35813;&#21457;&#29983;&#26356;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04605v1 Announce Type: new  Abstract: Deep neural networks are notoriously miscalibrated, i.e., their outputs do not reflect the true probability of the event we aim to predict. While networks for tabular or image data are usually overconfident, recent works have shown that graph neural networks (GNNs) show the opposite behavior for node-level classification. But what happens when we are predicting links? We show that, in this case, GNNs often exhibit a mixed behavior. More specifically, they may be overconfident in negative predictions while being underconfident in positive ones. Based on this observation, we propose IN-N-OUT, the first-ever method to calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions: i) attributing true/false labels to an edge while respecting a GNNs prediction should cause but small fluctuations in that edge's embedding; and, conversely, ii) if we label that same edge contradicting our GNN, embeddings should change more substa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Contrastive Continual Learning via Importance Sampling (CCLIS)&#20197;&#20445;&#30041;&#30693;&#35782;&#65292;&#20854;&#20013;&#36890;&#36807;&#37325;&#25773;&#32531;&#20914;&#21306;&#36873;&#25321;&#65288;RBS&#65289;&#31574;&#30053;&#24674;&#22797;&#20808;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#26368;&#23567;&#21270;&#20272;&#35745;&#26041;&#24046;&#20197;&#20445;&#23384;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21407;&#22411;-&#23454;&#20363;&#20851;&#31995;&#33976;&#39311;&#65288;PRD&#65289;&#25439;&#22833;&#26469;&#32500;&#25252;&#21407;&#22411;&#21644;&#26679;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.04599</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#35201;&#24615;&#37319;&#26679;&#21644;&#21407;&#22411;&#23454;&#20363;&#20851;&#31995;&#33976;&#39311;&#30340;&#23545;&#27604;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04599
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Contrastive Continual Learning via Importance Sampling (CCLIS)&#20197;&#20445;&#30041;&#30693;&#35782;&#65292;&#20854;&#20013;&#36890;&#36807;&#37325;&#25773;&#32531;&#20914;&#21306;&#36873;&#25321;&#65288;RBS&#65289;&#31574;&#30053;&#24674;&#22797;&#20808;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#26368;&#23567;&#21270;&#20272;&#35745;&#26041;&#24046;&#20197;&#20445;&#23384;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21407;&#22411;-&#23454;&#20363;&#20851;&#31995;&#33976;&#39311;&#65288;PRD&#65289;&#25439;&#22833;&#26469;&#32500;&#25252;&#21407;&#22411;&#21644;&#26679;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#37325;&#22797;&#35757;&#32451;&#30340;&#23545;&#27604;&#25345;&#32493;&#23398;&#20064;&#65292;&#20197;&#25506;&#32034;&#22914;&#20309;&#25345;&#32493;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#34920;&#31034;&#23884;&#20837;&#65292;&#36991;&#20813;&#20256;&#32479;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#23545;&#27604;&#25345;&#32493;&#23398;&#20064;&#65288;CCLIS&#65289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#37325;&#25773;&#32531;&#20914;&#21306;&#36873;&#25321;&#65288;RBS&#65289;&#31574;&#30053;&#26469;&#24674;&#22797;&#20808;&#21069;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#26368;&#23567;&#21270;&#20272;&#35745;&#26041;&#24046;&#20197;&#20445;&#23384;&#39640;&#36136;&#37327;&#30340;&#29992;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#38590;&#36127;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;-&#23454;&#20363;&#20851;&#31995;&#33976;&#39311;&#65288;PRD&#65289;&#25439;&#22833;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#33976;&#39311;&#36807;&#31243;&#26469;&#32500;&#25252;&#21407;&#22411;&#21644;&#26679;&#26412;&#34920;&#31034;&#20043;&#38388;&#20851;&#31995;&#30340;&#25216;&#26415;&#12290;&#22312;&#26631;&#20934;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04599v1 Announce Type: new  Abstract: Recently, because of the high-quality representations of contrastive learning methods, rehearsal-based contrastive continual learning has been proposed to explore how to continually learn transferable representation embeddings to avoid the catastrophic forgetting issue in traditional continual settings. Based on this framework, we propose Contrastive Continual Learning via Importance Sampling (CCLIS) to preserve knowledge by recovering previous data distributions with a new strategy for Replay Buffer Selection (RBS), which minimize estimated variance to save hard negative samples for representation learning with high quality. Furthermore, we present the Prototype-instance Relation Distillation (PRD) loss, a technique designed to maintain the relationship between prototypes and sample representations using a self-distillation process. Experiments on standard continual learning benchmarks reveal that our method notably outperforms existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04586</link><description>&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#23398;&#20064;&#39134;&#34892;&#25935;&#25463;&#24615;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Learning Agility Adaptation for Flight in Clutter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#23398;&#20064;&#36866;&#24212;&#20854;&#36816;&#21160;&#33021;&#21147;&#21644;&#25805;&#20316;&#29615;&#22659;&#30340;&#25935;&#25463;&#24615;&#12290;&#31227;&#21160;&#26426;&#22120;&#20154;&#20063;&#24212;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#23558;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26088;&#22312;&#36171;&#20104;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#36866;&#24212;&#25935;&#25463;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#65292;&#32467;&#21512;&#35797;&#38169;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#36712;&#36857;&#29983;&#25104;&#26041;&#27861;&#26469;&#20840;&#38754;&#23398;&#20064;&#25935;&#25463;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#26469;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#12290;&#22312;&#20223;&#30495;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#35813;&#31574;&#30053;&#23548;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04586v1 Announce Type: cross  Abstract: Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#26426;&#21046;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22797;&#29616;&#26377;&#26426;&#21453;&#24212;&#26426;&#29702;&#65292;&#23637;&#31034;&#20102;&#39044;&#27979;&#21453;&#24212;&#36884;&#24452;&#21644;&#26434;&#36136;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04580</link><description>&lt;p&gt;
&#36229;&#36234;&#20027;&#35201;&#20135;&#29289;&#39044;&#27979;&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#26426;&#21046;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#29616;&#21453;&#24212;&#26426;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beyond Major Product Prediction: Reproducing Reaction Mechanisms with Machine Learning Models Trained on a Large-Scale Mechanistic Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#26426;&#21046;&#25968;&#25454;&#38598;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#22797;&#29616;&#26377;&#26426;&#21453;&#24212;&#26426;&#29702;&#65292;&#23637;&#31034;&#20102;&#39044;&#27979;&#21453;&#24212;&#36884;&#24452;&#21644;&#26434;&#36136;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26426;&#21453;&#24212;&#30340;&#26426;&#29702;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#21453;&#24212;&#24320;&#21457;&#12289;&#26434;&#36136;&#39044;&#27979;&#65292;&#24182;&#19988;&#20174;&#21407;&#29702;&#19978;&#35762;&#65292;&#26377;&#21161;&#20110;&#21453;&#24212;&#30340;&#21457;&#29616;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#21453;&#24212;&#27169;&#26495;&#22312;&#23454;&#39564;&#25253;&#21578;&#30340;&#21453;&#24212;&#29289;&#21644;&#20135;&#29289;&#20043;&#38388;&#25554;&#20540;&#20986;&#20013;&#38388;&#20307;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;5,184,184&#20010;&#22522;&#26412;&#27493;&#39588;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#33021;&#21147;&#65292;&#30528;&#37325;&#20110;&#23427;&#20204;&#39044;&#27979;&#21453;&#24212;&#36884;&#24452;&#24182;&#37325;&#29616;&#20652;&#21270;&#21058;&#21644;&#35797;&#21058;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#21046;&#27169;&#22411;&#22312;&#39044;&#27979;&#26434;&#36136;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#22312;&#20256;&#32479;&#27169;&#22411;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04580v1 Announce Type: new  Abstract: Mechanistic understanding of organic reactions can facilitate reaction development, impurity prediction, and in principle, reaction discovery. While several machine learning models have sought to address the task of predicting reaction products, their extension to predicting reaction mechanisms has been impeded by the lack of a corresponding mechanistic dataset. In this study, we construct such a dataset by imputing intermediates between experimentally reported reactants and products using expert reaction templates and train several machine learning models on the resulting dataset of 5,184,184 elementary steps. We explore the performance and capabilities of these models, focusing on their ability to predict reaction pathways and recapitulate the roles of catalysts and reagents. Additionally, we demonstrate the potential of mechanistic models in predicting impurities, often overlooked by conventional models. We conclude by evaluating the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#21644;self-normalized&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32447;&#24615;&#28151;&#21512;MDPs&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.04568</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31639;&#27861;&#29992;&#20110;&#24102;&#26377;&#21290;&#22839;&#25152;&#24605;&#21453;&#39304;&#21644;&#26410;&#30693;&#36716;&#31227;&#30340;&#25932;&#23545;&#32447;&#24615;&#28151;&#21512;MDPs
&lt;/p&gt;
&lt;p&gt;
Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit Feedback and Unknown Transition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04568
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#21644;self-normalized&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32447;&#24615;&#28151;&#21512;MDPs&#20013;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#26410;&#30693;&#36716;&#31227;&#21644;&#22312;&#21290;&#22839;&#25152;&#24605;&#21453;&#39304;&#35774;&#32622;&#20013;&#30340;&#25932;&#23545;&#25439;&#22833;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36716;&#31227;&#26680;&#20026;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#30340;&#32447;&#24615;&#28151;&#21512;MDPs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#36798;&#21040;&#20102;$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$&#30340;&#36951;&#25022;&#20540;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#26144;&#23556;&#30340;&#32500;&#24230;&#65292;$S$&#26159;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$A$&#26159;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$H$&#26159;&#27599;&#38598;&#38271;&#24230;&#65292;$K$&#26159;&#38598;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20005;&#26684;&#25913;&#36827;&#20102;Zhao&#31561;&#20154;(2023a)&#20013;&#24050;&#30693;&#30340;&#26368;&#20339;$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$&#32467;&#26524;&#65292;&#22240;&#20026;$H \leq S$&#30001;&#23618;&#27425;MDP&#32467;&#26500;&#25104;&#31435;&#12290;&#25105;&#20204;&#30340;&#36827;&#23637;&#20027;&#35201;&#24402;&#22240;&#20110;(i)&#19968;&#31181;&#26032;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#36716;&#31227;&#21442;&#25968;&#65292;&#21033;&#29992;&#20102;&#25152;&#26377;&#29366;&#24577;&#30340;&#35775;&#38382;&#20449;&#24687;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#24037;&#20316;&#21482;&#29992;&#19968;&#20010;&#29366;&#24577;&#65292;&#20197;&#21450;(ii)&#19968;&#31181;&#26032;&#30340;self-normalized&#20998;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04568v1 Announce Type: new  Abstract: We study reinforcement learning with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, we focus on linear mixture MDPs whose transition kernel is a linear mixture model. We propose a new algorithm that attains an $\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability, where $d$ is the dimension of feature mappings, $S$ is the size of state space, $A$ is the size of action space, $H$ is the episode length and $K$ is the number of episodes. Our result strictly improves the previous best-known $\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a) since $H \leq S$ holds by the layered MDP structure. Our advancements are primarily attributed to (i) a new least square estimator for the transition parameter that leverages the visit information of all states, as opposed to only one state in prior work, and (ii) a new self-normalized conc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.04558</link><description>&lt;p&gt;
&#20943;&#23569;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#25913;&#21892;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#20943;&#23569;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22797;&#26434;&#24615;&#23545;&#20998;&#31867;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#36890;&#36807;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20174;&#24120;&#35268;&#21487;&#29992;&#30340;&#32452;&#32455;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#20020;&#24202;&#21487;&#25805;&#20316;&#35265;&#35299;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#30340;&#26631;&#27880;&#65292;&#36825;&#31181;&#26631;&#27880;&#31232;&#32570;&#19988;&#26114;&#36149;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#28040;&#38500;&#20102;&#36825;&#19968;&#38556;&#30861;&#65292;&#20801;&#35768;&#23545;&#38750;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;SSL&#26041;&#27861;&#37319;&#29992;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#25968;&#25454;&#37327;&#36805;&#36895;&#22686;&#21152;&#65292;&#30828;&#20214;&#35201;&#27714;&#21644;&#25972;&#20307;&#25104;&#26412;&#22686;&#21152;&#65292;&#20351;&#24471;&#24456;&#23569;&#26426;&#26500;&#33021;&#22815;&#33719;&#24471;&#36825;&#20123;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#30340;&#22797;&#26434;&#24615;&#19982;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#37327;&#12289;&#26550;&#26500;&#21644;&#31639;&#27861;&#30340;&#35843;&#25972;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, empha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;MLCommons CloudMask&#22522;&#20934;&#27979;&#35797;&#19978;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#21253;&#25324;&#26368;&#20339;&#27169;&#22411;&#12289;&#26368;&#39640;&#20934;&#30830;&#24615;&#21644;&#24179;&#22343;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.04553</link><description>&lt;p&gt;
MLCommons CloudMask&#22522;&#20934;&#27979;&#35797;&#30340;&#25913;&#36827;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improvements &amp; Evaluations on the MLCommons CloudMask Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;MLCommons CloudMask&#22522;&#20934;&#27979;&#35797;&#19978;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#21253;&#25324;&#26368;&#20339;&#27169;&#22411;&#12289;&#26368;&#39640;&#20934;&#30830;&#24615;&#21644;&#24179;&#22343;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#32445;&#32422;&#22823;&#23398;&#65288;NYU&#65289;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#38598;&#32676;NYU Greene&#19978;&#23545;MLCommons&#31185;&#23398;&#20113;&#36974;&#34109;&#22522;&#20934;&#27979;&#35797;&#19978;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;MLCommons&#26159;&#19968;&#20010;&#24320;&#21457;&#21644;&#32500;&#25252;&#20960;&#20010;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;&#30340;&#32852;&#30431;&#65292;&#21487;&#20197;&#20174;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20113;&#36974;&#34109;&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#30340;&#25551;&#36848;&#65292;&#26356;&#26032;&#30340;&#20195;&#30721;&#20197;&#21450;&#22312;&#20351;&#29992;&#25105;&#20204;&#36873;&#25321;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#26102;&#38024;&#23545;&#35813;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#21253;&#25324;&#22312;NYU&#31995;&#32479;&#19978;&#23454;&#29616;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#65292;&#20197;&#21450;&#22312;&#22810;&#27425;&#36816;&#34892;/&#31181;&#23376;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#25152;&#38656;&#30340;&#24179;&#22343;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#12290;MLCommons&#22242;&#38431;&#24050;&#32463;&#20102;&#35299;&#21040;&#25105;&#20204;&#30340;&#36827;&#23637;&#65292;&#24182;&#21487;&#33021;&#22312;&#20182;&#20204;&#30340;&#26410;&#26469;&#24037;&#20316;&#20013;&#20351;&#29992;&#24320;&#21457;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04553v1 Announce Type: cross  Abstract: In this paper, we report the performance benchmarking results of deep learning models on MLCommons' Science cloud-masking benchmark using a high-performance computing cluster at New York University (NYU): NYU Greene. MLCommons is a consortium that develops and maintains several scientific benchmarks that can benefit from developments in AI. We provide a description of the cloud-masking benchmark task, updated code, and the best model for this benchmark when using our selected hyperparameter settings. Our benchmarking results include the highest accuracy achieved on the NYU system as well as the average time taken for both training and inference on the benchmark across several runs/seeds. Our code can be found on GitHub. MLCommons team has been kept informed about our progress and may use the developed code for their future work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#38590;&#24230;&#31867;&#22411;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#38590;&#24230;&#34920;&#24449;&#20998;&#26512;&#24037;&#20855;&#21253;&#65288;H-CAT&#65289;&#65292;&#25903;&#25345;&#20840;&#38754;&#21644;&#23450;&#37327;&#35780;&#20272;&#19981;&#21516;&#38590;&#24230;&#31867;&#22411;&#19979;&#30340;HCMs&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#8220;&#38590;&#24230;&#8221;&#23450;&#20041;&#21644;&#35780;&#20272;&#30340;&#23450;&#37327;&#35782;&#21035;&#20219;&#21153;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.04551</link><description>&lt;p&gt;
&#32454;&#31350;&#26679;&#26412;&#38590;&#24230;: &#38024;&#23545;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#38590;&#24230;&#34920;&#24449;&#26041;&#27861;&#30340;&#31934;&#32454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#38590;&#24230;&#31867;&#22411;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24341;&#20837;&#20102;&#38590;&#24230;&#34920;&#24449;&#20998;&#26512;&#24037;&#20855;&#21253;&#65288;H-CAT&#65289;&#65292;&#25903;&#25345;&#20840;&#38754;&#21644;&#23450;&#37327;&#35780;&#20272;&#19981;&#21516;&#38590;&#24230;&#31867;&#22411;&#19979;&#30340;HCMs&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#8220;&#38590;&#24230;&#8221;&#23450;&#20041;&#21644;&#35780;&#20272;&#30340;&#23450;&#37327;&#35782;&#21035;&#20219;&#21153;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38590;&#20197;&#23398;&#20064;&#30340;&#26679;&#26412;&#36827;&#34892;&#34920;&#24449;&#23545;&#20110;&#24320;&#21457;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#26088;&#22312;&#35782;&#21035;&#8220;&#38590;&#8221;&#26679;&#26412;&#30340;&#38590;&#24230;&#34920;&#24449;&#26041;&#27861;&#65288;HCMs&#65289;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#8220;&#38590;&#24230;&#8221;&#30340;&#23450;&#20041;&#21644;&#35780;&#20272;&#32570;&#20047;&#20849;&#35782;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;HCMs&#20165;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#38590;&#24230;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36890;&#24120;&#20165;&#22312;&#36136;&#37327;&#25110;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#23450;&#24615;&#22320;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#22522;&#26412;&#30340;&#23450;&#37327;&#35782;&#21035;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#38590;&#24230;&#31867;&#22411;&#20998;&#31867;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38590;&#24230;&#34920;&#24449;&#20998;&#26512;&#24037;&#20855;&#21253;&#65288;H-CAT&#65289;&#65292;&#25903;&#25345;&#23545;&#38590;&#24230;&#20998;&#31867;&#19979;&#30340;HCMs&#36827;&#34892;&#20840;&#38754;&#21644;&#23450;&#37327;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#26032;&#30340;HCMs&#12289;&#38590;&#24230;&#31867;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;H-CAT&#26469;&#35780;&#20272;8&#31181;&#38590;&#24230;&#31867;&#22411;&#20013;&#30340;13&#31181;&#19981;&#21516;HCMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04551v1 Announce Type: new  Abstract: Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify "hard" samples. However, there is a lack of consensus regarding the definition and evaluation of "hardness". Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encomp
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#24179;&#34913;&#22312;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#21487;&#20197;&#37096;&#20998;&#25913;&#21892;&#20559;&#35265;&#38382;&#39064;&#65292;&#28982;&#32780;&#20250;&#23545;&#36136;&#37327;&#20135;&#29983;&#22797;&#26434;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04547</link><description>&lt;p&gt;
CLIP&#21435;&#20559;&#35265;&#65306;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#24179;&#34913;&#25968;&#25454;&#26377;&#22810;&#22823;&#29992;&#22788;&#65311;
&lt;/p&gt;
&lt;p&gt;
CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04547
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24179;&#34913;&#22312;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#21487;&#20197;&#37096;&#20998;&#25913;&#21892;&#20559;&#35265;&#38382;&#39064;&#65292;&#28982;&#32780;&#20250;&#23545;&#36136;&#37327;&#20135;&#29983;&#22797;&#26434;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#24179;&#34913;&#23545;&#20110;&#20943;&#36731;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20013;&#30340;&#20559;&#35265;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#30003;&#20102;&#20197;&#21069;&#30340;&#32467;&#35770;&#65292;&#21363;CLIP&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21560;&#25910;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#27169;&#24577;&#26102;&#21051;&#21305;&#37197;&#65288;M4&#65289;&#65292;&#26088;&#22312;&#20943;&#23569;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#21644;&#20851;&#32852;&#20559;&#35265;&#65288;&#21363;&#19968;&#38454;&#21644;&#20108;&#38454;&#32479;&#35745;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;M4&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#12289;&#34920;&#31034;&#21644;&#25968;&#25454;&#22823;&#23567;&#31561;&#21508;&#31181;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;CLIP&#23398;&#20064;&#21644;&#28040;&#38500;&#20559;&#35265;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#25269;&#21046;&#34920;&#31034;&#20559;&#35265;&#65292;&#20294;&#23545;&#20851;&#32852;&#20559;&#35265;&#30340;&#24433;&#21709;&#36880;&#28176;&#20943;&#24369;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#24179;&#34913;&#23545;&#36136;&#37327;&#26377;&#30528;&#22797;&#26434;&#30340;&#24433;&#21709;&#65306;&#23427;&#20542;&#21521;&#20110;&#25913;&#21892;&#20998;&#31867;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04547v1 Announce Type: cross  Abstract: We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Inte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#31649;&#29702;&#25928;&#29575;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#20419;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04546</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#30340;&#26550;&#26500;&#34013;&#22270;
&lt;/p&gt;
&lt;p&gt;
Architectural Blueprint For Heterogeneity-Resilient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#31649;&#29702;&#25928;&#29575;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#20419;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26550;&#26500;&#35299;&#20915;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#30456;&#23545;&#20110;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#31649;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#20419;&#36827;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04546v1 Announce Type: new  Abstract: This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#28145;&#24191;&#27531;&#24046;&#32593;&#32476;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#20351;&#20801;&#35768;&#32553;&#25918;&#22240;&#23376;&#38543;&#28145;&#24230;&#20943;&#23567;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.04545</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#36866;&#30340;&#32553;&#25918;&#22240;&#23376;&#25552;&#39640;&#28145;&#24191;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improve Generalization Ability of Deep Wide Residual Network with A Suitable Scaling Factor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04545
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#28145;&#24191;&#27531;&#24046;&#32593;&#32476;&#20013;&#20351;&#29992;&#36866;&#24403;&#30340;&#32553;&#25918;&#22240;&#23376;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#20351;&#20801;&#35768;&#32553;&#25918;&#22240;&#23376;&#38543;&#28145;&#24230;&#20943;&#23567;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;ResNets&#65289;&#22312;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28145;&#24191;&#27531;&#24046;&#32593;&#32476;&#20013;&#27531;&#24046;&#20998;&#25903;&#19978;&#30340;&#21512;&#36866;&#32553;&#25918;&#22240;&#23376;&#65288;&#29992;$\alpha$&#34920;&#31034;&#65289;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;$\alpha$&#26159;&#19968;&#20010;&#24120;&#25968;&#65292;&#30001;&#27531;&#24046;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;RNTK&#65289;&#24341;&#21457;&#30340;&#20989;&#25968;&#31867;&#22312;&#28145;&#24230;&#36235;&#36817;&#26080;&#31351;&#26102;&#26159;&#28176;&#36817;&#19981;&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65306;&#21363;&#20351;&#25105;&#20204;&#20801;&#35768;$\alpha$&#38543;&#30528;&#28145;&#24230;$L$&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#36864;&#21270;&#29616;&#35937;&#20173;&#21487;&#33021;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#24403;$\alpha$&#19982;$L$&#24555;&#36895;&#20943;&#23567;&#26102;&#65292;&#20351;&#29992;&#28145;&#23618;RNTK&#36827;&#34892;&#26680;&#22238;&#24402;&#65292;&#24182;&#19988;&#22312;&#26089;&#20572;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26368;&#22823;&#36895;&#29575;&#65292;&#21069;&#25552;&#26159;&#30446;&#26631;&#22238;&#24402;&#20989;&#25968;&#33853;&#22312;&#19982;&#26080;&#38480;&#28145;&#24230;RNTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27169;&#25311;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04545v1 Announce Type: new  Abstract: Deep Residual Neural Networks (ResNets) have demonstrated remarkable success across a wide range of real-world applications. In this paper, we identify a suitable scaling factor (denoted by $\alpha$) on the residual branch of deep wide ResNets to achieve good generalization ability. We show that if $\alpha$ is a constant, the class of functions induced by Residual Neural Tangent Kernel (RNTK) is asymptotically not learnable, as the depth goes to infinity. We also highlight a surprising phenomenon: even if we allow $\alpha$ to decrease with increasing depth $L$, the degeneration phenomenon may still occur. However, when $\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK with early stopping can achieve the minimax rate provided that the target regression function falls in the reproducing kernel Hilbert space associated with the infinite-depth RNTK. Our simulation studies on synthetic data and real classification task
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04529</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21319;&#25968;&#25454;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Enhancing Data Quality in Federated Fine-Tuning of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04529
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#65292;&#21487;&#20197;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#24403;&#21069;&#24773;&#26223;&#20013;&#65292;&#23384;&#22312;&#30528;&#23545;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#30340;&#26174;&#33879;&#20381;&#36182;&#65292;&#32780;&#26681;&#25454;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#24050;&#25509;&#36817;&#26543;&#31469;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#23558;&#22810;&#20010;&#19987;&#19994;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#28304;&#36827;&#34892;&#21327;&#20316;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22320;&#26041;&#24615;&#35757;&#32451;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#37030;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#12290;&#35813;&#31649;&#36947;&#35745;&#31639;&#21453;&#26144;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#20998;&#25968;&#65292;&#24182;&#30830;&#23450;&#19968;&#20010;&#20840;&#23616;&#38408;&#20540;&#20197;&#23454;&#29616;&#32479;&#19968;&#26631;&#20934;&#65292;&#26088;&#22312;&#25552;&#39640;&#20840;&#23616;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#36136;&#37327;&#25511;&#21046;&#31649;&#36947;&#20419;&#36827;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04529v1 Announce Type: cross  Abstract: In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#19979;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04526</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25289;&#26364;&#20809;&#35889;&#30340;&#39640;&#20809;&#35889;&#35299;&#28151;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#39640;&#20809;&#35889;&#35299;&#28151;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#19979;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26364;&#20809;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#38750;&#30772;&#22351;&#24615;&#12289;&#26080;&#26631;&#35760;&#30340;&#26041;&#24335;&#34920;&#24449;&#26679;&#21697;&#30340;&#21270;&#23398;&#32452;&#25104;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20174;&#28151;&#21512;&#20998;&#23376;&#29289;&#31181;&#30340;&#20449;&#21495;&#20013;&#35299;&#28151;&#65292;&#20197;&#35782;&#21035;&#20986;&#29616;&#30340;&#20010;&#20307;&#32452;&#20998;&#21450;&#20854;&#27604;&#20363;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;&#21270;&#23398;&#35745;&#37327;&#23398;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#22788;&#29702;&#23454;&#36341;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#28151;&#21512;&#24773;&#20917;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#22522;&#20110;&#39640;&#20809;&#35889;&#35299;&#28151;&#30340;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#20869;&#37096;&#21019;&#24314;&#30340;&#21512;&#25104;&#21644;&#23454;&#39564;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#36827;&#34892;&#31995;&#32479;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#35299;&#28151;&#26041;&#27861;&#30456;&#27604;&#65292;&#35299;&#28151;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22797;&#26434;&#29983;&#29289;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#20174;&#21333;&#26680;&#32454;&#32990;&#30340;&#20307;&#31215;&#25289;&#26364;&#25104;&#20687;&#25968;&#25454;&#20013;&#25913;&#21892;&#30340;&#29983;&#29289;&#21270;&#23398;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04526v1 Announce Type: cross  Abstract: Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04523</link><description>&lt;p&gt;
T-TAME&#65306;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers&#21644;&#20854;&#20182;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#24555;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#26159;&#22312;&#38656;&#35201;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#37319;&#29992;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#36866;&#24212;&#21040;&#35270;&#35273;Transformer&#30340;&#26032;&#33539;&#24335;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;Transformer&#20860;&#23481;&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#35299;&#37322;&#65292;&#36825;&#26159;&#19968;&#31181;&#35828;&#26126;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#21367;&#31215;&#25110;&#31867;&#20284;Vision Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#31934;&#31616;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;&#21518;&#65292;&#35299;&#37322;&#22270;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#20986;&#65307;&#36825;&#20123;&#35299;&#37322;&#22270;&#21487;&#20197;&#19982;Convolutional Neural Networks&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#22270;&#30456;&#23218;&#32654;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the "black box" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04493</link><description>&lt;p&gt;
&#20351;&#22270;&#20687;&#30495;&#23454;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What makes an image realistic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04493
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#30475;&#36215;&#26469;&#30495;&#23454;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#36824;&#26159;&#35270;&#39057;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#21270;&#29616;&#23454;&#20027;&#20041;&#65292;&#21363;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;&#20174;&#31639;&#27861;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#21333;&#29420;&#19981;&#33021;&#35299;&#20915;&#23427;&#65292;&#20197;&#21450;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#65292;&#19981;&#20687;&#23545;&#25239;&#24615;&#35780;&#35770;&#32773;&#37027;&#26679;&#38656;&#35201;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#29992;&#35780;&#35770;&#32773;&#24182;&#19981;&#31435;&#21363;&#23454;&#29992;&#65292;&#20294;&#23427;&#20204;&#26082;&#21487;&#20197;&#20316;&#20026;&#24341;&#23548;&#23454;&#38469;&#23454;&#29616;&#30340;&#21271;&#26497;&#26143;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04484</link><description>&lt;p&gt;
&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#65306;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#21307;&#23398;&#25104;&#20687;&#20998;&#31867;&#31639;&#27861;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#24120;&#21033;&#29992;ImageNet&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#20174;&#33258;&#28982;&#21040;&#21307;&#23398;&#22270;&#20687;&#30340;&#39046;&#22495;&#36716;&#21464;&#20419;&#20351;&#20102;&#35832;&#22914;RadImageNet &#31561;&#26367;&#20195;&#26041;&#26696;&#30340;&#20986;&#29616;&#65292;&#24448;&#24448;&#23637;&#31034;&#20986;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25552;&#21319;&#26159;&#26469;&#33258;&#20110;&#25913;&#21892;&#30340;&#27867;&#21270;&#36824;&#26159;&#24555;&#25463;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20844;&#24320;&#30340;&#33016;&#37096;X&#20809;&#29255;&#21644;CT&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;--&#26080;&#35770;&#26159;&#21512;&#25104;&#30340;&#36824;&#26159;&#20174;&#25968;&#25454;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;ImageNet &#21644; RadImageNet &#23454;&#29616;&#20102;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#28982;&#32780; ImageNet &#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#23637;&#31867;&#20284;&#23454;&#39564;&#26469;&#37325;&#26032;&#23457;&#35270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#21487;&#22312;https://github.com/DovileDo/source-mat &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04484v1 Announce Type: cross  Abstract: Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-mat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.04482</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#24863;&#30693;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
On the Topology Awareness and Generalization Performance of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04482
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22312;&#22270;&#19978;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#22270;&#30340;&#22266;&#26377;&#25299;&#25169;&#23646;&#24615;&#65292;&#21363;GNNs&#30340;&#25299;&#25169;&#24863;&#30693;&#12290;&#23613;&#31649;GNNs&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25299;&#25169;&#24863;&#30693;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#25506;&#35752;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19982;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;(I.I.D.)&#30340;&#20551;&#35774;&#32972;&#36947;&#32780;&#39536;&#30340;&#33410;&#28857;&#32423;&#20219;&#21153;&#12290;&#23545;&#20110;GNNs&#30340;&#25299;&#25169;&#24863;&#30693;&#30340;&#31934;&#30830;&#23450;&#20041;&#21644;&#34920;&#24449;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#19981;&#21516;&#25299;&#25169;&#29305;&#24449;&#30340;&#24773;&#20917;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;GNNs&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04482v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data. A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs. Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature. Using this framework, we
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35843;&#26597;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#23545;MLP&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20803;&#25968;&#25454;&#38598;TSBench&#65292;&#24378;&#35843;&#35843;&#25972;&#36825;&#20123;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04477</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;MLP&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35843;&#26597;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#23545;MLP&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20803;&#25968;&#25454;&#38598;TSBench&#65292;&#24378;&#35843;&#35843;&#25972;&#36825;&#20123;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35797;&#22270;&#36890;&#36807;&#20998;&#26512;&#36807;&#21435;&#30340;&#36235;&#21183;&#21644;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#23613;&#31649;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26576;&#20123;&#20851;&#38190;&#26041;&#38754;&#20173;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32771;&#23519;&#19982;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;&#22914;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;&#39564;&#35777;&#31574;&#30053;&#65289;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#29616;&#20195;MLP&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#28041;&#21450;20&#20010;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#25968;&#25454;&#38598;4800&#20010;&#37197;&#32622;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35843;&#25972;&#36825;&#20123;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36804;&#20170;&#20026;&#27490;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26368;&#22823;&#20803;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;TSBench&#65292;&#21253;&#25324;97200&#27425;&#35780;&#20272;&#65292;&#27604;&#35813;&#39046;&#22495;&#20808;&#21069;&#24037;&#20316;&#22686;&#21152;&#20102;20&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#21019;&#24314;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04477v1 Announce Type: new  Abstract: Time series forecasting attempts to predict future events by analyzing past trends and patterns. Although well researched, certain critical aspects pertaining to the use of deep learning in time series forecasting remain ambiguous. Our research primarily focuses on examining the impact of specific hyperparameters related to time series, such as context length and validation strategy, on the performance of the state-of-the-art MLP model in time series forecasting. We have conducted a comprehensive series of experiments involving 4800 configurations per dataset across 20 time series forecasting datasets, and our findings demonstrate the importance of tuning these parameters. Furthermore, in this work, we introduce the largest metadataset for timeseries forecasting to date, named TSBench, comprising 97200 evaluations, which is a twentyfold increase compared to previous works in the field. Finally, we demonstrate the utility of the created m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;&#65292;&#24182;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04468</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35843;&#26597;&#65306;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38754;&#20020;&#30340;&#19981;&#24179;&#34913;&#12289;&#22122;&#22768;&#12289;&#38544;&#31169;&#21644;OOD&#25361;&#25112;&#65292;&#24182;&#33268;&#21147;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04468v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#22495; &#25688;&#35201;: &#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#34920;&#29616;&#20986;&#26222;&#36866;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#28085;&#30422;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#21270;&#23398;&#12289;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27169;&#22411;&#30340;&#35757;&#32451;&#29615;&#22659;&#24448;&#24448;&#36828;&#38750;&#29702;&#24819;&#65292;&#30001;&#20110;&#21508;&#31181;&#19981;&#21033;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#12289;&#38169;&#35823;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#12289;&#25935;&#24863;&#20449;&#24687;&#30340;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#23545;&#20110;OOD&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#25913;&#21892;GNN&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04468v1 Announce Type: cross  Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive surv
&lt;/p&gt;</description></item><item><title>Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04453</link><description>&lt;p&gt;
Vlearn&#65306;&#20351;&#29992;&#39640;&#25928;&#29366;&#24577;&#20540;&#20989;&#25968;&#20272;&#35745;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04453
&lt;/p&gt;
&lt;p&gt;
Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#36825;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#21464;&#24471;&#26840;&#25163;&#12290;&#36825;&#20123;&#31639;&#27861;&#32463;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#21363;&#23427;&#20204;&#22312;&#22788;&#29702;&#32500;&#24230;&#28798;&#38590;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#32500;&#25252;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#21464;&#24471;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vlearn&#30340;&#26032;&#22411;&#31163;&#31574;&#30053;&#20449;&#20219;&#21306;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#35201;&#27714;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#20174;&#32780;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;Vlearn&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;Vlearn&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19982;&#32431;&#29366;&#24577;&#20540;&#20989;&#25968;&#23398;&#20064;&#30456;&#20851;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se
&lt;/p&gt;</description></item><item><title>&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.04451</link><description>&lt;p&gt;
&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#19982;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks and Privacy in Topic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04451
&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26356;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#20027;&#39064;&#27169;&#22411;&#65292;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#33258;&#20449;&#22320;&#35782;&#21035;Latent Dirichlet Allocation&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#38544;&#31169;&#39118;&#38505;&#24182;&#19981;&#20165;&#38480;&#20110;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20027;&#39064;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31169;&#23494;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;DP&#35789;&#27719;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#23637;&#31034;&#23427;&#19981;&#20165;&#25913;&#21892;&#20102;&#38544;&#31169;&#24615;&#65292;&#32780;&#19988;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;</title><link>https://arxiv.org/abs/2403.04447</link><description>&lt;p&gt;
FRRI&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRRI: a novel algorithm for fuzzy-rough rule induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04447
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19979;&#19968;&#20010;&#21069;&#27839;&#12290;&#22312;&#23547;&#25214;&#30333;&#30418;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;-&#19982;&#38543;&#26426;&#26862;&#26519;&#25110;&#31070;&#32463;&#32593;&#32476;&#31561;&#40657;&#30418;&#27169;&#22411;&#30456;&#23545;&#24212;&#65292;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#26159;&#19968;&#20010;&#21512;&#20046;&#36923;&#36753;&#19988;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#35268;&#21017;&#21487;&#20197;&#34987;&#20154;&#31867;&#36731;&#26494;&#29702;&#35299;&#12290;&#27169;&#31946;&#21644;&#31895;&#31961;&#38598;&#29702;&#35770;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31181;&#21407;&#22411;&#65292;&#20960;&#20046;&#24635;&#26159;&#20998;&#24320;&#24212;&#29992;&#12290;&#30001;&#20110;&#35268;&#21017;&#24402;&#32435;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#28041;&#21450;&#22522;&#20110;&#31561;&#20215;&#31867;&#27010;&#24565;&#30340;&#31890;&#35745;&#31639;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#26159;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;QuickRules&#31639;&#27861;&#26159;&#21033;&#29992;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#36827;&#34892;&#35268;&#21017;&#24402;&#32435;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#23427;&#22522;&#20110;QuickReduct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#20915;&#31574;&#32422;&#31616;&#30340;&#36138;&#23146;&#31639;&#27861;&#12290;QuickRules &#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#27604;&#20854;&#20182;&#35268;&#21017;&#24402;&#32435;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#35201;&#35780;&#20272;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23601;&#38656;&#35201;&#20174;&#22522;&#30784;&#24320;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04447v1 Announce Type: cross  Abstract: Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#21327;&#20316;&#36873;&#25321;&#26597;&#35810;&#20989;&#25968;&#30340;&#28857;&#65292;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.04442</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#20195;&#29702;&#30340;&#21512;&#20316;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cooperative Bayesian Optimization for Imperfect Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04442
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#21327;&#20316;&#36873;&#25321;&#26597;&#35810;&#20989;&#25968;&#30340;&#28857;&#65292;&#36890;&#36807;&#25112;&#30053;&#35268;&#21010;&#23454;&#29616;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#20248;&#21270;&#20004;&#20010;&#21464;&#37327;&#30340;&#40657;&#21283;&#23376;&#20989;&#25968;&#65292;&#20854;&#20013;&#20004;&#20010;&#20195;&#29702;&#32773;&#19968;&#36215;&#36873;&#25321;&#22312;&#21738;&#20123;&#28857;&#19978;&#26597;&#35810;&#20989;&#25968;&#65292;&#20294;&#27599;&#20010;&#21482;&#25511;&#21046;&#19968;&#20010;&#21464;&#37327;&#12290;&#36825;&#20010;&#35774;&#32622;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21512;&#20316;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#24110;&#21161;&#20854;&#29992;&#25143;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#26159;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#20026;&#39034;&#24207;&#20915;&#31574;&#65292;&#20854;&#20013;&#25105;&#20204;&#25511;&#21046;&#30340;&#20195;&#29702;&#25226;&#29992;&#25143;&#27169;&#25311;&#20026;&#23545;&#20989;&#25968;&#20855;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#35745;&#31639;&#26377;&#29702;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#26597;&#35810;&#30340;&#25112;&#30053;&#35268;&#21010;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#20840;&#23616;&#26368;&#22823;&#20540;&#65292;&#21482;&#35201;&#29992;&#25143;&#36991;&#20813;&#36807;&#24230;&#25506;&#32034;&#12290;&#36825;&#31181;&#35268;&#21010;&#26159;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#20197;&#21450;&#36171;&#20104;&#20195;&#29702;&#20855;&#26377;&#32771;&#34385;&#20445;&#23432;&#20449;&#24565;&#26356;&#26032;&#21644;&#25506;&#32034;&#24615;&#37319;&#26679;&#30340;&#29992;&#25143;&#27169;&#22411;&#32780;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04442v1 Announce Type: cross  Abstract: We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;"Human to Humanoid (H2O)"&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#21333;&#20010;RGB&#30456;&#26426;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#65292;&#24182;&#25104;&#21151;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#36965;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.04436</link><description>&lt;p&gt;
&#23398;&#20064;&#20154;&#26426;&#22120;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;"Human to Humanoid (H2O)"&#65292;&#23454;&#29616;&#20102;&#21033;&#29992;&#21333;&#20010;RGB&#30456;&#26426;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#65292;&#24182;&#25104;&#21151;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#36965;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;"Human to Humanoid (H2O)"&#36825;&#19968;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#20165;&#26377;&#30340;&#19968;&#20010;RGB&#30456;&#26426;&#23454;&#29616;&#23545;&#20840;&#23610;&#23544;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#23454;&#26102;&#20840;&#36523;&#36965;&#25805;&#20316;&#12290;&#20026;&#20102;&#20026;&#20154;&#24418;&#26426;&#22120;&#20154;&#21019;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#36816;&#21160;&#37325;&#23450;&#21521;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#8220;&#20174;&#27169;&#25311;&#21040;&#25968;&#25454;&#8221;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#19968;&#20010;&#29305;&#26435;&#36816;&#21160;&#27169;&#20223;&#22120;&#26469;&#31579;&#36873;&#21644;&#36873;&#25321;&#21487;&#34892;&#30340;&#21160;&#20316;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20351;&#29992;&#36825;&#20123;&#31934;&#32454;&#30340;&#21160;&#20316;&#35757;&#32451;&#19968;&#20010;&#31283;&#20581;&#30340;&#23454;&#26102;&#20154;&#24418;&#26426;&#22120;&#20154;&#21160;&#20316;&#27169;&#20223;&#22120;&#65292;&#24182;&#20197;&#38646;&#27425;&#35797;&#39564;&#30340;&#26041;&#24335;&#23558;&#20854;&#36801;&#31227;&#21040;&#30495;&#23454;&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#12290;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23545;&#21160;&#24577;&#20840;&#36523;&#36816;&#21160;&#30340;&#36965;&#25805;&#20316;&#65292;&#21253;&#25324;&#34892;&#36208;&#12289;&#21518;&#36339;&#12289;&#36386;&#29699;&#12289;&#36716;&#36523;&#12289;&#25381;&#25163;&#12289;&#25512;&#21160;&#12289;&#25331;&#20987;&#31561;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23454;&#29616;&#22522;&#20110;&#23398;&#20064;&#30340;&#23454;&#26102;&#20840;&#36523;&#20154;&#24418;&#26426;&#22120;&#20154;&#36965;&#25511;&#30340;&#39318;&#27425;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04436v1 Announce Type: cross  Abstract: We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#22823;&#26497;&#23567;&#20248;&#21270;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#32534;&#30721;&#35299;&#30721;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;</title><link>https://arxiv.org/abs/2403.04431</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#22686;&#24378;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Boosting Fairness and Robustness in Over-the-Air Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26497;&#22823;&#26497;&#23567;&#20248;&#21270;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#32534;&#30721;&#35299;&#30721;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#22312;&#20998;&#25955;&#24335;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#26497;&#22823;&#26497;&#23567;&#20248;&#21270;&#25552;&#20379;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38382;&#39064;&#30340;&#22806;&#23481;&#24418;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#31639;&#27861;&#25910;&#25947;&#20110;&#26497;&#22823;&#26497;&#23567;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#21453;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#36890;&#36807;&#22797;&#26434;&#30340;&#32534;&#30721;&#35299;&#30721;&#26041;&#26696;&#23545;&#20449;&#36947;&#31995;&#25968;&#36827;&#34892;&#37325;&#24314;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04431v1 Announce Type: new  Abstract: Over-the-Air Computation is a beyond-5G communication strategy that has recently been shown to be useful for the decentralized training of machine learning models due to its efficiency. In this paper, we propose an Over-the-Air federated learning algorithm that aims to provide fairness and robustness through minmax optimization. By using the epigraph form of the problem at hand, we show that the proposed algorithm converges to the optimal solution of the minmax problem. Moreover, the proposed approach does not require reconstructing channel coefficients by complex encoding-decoding schemes as opposed to state-of-the-art approaches. This improves both efficiency and privacy.
&lt;/p&gt;</description></item><item><title>&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#38656;&#37327;&#21270;&#30340;&#33021;&#25928;&#32852;&#21512;&#29983;&#25104;&#25193;&#25955;&#26041;&#27861;&#20197;&#35299;&#20915;&#22823;&#22411;GAI&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#28040;&#32791;&#21644;&#33021;&#37327;&#28040;&#32791;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04430</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#32511;&#33394;&#32852;&#21512;&#29983;&#25104;&#25193;&#25955;&#30340;&#25353;&#38656;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
On-demand Quantization for Green Federated Generative Diffusion in Mobile Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04430
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#38656;&#37327;&#21270;&#30340;&#33021;&#25928;&#32852;&#21512;&#29983;&#25104;&#25193;&#25955;&#26041;&#27861;&#20197;&#35299;&#20915;&#22823;&#22411;GAI&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#36890;&#20449;&#28040;&#32791;&#21644;&#33021;&#37327;&#28040;&#32791;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04430v1 &#20844;&#21578;&#31867;&#22411;:&#26032;     &#25688;&#35201;:&#29983;&#25104;&#20154;&#24037;&#26234;&#33021; (GAI) &#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#29983;&#20135;&#21147;&#21644;&#21019;&#36896;&#21147;&#65292;&#20363;&#22914;&#20803;&#23431;&#23449;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#12290;&#32852;&#21512;&#23398;&#20064;&#26159;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#26377;&#25928;&#35757;&#32451;GAI&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#35757;&#32451;&#22823;&#22411;GAI&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65289;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#19982;&#36890;&#20449;&#28040;&#32791;&#30456;&#20851;&#30340;&#26174;&#30528;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25152;&#28040;&#32791;&#30340;&#23454;&#38469;&#33021;&#37327;&#65292;&#21152;&#19978;&#36793;&#32536;&#35774;&#22791;&#30340;&#26377;&#38480;&#36164;&#28304;&#21644;&#32593;&#32476;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#20026;&#25552;&#39640;GAI&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#30340;&#25353;&#38656;&#37327;&#21270;&#33021;&#25928;&#32852;&#21512;&#25193;&#25955;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#32593;&#32476;&#21160;&#24577;&#37327;&#21270;&#30340;&#32852;&#21512;&#25193;&#25955;&#35757;&#32451;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04430v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GAI) shows remarkable productivity and creativity in Mobile Edge Networks, such as the metaverse and the Industrial Internet of Things. Federated learning is a promising technique for effectively training GAI models in mobile edge networks due to its data distribution. However, there is a notable issue with communication consumption when training large GAI models like generative diffusion models in mobile edge networks. Additionally, the substantial energy consumption associated with training diffusion-based models, along with the limited resources of edge devices and complexities of network environments, pose challenges for improving the training efficiency of GAI models. To address this challenge, we propose an on-demand quantized energy-efficient federated diffusion approach for mobile edge networks. Specifically, we first design a dynamic quantized federated diffusion training scheme considering va
&lt;/p&gt;</description></item><item><title>&#38477;&#32500;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#25972;&#21512;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.04429</link><description>&lt;p&gt;
&#25506;&#31350;&#38477;&#32500;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Dimensionality Reduction on Anomaly Detection Performance in Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04429
&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#25972;&#21512;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;MUTANT&#21644;Anomaly-Transformer&#27169;&#22411;&#65292;&#23545;&#38477;&#32500;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#25972;&#21512;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#28041;&#21450;&#23545;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;MSL&#12289;SMAP&#21644;SWaT&#65289;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25152;&#30740;&#31350;&#30340;&#38477;&#32500;&#25216;&#26415;&#21253;&#25324;PCA&#12289;UMAP&#12289;&#38543;&#26426;&#25237;&#24433;&#21644;t-SNE&#65292;&#27599;&#31181;&#25216;&#26415;&#22312;&#31616;&#21270;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#37117;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#38477;&#32500;&#19981;&#20165;&#26377;&#21161;&#20110;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#32780;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#35266;&#23519;&#21040;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#21516;&#26102;&#65292;&#24403;&#32500;&#25968;&#20943;&#21322;&#26102;&#65292;&#35757;&#32451;&#26102;&#38388;&#20998;&#21035;&#20943;&#23569;&#20102;&#32422;300\%&#21644;650\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04429v1 Announce Type: new  Abstract: This paper presents an extensive empirical study on the integration of dimensionality reduction techniques with advanced unsupervised time series anomaly detection models, focusing on the MUTANT and Anomaly-Transformer models. The study involves a comprehensive evaluation across three different datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing for a robust assessment of the models' capabilities in varied contexts. The dimensionality reduction techniques examined include PCA, UMAP, Random Projection, and t-SNE, each offering distinct advantages in simplifying high-dimensional data. Our findings reveal that dimensionality reduction not only aids in reducing computational complexity but also significantly enhances anomaly detection performance in certain scenarios. Moreover, a remarkable reduction in training times was observed, with reductions by approximately 300\% and 650\% when dimensionality was halved and m
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04405</link><description>&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04405
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Functional Isolation Forest (FIF)&#26159;&#19968;&#31181;&#38024;&#23545;&#21151;&#33021;&#25968;&#25454;&#35774;&#35745;&#30340;&#26368;&#26032;&#19968;&#27969;&#24322;&#24120;&#26816;&#27979;(AD)&#31639;&#27861;&#12290;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#26641;&#20998;&#21306;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26354;&#32447;&#35266;&#27979;&#25237;&#24433;&#21040;&#36890;&#36807;&#32447;&#24615;&#20869;&#31215;&#32472;&#21046;&#30340;&#35789;&#20856;&#19978;&#26469;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;Signature Isolation Forest&#8221;&#65292;&#19968;&#31181;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#31614;&#21517;&#21464;&#25442;&#30340;&#26032;&#39062;AD&#31639;&#27861;&#31867;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#26469;&#28040;&#38500;FIF&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#29305;&#21035;&#38024;&#23545;FIF&#20869;&#31215;&#30340;&#32447;&#24615;&#24615;&#21644;&#35789;&#20856;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#25935;&#24863;</title><link>https://arxiv.org/abs/2403.04385</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impacts of Color and Texture Distortions on Earth Observation Data in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#20013;&#39068;&#33394;&#21644;&#32441;&#29702;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#25935;&#24863;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29289;&#35206;&#30422;&#20998;&#31867;&#21644;&#21464;&#21270;&#26816;&#27979;&#26159;&#36965;&#24863;&#21644;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#30340;&#20004;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#20013;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#30410;&#22788;&#12290;&#21367;&#31215;&#21644;&#22522;&#20110;Transformer&#30340;U-net&#27169;&#22411;&#26159;&#36825;&#20123;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#22823;&#35268;&#27169;&#26631;&#27880;EO&#25968;&#25454;&#38598;&#30340;&#22686;&#21152;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#23545;&#36755;&#20837;EO&#25968;&#25454;&#30340;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#25512;&#26029;&#26399;&#38388;&#23545;&#36755;&#20837;EO&#25968;&#25454;&#36827;&#34892;&#20960;&#31181;&#22522;&#20110;&#39068;&#33394;&#21644;&#32441;&#29702;&#30340;&#22833;&#30495;&#26102;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#32771;&#34385;&#21040;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#27809;&#26377;&#36825;&#31181;&#22833;&#30495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22320;&#29289;&#35206;&#30422;&#20998;&#31867;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#36890;&#24120;&#23545;&#32441;&#29702;&#22833;&#30495;&#27604;&#39068;&#33394;&#22833;&#30495;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04385v1 Announce Type: cross  Abstract: Land cover classification and change detection are two important applications of remote sensing and Earth observation (EO) that have benefited greatly from the advances of deep learning. Convolutional and transformer-based U-net models are the state-of-the-art architectures for these tasks, and their performances have been boosted by an increased availability of large-scale annotated EO datasets. However, the influence of different visual characteristics of the input EO data on a model's predictions is not well understood. In this work we systematically examine model sensitivities with respect to several color- and texture-based distortions on the input EO data during inference, given models that have been trained without such distortions. We conduct experiments with multiple state-of-the-art segmentation networks for land cover classification and show that they are in general more sensitive to texture than to color distortions. Beyond
&lt;/p&gt;</description></item><item><title>LoCoDL&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#30340;&#24378;&#20984;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.04348</link><description>&lt;p&gt;
LoCoDL: &#20855;&#26377;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#30340;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04348
&lt;/p&gt;
&lt;p&gt;
LoCoDL&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#30340;&#24378;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#20013;&#65292;&#29978;&#33267;&#22312;&#29616;&#20195;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#30001;&#20110;&#36890;&#20449;&#36895;&#24230;&#24930;&#19988;&#25104;&#26412;&#39640;&#65292;&#36890;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LoCoDL&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#36825;&#20004;&#31181;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#26412;&#22320;&#35757;&#32451;&#38477;&#20302;&#20102;&#36890;&#20449;&#39057;&#29575;&#65292;&#21387;&#32553;&#21017;&#26159;&#21457;&#36865;&#30701;&#30340;&#27604;&#29305;&#27969;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#28014;&#28857;&#25968;&#21521;&#37327;&#12290;LoCoDL&#36866;&#29992;&#20110;&#22823;&#31867;&#21035;&#30340;&#26080;&#20559;&#21387;&#32553;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#26041;&#27861;&#12290;LoCoDL&#22312;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36825;&#21462;&#20915;&#20110;&#20989;&#25968;&#30340;&#26465;&#20214;&#25968;&#21644;&#27169;&#22411;&#32500;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#24378;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;LoCoDL&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.
&lt;/p&gt;</description></item><item><title>RL-CFR&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#21160;&#20316;&#25277;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;MDP&#20844;&#24335;&#26500;&#24314;&#28216;&#25103;&#26641;&#65292;&#24182;&#21033;&#29992;CFR&#36827;&#34892;&#31574;&#30053;&#25512;&#23548;&#65292;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#24418;&#24335;&#21338;&#24328;&#20013;&#21462;&#24471;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04344</link><description>&lt;p&gt;
RL-CFR: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#24418;&#24335;&#21338;&#24328;&#30340;&#21160;&#20316;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
RL-CFR: Improving Action Abstraction for Imperfect Information Extensive-Form Games with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04344
&lt;/p&gt;
&lt;p&gt;
RL-CFR&#26159;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#21160;&#20316;&#25277;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;MDP&#20844;&#24335;&#26500;&#24314;&#28216;&#25103;&#26641;&#65292;&#24182;&#21033;&#29992;CFR&#36827;&#34892;&#31574;&#30053;&#25512;&#23548;&#65292;&#33021;&#22815;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#24418;&#24335;&#21338;&#24328;&#20013;&#21462;&#24471;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#21160;&#20316;&#25277;&#35937;&#22312;&#24212;&#23545;&#19981;&#23436;&#20840;&#20449;&#24687;&#24191;&#20041;&#24418;&#24335;&#21338;&#24328;&#65288;IIEFGs&#65289;&#20013;&#22823;&#35268;&#27169;&#21160;&#20316;&#31354;&#38388;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;IIEFGs&#20013;&#24222;&#22823;&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25277;&#35937;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;RL-CFR&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#21160;&#20316;&#25277;&#35937;&#12290;RL-CFR&#22522;&#20110;&#25105;&#20204;&#21019;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#34920;&#36848;&#65292;&#20854;&#20013;&#29366;&#24577;&#23545;&#24212;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#21160;&#20316;&#34920;&#31034;&#20026;&#25351;&#31034;&#29305;&#23450;&#21160;&#20316;&#25277;&#35937;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22870;&#21169;&#34987;&#23450;&#20041;&#20026;&#25152;&#36873;&#21160;&#20316;&#25277;&#35937;&#21644;&#40664;&#35748;&#21160;&#20316;&#25277;&#35937;&#20043;&#38388;&#30340;&#39044;&#26399;&#25910;&#30410;&#24046;&#24322;&#12290;RL-CFR&#26500;&#24314;&#19968;&#20010;&#24102;&#26377;RL&#24341;&#23548;&#21160;&#20316;&#25277;&#35937;&#30340;&#28216;&#25103;&#26641;&#65292;&#24182;&#21033;&#29992;&#23545;&#30424;&#24724;&#24680;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#36827;&#34892;&#31574;&#30053;&#25512;&#23548;&#12290;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04344v1 Announce Type: cross  Abstract: Effective action abstraction is crucial in tackling challenges associated with large action spaces in Imperfect Information Extensive-Form Games (IIEFGs). However, due to the vast state space and computational complexity in IIEFGs, existing methods often rely on fixed abstractions, resulting in sub-optimal performance. In response, we introduce RL-CFR, a novel reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR builds upon our innovative Markov Decision Process (MDP) formulation, with states corresponding to public information and actions represented as feature vectors indicating specific action abstractions. The reward is defined as the expected payoff difference between the selected and default action abstractions. RL-CFR constructs a game tree with RL-guided action abstractions and utilizes counterfactual regret minimization (CFR) for strategy derivation. Impressively, it can be trained from scratch, achievin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#20887;&#20313;NVM&#20869;&#23384;&#20889;&#39044;&#27979;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;XAI&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04337</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65306;&#38745;&#24577;&#20887;&#20313;NVM&#20869;&#23384;&#20889;&#39044;&#27979;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Embedded Systems Design: A Case Study of Static Redundant NVM Memory Write Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#38745;&#24577;&#20887;&#20313;NVM&#20869;&#23384;&#20889;&#39044;&#27979;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;XAI&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35774;&#35745;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23427;&#35299;&#20915;&#20102;&#38745;&#24577;&#38745;&#40664;&#23384;&#20648;&#39044;&#27979;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#36825;&#28041;&#21450;&#20165;&#22522;&#20110;&#38745;&#24577;&#31243;&#24207;&#29305;&#24449;&#35782;&#21035;&#20887;&#20313;&#20869;&#23384;&#20889;&#20837;&#12290;&#28040;&#38500;&#36825;&#31181;&#23384;&#20648;&#20250;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#21644;&#24635;&#32447;&#27969;&#37327;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#38750;&#26131;&#22833;&#24615;&#20869;&#23384;&#25216;&#26415;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#35299;&#37322;&#38745;&#40664;&#23384;&#20648;&#39044;&#27979;&#24320;&#21457;&#30456;&#20851;&#30340;ML&#27169;&#22411;&#65292;&#20197;&#21450;2&#65289;&#24212;&#29992;XAI&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;XAI&#26041;&#27861;&#26469;&#20998;&#26512;&#38745;&#40664;&#23384;&#20648;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#20026;&#38745;&#40664;&#23384;&#20648;&#39044;&#27979;&#25552;&#20379;&#20102;&#35299;&#37322;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04337v1 Announce Type: new  Abstract: This paper investigates the application of eXplainable Artificial Intelligence (XAI) in the design of embedded systems using machine learning (ML). As a case study, it addresses the challenging problem of static silent store prediction. This involves identifying redundant memory writes based only on static program features. Eliminating such stores enhances performance and energy efficiency by reducing memory access and bus traffic, especially in the presence of emerging non-volatile memory technologies. To achieve this, we propose a methodology consisting of: 1) the development of relevant ML models for explaining silent store prediction, and 2) the application of XAI to explain these models. We employ two state-of-the-art model-agnostic XAI methods to analyze the causes of silent stores. Through the case study, we evaluate the effectiveness of the methods. We find that these methods provide explanations for silent store predictions, whi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#19979;&#30340;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#31574;&#30053;&#20197;&#20445;&#35777;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04329</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#26426;&#32764;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A mechanism-informed reinforcement learning framework for shape optimization of airfoils
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#19979;&#30340;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#31574;&#30053;&#20197;&#20445;&#35777;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#32764;&#27668;&#21160;&#22806;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#26174;&#33879;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21463;&#27969;&#20307;&#21160;&#21147;&#23398;&#25511;&#21046;&#30340;&#22806;&#22411;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#37319;&#29992;&#22522;&#20110;PDEs&#30340;&#27714;&#35299;&#22120;&#20197;&#20445;&#35777;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#37197;&#32622;&#21644;&#20960;&#20309;&#24418;&#29366;&#21457;&#29983;&#20102;&#26497;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#21452;&#21152;&#26435;&#27531;&#24046;&#32593;&#26684;&#32454;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#30446;&#26631;&#20989;&#25968;&#30340;&#31934;&#30830;&#35745;&#31639;&#12290;&#20026;&#20102;&#31616;&#21270;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#24182;&#22788;&#29702;&#20960;&#20309;&#21464;&#24418;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#25289;&#26222;&#25289;&#26031;&#24179;&#28369;&#12289;&#33258;&#36866;&#24212;&#32454;&#21270;&#21644;B\'ezier&#25311;&#21512;&#31574;&#30053;&#12290;&#36825;&#31181;&#32452;&#21512;&#19981;&#20165;&#28040;&#38500;&#20102;&#32593;&#26684;&#32416;&#32544;&#65292;&#36824;&#20445;&#35777;&#20102;&#23545;&#26426;&#32764;&#20960;&#20309;&#30340;&#31934;&#30830;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21033;&#29992;B\'ezier&#26354;&#32447;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04329v1 Announce Type: cross  Abstract: In this study, we present the mechanism-informed reinforcement learning framework for airfoil shape optimization. By leveraging the twin delayed deep deterministic policy gradient algorithm for its notable stability, our approach addresses the complexities of optimizing shapes governed by fluid dynamics. The PDEs-based solver is adopted for its accuracy even when the configurations and geometries are extraordinarily changed during the exploration. Dual-weighted residual-based mesh refinement strategy is applied to ensure the accurate calculation of target functionals. To streamline the iterative optimization process and handle geometric deformations, our approach integrates Laplacian smoothing, adaptive refinement, and a B\'ezier fitting strategy. This combination not only remits mesh tangling but also guarantees a precise manipulation of the airfoil geometry. Our neural network architecture leverages B\'ezier curves for efficient dime
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21019;&#24314;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#24314;&#31569;&#27668;&#20505;&#24314;&#27169;&#65292;&#24182;&#22312;&#36793;&#32536;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#24314;&#31569;&#29289;&#36816;&#33829;&#12290;</title><link>https://arxiv.org/abs/2403.04326</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#30340;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#26234;&#33021;&#24314;&#31569;&#23460;&#20869;&#27668;&#20505;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04326
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#21019;&#24314;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#24314;&#31569;&#27668;&#20505;&#24314;&#27169;&#65292;&#24182;&#22312;&#36793;&#32536;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#24314;&#31569;&#29289;&#36816;&#33829;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04326v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#24314;&#31569;&#29615;&#22659;&#20013;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#29992;&#20110;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20197;&#20248;&#21270;&#24314;&#31569;&#36816;&#33829;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#12289;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#23545;&#24314;&#31569;&#29289;&#27668;&#20505;&#30340;&#29702;&#35299;&#12290;&#20351;&#29992;&#26412;&#20307;&#21019;&#24314;&#30340;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#30830;&#20445;&#36328;&#19981;&#21516;&#24314;&#31569;&#35774;&#22791;&#30340;&#22810;&#26679;&#26381;&#21153;&#31995;&#32479;&#20013;&#20445;&#25345;&#19968;&#33268;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#22522;&#20110;&#21019;&#24314;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#23460;&#20869;&#27668;&#20505;&#20013;&#30340;&#27169;&#24335;&#24182;&#25552;&#20379;&#35265;&#35299;&#12290;&#21442;&#25968;&#21270;&#25968;&#23383;&#23402;&#29983;&#20307;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22343;&#37096;&#32626;&#22312;&#36793;&#32536;&#19978;&#65292;&#20197;&#23454;&#29616;&#20302;&#24310;&#36831;&#21644;&#38544;&#31169;&#21512;&#35268;&#24615;&#12290;&#20316;&#20026;&#31034;&#33539;&#65292;&#23545;&#29790;&#20856;&#22885;&#26031;&#27888;&#32422;&#29305;&#20848;&#22320;&#21306;&#30340;&#19968;&#24231;&#21382;&#21490;&#24314;&#31569;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#20116;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26102;&#24207;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04326v1 Announce Type: cross  Abstract: Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations. This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings. Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings. Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights. Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance. As a demonstration, a case study was conducted in a historic building in \"Osterg\"otland, Sweden, to compare the performance of five deep learning architectures. The results indicate that the time-seri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#32858;&#31867;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#39046;&#22495;&#23581;&#35797;&#23450;&#20041;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04322</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#27169;&#25311;&#30340;&#24046;&#20998;&#36827;&#21270;&#26041;&#27861;&#29992;&#20110;&#21322;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Memetic Differential Evolution Methods for Semi-Supervised Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#32858;&#31867;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#39046;&#22495;&#23581;&#35797;&#23450;&#20041;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#21322;&#30417;&#30563;&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;(MSSC)&#38382;&#39064;&#65292;&#20854;&#20013;&#32972;&#26223;&#30693;&#35782;&#20197;&#23454;&#20363;&#32423;&#32422;&#26463;&#30340;&#24418;&#24335;&#32473;&#23450;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#8220;&#24517;&#36830;&#25509;&#8221;&#21644;&#8220;&#38750;&#36830;&#25509;&#8221;&#32422;&#26463;&#65292;&#27599;&#20010;&#32422;&#26463;&#25351;&#31034;&#20004;&#20010;&#25968;&#25454;&#38598;&#28857;&#26159;&#21542;&#24212;&#35813;&#20851;&#32852;&#21040;&#21516;&#19968;&#20010;&#25110;&#19981;&#21516;&#30340;&#31751;&#20013;&#12290;&#36825;&#20123;&#32422;&#26463;&#30340;&#23384;&#22312;&#20351;&#24471;&#38382;&#39064;&#33267;&#23569;&#19982;&#20854;&#26080;&#30417;&#30563;&#29256;&#26412;&#19968;&#26679;&#22256;&#38590;&#65306;&#19981;&#20877;&#27599;&#20010;&#28857;&#37117;&#20851;&#32852;&#21040;&#20854;&#26368;&#36817;&#30340;&#31751;&#20013;&#24515;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#20851;&#38190;&#25805;&#20316;&#65288;&#22914;&#20998;&#37197;&#27493;&#39588;&#65289;&#20013;&#36827;&#34892;&#19968;&#20123;&#20462;&#25913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#30452;&#25509;&#25193;&#23637;&#20102;&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#32858;&#31867;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#26032;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#20195;&#34920;&#20102;&#31532;&#19968;&#27425;&#23581;&#35797;&#23450;&#20041;&#19968;&#20010;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04322v1 Announce Type: cross  Abstract: In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account "must-link" and "cannot-link" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.04317</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Online Adaptation of Language Models with a Memory of Amortized Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20449;&#24687;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#20256;&#25773;&#65292;&#21363;&#20351;&#24320;&#21457;&#25104;&#26412;&#24040;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#24456;&#24555;&#36807;&#26102;&#12290;&#37492;&#20110;&#20445;&#25345;&#27169;&#22411;&#26356;&#26032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;LLMs&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19981;&#26029;&#25193;&#22823;&#30340;&#26410;&#35265;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#29616;&#20195;LLMs&#30340;&#22823;&#21442;&#25968;&#31354;&#38388;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Memory of Amortized Contexts&#65288;MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#29305;&#24449;&#25552;&#21462;&#21644;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#26032;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#24182;&#25552;&#21462;&#20026;&#23384;&#20648;&#22312;&#35760;&#24518;&#24211;&#20013;&#30340;&#32039;&#20945;&#35843;&#21046;&#12290;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20851;&#27880;&#24182;&#20174;&#35813;&#35760;&#24518;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#35843;&#21046;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>MedM2G&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#22810;&#27169;&#24577;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#23545;&#21307;&#23398;&#22810;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#30693;&#35782;&#24182;&#22686;&#24378;&#29305;&#23450;&#21307;&#23398;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.04290</link><description>&lt;p&gt;
MedM2G&#65306;&#36890;&#36807;&#20132;&#21449;&#24341;&#23548;&#25193;&#25955;&#32479;&#19968;&#21307;&#23398;&#22810;&#27169;&#24577;&#29983;&#25104;&#19982;&#35270;&#35273;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04290
&lt;/p&gt;
&lt;p&gt;
MedM2G&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#23398;&#22810;&#27169;&#24577;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#23545;&#21307;&#23398;&#22810;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#30693;&#35782;&#24182;&#22686;&#24378;&#29305;&#23450;&#21307;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29983;&#25104;&#27169;&#22411;&#20197;&#20854;&#39640;&#36136;&#37327;&#26679;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38024;&#23545;&#19981;&#21516;&#21307;&#23398;&#20219;&#21153;&#30340;&#21333;&#29420;&#21307;&#23398;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#19988;&#21463;&#21040;&#19981;&#36275;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#30693;&#35782;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21307;&#23398;&#20840;&#38754;&#35786;&#26029;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedM2G&#65292;&#19968;&#31181;&#21307;&#23398;&#22810;&#27169;&#24577;&#29983;&#25104;&#26694;&#26550;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#32479;&#19968;&#27169;&#22411;&#23545;&#21307;&#23398;&#22810;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#32479;&#19968;&#31354;&#38388;&#20013;&#30340;&#20013;&#24515;&#23545;&#40784;&#26041;&#27861;&#39640;&#25928;&#22320;&#23545;&#21307;&#23398;&#22810;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#19981;&#20165;&#38480;&#20110;&#21333;&#19968;&#25110;&#20004;&#31181;&#21307;&#23398;&#27169;&#24577;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20445;&#30041;&#27599;&#31181;&#25104;&#20687;&#27169;&#24577;&#30340;&#21307;&#23398;&#35270;&#35273;&#19981;&#21464;&#24615;&#26469;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#29305;&#23450;&#21307;&#23398;&#20449;&#24687;&#12290;&#36890;&#36807;&#35843;&#33410;&#33258;&#36866;&#24212;&#30340;&#20132;&#21449;&#24341;&#23548;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04290v1 Announce Type: cross  Abstract: Medical generative models, acknowledged for their high-quality sample generation ability, have accelerated the fast growth of medical applications. However, recent works concentrate on separate medical generation models for distinct medical tasks and are restricted to inadequate medical multi-modal knowledge, constraining medical comprehensive diagnosis. In this paper, we propose MedM2G, a Medical Multi-Modal Generative framework, with the key innovation to align, extract, and generate medical multi-modal within a unified model. Extending beyond single or two medical modalities, we efficiently align medical multi-modal through the central alignment approach in the unified space. Significantly, our framework extracts valuable clinical knowledge by preserving the medical visual invariant of each imaging modal, thereby enhancing specific medical information for multi-modal generation. By conditioning the adaptive cross-guided parameters i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04283</link><description>&lt;p&gt;
Proxy-RLHF&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#20195;&#29702;&#35299;&#32806;&#29983;&#25104;&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;Proxy-RLHF&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#36807;&#31243;&#35299;&#32806;&#65292;&#23454;&#29616;&#20102;&#20197;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RLHF&#26041;&#27861;&#38656;&#35201;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;RLHF&#21516;&#26102;&#23558;&#29983;&#25104;&#21644;&#23545;&#40784;&#20219;&#21153;&#20998;&#37197;&#32473;LLM&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Proxy-RLHF&#65292;&#23427;&#35299;&#32806;&#20102;LLMs&#30340;&#29983;&#25104;&#21644;&#23545;&#40784;&#27969;&#31243;&#65292;&#20197;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#20174;&#20026;&#23545;&#40784;&#36807;&#31243;&#35774;&#35745;&#30340;&#26032;&#22411;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#30417;&#30563;LLM&#30340;&#26631;&#35760;&#29983;&#25104;&#65292;&#32780;&#19981;&#25913;&#21464;LLM&#26412;&#36523;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30340;1%&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#21487;&#27604;&#27700;&#24179;&#30340;&#23545;&#40784;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\% of the training parameters of other methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QWAS&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#25628;&#32034;&#27599;&#20010;&#38454;&#27573;&#30340;&#19968;&#37327;&#23376;&#20301;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#20123;&#23454;&#38469;&#20219;&#21153;&#20013;&#24179;&#34913;&#30005;&#36335;&#24615;&#33021;&#21644;&#22823;&#23567;&#30340;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.04268</link><description>&lt;p&gt;
Qubit-Wise&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#29992;&#20110;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Qubit-Wise Architecture Search Method for Variational Quantum Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QWAS&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#25628;&#32034;&#27599;&#20010;&#38454;&#27573;&#30340;&#19968;&#37327;&#23376;&#20301;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19968;&#20123;&#23454;&#38469;&#20219;&#21153;&#20013;&#24179;&#34913;&#30005;&#36335;&#24615;&#33021;&#21644;&#22823;&#23567;&#30340;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#22122;&#38899;&#32423;&#21035;&#30340;&#38480;&#21046;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36739;&#23569;&#37327;&#23376;&#38376;&#30340;&#39640;&#24615;&#33021;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#26550;&#26500;&#12290;&#19982;&#32463;&#20856;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31867;&#20284;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65288;QAS&#65289;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#36229;&#32593;&#20248;&#21270;&#31561;&#26041;&#27861;&#26469;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Qubit-Wise&#26550;&#26500;&#25628;&#32034;&#65288;QWAS&#65289;&#26041;&#27861;&#65292;&#36880;&#27493;&#25628;&#32034;&#27599;&#20010;&#38454;&#27573;&#30340;&#19968;&#37327;&#23376;&#20301;&#37197;&#32622;&#65292;&#24182;&#32467;&#21512;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#23558;&#25628;&#32034;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20010;&#33391;&#22909;&#21644;&#19981;&#33391;&#23376;&#21306;&#22495;&#26469;&#25214;&#21040;&#33391;&#22909;&#30340;&#37327;&#23376;&#26550;&#26500;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19968;&#20123;&#29616;&#23454;&#20219;&#21153;&#20013;&#65288;&#22914;MNIST&#12289;&#26102;&#23578;&#21644;MOSI&#31561;&#65289;&#24179;&#34913;&#30005;&#36335;&#24615;&#33021;&#21644;&#22823;&#23567;&#30340;&#25506;&#32034;&#19982;&#24320;&#21457;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;QWAS&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04268v1 Announce Type: cross  Abstract: Considering the noise level limit, one crucial aspect for quantum machine learning is to design a high-performing variational quantum circuit architecture with small number of quantum gates. As the classical neural architecture search (NAS), quantum architecture search methods (QAS) employ methods like reinforcement learning, evolutionary algorithms and supernet optimiza-tion to improve the search efficiency. In this paper, we propose a novel qubit-wise architec-ture search (QWAS) method, which progres-sively search one-qubit configuration per stage, and combine with Monte Carlo Tree Search al-gorithm to find good quantum architectures by partitioning the search space into several good and bad subregions. The numerical experimental results indicate that our proposed method can balance the exploration and exploitation of cir-cuit performance and size in some real-world tasks, such as MNIST, Fashion and MOSI. As far as we know, QWAS achi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04260</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#25104;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#30340;&#33391;&#22909;&#25512;&#29702;&#32773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Small Language Models be Good Reasoners for Sequential Recommendation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#36164;&#28304;&#38656;&#27714;&#30340;&#38590;&#39064;&#65292;&#20351;&#20854;&#33021;&#20197;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#30340;&#20986;&#33394;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#24320;&#25299;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#35201;&#25104;&#21151;&#23454;&#29616;&#30001;LLMs&#36171;&#33021;&#30340;&#39034;&#24207;&#25512;&#33616;&#36824;&#26377;&#35768;&#22810;&#25361;&#25112;&#38656;&#35201;&#35299;&#20915;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#36890;&#24120;&#22797;&#26434;&#65292;&#20165;&#20165;&#20381;&#38752;LLMs&#30340;&#19968;&#27493;&#25512;&#29702;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;LLMs&#65288;&#20363;&#22914;ChatGPT-175B&#65289;&#26497;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#26159;&#38590;&#20197;&#25215;&#21463;&#19988;&#22312;&#23454;&#38469;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36880;&#27493;&#30693;&#35782;&#25552;&#21462;&#26694;&#26550;&#29992;&#20110;&#25512;&#33616;&#65288;SLIM&#65289;&#65292;&#20026;&#39034;&#24207;&#25512;&#33616;&#22120;&#20197;&#8220;&#30246;&#8221;&#65288;&#21363;&#36164;&#28304;&#39640;&#25928;&#65289;&#30340;&#26041;&#24335;&#20139;&#21463;LLMs&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#38138;&#24179;&#20102;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;CoT&#25552;&#31034;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04260v1 Announce Type: cross  Abstract: Large language models (LLMs) open up new horizons for sequential recommendations, owing to their remarkable language comprehension and generation capabilities. However, there are still numerous challenges that should be addressed to successfully implement sequential recommendations empowered by LLMs. Firstly, user behavior patterns are often complex, and relying solely on one-step reasoning from LLMs may lead to incorrect or task-irrelevant responses. Secondly, the prohibitively resource requirements of LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real sequential recommender systems. In this paper, we propose a novel Step-by-step knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising path for sequential recommenders to enjoy the exceptional reasoning capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We introduce CoT prompting based on user behavior sequences for the larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#21644;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#38598;&#20013;&#24335;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.04259</link><description>&lt;p&gt;
&#20998;&#25955;&#19988;&#20844;&#24179;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Decentralized and Equitable Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#25955;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#21644;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#38598;&#20013;&#24335;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#25955;&#65288;&#31163;&#25955;&#65289;&#26368;&#20248;&#36755;&#36816;&#65288;D-OT&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#32452;&#20195;&#29702;&#20154;&#20849;&#21516;&#35774;&#35745;&#36816;&#36755;&#26041;&#26696;&#65292;&#20854;&#20013;&#25104;&#26412;&#20989;&#25968;&#26159;&#27599;&#20010;&#20195;&#29702;&#20154;&#25345;&#26377;&#30340;&#25104;&#26412;&#20043;&#21644;&#12290;&#25105;&#20204;&#23558;D-OT&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#32422;&#26463;&#32806;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon})&#30340;&#21333;&#24490;&#29615;&#20998;&#25955;&#31639;&#27861;&#65292;&#19982;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#19968;&#38454;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#20844;&#24179;&#26368;&#20248;&#36755;&#36816;&#65288;DE-OT&#65289;&#38382;&#39064;&#12290;&#22312;DE-OT&#20013;&#65292;&#20195;&#29702;&#19981;&#20165;&#21327;&#20316;&#35774;&#35745;&#26368;&#23567;&#21270;&#36816;&#36755;&#25104;&#26412;&#30340;&#36816;&#36755;&#35745;&#21010;&#65292;&#36824;&#21162;&#21147;&#30830;&#20445;&#21508;&#33258;&#25104;&#26412;&#30340;&#20844;&#24179;&#24615;&#12290;&#35299;&#20915;DE-OT&#30340;&#26041;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20063;&#26159;O(1/{\epsilon})&#65292;&#36825;&#19968;&#36895;&#29575;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#38598;&#20013;&#24335;&#31639;&#27861;&#65292;&#20854;&#20013;&#33719;&#24471;&#30340;&#26368;&#20339;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O(1/{\epsilon}^2)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04259v1 Announce Type: cross  Abstract: This paper considers the decentralized (discrete) optimal transport (D-OT) problem. In this setting, a network of agents seeks to design a transportation plan jointly, where the cost function is the sum of privately held costs for each agent. We reformulate the D-OT problem as a constraint-coupled optimization problem and propose a single-loop decentralized algorithm with an iteration complexity of O(1/{\epsilon}) that matches existing centralized first-order approaches. Moreover, we propose the decentralized equitable optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively designing a transportation plan that minimizes transportation costs, agents seek to ensure equity in their individual costs. The iteration complexity of the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves existing centralized algorithms, where the best iteration complexity obtained is O(1/{\epsilon}^2).
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#19968;&#31867;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19990;&#30028;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Recall to Imagine&#65288;R2I&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#21644;&#38271;&#36317;&#31163;&#22870;&#21169;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#22870;&#21169;&#20998;&#37197;&#26041;&#38754;&#30340;&#36229;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.04253</link><description>&lt;p&gt;
&#29992;&#19990;&#30028;&#27169;&#22411;&#25484;&#25569;&#35760;&#24518;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Mastering Memory Tasks with World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04253
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#31867;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#19990;&#30028;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Recall to Imagine&#65288;R2I&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#21644;&#38271;&#36317;&#31163;&#22870;&#21169;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#22312;&#35760;&#24518;&#20219;&#21153;&#21644;&#22870;&#21169;&#20998;&#37197;&#26041;&#38754;&#30340;&#36229;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20195;&#29702;&#22312;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#35299;&#20915;&#28041;&#21450;&#21160;&#20316;&#21644;&#32467;&#26524;&#20043;&#38388;&#23384;&#22312;&#26102;&#38388;&#38388;&#38548;&#25110;&#38656;&#35201;&#22238;&#24819;&#36828;&#36317;&#31163;&#35266;&#23519;&#20197;&#25351;&#23548;&#24403;&#21069;&#21160;&#20316;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25913;&#21892;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23558;&#19968;&#31867;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#25972;&#21512;&#21040;MBRL&#20195;&#29702;&#30340;&#19990;&#30028;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Recall to Imagine&#65288;R2I&#65289;&#12290;&#36825;&#31181;&#25972;&#21512;&#26088;&#22312;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#21644;&#38271;&#36317;&#31163;&#22870;&#21169;&#20998;&#37197;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#31034;&#20363;&#20219;&#21153;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23637;&#31034;&#20102;R2I&#19981;&#20165;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35760;&#24518;&#21644;&#22870;&#21169;&#20998;&#37197;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30830;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#22914;BSuite&#21644;POPGym&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;Memory Maze&#36825;&#19968;&#22797;&#26434;&#30340;&#35760;&#24518;&#39046;&#22495;&#20013;&#36229;&#36234;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#22312;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;Atari&#21644;DMC&#20013;&#65292;&#23427;&#20445;&#25345;&#20102;&#21487;&#20197;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04253v1 Announce Type: new  Abstract: Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggestin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN-LSTM&#19977;&#38454;&#27573;&#27169;&#22411;PEnet&#65292;&#36890;&#36807;CNN&#23545;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#38271;&#24207;&#21015;&#35266;&#27979;&#30340;&#22686;&#24378;&#25512;&#26029;&#36895;&#24230;&#21644;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;SDE&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.04246</link><description>&lt;p&gt;
&#22522;&#20110;CNN-LSTM&#30340;Levy&#39537;&#21160;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#39640;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN-LSTM&#19977;&#38454;&#27573;&#27169;&#22411;PEnet&#65292;&#36890;&#36807;CNN&#23545;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20197;&#21450;&#38271;&#24207;&#21015;&#35266;&#27979;&#30340;&#22686;&#24378;&#25512;&#26029;&#36895;&#24230;&#21644;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#20272;&#35745;SDE&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#30001;&#38750;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20215;&#26684;&#27874;&#21160;&#21644;&#20256;&#26579;&#30149;&#20256;&#25773;&#31561;&#21160;&#24577;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;LSTM&#32593;&#32476;&#22312;&#20272;&#35745;alpha&#31283;&#23450;Levy&#39537;&#21160;&#30340;SDE&#21442;&#25968;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#39640;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;LSTM&#38142;&#25509;&#23646;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PEnet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CNN-LSTM&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;CNN&#23545;&#21021;&#22987;&#25968;&#25454;&#29305;&#24449;&#36827;&#34892;&#27987;&#32553;&#65292;&#20026;&#38271;&#24207;&#21015;&#35266;&#27979;&#25552;&#20379;&#22686;&#24378;&#30340;&#25512;&#26029;&#36895;&#24230;&#65292;&#24182;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20801;&#35768;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#30340;SDE&#22330;&#26223;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;PEnet&#22312;&#20272;&#35745;SDE&#20013;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04246v1 Announce Type: cross  Abstract: This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#20013;&#20002;&#22833;&#35270;&#39057;&#24103;&#23548;&#33268;&#30340;&#36741;&#21161;&#27169;&#24577;&#20559;&#24046;&#23545;&#31995;&#32479;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#20559;&#24046;&#20551;&#35774;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#24067;&#36817;&#20284;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#23545;&#38899;&#39057;&#27169;&#24577;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24182;&#20445;&#25345;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04245</link><description>&lt;p&gt;
&#30740;&#31350;&#20002;&#22833;&#35270;&#39057;&#24103;&#23548;&#33268;&#30340;&#36741;&#21161;&#27169;&#24577;&#20559;&#24046;&#23545;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#20013;&#20002;&#22833;&#35270;&#39057;&#24103;&#23548;&#33268;&#30340;&#36741;&#21161;&#27169;&#24577;&#20559;&#24046;&#23545;&#31995;&#32479;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#20559;&#24046;&#20551;&#35774;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#24067;&#36817;&#20284;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#23545;&#38899;&#39057;&#27169;&#24577;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24182;&#20445;&#25345;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#38899;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24050;&#34987;&#35266;&#23519;&#21040;&#23545;&#20002;&#22833;&#35270;&#39057;&#24103;&#25935;&#24863;&#65292;&#29978;&#33267;&#34920;&#29616;&#27604;&#21333;&#27169;&#24577;&#27169;&#22411;&#26356;&#24046;&#12290;&#23613;&#31649;&#23558;&#36749;&#23398;&#25216;&#26415;&#24212;&#29992;&#20110;&#35270;&#39057;&#27169;&#24577;&#21487;&#20197;&#22686;&#24378;&#23545;&#20002;&#22833;&#24103;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#21516;&#26102;&#22312;&#22788;&#29702;&#23436;&#25972;&#25968;&#25454;&#36755;&#20837;&#26102;&#20250;&#23548;&#33268;&#24615;&#33021;&#25439;&#22833;&#12290;&#26412;&#25991;&#20174;&#27169;&#24577;&#20559;&#24046;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#36825;&#19968;&#30683;&#30462;&#29616;&#35937;&#65292;&#24182;&#25581;&#31034;&#20102;&#36749;&#23398;&#23548;&#33268;&#30340;&#23545;&#38899;&#39057;&#30340;&#36807;&#24230;&#27169;&#24577;&#20559;&#24046;&#26159;&#28508;&#22312;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#24577;&#20559;&#24046;&#20551;&#35774;&#65288;MBH&#65289;&#65292;&#31995;&#32479;&#22320;&#25551;&#36848;&#20102;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#27169;&#24577;&#20559;&#24046;&#19982;&#23545;&#20002;&#22833;&#27169;&#24577;&#30340;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22810;&#27169;&#24577;&#20998;&#24067;&#36817;&#20284;&#65288;MDA-KD&#65289;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#23545;&#38899;&#39057;&#27169;&#24577;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24182;&#20445;&#25345;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04245v1 Announce Type: cross  Abstract: Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to be sensitive to missing video frames, performing even worse than single-modality models. While applying the dropout technique to the video modality enhances robustness to missing frames, it simultaneously results in a performance loss when dealing with complete data input. In this paper, we investigate this contrasting phenomenon from the perspective of modality bias and reveal that an excessive modality bias on the audio caused by dropout is the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH) to systematically describe the relationship between modality bias and robustness against missing modality in multimodal systems. Building on these findings, we propose a novel Multimodal Distribution Approximation with Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio modality and to maintain performance and robust
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#21017;&#21270;DeepIV&#65288;RDIV&#65289;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;IV&#22238;&#24402;&#21807;&#19968;&#26631;&#35782;&#12289;&#26497;&#23567;&#26497;&#22823;&#35745;&#31639;&#39044;&#35328;&#21644;&#32570;&#20047;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#31561;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.04236</link><description>&lt;p&gt;
&#20855;&#26377;&#27169;&#22411;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;DeepIV
&lt;/p&gt;
&lt;p&gt;
Regularized DeepIV with Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#21017;&#21270;DeepIV&#65288;RDIV&#65289;&#22238;&#24402;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#36991;&#20813;IV&#22238;&#24402;&#21807;&#19968;&#26631;&#35782;&#12289;&#26497;&#23567;&#26497;&#22823;&#35745;&#31639;&#39044;&#35328;&#21644;&#32570;&#20047;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#31561;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#22238;&#24402;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#24341;&#20837;&#20102;&#28789;&#27963;&#30340;IV&#20272;&#35745;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#36935;&#21040;&#20197;&#19979;&#19968;&#20010;&#25110;&#22810;&#20010;&#38480;&#21046;&#65306;&#65288;1&#65289;&#23558;IV&#22238;&#24402;&#38480;&#21046;&#20026;&#21807;&#19968;&#26631;&#35782;&#65307;&#65288;2&#65289;&#38656;&#35201;&#26497;&#23567;&#26497;&#22823;&#35745;&#31639;&#39044;&#35328;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#19981;&#31283;&#23450;&#65307;&#65288;3&#65289;&#32570;&#20047;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36991;&#20813;&#25152;&#26377;&#19977;&#20010;&#38480;&#21046;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#21644;&#20998;&#26512;&#65292;&#21516;&#26102;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#21017;&#21270;DeepIV&#65288;RDIV&#65289;&#22238;&#24402;&#30340;&#26080;&#26497;&#23567;&#26497;&#22823;&#35745;&#31639;&#39044;&#35328;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#23567;&#33539;&#25968;IV&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#21327;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#25152;&#23398;&#21040;&#30340;&#20998;&#24067;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Tikhonov&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04236v1 Announce Type: new  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We furth
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33021;&#30001;&#31561;&#25928;&#39640;&#26031;&#27169;&#22411;&#21644;&#26377;&#25928;&#20808;&#39564;&#30830;&#23450;&#65292;&#20449;&#21495;&#37325;&#26500;&#38656;&#35201;&#22686;&#38271;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#28176;&#36817;&#35823;&#24046;&#29305;&#24449;&#21270;&#21644;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04234</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental limits of Non-Linear Low-Rank Matrix Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04234
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33021;&#30001;&#31561;&#25928;&#39640;&#26031;&#27169;&#22411;&#21644;&#26377;&#25928;&#20808;&#39564;&#30830;&#23450;&#65292;&#20449;&#21495;&#37325;&#26500;&#38656;&#35201;&#22686;&#38271;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#32447;&#24615;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#30340;&#28176;&#36817;&#35823;&#24046;&#29305;&#24449;&#21270;&#21644;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#38750;&#32447;&#24615;&#21644;&#22024;&#26434;&#35266;&#27979;&#20013;&#20272;&#35745;&#20302;&#31209;&#30697;&#38453;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26222;&#36866;&#24615;&#32467;&#26524;&#65292;&#34920;&#26126;&#36125;&#21494;&#26031;&#26368;&#20248;&#34920;&#29616;&#30001;&#19968;&#20010;&#31561;&#25928;&#39640;&#26031;&#27169;&#22411;&#21644;&#19968;&#20010;&#26377;&#25928;&#20808;&#39564;&#25152;&#20915;&#23450;&#65292;&#20854;&#21442;&#25968;&#23436;&#20840;&#30001;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#23637;&#24320;&#30830;&#23450;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#20026;&#20102;&#20934;&#30830;&#37325;&#26500;&#20449;&#21495;&#65292;&#20449;&#22122;&#27604;&#38656;&#35201;&#22686;&#38271;&#20026;$N^{\frac 12 (1-1/k_F)}$&#65292;&#20854;&#20013;$k_F$&#26159;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;&#38750;&#38646;&#36153;&#33293;&#23572;&#20449;&#24687;&#31995;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#23567;&#21487;&#36798;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#30340;&#28176;&#36817;&#29305;&#24449;&#21270;&#21644;&#19968;&#31181;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#31867;&#20284;&#20110;&#38382;&#39064;&#30340;&#32447;&#24615;&#29256;&#26412;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;MMSE&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36890;&#36807;&#26041;&#27861;&#20363;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#32467;&#21512;&#36125;&#21494;&#26031;&#38477;&#22122;&#23454;&#29616;&#30340;&#28176;&#36817;&#35823;&#24046;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;MMSE&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04234v1 Announce Type: cross  Abstract: We consider the task of estimating a low-rank matrix from non-linear and noisy observations. We prove a strong universality result showing that Bayes-optimal performances are characterized by an equivalent Gaussian model with an effective prior, whose parameters are entirely determined by an expansion of the non-linear function. In particular, we show that to reconstruct the signal accurately, one requires a signal-to-noise ratio growing as $N^{\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher information coefficient of the function. We provide asymptotic characterization for the minimal achievable mean squared error (MMSE) and an approximate message-passing algorithm that reaches the MMSE under conditions analogous to the linear version of the problem. We also provide asymptotic errors achieved by methods such as principal component analysis combined with Bayesian denoising, and compare them with Bayes-optimal MMSE.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22522;&#20110;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#30340;&#27867;&#21270;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#25511;&#21046;&#24182;&#36816;&#29992;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.04232</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Cooperative Eco-driving via Multi-residual Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04232
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22522;&#20110;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#30340;&#27867;&#21270;&#21327;&#21516;&#29983;&#24577;&#39550;&#39542;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#25511;&#21046;&#24182;&#36816;&#29992;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25511;&#21046;&#65292;&#22914;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36890;&#24120;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#20854;&#25928;&#29575;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#35201;&#24212;&#23545;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#21516;&#20132;&#36890;&#22330;&#26223;&#65292;&#36825;&#23545;&#36825;&#20123;&#35268;&#21010;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#36825;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20294;&#23398;&#20064;DRL&#25511;&#21046;&#31574;&#30053;&#20197;&#27867;&#21270;&#21040;&#22810;&#20010;&#20132;&#36890;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27531;&#24046;&#20219;&#21153;&#23398;&#20064;&#65288;MRTL&#65289;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#26159;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#65292;&#38024;&#23545;&#19968;&#32452;&#20219;&#21153;&#22330;&#26223;&#65292;&#23558;&#25511;&#21046;&#20998;&#35299;&#20026;&#26377;&#25928;&#30001;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#35299;&#20915;&#30340;&#21517;&#20041;&#20998;&#37327;&#21644;&#30001;&#23398;&#20064;&#35299;&#20915;&#30340;&#27531;&#24046;&#39033;&#12290;&#25105;&#20204;&#20351;&#29992;MRTL&#26469;&#22312;&#28151;&#21512;&#20132;&#36890;&#20013;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#23454;&#29616;&#36710;&#38431;&#32423;&#21035;&#30340;&#25490;&#25918;&#20943;&#23569;&#20316;&#20026;&#31995;&#32479;&#25511;&#21046;&#25163;&#27573;&#12290;&#36890;&#36807;&#20998;&#26512;MR&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04232v1 Announce Type: cross  Abstract: Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MR
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.04221</link><description>&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20309;&#20855;&#26377;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why Online Reinforcement Learning is Causal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04221
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22240;&#26524;&#24314;&#27169;&#33258;&#28982;&#20114;&#34917;&#12290;&#22240;&#26524;&#24314;&#27169;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#22823;&#21270;&#20195;&#29702;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#30340;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#20004;&#20010;&#26368;&#24378;&#22823;&#20449;&#24687;&#28304;&#65306;&#26102;&#38388;&#39034;&#24207;&#21644;&#23545;&#29615;&#22659;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#20174;&#22240;&#26524;&#24314;&#27169;&#20013;&#21463;&#30410;&#65292;&#20197;&#21450;&#22914;&#20309;&#21463;&#30410;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26377;&#33021;&#21147;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20174;&#25506;&#32034;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#65292;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#26465;&#20214;&#27010;&#29575;&#26159;&#22240;&#26524;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#26377;&#28508;&#21147;&#20135;&#29983;&#24046;&#24322;&#30340;&#29615;&#22659;&#12290;&#22522;&#26412;&#19978;&#65292;&#21407;&#22240;&#22312;&#20110;&#24403;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#26159;&#30001;&#20854;&#23545;&#29615;&#22659;&#30340;&#35748;&#35782;&#25152;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#35774;&#22791;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#30340;&#19981;&#21516;&#23548;&#33268;&#30340;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;HeteroSwitch&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04207</link><description>&lt;p&gt;
HeteroSwitch&#65306;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#34920;&#24449;&#19982;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
HeteroSwitch: Characterizing and Taming System-Induced Data Heterogeneity in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#35774;&#22791;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#30340;&#19981;&#21516;&#23548;&#33268;&#30340;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24322;&#36136;&#24615;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;HeteroSwitch&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#29992;&#25143;&#35774;&#22791;&#21327;&#20316;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#22312;FL&#20013;&#65292;&#21442;&#19982;&#30340;&#29992;&#25143;&#35774;&#22791;&#22312;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#26041;&#38754;&#39640;&#24230;&#30862;&#29255;&#21270;&#12290;&#36825;&#31181;&#30862;&#29255;&#21270;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#21363;\textit{&#31995;&#32479;&#35825;&#23548;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;}&#65292;&#22240;&#20026;&#27599;&#20010;&#35774;&#22791;&#26681;&#25454;&#20854;&#30828;&#20214;&#21644;&#36719;&#20214;&#37197;&#32622;&#29983;&#25104;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#24449;&#20102;&#31995;&#32479;&#35825;&#23548;&#25968;&#25454;&#24322;&#36136;&#24615;&#23545;FL&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#36328;&#20379;&#24212;&#21830;&#21644;&#24615;&#33021;&#23618;&#32423;&#21464;&#21270;&#30340;&#24322;&#26500;&#35774;&#22791;&#25910;&#38598;&#20102;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;\textit{&#31995;&#32479;&#35825;&#23548;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;}&#23545;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#24694;&#21270;&#20102;FL&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HeteroSwitch&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04207v1 Announce Type: new  Abstract: Federated Learning (FL) is a practical approach to train deep learning models collaboratively across user-end devices, protecting user privacy by retaining raw data on-device. In FL, participating user-end devices are highly fragmented in terms of hardware and software configurations. Such fragmentation introduces a new type of data heterogeneity in FL, namely \textit{system-induced data heterogeneity}, as each device generates distinct data depending on its hardware and software configurations. In this paper, we first characterize the impact of system-induced data heterogeneity on FL model performance. We collect a dataset using heterogeneous devices with variations across vendors and performance tiers. By using this dataset, we demonstrate that \textit{system-induced data heterogeneity} negatively impacts accuracy, and deteriorates fairness and domain generalization problems in FL. To address these challenges, we propose HeteroSwitch, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#24674;&#22797;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#24179;&#22374;&#21306;&#22495;&#20026;&#20248;&#20808;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#36136;&#37327;</title><link>https://arxiv.org/abs/2403.04206</link><description>&lt;p&gt;
GRAWA: &#22522;&#20110;&#26799;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GRAWA: Gradient-based Weighted Averaging for Distributed Training of Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#31639;&#27861;&#65292;&#20197;&#24674;&#22797;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#24179;&#22374;&#21306;&#22495;&#20026;&#20248;&#20808;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21608;&#26399;&#24615;&#22320;&#23558;&#24037;&#20316;&#32773;&#25289;&#21521;&#20316;&#20026;&#24037;&#20316;&#32773;&#21152;&#26435;&#24179;&#22343;&#20540;&#35745;&#31639;&#30340;&#20013;&#24515;&#21464;&#37327;&#65292;&#20854;&#20013;&#26435;&#37325;&#19982;&#24037;&#20316;&#32773;&#30340;&#26799;&#24230;&#33539;&#25968;&#25104;&#21453;&#27604;&#65292;&#20174;&#32780;&#20248;&#20808;&#24674;&#22797;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#24179;&#22374;&#21306;&#22495;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#20004;&#31181;&#24322;&#27493;&#21464;&#20307;&#65292;&#20998;&#21035;&#31216;&#20026;&#27169;&#22411;&#32423;&#21644;&#23618;&#32423;&#26799;&#24230;&#21152;&#26435;&#24179;&#22343;&#65288;MGRAWA&#21644;LGRAWA&#65289;&#65292;&#22312;&#21152;&#26435;&#26041;&#26696;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#35201;&#20040;&#26159;&#38024;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#65292;&#35201;&#20040;&#26159;&#36880;&#23618;&#24212;&#29992;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#20984;&#24615;&#21644;&#38750;&#20984;&#24615;&#29615;&#22659;&#20013;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24674;&#22797;&#26356;&#22909;&#30340;&#36136;&#37327;&#32780;&#32988;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04206v1 Announce Type: new  Abstract: We study distributed training of deep learning models in time-constrained environments. We propose a new algorithm that periodically pulls workers towards the center variable computed as a weighted average of workers, where the weights are inversely proportional to the gradient norms of the workers such that recovering the flat regions in the optimization landscape is prioritized. We develop two asynchronous variants of the proposed algorithm that we call Model-level and Layer-level Gradient-based Weighted Averaging (resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that is either done with respect to the entire model or is applied layer-wise. On the theoretical front, we prove the convergence guarantee for the proposed approach in both convex and non-convex settings. We then experimentally demonstrate that our algorithms outperform the competitor methods by achieving faster convergence and recovering better quality
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#27700;&#24211;&#36816;&#34892;&#20915;&#31574;&#19982;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#21160;&#24577;&#35268;&#21010;&#21644;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#20102;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04195</link><description>&lt;p&gt;
&#12298;&#35013;&#22635;&#19982;&#28322;&#20986;&#65306;&#29992;&#20110;&#27700;&#24211;&#36816;&#34892;&#20915;&#31574;&#19982;&#25511;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for Reservoir Operation Decision and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04195
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#27700;&#24211;&#36816;&#34892;&#20915;&#31574;&#19982;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#21160;&#24577;&#35268;&#21010;&#21644;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#35299;&#20915;&#20102;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#21464;&#21270;&#12289;&#21508;&#31181;&#27700;&#25991;&#36755;&#20837;&#21644;&#29615;&#22659;&#21387;&#21147;&#26159;&#27700;&#36164;&#28304;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#32463;&#24120;&#38754;&#20020;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290; &#36825;&#20123;&#38382;&#39064;&#24341;&#36215;&#20154;&#20204;&#23545;&#24212;&#29992;&#19981;&#21516;&#25216;&#26415;&#26469;&#30830;&#23450;&#27700;&#24211;&#36816;&#34892;&#25919;&#31574;&#20915;&#31574;&#30340;&#20852;&#36259;&#12290; &#38543;&#30528;&#20998;&#26512;&#30340;&#20998;&#36776;&#29575;&#25552;&#39640;&#65292;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#22914;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#21644;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#65288;SDP&#65289;&#26469;&#30830;&#23450;&#26368;&#20339;&#27700;&#24211;&#36816;&#34892;&#25919;&#31574;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#38590;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290; &#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#20272;&#35745;&#20855;&#26377;&#32473;&#23450;&#31934;&#24230;&#27700;&#24179;&#20219;&#24847;&#20989;&#25968;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#38543;&#30528;&#20989;&#25968;&#30340;&#36755;&#20837;&#21464;&#37327;&#25968;&#37327;&#65288;&#21363;&#32500;&#25968;&#65289;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290; &#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26159;&#19968;&#31181;&#26234;&#33021;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#27700;&#24211;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04195v1 Announce Type: new  Abstract: Changes in demand, various hydrological inputs, and environmental stressors are among the issues that water managers and policymakers face on a regular basis. These concerns have sparked interest in applying different techniques to determine reservoir operation policy decisions. As the resolution of the analysis increases, it becomes more difficult to effectively represent a real-world system using traditional methods such as Dynamic Programming (DP) and Stochastic Dynamic Programming (SDP) for determining the best reservoir operation policy. One of the challenges is the "curse of dimensionality," which means the number of samples needed to estimate an arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function. Deep Reinforcement Learning (DRL) is an intelligent approach to overcome the curses of stochastic optimization problems for reservoir 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.04190</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Synthetic Data Generation: Methods, Challenges and the Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04190
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#26174;&#33879;&#36716;&#21464;&#12290;&#23427;&#20204;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30456;&#23218;&#32654;&#30340;&#33021;&#21147;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#23450;&#20301;&#20026;&#35299;&#20915;&#20302;&#36164;&#28304;&#25361;&#25112;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#24222;&#22823;&#30340;LLMs&#29983;&#25104;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#25216;&#26415;&#21644;&#23454;&#38469;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;2.5D&#24179;&#21488;&#20013;&#21033;&#29992;&#20809;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#21534;&#21520;&#37327;&#39640;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20307;&#31995;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.04189</link><description>&lt;p&gt;
&#20811;&#26381;&#35268;&#27169;&#21270;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#36890;&#20449;&#29942;&#39048;&#30340;&#30789;&#20809;&#23376;2.5D&#20114;&#36830;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Silicon Photonic 2.5D Interposer Networks for Overcoming Communication Bottlenecks in Scale-out Machine Learning Hardware Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;2.5D&#24179;&#21488;&#20013;&#21033;&#29992;&#20809;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#21534;&#21520;&#37327;&#39640;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#31243;&#24207;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#21333;&#29255;&#21152;&#36895;&#22120;&#20307;&#31995;&#32467;&#26500;&#26080;&#27861;&#28385;&#36275;&#20854;&#33021;&#25928;&#21644;&#21534;&#21520;&#35201;&#27714;&#12290;&#23613;&#31649;&#29616;&#20195;&#25968;&#23383;&#30005;&#23376;&#21152;&#36895;&#22120;&#36880;&#28176;&#37319;&#29992;&#20855;&#26377;&#22810;&#20010;&#36739;&#23567;&#33455;&#29255;&#32452;&#30340;2.5D&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#20381;&#36182;&#32531;&#24930;&#30340;&#37329;&#23646;&#20114;&#36830;&#32780;&#38754;&#20020;&#22522;&#26412;&#38480;&#21046;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22914;&#20309;&#22312;2.5D&#24179;&#21488;&#20013;&#21033;&#29992;&#20809;&#36890;&#20449;&#21644;&#35745;&#31639;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#21534;&#21520;&#37327;&#39640;&#30340;2.5D ML&#21152;&#36895;&#22120;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04189v1 Announce Type: cross  Abstract: Modern machine learning (ML) applications are becoming increasingly complex and monolithic (single chip) accelerator architectures cannot keep up with their energy efficiency and throughput demands. Even though modern digital electronic accelerators are gradually adopting 2.5D architectures with multiple smaller chiplets to improve scalability, they face fundamental limitations due to a reliance on slow metallic interconnects. This paper outlines how optical communication and computation can be leveraged in 2.5D platforms to realize energy-efficient and high throughput 2.5D ML accelerator architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04180</link><description>&lt;p&gt;
RATSF&#65306;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26469;&#36171;&#33021;&#23458;&#26381;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#30340;&#23458;&#26381;&#31649;&#29702;&#31995;&#32479;&#21462;&#20915;&#20110;&#23545;&#26381;&#21153;&#37327;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#26126;&#26174;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#30340;&#39044;&#27979;&#20005;&#37325;&#20381;&#36182;&#20110;&#35782;&#21035;&#21644;&#21033;&#29992;&#31867;&#20284;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#29616;&#26377;&#22522;&#20110;RNN&#25110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#28789;&#27963;&#21644;&#26377;&#25928;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#36866;&#24212;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#31216;&#20026;RACA&#65292;&#23427;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#26377;&#25928;&#21033;&#29992;&#20102;&#21382;&#21490;&#27573;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#21382;&#21490;&#24207;&#21015;&#26597;&#35810;&#34920;&#31034;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#30340;&#35774;&#35745;&#12290;&#36825;&#20123;&#20851;&#38190;&#32452;&#20214;&#20849;&#21516;&#26500;&#25104;&#20102;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65288;RATSF&#65289;&#12290;RATSF&#19981;&#20165;&#22312;&#33778;&#40481;&#37202;&#24215;&#26381;&#21153;&#37327;&#39044;&#27979;&#29615;&#22659;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#24615;&#33021;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04180v1 Announce Type: new  Abstract: An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#25506;&#32034;&#20013;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.04162</link><description>&lt;p&gt;
&#29992;&#20110;&#25506;&#32034;&#30340;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Noisy Spiking Actor Network for Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#25506;&#32034;&#20013;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#30340;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;NoisyNet&#33021;&#22815;&#20135;&#29983;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#30001;&#20110;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20855;&#26377;&#20108;&#36827;&#21046;&#21457;&#25918;&#26426;&#21046;&#65292;&#23545;&#20110;&#22122;&#22768;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#38590;&#20197;&#36890;&#36807;&#23616;&#37096;&#24178;&#25200;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#30340;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#26469;&#20026;&#20195;&#29702;&#25214;&#21040;&#31283;&#23450;&#31574;&#30053;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;OpenAI gym&#30340;&#24191;&#27867;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04162v1 Announce Type: new  Abstract: As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;SDE&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#65292;&#31283;&#23450;&#31574;&#30053;&#26799;&#24230;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.04154</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#31283;&#23450;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Policy Gradients for Stochastic Differential Equations via Consistency with Perturbation Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;SDE&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#19968;&#33268;&#24615;&#65292;&#31283;&#23450;&#31574;&#30053;&#26799;&#24230;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20135;&#29983;&#39640;&#22238;&#25253;&#26679;&#26412;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20248;&#21270;&#21442;&#25968;&#21270;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#65292;&#36825;&#26159;&#20855;&#26377;&#39640;&#21487;&#34920;&#36798;&#24615;&#30340;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#31639;&#27861;&#31574;&#30053;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;&#20110;SDE&#26102;&#65292;&#30001;&#20110;&#31574;&#30053;&#26799;&#24230;&#26159;&#22312;&#26377;&#38480;&#36712;&#36857;&#38598;&#19978;&#20272;&#35745;&#30340;&#65292;&#23427;&#21487;&#33021;&#26159;&#19981;&#26126;&#30830;&#23450;&#20041;&#30340;&#65292;&#24182;&#19988;&#25968;&#25454;&#31232;&#32570;&#21306;&#22495;&#30340;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#26080;&#27861;&#25511;&#21046;&#12290;&#36825;&#19968;&#25361;&#25112;&#24433;&#21709;&#20102;&#31574;&#30053;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23545;&#26679;&#26412;&#22797;&#26434;&#24230;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;SDE&#38480;&#21046;&#20026;&#19982;&#20854;&#30456;&#20851;&#30340;&#25200;&#21160;&#36807;&#31243;&#20445;&#25345;&#19968;&#33268;&#12290;&#30001;&#20110;&#25200;&#21160;&#36807;&#31243;&#35206;&#30422;&#25972;&#20010;&#31354;&#38388;&#19988;&#26131;&#20110;&#25277;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#32531;&#35299;&#21069;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20197;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04154v1 Announce Type: new  Abstract: Considering generating samples with high rewards, we focus on optimizing deep neural networks parameterized stochastic differential equations (SDEs), the advanced generative models with high expressiveness, with policy gradient, the leading algorithm in reinforcement learning. Nevertheless, when applying policy gradients to SDEs, since the policy gradient is estimated on a finite set of trajectories, it can be ill-defined, and the policy behavior in data-scarce regions may be uncontrolled. This challenge compromises the stability of policy gradients and negatively impacts sample complexity. To address these issues, we propose constraining the SDE to be consistent with its associated perturbation process. Since the perturbation process covers the entire space and is easy to sample, we can mitigate the aforementioned problems. Our framework offers a general approach allowing for a versatile selection of policy gradient methods to effective
&lt;/p&gt;</description></item><item><title>FL-GUARD&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#39069;&#22806;&#25104;&#26412;&#21644;&#28010;&#36153;&#23398;&#20064;&#36718;&#27425;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04146</link><description>&lt;p&gt;
FL-GUARD: &#19968;&#20010;&#29992;&#20110;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#36816;&#34892;&#26102;&#26816;&#27979;&#21644;&#24674;&#22797;&#30340;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04146
&lt;/p&gt;
&lt;p&gt;
FL-GUARD&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#24674;&#22797;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#39069;&#22806;&#25104;&#26412;&#21644;&#28010;&#36153;&#23398;&#20064;&#36718;&#27425;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20174;&#20998;&#24067;&#22312;&#22823;&#37327;&#23458;&#25143;&#31471;&#19978;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#22411;&#19988;&#19981;&#26292;&#38706;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#29702;&#24819;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25928;&#26524;&#26174;&#33879;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20998;&#20139;&#21516;&#36136;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#23398;&#20064;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#24403;&#32852;&#37030;&#23398;&#20064;&#19981;&#29702;&#24819;&#26102;&#65292;FL&#21487;&#33021;&#26080;&#27861;&#27491;&#24120;&#36816;&#20316;&#65292;&#23548;&#33268;&#36127;&#38754;&#32852;&#37030;&#23398;&#20064;&#65288;NFL&#65289;&#30340;&#19981;&#33391;&#29366;&#24577;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;NFL&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#65288;1&#65289;&#39044;&#20808;&#38450;&#27490;&#25972;&#20010;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;NFL&#65292;&#35201;&#20040;&#65288;2&#65289;&#22312;&#22823;&#37327;&#23398;&#20064;&#36718;&#27425;&#20043;&#21518;&#35299;&#20915;NFL&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#35201;&#20040;&#65288;1&#65289;&#22312;FL&#27809;&#26377;&#36825;&#20123;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26080;&#24046;&#21035;&#22686;&#21152;&#39069;&#22806;&#25104;&#26412;&#65292;&#35201;&#20040;&#65288;2&#65289;&#28010;&#36153;&#22823;&#37327;&#23398;&#20064;&#36718;&#27425;&#12290;&#21478;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#27809;&#26377;&#32771;&#34385;&#21487;&#33021;&#19981;&#24895;&#24847;/&#26080;&#27861;&#36981;&#24490;&#25552;&#20986;&#30340;NFL&#35299;&#20915;&#26041;&#26696;&#30340;&#23458;&#25143;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedClust&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#19968;&#27425;&#24615;&#30340;&#25805;&#20316;&#20013;&#23558;&#23458;&#25143;&#31471;&#20998;&#32452;&#25104;&#38598;&#32676;&#12290;</title><link>https://arxiv.org/abs/2403.04144</link><description>&lt;p&gt;
FedClust&#65306;&#36890;&#36807;&#22522;&#20110;&#26435;&#37325;&#39537;&#21160;&#30340;&#23458;&#25143;&#31471;&#32858;&#31867;&#20248;&#21270;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedClust: Optimizing Federated Learning on Non-IID Data through Weight-Driven Client Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedClust&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#19968;&#27425;&#24615;&#30340;&#25805;&#20316;&#20013;&#23558;&#23458;&#25143;&#31471;&#20998;&#32452;&#25104;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#25955;&#35774;&#22791;&#19978;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19981;&#20250;&#26292;&#38706;&#23427;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#22312;FL&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#36829;&#21453;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#35757;&#32451;&#26679;&#26412;&#30340;&#20551;&#35774;&#12290;&#38598;&#32676;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#36890;&#36807;&#26681;&#25454;&#25968;&#25454;&#20998;&#24067;&#30340;&#30456;&#20284;&#24615;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#20998;&#32452;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CFL&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#36890;&#20449;&#24448;&#36820;&#26469;&#31283;&#23450;&#38598;&#32676;&#24418;&#25104;&#65292;&#24182;&#19988;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#38598;&#32676;&#25968;&#37327;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedClust&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;CFL&#26041;&#27861;&#65292;&#21033;&#29992;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;FedClust&#20351;&#29992;&#31574;&#30053;&#24615;&#36873;&#25321;&#30340;&#21442;&#25968;&#19968;&#27425;&#24615;&#23558;&#23458;&#25143;&#31471;&#20998;&#32452;&#25104;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04144v1 Announce Type: cross  Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm enabling collaborative model training on decentralized devices without exposing their local data. A key challenge in FL is the uneven data distribution across client devices, violating the well-known assumption of independent-and-identically-distributed (IID) training samples in conventional machine learning. Clustered federated learning (CFL) addresses this challenge by grouping clients based on the similarity of their data distributions. However, existing CFL approaches require a large number of communication rounds for stable cluster formation and rely on a predefined number of clusters, thus limiting their flexibility and adaptability. This paper proposes FedClust, a novel CFL approach leveraging correlations between local model weights and client data distributions. FedClust groups clients into clusters in a one-shot manner using strategically selected pa
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21270;RCA&#20013;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#35786;&#26029;&#20449;&#24687;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04123</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring LLM-based Agents for Root Cause Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04123
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#33258;&#21160;&#21270;RCA&#20013;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#35786;&#26029;&#20449;&#24687;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36719;&#20214;&#31995;&#32479;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23548;&#33268;&#20107;&#20214;&#31649;&#29702;&#24050;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#26159;&#20107;&#20214;&#31649;&#29702;&#36807;&#31243;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#23545;&#20540;&#29677;&#24037;&#31243;&#24072;&#26469;&#35828;&#26159;&#19968;&#39033;&#20005;&#23803;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#23545;&#22242;&#38431;&#29305;&#23450;&#26381;&#21153;&#30340;&#24191;&#27867;&#32463;&#39564;&#12290;&#33258;&#21160;&#21270;RCA&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#20943;&#36731;&#20540;&#29677;&#24037;&#31243;&#24072;&#22312;&#20107;&#20214;&#31649;&#29702;&#19978;&#30340;&#36127;&#25285;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;RCA&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#21160;&#24577;&#25910;&#38598;&#39069;&#22806;&#30340;&#35786;&#26029;&#20449;&#24687;&#65292;&#22914;&#19982;&#20107;&#20214;&#30456;&#20851;&#30340;&#26085;&#24535;&#12289;&#25351;&#26631;&#25110;&#25968;&#25454;&#24211;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#35786;&#26029;&#26681;&#26412;&#21407;&#22240;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#20195;&#29702;&#29992;&#20110;RCA&#20197;&#35299;&#20915;&#27492;&#38480;&#21046;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04123v1 Announce Type: cross  Abstract: The growing complexity of cloud based software systems has resulted in incident management becoming an integral part of the software development lifecycle. Root cause analysis (RCA), a critical part of the incident management process, is a demanding task for on-call engineers, requiring deep domain knowledge and extensive experience with a team's specific services. Automation of RCA can result in significant savings of time, and ease the burden of incident management on on-call engineers. Recently, researchers have utilized Large Language Models (LLMs) to perform RCA, and have demonstrated promising results. However, these approaches are not able to dynamically collect additional diagnostic information such as incident related logs, metrics or databases, severely restricting their ability to diagnose root causes. In this work, we explore the use of LLM based agents for RCA to address this limitation. We present a thorough empirical eva
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.04121</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Reason and Plan?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#65292;&#26080;&#27861;&#20687;&#20154;&#31867;&#19968;&#26679;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20154;&#31867;&#26377;&#26102;&#20505;&#34920;&#29616;&#20986;&#33021;&#22815;&#36890;&#36807;&#33258;&#25105;&#25209;&#35780;&#32416;&#27491;&#33258;&#24049;&#38169;&#35823;&#29468;&#27979;&#30340;&#33021;&#21147;&#65292;&#20294;&#20284;&#20046;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#27809;&#26377;&#20381;&#25454;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04121v1 Announce Type: new  Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#65292;&#21487;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21644;&#20854;&#23545;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04118</link><description>&lt;p&gt;
&#20840;&#23616;&#31283;&#23450;&#30340;&#31070;&#32463;&#20223;&#30495;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Globally Stable Neural Imitation Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#65292;&#21487;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21644;&#20854;&#23545;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20174;&#22836;&#24320;&#22987;&#22312;&#35299;&#20915;&#31354;&#38388;&#20013;&#23398;&#20064;&#25919;&#31574;&#30340;&#36164;&#28304;&#23494;&#38598;&#21644;&#32791;&#26102;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#32467;&#26524;&#25919;&#31574;&#21487;&#20197;&#21487;&#38752;&#22320;&#27169;&#20223;&#19987;&#23478;&#28436;&#31034;&#65292;&#20294;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#26410;&#25506;&#32034;&#21306;&#22495;&#20013;&#24120;&#24120;&#32570;&#20047;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#32473;&#22312;&#38754;&#23545;&#25200;&#21160;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#65288;SNDS&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#20855;&#26377;&#27491;&#24335;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;&#25919;&#31574;&#30340;&#20223;&#30495;&#23398;&#20064;&#21046;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#25919;&#31574;&#26550;&#26500;&#65292;&#20419;&#36827;&#22522;&#20110;&#26446;&#20122;&#26222;&#35834;&#22827;&#23450;&#29702;&#30340;&#31283;&#23450;&#24615;&#34920;&#31034;&#65292;&#24182;&#32852;&#21512;&#35757;&#32451;&#25919;&#31574;&#21450;&#20854;&#30456;&#24212;&#30340;&#26446;&#20122;&#26222;&#35834;&#22827;&#20505;&#36873;&#32773;&#65292;&#20197;&#30830;&#20445;&#20840;&#23616;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23558;&#32463;&#36807;&#35757;&#32451;&#30340;&#25919;&#31574;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#26800;&#25163;&#33218;&#19978;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;SNDS&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04118v1 Announce Type: cross  Abstract: Imitation learning presents an effective approach to alleviate the resource-intensive and time-consuming nature of policy learning from scratch in the solution space. Even though the resulting policy can mimic expert demonstrations reliably, it often lacks predictability in unexplored regions of the state-space, giving rise to significant safety concerns in the face of perturbations. To address these challenges, we introduce the Stable Neural Dynamical System (SNDS), an imitation learning regime which produces a policy with formal stability guarantees. We deploy a neural policy architecture that facilitates the representation of stability based on Lyapunov theorem, and jointly train the policy and its corresponding Lyapunov candidate to ensure global stability. We validate our approach by conducting extensive experiments in simulation and successfully deploying the trained policies on a real-world manipulator arm. The experimental resu
&lt;/p&gt;</description></item><item><title>COV-NeRF&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#36741;&#21161;&#23556;&#32447;&#20307;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30495;&#23454;&#22270;&#20687;&#20013;&#25552;&#21462;&#23545;&#35937;&#24182;&#21512;&#25104;&#26032;&#22330;&#26223;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#20174;&#34394;&#25311;&#21040;&#23454;&#38469;&#20013;&#30340;&#25968;&#25454;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04114</link><description>&lt;p&gt;
&#29992;&#21487;&#32452;&#21512;&#29289;&#20307;&#30340;&#31070;&#32463;&#36741;&#21161;&#23556;&#32447;&#20307;&#36924;&#30495;&#22320;&#32553;&#23567;&#35270;&#35273;&#20174;&#34394;&#25311;&#21040;&#23454;&#38469;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04114
&lt;/p&gt;
&lt;p&gt;
COV-NeRF&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#36741;&#21161;&#23556;&#32447;&#20307;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#30495;&#23454;&#22270;&#20687;&#20013;&#25552;&#21462;&#23545;&#35937;&#24182;&#21512;&#25104;&#26032;&#22330;&#26223;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#20174;&#34394;&#25311;&#21040;&#23454;&#38469;&#20013;&#30340;&#25968;&#25454;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#28508;&#21147;&#65292;&#20294;&#33719;&#21462;&#30495;&#23454;&#19990;&#30028;&#30340;&#35757;&#32451;&#25968;&#25454;&#25104;&#26412;&#39640;&#65292;&#23545;&#20110;&#19968;&#20123;&#20219;&#21153;&#21487;&#33021;&#38590;&#20197;&#23454;&#38469;&#25805;&#20316;&#12290;&#36890;&#36807;&#39046;&#22495;&#38543;&#26426;&#21270;&#36827;&#34892;&#20174;&#34394;&#25311;&#21040;&#23454;&#38469;&#30340;&#36716;&#31227;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#65292;&#24182;&#23548;&#33268;&#27169;&#22411;&#23545;&#20110;&#34394;&#25311;&#21644;&#30495;&#23454;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#32452;&#21512;&#23545;&#35937;&#20307;&#31215;&#31070;&#32463;&#36741;&#21161;&#23556;&#32447;&#20307;&#65288;COV-NeRF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#30495;&#23454;&#21040;&#34394;&#25311;&#31649;&#36947;&#20026;&#20013;&#24515;&#30340;&#29289;&#20307;&#21487;&#32452;&#21512;&#30340;NeRF&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#25104;&#38754;&#21521;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#21644;&#23545;&#35937;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;COV-NeRF&#20174;&#30495;&#23454;&#22270;&#20687;&#20013;&#25552;&#21462;&#23545;&#35937;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#25104;&#20026;&#26032;&#22330;&#26223;&#65292;&#29983;&#25104;&#36924;&#30495;&#30340;&#28210;&#26579;&#20197;&#21450;&#22810;&#31181;&#31867;&#22411;&#30340;2D&#21644;3D&#30417;&#30563;&#65292;&#21253;&#25324;&#28145;&#24230;&#22270;&#65292;&#20998;&#21106;&#36974;&#32617;&#21644;&#32593;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;COV-NeRF&#19982;&#29616;&#20195;NeRF&#30340;&#28210;&#26579;&#36136;&#37327;&#30456;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04114v1 Announce Type: cross  Abstract: Deep learning methods for perception are the cornerstone of many robotic systems. Despite their potential for impressive performance, obtaining real-world training data is expensive, and can be impractically difficult for some tasks. Sim-to-real transfer with domain randomization offers a potential workaround, but often requires extensive manual tuning and results in models that are brittle to distribution shift between sim and real. In this work, we introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF model that is the centerpiece of a real-to-sim pipeline for synthesizing training data targeted to scenes and objects from the real world. COV-NeRF extracts objects from real images and composes them into new scenes, generating photorealistic renderings and many types of 2D and 3D supervision, including depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the rendering quality of modern NeRF 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20307;&#34920;&#29616;&#33258;&#21160;&#29983;&#25104;&#19981;&#21516;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#39118;&#24739;&#32773;&#22312;&#36798;&#21040;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#21464;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.04109</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#26641;&#20272;&#35745;&#20013;&#39118;&#21518;&#20010;&#20307;&#30340;&#20010;&#24615;&#21270;&#20219;&#21153;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Using Causal Trees to Estimate Personalized Task Difficulty in Post-Stroke Individuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04109
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20307;&#34920;&#29616;&#33258;&#21160;&#29983;&#25104;&#19981;&#21516;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#39118;&#24739;&#32773;&#22312;&#36798;&#21040;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#35757;&#32451;&#35745;&#21010;&#23545;&#20013;&#39118;&#21518;&#30340;&#24247;&#22797;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#35843;&#25972;&#30340;&#35745;&#21010;&#21462;&#20915;&#20110;&#22914;&#20309;&#37327;&#21270;&#20219;&#21153;&#23545;&#29305;&#23450;&#20010;&#20307;&#22312;&#24247;&#22797;&#36807;&#31243;&#20013;&#30340;&#38590;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#26681;&#25454;&#20010;&#20307;&#30340;&#34920;&#29616;&#33258;&#21160;&#29983;&#25104;&#19981;&#21516;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#27604;&#20808;&#21069;&#20272;&#35745;&#20219;&#21153;&#38590;&#24230;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#35299;&#37322;&#20102;&#36798;&#21040;&#20219;&#21153;&#30340;&#29992;&#25143;&#34920;&#29616;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04109v1 Announce Type: cross  Abstract: Adaptive training programs are crucial for recovery post stroke. However, developing programs that automatically adapt depends on quantifying how difficult a task is for a specific individual at a particular stage of their recovery. In this work, we propose a method that automatically generates regions of different task difficulty levels based on an individual's performance. We show that this technique explains the variance in user performance for a reaching task better than previous approaches to estimating task difficulty.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Many-objective multi-solution Transport (MosT) &#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20247;&#22810;&#30446;&#26631;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#25214;&#21040;&#22810;&#20010;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#21152;&#26435;&#30446;&#26631;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.04099</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#22810;&#35299;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Many-Objective Multi-Solution Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Many-objective multi-solution Transport (MosT) &#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20247;&#22810;&#30446;&#26631;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#25214;&#21040;&#22810;&#20010;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#21152;&#26435;&#30446;&#26631;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#35768;&#22810;&#30446;&#26631;&#65288;&#30001;&#20219;&#21153;&#25110;&#23458;&#25143;&#31471;&#23454;&#20363;&#21270;&#65289;&#19982;&#23569;&#25968;&#24085;&#32047;&#25176;&#22266;&#23450;&#35299;&#20915;&#26041;&#26696;&#65288;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#23569;&#37327;&#30446;&#26631;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22810;&#20010;&#25968;&#30446;&#36229;&#36807;&#35299;&#20915;&#26041;&#26696;&#30340;&#30446;&#26631;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#25110;&#34987;&#24573;&#30053;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Many-objective multi-solution Transport&#65288;MosT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#30446;&#26631;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#25214;&#21040;&#22810;&#20010;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#23547;&#25214;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#37117;&#20687;&#39046;&#22495;&#19987;&#23478;&#19968;&#26679;&#25191;&#34892;&#65292;&#24182;&#19987;&#27880;&#20110;&#29305;&#23450;&#23376;&#30446;&#26631;&#38598;&#65292;&#21516;&#26102;&#20849;&#21516;&#28085;&#30422;&#25152;&#26377;&#30446;&#26631;&#12290;MosT&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#21152;&#26435;&#30446;&#26631;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20854;&#20013;&#26435;&#37325;&#30001;&#30446;&#26631;&#21644;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#25910;&#25947;&#21040;&#24085;&#32047;&#25176;&#31449;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04099v1 Announce Type: new  Abstract: Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few number of objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce Many-objective multi-solution Transport (MosT), a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between the objectives and solutions. Our algorithm ensures convergence to Pareto station
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.04086</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#33258;&#21160;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#25968;&#23383;&#21270;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#26512;EHR&#25968;&#25454;&#20197;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#20854;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20513;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26469;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#30446;&#26631;&#30142;&#30149;&#65292;&#20197;&#25552;&#39640;&#21333;&#20219;&#21153;&#23398;&#20064;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;EHR&#25968;&#25454;&#30340;MTL&#26694;&#26550;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#26469;&#35782;&#21035;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#20219;&#21153;&#32452;&#21644;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#65292;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#20026;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#24182;&#25913;&#36827;&#26694;&#26550;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25628;&#32034;&#20219;&#21153;&#20998;&#32452;&#21644;&#26550;&#26500;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#28085;&#30422;&#20219;&#21153;&#32452;&#21512;&#21644;&#26550;&#26500;&#30340;&#24191;&#27867;&#32852;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04086v1 Announce Type: new  Abstract: In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing ta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;</title><link>https://arxiv.org/abs/2403.04082</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#36827;&#34892;&#25512;&#26029;&#65306;&#23545;&#27604;&#34920;&#31034;&#21487;&#35777;&#26126;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#22914;&#20309;&#22238;&#31572;&#35832;&#22914;&#8220;&#26410;&#26469;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#25105;&#20204;&#26159;&#22914;&#20309;&#21040;&#36798;&#36825;&#37324;&#30340;&#65311;&#8221;&#36825;&#31867;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#22312;&#35266;&#27979;&#20540;&#20026;&#39640;&#32500;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#30340;&#32039;&#20945;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#27010;&#29575;&#27604;&#12290;&#36890;&#36807;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#20197;&#34920;&#26126;&#34920;&#31034;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#34920;&#31034;&#30340;&#32852;&#21512;&#20998;&#24067;&#20063;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#19968;&#31181;&#22270;&#24418;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#34920;&#31034;&#36827;&#34892;&#30340;&#25512;&#26029;&#65288;&#20363;&#22914;&#39044;&#27979;&#12289;&#35268;&#21010;&#65289;&#23545;&#24212;&#20110;&#21453;&#28436;&#20302;&#32500;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#19979;&#38477;&#38382;&#39064;&#65292;&#21033;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#24320;&#21457;&#19978;&#30028;&#24182;&#33719;&#24471;&#25910;&#25947;&#20445;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.04081</link><description>&lt;p&gt;
&#26041;&#21521;&#24179;&#28369;&#24230;&#19982;&#26799;&#24230;&#26041;&#27861;&#65306;&#25910;&#25947;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Directional Smoothness and Gradient Methods: Convergence and Adaptivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#19979;&#38477;&#38382;&#39064;&#65292;&#21033;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#24320;&#21457;&#19978;&#30028;&#24182;&#33719;&#24471;&#25910;&#25947;&#20445;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#65292;&#20854;&#21462;&#20915;&#20110;&#27839;&#20248;&#21270;&#36335;&#24452;&#30340;&#30446;&#26631;&#26465;&#20214;&#24615;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#26368;&#22351;&#24773;&#20917;&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20851;&#38190;&#26159;&#26041;&#21521;&#24179;&#28369;&#24230;&#65292;&#36825;&#26159;&#25105;&#20204;&#29992;&#26469;&#24320;&#21457;&#30446;&#26631;&#19978;&#30028;&#30340;&#26799;&#24230;&#21464;&#21270;&#24230;&#37327;&#12290;&#26368;&#23567;&#21270;&#36825;&#20123;&#19978;&#30028;&#38656;&#35201;&#35299;&#20915;&#38544;&#24335;&#26041;&#31243;&#20197;&#33719;&#24471;&#19968;&#31995;&#21015;&#24378;&#36866;&#24212;&#30340;&#27493;&#38271;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20984;&#20108;&#27425;&#20989;&#25968;, &#36825;&#20123;&#26041;&#31243;&#24456;&#23481;&#26131;&#27714;&#35299;&#65292;&#24182;&#20026;&#20004;&#31181;&#32463;&#20856;&#27493;&#38271;&#25552;&#20379;&#20102;&#26032;&#30340;&#20445;&#35777;&#12290;&#23545;&#20110;&#19968;&#33324;&#20989;&#25968;, &#25105;&#20204;&#35777;&#26126;Polyak&#27493;&#38271;&#21644;&#24402;&#19968;&#21270;GD&#33719;&#24471;&#24555;&#36895;&#30340;&#12289;&#36335;&#24452;&#30456;&#20851;&#30340;&#36895;&#29575;&#65292;&#23613;&#31649;&#19981;&#20351;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290; Logistic&#22238;&#24402;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#27604;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#32463;&#20856;&#29702;&#35770;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04081v1 Announce Type: new  Abstract: We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#26367;&#20195;&#36710;&#36742;&#26368;&#20339;&#20301;&#32622;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04072</link><description>&lt;p&gt;
&#39044;&#27979;&#21644;&#32531;&#35299;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#30340;&#20013;&#26029;
&lt;/p&gt;
&lt;p&gt;
Forecasting and Mitigating Disruptions in Public Bus Transit Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04072
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#20844;&#20132;&#26381;&#21153;&#20013;&#26367;&#20195;&#36710;&#36742;&#26368;&#20339;&#20301;&#32622;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#20132;&#36890;&#31995;&#32479;&#32463;&#24120;&#36973;&#21463;&#38656;&#27714;&#30340;&#24847;&#22806;&#27874;&#21160;&#21644;&#20013;&#26029;&#65292;&#20363;&#22914;&#26426;&#26800;&#25925;&#38556;&#21644;&#21307;&#30103;&#32039;&#24613;&#24773;&#20917;&#12290;&#36825;&#20123;&#27874;&#21160;&#21644;&#20013;&#26029;&#23548;&#33268;&#24310;&#35823;&#21644;&#25317;&#25380;&#65292;&#36825;&#23545;&#20056;&#23458;&#30340;&#20307;&#39564;&#21644;&#20844;&#20849;&#20132;&#36890;&#26381;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#37117;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#31215;&#26497;&#20943;&#23569;&#36825;&#31867;&#20107;&#20214;&#65292;&#35768;&#22810;&#36816;&#36755;&#26426;&#26500;&#22312;&#20854;&#26381;&#21153;&#33539;&#22260;&#20869;&#35774;&#32622;&#26367;&#20195;&#65288;&#22791;&#29992;&#65289;&#36710;&#36742;&#65292;&#23427;&#20204;&#21487;&#20197;&#35843;&#24230;&#36825;&#20123;&#36710;&#36742;&#20197;&#22686;&#21152;&#25110;&#26367;&#20195;&#36973;&#21463;&#25317;&#25380;&#25110;&#20013;&#26029;&#30340;&#36335;&#32447;&#19978;&#30340;&#36710;&#36742;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20013;&#26029;&#30340;&#22266;&#26377;&#38543;&#26426;&#24615;&#21644;&#22312;&#22478;&#24066;&#21508;&#22320;&#36873;&#25321;&#20301;&#32622;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#30830;&#23450;&#26367;&#20195;&#36710;&#36742;&#24212;&#35813;&#35774;&#32622;&#30340;&#26368;&#20339;&#20301;&#32622;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19982;&#30000;&#32435;&#35199;&#24030;&#32435;&#20160;&#32500;&#23572;&#30340;&#36816;&#36755;&#26426;&#26500;&#21512;&#20316;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04072v1 Announce Type: new  Abstract: Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04070</link><description>&lt;p&gt;
&#21033;&#29992;&#33030;&#24369;&#24615;&#24863;&#30693;&#25200;&#21160;&#39044;&#31639;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Training using Vulnerability-Aware Perturbation Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;(Adversarial Training, AT)&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#24120;&#65292;AT&#28041;&#21450;&#20351;&#29992;&#22312;&#39044;&#23450;&#20041;&#12289;&#22266;&#23450;&#25200;&#21160;&#30028;&#38480;&#20869;&#33719;&#21462;&#30340;&#23545;&#25239;&#31034;&#20363;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29992;&#20110;&#21046;&#20316;&#36825;&#20123;&#23545;&#25239;&#31034;&#20363;&#30340;&#20010;&#21035;&#33258;&#28982;&#31034;&#20363;&#23637;&#31034;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#22266;&#26377;&#33030;&#24369;&#24615;&#65292;&#22240;&#27492;&#65292;&#20026;&#25152;&#26377;&#23454;&#20363;&#35774;&#23450;&#22266;&#23450;&#25200;&#21160;&#21322;&#24452;&#26469;&#21046;&#20316;&#23545;&#25239;&#31034;&#20363;&#21487;&#33021;&#19981;&#36275;&#20197;&#20805;&#20998;&#21457;&#25381;AT&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#12289;&#35745;&#31639;&#24265;&#20215;&#30340;&#22522;&#20110;&#33030;&#24369;&#24615;&#24863;&#30693;&#30340;&#37325;&#26032;&#21152;&#26435;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#29992;&#20110;AT&#30340;&#23545;&#25239;&#31034;&#20363;&#20998;&#37197;&#25200;&#21160;&#30028;&#38480;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#36793;&#38469;&#21152;&#26435;&#25200;&#21160;&#39044;&#31639;&#65288;MWPB&#65289;&#21644;&#26631;&#20934;&#24046;&#21152;&#26435;&#25200;&#21160;&#39044;&#31639;&#65288;SDWPB&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26681;&#25454;&#20854;&#23545;&#24212;&#33030;&#24369;&#24615;&#20026;&#21333;&#20010;&#23545;&#25239;&#26679;&#26412;&#20998;&#37197;&#25200;&#21160;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04070v1 Announce Type: cross  Abstract: Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their correspon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24565;&#29366;&#24577;&#25512;&#29702;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#29366;&#24577;&#20928;&#21270;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#24754;&#35266;&#31574;&#30053;&#65292;&#20197;&#23545;&#25239;&#20195;&#29702;&#23545;&#30495;&#23454;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04050</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24565;&#20016;&#23500;&#30340;&#24754;&#35266;Q&#23398;&#20064;&#25269;&#25239;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04050
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24565;&#29366;&#24577;&#25512;&#29702;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#29366;&#24577;&#20928;&#21270;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#24754;&#35266;&#31574;&#30053;&#65292;&#20197;&#23545;&#25239;&#20195;&#29702;&#23545;&#30495;&#23454;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#28431;&#27934;&#65292;&#21487;&#20197;&#34987;&#24694;&#24847;&#23545;&#25163;&#21033;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#38454;&#27573;&#26377;&#31574;&#30053;&#22320;&#25200;&#20081;&#20854;&#29366;&#24577;&#35266;&#23519;&#65292;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;RL&#20195;&#29702;&#24456;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#20197;&#25913;&#21892;&#21463;&#25200;&#21160;&#24433;&#21709;&#35757;&#32451;&#31574;&#30053;&#30340;&#24179;&#28369;&#24615;&#65292;&#35201;&#20040;&#20998;&#21035;&#35757;&#32451;&#20195;&#29702;&#30340;&#31574;&#30053;&#21644;&#25915;&#20987;&#32773;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#26469;&#25269;&#24481;&#24378;&#25915;&#20987;&#65292;&#32780;&#21518;&#32773;&#23545;&#20110;&#22823;&#29615;&#22659;&#32780;&#35328;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#20986;&#19968;&#31181;&#24754;&#35266;&#31574;&#30053;&#65292;&#20197;&#38450;&#33539;&#20195;&#29702;&#23545;&#30495;&#23454;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#32467;&#21512;&#20102;&#20449;&#24565;&#29366;&#24577;&#25512;&#29702;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#29366;&#24577;&#20928;&#21270;&#65292;&#20197;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04050v1 Announce Type: new  Abstract: Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce unc
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#30830;&#23450;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#35268;&#27169;&#65292;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23376;&#32452;&#20013;&#20272;&#35745;&#26465;&#20214;&#21453;&#20107;&#23454;&#26399;&#26395;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#30446;&#26631;&#36716;&#21270;&#20026;&#21516;&#26102;&#25512;&#26029;&#38382;&#39064;&#26469;&#35299;&#20915;&#20272;&#35745;&#35823;&#24046;&#22686;&#21152;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20801;&#35768;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#39044;&#31639;&#19979;&#36870;&#36716;&#38382;&#39064;&#20197;&#30830;&#23450;&#21487;&#34892;&#30340;&#27835;&#30103;&#25163;&#33218;&#25968;&#37327;&#25110;&#20998;&#21306;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04039</link><description>&lt;p&gt;
&#29992;&#20110;&#26465;&#20214;&#21453;&#20107;&#23454;&#22343;&#20540;&#20272;&#35745;&#30340;&#26679;&#26412;&#35268;&#27169;&#35268;&#21010;: &#19968;&#20010;K&#33218;&#38543;&#26426;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Sample size planning for conditional counterfactual mean estimation with a K-armed randomized experiment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04039
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#30830;&#23450;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#35268;&#27169;&#65292;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23376;&#32452;&#20013;&#20272;&#35745;&#26465;&#20214;&#21453;&#20107;&#23454;&#26399;&#26395;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#30446;&#26631;&#36716;&#21270;&#20026;&#21516;&#26102;&#25512;&#26029;&#38382;&#39064;&#26469;&#35299;&#20915;&#20272;&#35745;&#35823;&#24046;&#22686;&#21152;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20801;&#35768;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#22823;&#23567;&#39044;&#31639;&#19979;&#36870;&#36716;&#38382;&#39064;&#20197;&#30830;&#23450;&#21487;&#34892;&#30340;&#27835;&#30103;&#25163;&#33218;&#25968;&#37327;&#25110;&#20998;&#21306;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#30830;&#23450;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#35268;&#27169;&#65292;&#20197;&#20272;&#35745;&#25968;&#25454;&#39537;&#21160;&#23376;&#32452;&#20013;&#30340;&#26465;&#20214;&#21453;&#20107;&#23454;&#26399;&#26395;&#65292;&#35813;&#23376;&#32452;&#21487;&#20197;&#30001;&#20219;&#20309;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#31639;&#27861;&#36755;&#20986;&#65292;&#21253;&#25324;&#26681;&#25454;&#39044;&#27979;&#20998;&#25968;&#30456;&#20284;&#30340;&#29992;&#25143;&#36827;&#34892;&#20998;&#32452;&#25110;&#26681;&#25454;&#23398;&#20064;&#30340;&#31574;&#30053;&#26641;&#36827;&#34892;&#20998;&#32452;&#12290;&#22312;&#20180;&#32454;&#35268;&#23450;&#25512;&#26029;&#30446;&#26631;&#12289;&#26368;&#23567;&#32622;&#20449;&#27700;&#24179;&#21644;&#26368;&#22823;&#35823;&#24046;&#36793;&#38469;&#21518;&#65292;&#20851;&#38190;&#26159;&#23558;&#21407;&#22987;&#30446;&#26631;&#36716;&#21270;&#20026;&#19968;&#20010;&#21516;&#26102;&#25512;&#26029;&#38382;&#39064;&#65292;&#25512;&#33616;&#30340;&#26679;&#26412;&#22823;&#23567;&#20197;&#25269;&#28040;&#20272;&#35745;&#35823;&#24046;&#30340;&#22686;&#21152;&#21487;&#33021;&#24615;&#30452;&#25509;&#19982;&#35201;&#36827;&#34892;&#30340;&#25512;&#26029;&#25968;&#37327;&#30456;&#20851;&#12290;&#22312;&#32473;&#23450;&#22266;&#23450;&#26679;&#26412;&#35268;&#27169;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#38382;&#39064;&#21453;&#36716;&#20026;&#20851;&#20110;&#21487;&#34892;&#27835;&#30103;&#25163;&#33218;&#25968;&#37327;&#25110;&#20998;&#21306;&#22797;&#26434;&#24615;&#65288;&#20363;&#22914;&#65292;&#20915;&#31574;&#26641;&#21494;&#23376;&#25968;&#37327;&#65289;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#31574;&#30053;&#26641;&#23398;&#20064;&#23376;&#32452;&#65292;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04039v1 Announce Type: new  Abstract: We cover how to determine a sufficiently large sample size for a $K$-armed randomized experiment in order to estimate conditional counterfactual expectations in data-driven subgroups. The sub-groups can be output by any feature space partitioning algorithm, including as defined by binning users having similar predictive scores or as defined by a learned policy tree. After carefully specifying the inference target, a minimum confidence level, and a maximum margin of error, the key is to turn the original goal into a simultaneous inference problem where the recommended sample size to offset an increased possibility of estimation error is directly related to the number of inferences to be conducted. Given a fixed sample size budget, our result allows us to invert the question to one about the feasible number of treatment arms or partition complexity (e.g. number of decision tree leaves). Using policy trees to learn sub-groups, we evaluate o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OCD-FL&#30340;&#26032;&#26041;&#26696;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;</title><link>https://arxiv.org/abs/2403.04037</link><description>&lt;p&gt;
OCD-FL: &#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#36873;&#25321;&#30340;&#36890;&#20449;&#39640;&#25928;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OCD-FL: A Novel Communication-Efficient Peer Selection-based Decentralized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OCD-FL&#30340;&#26032;&#26041;&#26696;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#30340;&#32467;&#21512;&#24320;&#21019;&#20102;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#65292;&#32852;&#37030;&#23398;&#20064;(FL)&#20316;&#20026;&#26368;&#31361;&#20986;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#36825;&#20123;&#23398;&#20064;&#26041;&#26696;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#35299;&#20915;&#23427;&#20204;&#26368;&#22522;&#26412;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#20107;&#23454;&#19978;&#65292;&#20855;&#26377;&#20013;&#24515;&#32858;&#21512;&#22120;&#30340;&#20256;&#32479;FL&#23384;&#22312;&#21333;&#28857;&#25925;&#38556;&#21644;&#32593;&#32476;&#29942;&#39048;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33410;&#28857;&#22312;&#28857;&#23545;&#28857;&#32593;&#32476;&#20013;&#21327;&#20316;&#30340;&#21435;&#20013;&#24515;&#21270;FL&#12290;&#23613;&#31649;&#21518;&#32773;&#25928;&#29575;&#39640;&#65292;&#20294;&#22312;&#21435;&#20013;&#24515;&#21270;FL&#20013;&#65292;&#36890;&#20449;&#25104;&#26412;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#20173;&#28982;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#20250;&#20027;&#20041;&#36890;&#20449;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;(OCD-FL)&#30340;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#21253;&#25324;&#31995;&#32479;&#21270;&#30340;FL&#23545;&#31561;&#36873;&#25321;&#20197;&#36827;&#34892;&#21327;&#20316;&#65292;&#26088;&#22312;&#23454;&#29616;&#26368;&#22823;&#30340;FL&#30693;&#35782;&#22686;&#30410;&#21516;&#26102;&#20943;&#23569;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04037v1 Announce Type: new  Abstract: The conjunction of edge intelligence and the ever-growing Internet-of-Things (IoT) network heralds a new era of collaborative machine learning, with federated learning (FL) emerging as the most prominent paradigm. With the growing interest in these learning schemes, researchers started addressing some of their most fundamental limitations. Indeed, conventional FL with a central aggregator presents a single point of failure and a network bottleneck. To bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer network has been proposed. Despite the latter's efficiency, communication costs and data heterogeneity remain key challenges in decentralized FL. In this context, we propose a novel scheme, called opportunistic communication-efficient decentralized federated learning, a.k.a., OCD-FL, consisting of a systematic FL peer selection for collaboration, aiming to achieve maximum FL knowledge gain while reducing energy co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#22788;&#29702;RF&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04036</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25269;&#25239;&#26102;&#22495;&#28418;&#31227;&#19979;&#30340;&#31283;&#20581;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#23556;&#39057;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#20013;&#30340;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#36317;&#31163;&#24230;&#37327;&#22788;&#29702;RF&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#39640;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#65288;RF&#65289;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#33258;&#21160;&#26080;&#32447;&#35774;&#22791;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#28508;&#22312;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20449;&#36947;&#26465;&#20214;&#21644;&#29615;&#22659;&#35774;&#32622;&#30340;&#21464;&#21270;&#21487;&#33021;&#24341;&#36215;&#30340;&#39046;&#22495;&#28418;&#31227;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#22522;&#20110;RF&#30340;&#35774;&#22791;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#22312;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#36317;&#31163;&#24230;&#37327;&#65292;&#20351;&#24471;&#27491;&#23545;&#32452;&#22312;&#23398;&#20064;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#27604;&#36127;&#23545;&#26356;&#25509;&#36817;&#65288;&#21363;&#26356;&#30456;&#20284;&#65289;&#12290;&#24403;&#24212;&#29992;&#20110;RF&#25351;&#32441;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26469;&#33258;&#30456;&#21516;&#20256;&#36755;&#30340;RF&#20449;&#21495;&#35270;&#20026;&#27491;&#23545;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;&#20256;&#36755;&#30340;&#20449;&#21495;&#35270;&#20026;&#36127;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04036v1 Announce Type: cross  Abstract: Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. T
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#23398;&#20064;&#20013;&#36890;&#36807;&#20803;&#31639;&#27861;&#32467;&#21512;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#32422;&#26463;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#39640;&#27010;&#29575;&#22320;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#31639;&#27861;&#30340;&#21518;&#24724;&#21463;&#21040;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#38480;&#21046;&#65292;&#27169;&#22411;&#31867;&#20013;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.04033</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04033
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#20013;&#36890;&#36807;&#20803;&#31639;&#27861;&#32467;&#21512;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#32422;&#26463;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#39640;&#27010;&#29575;&#22320;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#12290;&#31639;&#27861;&#30340;&#21518;&#24724;&#21463;&#21040;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#38480;&#21046;&#65292;&#27169;&#22411;&#31867;&#20013;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#22312;&#27599;&#19968;&#36718;&#24517;&#39035;&#36981;&#23432;&#19968;&#20010;&#26410;&#30693;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#30446;&#26631;&#26159;&#22312;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#24182;&#22312;&#21518;&#35270;&#19979;&#26368;&#23567;&#21270;&#23545;&#26368;&#20339;&#23433;&#20840;&#21160;&#20316;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#22238;&#24402;&#39044;&#27979;&#22120;&#26469;&#20272;&#35745;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23558;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#36716;&#25442;&#20026;&#31526;&#21512;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#39044;&#27979;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21518;&#24724;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#21644;&#22312;&#32447;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#21518;&#24724;&#12289;&#21253;&#21547;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#27169;&#22411;&#31867;&#30340;&#36867;&#36991;&#32500;&#24230;&#65292;&#20197;&#21450;&#19968;&#20010;&#25429;&#25417;&#23433;&#20840;&#23398;&#20064;&#22256;&#38590;&#31243;&#24230;&#30340;&#26032;&#39062;&#22797;&#26434;&#24230;&#24230;&#37327;&#26469;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04033v1 Announce Type: cross  Abstract: We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#21487;&#24341;&#23548;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#36923;&#36753;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#28436;&#32462;&#25628;&#32034;&#65292;&#20174;&#22823;&#22411;&#25512;&#29702;&#20307;&#20013;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#25171;&#30772;&#25512;&#29702;&#31995;&#32479;&#30340;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#25512;&#29702;&#38142;&#30340;&#38271;&#36828;&#21457;&#23637;&#21644;&#26032;&#39062;&#35777;&#26126;&#24605;&#36335;&#30340;&#20135;&#29983;</title><link>https://arxiv.org/abs/2403.04017</link><description>&lt;p&gt;
&#23398;&#20064;&#24341;&#23548;&#30340;&#33258;&#21160;&#25512;&#29702;&#65306;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning Guided Automated Reasoning: A Brief Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04017
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#24341;&#23548;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#36923;&#36753;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#28436;&#32462;&#25628;&#32034;&#65292;&#20174;&#22823;&#22411;&#25512;&#29702;&#20307;&#20013;&#22521;&#35757;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#25171;&#30772;&#25512;&#29702;&#31995;&#32479;&#30340;&#32452;&#21512;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20419;&#36827;&#25512;&#29702;&#38142;&#30340;&#38271;&#36828;&#21457;&#23637;&#21644;&#26032;&#39062;&#35777;&#26126;&#24605;&#36335;&#30340;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#24418;&#24335;&#21270;&#35777;&#26126;&#21161;&#25163;&#26159;&#36890;&#29992;&#30340;&#25512;&#29702;&#31995;&#32479;&#65292;&#29702;&#35770;&#19978;&#33021;&#22815;&#35777;&#26126;&#20219;&#24847;&#22256;&#38590;&#30340;&#23450;&#29702;&#65292;&#20174;&#32780;&#35299;&#20915;&#21487;&#24402;&#32422;&#20026;&#25968;&#23398;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#20219;&#24847;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#38754;&#20020;&#30528;&#26497;&#22823;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#22240;&#27492;&#21253;&#21547;&#35768;&#22810;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#36873;&#25321;&#28857;&#65292;&#36825;&#20123;&#26041;&#27861;&#21644;&#36873;&#25321;&#28857;&#26497;&#22823;&#22320;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#36825;&#20026;&#21463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#21487;&#20197;&#24341;&#23548;&#36825;&#20123;&#25512;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#12290;&#30456;&#21453;&#65292;&#30001;&#36923;&#36753;&#19978;&#26377;&#25928;&#35777;&#26126;&#27010;&#24565;&#25903;&#25345;&#30340;&#28436;&#32462;&#25628;&#32034;&#20801;&#35768;&#22312;&#22823;&#22411;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#36825;&#20123;&#35777;&#26126;&#20307;&#36890;&#24120;&#26159;&#36890;&#36807;&#26500;&#24314;&#27491;&#30830;&#30340;&#65292;&#24403;&#19982;&#26356;&#21152;&#31934;&#30830;&#30340;&#35757;&#32451;&#24341;&#23548;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#23558;&#23427;&#20204;&#25552;&#21319;&#20026;&#38750;&#24120;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#65292;&#20855;&#26377;&#36234;&#26469;&#36234;&#38271;&#30340;&#25512;&#29702;&#38142;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26032;&#39062;&#30340;&#35777;&#26126;&#24605;&#36335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04017v1 Announce Type: new  Abstract: Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.04015</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#20010;&#39044;&#35757;&#32451;&#30340;&#22686;&#24378;&#22411;&#20195;&#29702;&#24341;&#23548;&#30340;&#27169;&#25311;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#29305;&#24449;&#26469;&#20934;&#22791;&#25968;&#25454;&#30340;AI&#21487;&#29992;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;i&#65289;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#30446;&#26631;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#35782;&#21035;&#26368;&#20339;&#29305;&#24449;&#23376;&#38598;&#65307;ii&#65289;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#25429;&#33719;&#29305;&#24449;&#38598;&#20013;&#30340;&#22522;&#26412;&#20449;&#24687;&#32780;&#38750;&#20351;&#29992;&#30446;&#26631;&#21464;&#37327;&#26469;&#20943;&#23569;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#30446;&#26631;&#21464;&#37327;&#21644;&#19979;&#28216;ML&#20219;&#21153;&#32780;&#23548;&#33268;&#32791;&#26102;&#19988;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#26080;&#30417;&#30563;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21463;&#38480;&#20110;&#25512;&#23548;&#20986;&#30340;&#29305;&#24449;&#31354;&#38388;&#26159;&#28508;&#22312;&#19988;&#19981;&#21487;&#36861;&#36394;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#29305;&#24449;&#25351;&#23548;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#35782;&#21035;&#26368;&#20339;&#26377;&#25928;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04015v1 Announce Type: cross  Abstract: Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves gener
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.04012</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#30340;&#26102;&#38388;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Temporal Cross-Attention for Dynamic Embedding and Tokenization of Multimodal Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#24191;&#24230;&#12289;&#35268;&#27169;&#21644;&#26102;&#38388;&#31890;&#24230;&#20026;&#20351;&#29992;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#20010;&#24615;&#21270;&#21644;&#32972;&#26223;&#24739;&#32773;&#20581;&#24247;&#36712;&#36857;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#39640;&#32500;&#24230;&#12289;&#31232;&#30095;&#24615;&#12289;&#22810;&#27169;&#24577;&#24615;&#12289;&#19981;&#35268;&#21017;&#21644;&#21464;&#37327;&#29305;&#23450;&#30340;&#35760;&#24405;&#39057;&#29575;&#20197;&#21450;&#22312;&#21516;&#26102;&#35760;&#24405;&#22810;&#20010;&#27979;&#37327;&#26102;&#25139;&#37325;&#22797;&#65292;&#23398;&#20064;&#26377;&#29992;&#30340;EHR&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#21162;&#21147;&#23558;&#32467;&#26500;&#21270;EHR&#21644;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#31508;&#35760;&#34701;&#21512;&#65292;&#34920;&#26126;&#20102;&#26356;&#20934;&#30830;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#30340;&#28508;&#21147;&#65292;&#20294;&#23545;&#30452;&#25509;&#35299;&#20915;&#26102;&#38388;EHR&#25361;&#25112;&#30340;EHR&#23884;&#20837;&#26041;&#27861;&#30340;&#20851;&#27880;&#36739;&#23569;&#8212;&#8212;&#21363;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#27169;&#24577;&#24739;&#32773;&#26102;&#38388;&#24207;&#21015;&#30340;&#20855;&#26377;&#26102;&#38388;&#24863;&#30693;&#30340;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#31934;&#30830;&#34920;&#31034;&#22810;&#27169;&#24577;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#23884;&#20837;&#21644;&#26631;&#35760;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04012v1 Announce Type: new  Abstract: The breadth, scale, and temporal granularity of modern electronic health records (EHR) systems offers great potential for estimating personalized and contextual patient health trajectories using sequential deep learning. However, learning useful representations of EHR data is challenging due to its high dimensionality, sparsity, multimodality, irregular and variable-specific recording frequency, and timestamp duplication when multiple measurements are recorded simultaneously. Although recent efforts to fuse structured EHR and unstructured clinical notes suggest the potential for more accurate prediction of clinical outcomes, less focus has been placed on EHR embedding approaches that directly address temporal EHR challenges by learning time-aware representations from multimodal patient time series. In this paper, we introduce a dynamic embedding and tokenization framework for precise representation of multimodal clinical time series that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#31163;&#32676;&#20540;&#27880;&#20837;&#12289;&#28040;&#24687;&#20256;&#36882;&#21644;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#19977;&#20010;&#26041;&#38754;&#37325;&#26032;&#23457;&#35270;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#23545;&#25552;&#39640;&#24615;&#33021;&#30340;&#36890;&#29992;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.04010</link><description>&lt;p&gt;
&#19977;&#27425;&#37325;&#28201;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#65306;&#31163;&#32676;&#20540;&#12289;&#28040;&#24687;&#20256;&#36882;&#21644;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message Passing and Hyperbolic Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31163;&#32676;&#20540;&#27880;&#20837;&#12289;&#28040;&#24687;&#20256;&#36882;&#21644;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#19977;&#20010;&#26041;&#38754;&#37325;&#26032;&#23457;&#35270;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#23545;&#25552;&#39640;&#24615;&#33021;&#30340;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#23454;&#20363;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38459;&#30861;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#26412;&#25991;&#20174;&#19977;&#20010;&#26041;&#38754;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#26080;&#30417;&#30563;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#24120;&#27880;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#25968;&#25454;&#38598;&#20013;&#21019;&#24314;&#26356;&#22810;&#19981;&#21516;&#30340;&#22522;&#20110;&#22270;&#30340;&#24322;&#24120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#19982;&#19981;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#19982;&#28040;&#24687;&#20256;&#36882;&#30456;&#20851;&#30340;&#24615;&#33021;&#24847;&#22806;&#19979;&#38477;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#25351;&#23450;&#20102;&#20851;&#38190;&#30340;&#26550;&#26500;&#21644;&#25439;&#22833;&#35774;&#35745;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#25913;&#36827;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#30340;&#19968;&#33324;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04010v1 Announce Type: new  Abstract: Graph anomaly detection plays a vital role for identifying abnormal instances in complex networks. Despite advancements of methodology based on deep learning in recent years, existing benchmarking approaches exhibit limitations that hinder a comprehensive comparison. In this paper, we revisit datasets and approaches for unsupervised node-level graph anomaly detection tasks from three aspects. Firstly, we introduce outlier injection methods that create more diverse and graph-based anomalies in graph datasets. Secondly, we compare methods employing message passing against those without, uncovering the unexpected decline in performance associated with message passing. Thirdly, we explore the use of hyperbolic neural networks, specifying crucial architecture and loss design that contribute to enhanced performance. Through rigorous experiments and evaluations, our study sheds light on general strategies for improving node-level graph anomaly 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#26102;&#21516;&#26102;&#28385;&#36275;&#30828;&#24615;&#23433;&#20840;&#32422;&#26463;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.04007</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#26102;&#21516;&#26102;&#28385;&#36275;&#30828;&#24615;&#23433;&#20840;&#32422;&#26463;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#25511;&#21046;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#24615;&#21644;&#25910;&#25947;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#24357;&#21512;&#20102;&#25511;&#21046;&#29702;&#35770;&#30340;&#20005;&#26684;&#23433;&#20840;&#24615;&#20445;&#35777;&#21644;RL&#29702;&#35770;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30828;&#32422;&#26463;&#28385;&#36275;&#65292;&#23398;&#20064;RL&#25511;&#21046;&#22120;&#22312;&#25972;&#20010;&#35757;&#32451;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#28385;&#36275;&#20005;&#26684;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#21516;&#26102;&#20139;&#26377;&#32463;&#20856;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04007v1 Announce Type: new  Abstract: We develop provably safe and convergent reinforcement learning (RL) algorithms for control of nonlinear dynamical systems, bridging the gap between the hard safety guarantees of control theory and the convergence guarantees of RL theory. Recent advances at the intersection of control and RL follow a two-stage, safety filter approach to enforcing hard safety constraints: model-free RL is used to learn a potentially unsafe controller, whose actions are projected onto safe sets prescribed, for example, by a control barrier function. Though safe, such approaches lose any convergence guarantees enjoyed by the underlying RL methods. In this paper, we develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment. We validate the efficacy of our approach in simulation, including safe c
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#39640;&#25928;&#30340;&#36924;&#36817;&#25216;&#26415;&#65292;&#29992;&#20110;&#24207;&#36143;&#27169;&#22411;&#30340;&#36793;&#32536;&#21270;&#65292;&#36825;&#20123;&#25216;&#26415;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19979;&#19968;&#27493;&#26465;&#20214;&#20998;&#24067;&#30340;&#35775;&#38382;&#21644;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2403.04005</link><description>&lt;p&gt;
&#20851;&#20110;&#27010;&#29575;&#24207;&#21015;&#27169;&#22411;&#36793;&#32536;&#21270;&#30340;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Efficient Marginalization of Probabilistic Sequence Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04005
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#39640;&#25928;&#30340;&#36924;&#36817;&#25216;&#26415;&#65292;&#29992;&#20110;&#24207;&#36143;&#27169;&#22411;&#30340;&#36793;&#32536;&#21270;&#65292;&#36825;&#20123;&#25216;&#26415;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19979;&#19968;&#27493;&#26465;&#20214;&#20998;&#24067;&#30340;&#35775;&#38382;&#21644;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#32463;&#24120;&#34920;&#29616;&#20986;&#24207;&#21015;&#20381;&#36182;&#24615;&#65292;&#28085;&#30422;&#20154;&#31867;&#34892;&#20026;&#12289;&#21307;&#23398;&#12289;&#37329;&#34701;&#21644;&#27668;&#20505;&#27169;&#25311;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#27010;&#29575;&#26041;&#27861;&#25429;&#25417;&#20102;&#36825;&#20123;&#32972;&#26223;&#19979;&#39044;&#27979;&#30456;&#20851;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#33258;&#22238;&#24402;&#27169;&#22411;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#35770;&#25991;&#30528;&#37325;&#20110;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#22238;&#31572;&#36229;&#20986;&#21333;&#27493;&#39044;&#27979;&#33539;&#22260;&#30340;&#22797;&#26434;&#27010;&#29575;&#26597;&#35810;&#65292;&#27604;&#22914;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#23433;&#25490;&#25110;&#26576;&#19968;&#20107;&#20214;&#21457;&#29983;&#22312;&#21478;&#19968;&#20107;&#20214;&#20043;&#21069;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#39640;&#25928;&#30340;&#36924;&#36817;&#25216;&#26415;&#65292;&#29992;&#20110;&#24207;&#36143;&#27169;&#22411;&#30340;&#36793;&#32536;&#21270;&#65292;&#36825;&#20123;&#25216;&#26415;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19979;&#19968;&#27493;&#26465;&#20214;&#20998;&#24067;&#30340;&#35775;&#38382;&#21644;&#37319;&#26679;&#65292;&#21253;&#25324;&#20256;&#32479;&#21442;&#25968;&#27169;&#22411;&#20197;&#21450;&#26368;&#26032;&#30340;&#31070;&#32463;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04005v1 Announce Type: cross  Abstract: Real-world data often exhibits sequential dependence, across diverse domains such as human behavior, medicine, finance, and climate modeling. Probabilistic methods capture the inherent uncertainty associated with prediction in these contexts, with autoregressive models being especially prominent. This dissertation focuses on using autoregressive models to answer complex probabilistic queries that go beyond single-step prediction, such as the timing of future events or the likelihood of a specific event occurring before another. In particular, we develop a broad class of novel and efficient approximation techniques for marginalization in sequential models that are model-agnostic. These techniques rely solely on access to and sampling from next-step conditional distributions of a pre-trained autoregressive model, including both traditional parametric models as well as more recent neural autoregressive models. Specific approaches are pres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2403.04001</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#21644;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#30340;&#26032;&#20852;&#20219;&#21153;&#25490;&#24207;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04001
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#21644;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#22330;&#26223;&#65292;&#21487;&#20197;&#21551;&#21457;&#26426;&#22120;&#20154;&#30340;&#26032;&#22411;&#25511;&#21046;&#21644;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#36890;&#36807;&#28608;&#21457;&#20154;&#31867;&#22914;&#20309;&#33719;&#21462;&#30693;&#35782;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20256;&#36882;&#25216;&#33021;&#26469;&#23637;&#31034;&#36825;&#26679;&#19968;&#31181;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#21452;&#21521;&#36827;&#27493;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#33410;&#22238;&#24402;&#36827;&#23637;&#65288;ERP-BPNN&#65289;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;ERP-BPNN&#27169;&#22411;&#65288;1&#65289;&#36890;&#36807;&#20154;&#31867;&#33324;&#20132;&#21449;&#30340;&#26041;&#24335;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#28608;&#21169;&#20449;&#21495;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#20999;&#25442;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#65288;3&#65289;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#21452;&#21521;&#25216;&#33021;&#20256;&#36882;&#12290;ERP-BPNN&#26159;&#19968;&#20010;&#36890;&#29992;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#20960;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20854;&#31070;&#32463;&#32467;&#26500;&#30340;&#32454;&#33410;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#19981;&#21516;&#26426;&#22120;&#20154;&#24418;&#24577;&#30340;&#35302;&#30896;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#21644;&#25216;&#33021;&#20256;&#36882;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04001v1 Announce Type: cross  Abstract: Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressiv
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#20851;&#31995;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03994</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Video Relationship Detection Using Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03994
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#20851;&#31995;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#36830;&#25509;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#24046;&#36317;&#65292;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#26426;&#22120;&#29702;&#35299;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#36830;&#25509;&#35270;&#35273;&#21644;&#35821;&#35328;&#23384;&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24046;&#36317;&#65292;&#38590;&#20197;&#31934;&#30830;&#30830;&#23450;&#32473;&#23450;&#20195;&#29702;&#20316;&#29992;&#20110;&#21738;&#20010;&#23545;&#35937;&#24182;&#36890;&#36807;&#35821;&#35328;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#30001;&#21333;&#19968;&#30340;&#25972;&#20307;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36890;&#24120;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;MoE-VRD&#65292;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;MoE-VRD&#36890;&#36807;&#22312;&#35270;&#35273;&#22788;&#29702;&#20013;&#25552;&#21462;&#20197;&lt;&#20027;&#35821;&#65292;&#35859;&#35789;&#65292;&#23486;&#35821;&gt;&#20803;&#32452;&#24418;&#24335;&#30340;&#35821;&#35328;&#19977;&#20803;&#32452;&#26469;&#35782;&#21035;&#20851;&#31995;&#12290;&#21033;&#29992;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;MoE-VRD&#35299;&#20915;&#20102;&#22312;&#24314;&#31435;&#20027;&#20307;&#65288;&#25191;&#34892;&#21160;&#20316;&#65289;&#21644;&#23458;&#20307;&#65288;&#21463;&#21040;&#20316;&#29992;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#26102;&#23545;&#21160;&#20316;&#35782;&#21035;&#30340;&#35201;&#27714;&#12290;&#19982;&#21333;&#19968;&#25972;&#20307;&#32593;&#32476;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03994v1 Announce Type: cross  Abstract: Machine comprehension of visual information from images and videos by neural networks faces two primary challenges. Firstly, there exists a computational and inference gap in connecting vision and language, making it difficult to accurately determine which object a given agent acts on and represent it through language. Secondly, classifiers trained by a single, monolithic neural network often lack stability and generalization. To overcome these challenges, we introduce MoE-VRD, a novel approach to visual relationship detection utilizing a mixture of experts. MoE-VRD identifies language triplets in the form of &lt; subject, predicate, object&gt; tuples to extract relationships from visual processing. Leveraging recent advancements in visual relationship detection, MoE-VRD addresses the requirement for action recognition in establishing relationships between subjects (acting) and objects (being acted upon). In contrast to single monolithic net
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.03967</link><description>&lt;p&gt;
&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#23545;&#23545;&#25239;&#33030;&#24369;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29615;&#22659;-&#22266;&#26377;&#32500;&#24230;&#24046;&#24322;&#30340;&#27010;&#24565;&#65292;&#35770;&#25991;&#35777;&#26126;&#20102;&#32500;&#24230;&#24046;&#24322;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23384;&#22312;&#19988;&#23545;&#20154;&#31867;&#26469;&#35828;&#20960;&#20046;&#26080;&#27861;&#23519;&#35273;&#36825;&#19968;&#20107;&#23454;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#30456;&#24403;&#31070;&#31192;&#12290;&#25991;&#31456;&#24341;&#20837;&#20102;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#27010;&#24565;&#65306;&#33258;&#28982;&#25110;&#22312;&#27969;&#24418;&#19978;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#21487;&#20197;&#34987;&#20154;&#31867;/&#31070;&#35861;&#24863;&#30693;&#21040;&#30340;&#65307;&#38750;&#33258;&#28982;&#25110;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21017;&#26080;&#27861;&#34987;&#24863;&#30693;&#21040;&#12290;&#25991;&#31456;&#35748;&#20026;&#33073;&#31163;&#27969;&#24418;&#30340;&#25915;&#20987;&#23384;&#22312;&#26159;&#25968;&#25454;&#22266;&#26377;&#32500;&#24230;&#19982;&#29615;&#22659;&#32500;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;&#23545;&#20110;2&#23618;ReLU&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#32500;&#24230;&#24046;&#24322;&#19981;&#24433;&#21709;&#20174;&#35266;&#27979;&#25968;&#25454;&#31354;&#38388;&#20013;&#25277;&#21462;&#26679;&#26412;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23427;&#20173;&#20250;&#20351;&#24178;&#20928;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#31354;&#38388;&#33073;&#31163;&#27969;&#24418;&#26041;&#21521;&#30340;&#23545;&#25239;&#25200;&#21160;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25552;&#20379;&#20102;&#22312;/&#33073;&#31163;&#27969;&#24418;&#25915;&#20987;&#30340;$\ell_2,\ell_{\infty}$&#25915;&#20987;&#24378;&#24230;&#19982;&#32500;&#24230;&#24046;&#24322;&#20043;&#38388;&#26126;&#30830;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03967v1 Announce Type: new  Abstract: The existence of adversarial attacks on machine learning models imperceptible to a human is still quite a mystery from a theoretical perspective. In this work, we introduce two notions of adversarial attacks: natural or on-manifold attacks, which are perceptible by a human/oracle, and unnatural or off-manifold attacks, which are not. We argue that the existence of the off-manifold attacks is a natural consequence of the dimension gap between the intrinsic and ambient dimensions of the data. For 2-layer ReLU networks, we prove that even though the dimension gap does not affect generalization performance on samples drawn from the observed data space, it makes the clean-trained model more vulnerable to adversarial perturbations in the off-manifold direction of the data space. Our main results provide an explicit relationship between the $\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the dimension gap.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#26080;&#27169;&#26495;&#36870;&#21512;&#25104;&#27169;&#22411;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;&#23613;&#31649;&#20854;&#22312;&#39044;&#27979;&#26032;&#21512;&#25104;&#35268;&#21017;&#30340;&#21069;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#36229;&#20986;&#20998;&#24067;&#21453;&#24212;&#20013;&#30340;&#31934;&#20934;&#21305;&#37197;&#20934;&#30830;&#29575;&#24456;&#20302;&#65292;&#19988;&#20854;&#39044;&#27979;&#30340;&#26032;&#39062;&#21453;&#24212;&#22823;&#37096;&#20998;&#37117;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.03960</link><description>&lt;p&gt;
&#35780;&#20272;&#26080;&#27169;&#26495;&#36870;&#21512;&#25104;&#27169;&#22411;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing the Extrapolation Capability of Template-Free Retrosynthesis Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03960
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#26080;&#27169;&#26495;&#36870;&#21512;&#25104;&#27169;&#22411;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;&#23613;&#31649;&#20854;&#22312;&#39044;&#27979;&#26032;&#21512;&#25104;&#35268;&#21017;&#30340;&#21069;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#36229;&#20986;&#20998;&#24067;&#21453;&#24212;&#20013;&#30340;&#31934;&#20934;&#21305;&#37197;&#20934;&#30830;&#29575;&#24456;&#20302;&#65292;&#19988;&#20854;&#39044;&#27979;&#30340;&#26032;&#39062;&#21453;&#24212;&#22823;&#37096;&#20998;&#37117;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26080;&#27169;&#26495;&#27169;&#22411;&#22312;&#25506;&#32034;&#36870;&#21512;&#25104;&#39044;&#27979;&#20013;&#26410;&#35265;&#21453;&#24212;&#31354;&#38388;&#26041;&#38754;&#20855;&#26377;&#20844;&#35748;&#30340;&#33021;&#21147;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#27169;&#26495;&#30340;&#27169;&#22411;&#65292;&#23427;&#20204;&#36229;&#36234;&#24050;&#24314;&#31435;&#36793;&#30028;&#30340;&#33021;&#21147;&#20173;&#30456;&#23545;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#32452;&#35013;&#22823;&#37327;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21453;&#24212;&#65292;&#20174;&#32463;&#39564;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#26080;&#27169;&#26495;&#27169;&#22411;&#22312;&#39044;&#27979;&#20855;&#26377;&#26032;&#21512;&#25104;&#35268;&#21017;&#30340;&#21069;&#20307;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;OOD&#21453;&#24212;&#20013;&#30340;&#21069;&#21313;&#20010;&#31934;&#20934;&#21305;&#37197;&#20934;&#30830;&#29575;&#20196;&#20154;&#24778;&#35766;&#22320;&#19981;&#36275;&#65288;&lt;1%&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#21453;&#24212;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#19968;&#20010;&#21453;&#22797;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#27169;&#26495;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#21322;&#20197;&#19978;&#26032;&#39062;&#21453;&#24212;&#22312;&#21270;&#23398;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20513;&#23548;&#26410;&#26469;&#24320;&#21457;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03960v1 Announce Type: cross  Abstract: Despite the acknowledged capability of template-free models in exploring unseen reaction spaces compared to template-based models for retrosynthesis prediction, their ability to venture beyond established boundaries remains relatively uncharted. In this study, we empirically assess the extrapolation capability of state-of-the-art template-free models by meticulously assembling an extensive set of out-of-distribution (OOD) reactions. Our findings demonstrate that while template-free models exhibit potential in predicting precursors with novel synthesis rules, their top-10 exact-match accuracy in OOD reactions is strikingly modest (&lt; 1%). Furthermore, despite the capability of generating novel reactions, our investigation highlights a recurring issue where more than half of the novel reactions predicted by template-free models are chemically implausible. Consequently, we advocate for the future development of template-free models that in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;</title><link>https://arxiv.org/abs/2403.03905</link><description>&lt;p&gt;
&#40657;&#30418;$k$-to-$1$-PCA&#38477;&#32500;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03905
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#20110;$k$-PCA&#31639;&#27861;&#20013;&#36817;&#20284;&#21442;&#25968;&#36864;&#21270;&#30340;&#36793;&#30028;&#24471;&#21040;&#20102;&#26174;&#33879;&#26356;&#20026;&#31934;&#30830;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$k$-&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;$k$-PCA&#65289;&#38382;&#39064;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#31639;&#27861;&#21407;&#35821;&#65292;&#22312;&#25968;&#25454;&#20998;&#26512;&#21644;&#38477;&#32500;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#32479;&#35745;&#29615;&#22659;&#20013;&#65292;$k$-PCA&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#19968;&#20010;&#20998;&#24067;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39030;&#37096;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#38544;&#24335;&#35775;&#38382;&#36825;&#20010;&#30697;&#38453;&#12290;&#21463;&#36825;&#20123;&#38544;&#24335;&#35774;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#40657;&#30418;&#32553;&#20943;&#26041;&#27861;&#20316;&#20026;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#36890;&#36807;&#40657;&#30418;$1$-PCA&#39044;&#35328;&#27169;&#25311;&#23545;&#26410;&#30693;&#30446;&#26631;&#30697;&#38453;&#30340;&#35775;&#38382;&#65292;&#35813;&#39044;&#35328;&#36820;&#22238;&#19968;&#20010;&#36817;&#20284;&#30340;&#39030;&#37096;&#29305;&#24449;&#21521;&#37327;&#65292;&#26681;&#25454;&#20004;&#20010;&#27969;&#34892;&#30340;&#36817;&#20284;&#27010;&#24565;&#12290;&#23613;&#31649;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#21487;&#33021;&#26159;&#35774;&#35745;$k$-PCA&#31639;&#27861;&#20013;&#26368;&#33258;&#28982;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#36882;&#24402;&#35843;&#29992;$1$-PCA&#39044;&#35328;&#35843;&#29992;&#20102;$k$&#27425;&#65292;&#20197;&#21069;&#24456;&#38590;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03905v1 Announce Type: cross  Abstract: The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood.   Our main contribution is significantly sharper bounds on the approximation parameter degradation of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35270;&#20026;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28909;&#26680;&#29983;&#25104;&#30340;&#25193;&#25955;&#31354;&#38388;&#65292;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#20351;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.03669</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#22312;&#27969;&#24418;&#19978;&#30340;&#35889;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spectral Algorithms on Manifolds through Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35270;&#20026;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28909;&#26680;&#29983;&#25104;&#30340;&#25193;&#25955;&#31354;&#38388;&#65292;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#20351;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#24212;&#29992;&#30340;&#35889;&#31639;&#27861;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#33324;&#26680;&#20989;&#25968;&#19978;&#65292;&#32463;&#24120;&#24573;&#30053;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#30340;&#22266;&#26377;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#20027;&#24352;&#36755;&#20837;&#25968;&#25454;&#20301;&#20110;&#19968;&#20010;&#23884;&#20837;&#21040;&#26356;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20869;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RKHS&#20013;&#35889;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#30001;&#28909;&#26680;&#29983;&#25104;&#30340;&#65292;&#34987;&#31216;&#20026;&#25193;&#25955;&#31354;&#38388;&#30340;&#31354;&#38388;&#12290;&#36890;&#36807;&#32467;&#21512;&#36755;&#20837;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#25105;&#20204;&#37319;&#29992;&#31215;&#20998;&#31639;&#23376;&#25216;&#26415;&#25512;&#23548;&#20102;&#20851;&#20110;&#24191;&#20041;&#33539;&#25968;&#30340;&#32039;&#25910;&#25947;&#19978;&#30028;&#65292;&#36825;&#34920;&#26126;&#20272;&#35745;&#22120;&#22312;&#24378;&#24847;&#20041;&#19979;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#65292;&#24847;&#21619;&#30528;&#20989;&#25968;&#26412;&#36523;&#21450;&#20854;&#23548;&#25968;&#21516;&#26102;&#25910;&#25947;&#12290;&#36825;&#20123;&#30028;&#25552;&#20379;&#20102;&#20004;&#20010;&#37325;&#35201;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#26159;&#23436;&#20840;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03669v1 Announce Type: cross  Abstract: The existing research on spectral algorithms, applied within a Reproducing Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions, often neglecting the inherent structure of the input feature space. Our paper introduces a new perspective, asserting that input data are situated within a low-dimensional manifold embedded in a higher-dimensional Euclidean space. We study the convergence performance of spectral algorithms in the RKHSs, specifically those generated by the heat kernels, known as diffusion spaces. Incorporating the manifold structure of the input, we employ integral operator techniques to derive tight convergence upper bounds concerning generalized norms, which indicates that the estimators converge to the target function in strong sense, entailing the simultaneous convergence of the function itself and its derivatives. These bounds offer two significant advantages: firstly, they are exclusively contin
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.03643</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03643
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#36825;&#19968;&#38382;&#39064;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#30340;&#25361;&#25112;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#24037;&#19994;&#21644;&#26085;&#24120;&#29983;&#27963;&#31561;&#21508;&#20010;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#12290;&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#20197;&#21450;&#23545;&#23454;&#26102;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20256;&#32479;&#31639;&#27861;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#21387;&#21147;&#65292;&#38590;&#20197;&#23454;&#29616;&#26368;&#20339;&#25928;&#29575;&#21644;&#23454;&#26102;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#35745;&#31639;&#26426;&#35745;&#31639;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#20123;&#36827;&#23637;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#35299;&#20915;&#26041;&#27861;&#25910;&#25947;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#31561;&#20248;&#21183;&#65292;&#20026;&#35299;&#20915;&#31354;&#38388;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03643v1 Announce Type: cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03542</link><description>&lt;p&gt;
DPOT: &#33258;&#22238;&#24402;&#21435;&#22122;&#36816;&#31639;&#22120;&#21464;&#25442;&#22120;&#29992;&#20110;&#22823;&#35268;&#27169;PDE&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#36731;&#26494;&#25193;&#23637;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#24050;&#32463;&#34987;&#30740;&#31350;&#29992;&#26469;&#25552;&#39640;&#22312;&#25968;&#25454;&#31232;&#32570;&#29615;&#22659;&#20013;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#22914;&#38271;&#36712;&#36857;&#12289;&#22810;&#20010;&#23610;&#24230;&#21644;&#19981;&#21516;&#32500;&#24230;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#21435;&#22122;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#31283;&#23450;&#12289;&#26356;&#39640;&#25928;&#22320;&#22312;PDE&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#27867;&#21270;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22522;&#20110;&#20613;&#37324;&#21494;&#27880;&#24847;&#21147;&#30340;&#28789;&#27963;&#21487;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#27169;&#22411;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;10+&#20010;PDE&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20855;&#26377;&#36229;&#36807;0.5B&#21442;&#25968;&#30340;PDE&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;&#36229;&#36807;100k&#36712;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;SOTA&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03542v1 Announce Type: new  Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performanc
&lt;/p&gt;</description></item><item><title>CoRMF&#26159;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21464;&#20998;&#22343;&#22330;&#21644; RNN &#20043;&#38388;&#30340;&#32479;&#19968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2403.03391</link><description>&lt;p&gt;
CoRMF: &#20020;&#30028;&#26377;&#24207;&#24490;&#29615;&#22343;&#22330;&#20234;&#36763;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03391
&lt;/p&gt;
&lt;p&gt;
CoRMF&#26159;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21464;&#20998;&#22343;&#22330;&#21644; RNN &#20043;&#38388;&#30340;&#32479;&#19968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;&#30340;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RNN&#30340;&#39640;&#25928;&#20234;&#36763;&#27169;&#22411;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Criticality-ordered Recurrent Mean Field (CoRMF)&#65292;&#29992;&#20110;&#21069;&#21521;&#20234;&#36763;&#38382;&#39064;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#36890;&#36807;&#36138;&#23146;&#31639;&#27861;&#23545;N&#20010;&#33258;&#26059;&#30340;&#20234;&#36763;&#27169;&#22411;&#36827;&#34892;&#20102;&#20851;&#38190;&#26377;&#24207;&#33258;&#26059;&#24207;&#21015;&#30340;&#24341;&#20837;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#33258;&#22238;&#24402;&#22343;&#22330;&#22240;&#23376;&#20998;&#35299;&#65292;&#24182;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#65306;(i)&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#20234;&#36763;&#22270;&#30340;&#36817;&#20284;&#26641;&#32467;&#26500;&#65292;&#26032;&#33719;&#24471;&#30340;&#20851;&#38190;&#24615;&#39034;&#24207;&#20351;&#21464;&#20998;&#22343;&#22330;&#21644;RNN&#20043;&#38388;&#24471;&#20197;&#32479;&#19968;&#65292;&#20174;&#32780;&#20801;&#35768;&#26377;&#25928;&#22320;&#21033;&#29992;&#27010;&#29575;&#25512;&#26029;&#26469;&#25506;&#31350;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#30340;&#20234;&#36763;&#27169;&#22411;;(ii)&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#27169;&#22359;&#21270;&#12289;&#29420;&#31435;&#20110;&#27169;&#22411;&#32780;&#21448;&#36275;&#22815;&#34920;&#36798;&#33021;&#21147;&#65292;&#22240;&#27492;&#21487;&#20197;&#23436;&#20840;&#36866;&#29992;&#20110;&#20219;&#20309;&#21069;&#21521;&#20234;&#36763;&#25512;&#29702;&#38382;&#39064;&#65292;&#32780;&#19988;&#24037;&#20316;&#37327;&#26497;&#23567;&#12290;&#35745;&#31639;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26041;&#24046;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03391v1 Announce Type: cross  Abstract: We propose an RNN-based efficient Ising model solver, the Criticality-ordered Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a criticality-ordered spin sequence of an $N$-spin Ising model is introduced by sorting mission-critical edges with greedy algorithm, such that an autoregressive mean-field factorization can be utilized and optimized with Recurrent Neural Networks (RNNs). Our method has two notable characteristics: (i) by leveraging the approximated tree structure of the underlying Ising graph, the newly-obtained criticality order enables the unification between variational mean-field and RNN, allowing the generally intractable Ising model to be efficiently probed with probabilistic inference; (ii) it is well-modulized, model-independent while at the same time expressive enough, and hence fully applicable to any forward Ising inference problems with minimal effort. Computationally, by using a variance-redu
&lt;/p&gt;</description></item><item><title>&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;&#26159;&#26412;&#35770;&#12098;&#30340;&#12032;&#39033;&#37325;&#35201;&#21019;&#26032;&#65292;&#31616;&#21270;&#20102;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#21160;&#24577;&#12175;&#20026;&#39044;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03223</link><description>&lt;p&gt;
&#22312;&#39034;&#24207;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exact Enforcement of Temporal Continuity in Sequential Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03223
&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25191;&#12175;&#26102;&#38388;&#36830;&#32493;&#24615;&#26159;&#26412;&#35770;&#12098;&#30340;&#12032;&#39033;&#37325;&#35201;&#21019;&#26032;&#65292;&#31616;&#21270;&#20102;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#21160;&#24577;&#12175;&#20026;&#39044;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#12132;&#20195;&#34920;&#20102;&#12079;&#31243;&#38382;&#39064;&#35299;&#20915;&#12101;&#27861;&#30340;&#28508;&#22312;&#33539;&#24335;&#36716;&#21464;&#12290;&#26368;&#26174;&#33879;&#30340;&#21457;&#23637;&#20043;&#12032;&#26159;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#12153;&#32476;&#65288;PINNs&#65289;&#65292;&#20854;&#20013;&#31070;&#32463;&#12153;&#32476;&#34987;&#35757;&#32451;&#20197;&#28385;&#12188;&#20559;&#24494;&#20998;&#12101;&#31243;&#65288;PDEs&#65289;&#21644;/&#25110;&#35266;&#23519;&#25968;&#25454;&#12290;&#23613;&#31649;&#27492;&#12101;&#27861;&#26377;&#24076;&#26395;&#65292;&#20294;&#26631;&#20934;&#29256;&#26412;&#24050;&#34987;&#35777;&#26126;&#22312;&#20934;&#30830;&#39044;&#27979;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#30340;&#21160;&#24577;&#12175;&#20026;&#12101;&#12207;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#12032;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#12101;&#27861;&#23558;&#26102;&#38388;&#22495;&#20998;&#35299;&#20026;&#22810;&#20010;&#27573;&#65292;&#27599;&#20010;&#27573;&#20013;&#20351;&#12132;&#12032;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#12153;&#32476;&#65292;&#24182;&#30452;&#25509;&#22312;&#26368;&#12073;&#21270;&#38382;&#39064;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#23558;&#23427;&#20204;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#32435;&#12042;&#20854;&#20013;&#12290;&#22312;&#26412;&#12079;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#12042;&#12032;&#31181;&#36890;&#36807;&#35299;&#26512;&#35299;&#31934;&#30830;&#24378;&#21046;&#23454;&#29616;&#36830;&#32493;&#24615;&#30340;&#12101;&#27861;&#12290;&#36825;&#31181;&#12101;&#27861;&#31616;&#21333;&#26131;&#12132;&#65292;&#33021;&#22815;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03223v1 Announce Type: new  Abstract: The use of deep learning methods in scientific computing represents a potential paradigm shift in engineering problem solving. One of the most prominent developments is Physics-Informed Neural Networks (PINNs), in which neural networks are trained to satisfy partial differential equations (PDEs) and/or observed data. While this method shows promise, the standard version has been shown to struggle in accurately predicting the dynamic behavior of time-dependent problems. To address this challenge, methods have been proposed that decompose the time domain into multiple segments, employing a distinct neural network in each segment and directly incorporating continuity between them in the loss function of the minimization problem. In this work we introduce a method to exactly enforce continuity between successive time segments via a solution ansatz. This hard constrained sequential PINN (HCS-PINN) method is simple to implement and eliminates 
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23558;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#30340;&#29702;&#35770;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#25152;&#38656;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2403.02741</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#36793;&#20449;&#24687;&#30340;&#29366;&#24577;&#32422;&#26463;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
State-Constrained Zero-Sum Differential Games with One-Sided Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02741
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#30340;&#29702;&#35770;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#25152;&#38656;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#21644;&#21333;&#36793;&#20449;&#24687;&#30340;&#38646;&#21644;&#24494;&#20998;&#21338;&#24328;&#65292;&#20854;&#20013;&#30693;&#24773;&#29609;&#23478;&#65288;&#29609;&#23478;1&#65289;&#20855;&#26377;&#23545;&#20110;&#26410;&#30693;&#20110;&#19981;&#30693;&#24773;&#29609;&#23478;&#65288;&#29609;&#23478;2&#65289;&#30340;&#20998;&#31867;&#25903;&#20184;&#31867;&#22411;&#12290;&#29609;&#23478;1&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#36829;&#21453;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20182;&#30340;&#25903;&#20184;&#65292;&#32780;&#29609;&#23478;2&#30340;&#30446;&#26631;&#26159;&#35201;&#20040;&#36829;&#21453;&#29366;&#24577;&#32422;&#26463;&#65292;&#35201;&#20040;&#26368;&#22823;&#21270;&#25903;&#20184;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#26159;&#23558;Cardaliaguet&#65288;2007&#65289;&#23545;&#20110;&#27809;&#26377;&#29366;&#24577;&#32422;&#26463;&#30340;&#31867;&#20284;&#21338;&#24328;&#20215;&#20540;&#23384;&#22312;&#24615;&#21644;&#23545;&#20110;&#29609;&#23478;&#30340;&#20849;&#21516;&#20449;&#20208;&#20984;&#24615;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#20855;&#26377;&#29366;&#24577;&#32422;&#26463;&#30340;&#24494;&#20998;&#21338;&#24328;&#65292;&#24182;&#25512;&#23548;&#20102;&#29992;&#20110;&#35745;&#31639;&#34892;&#20026;&#31574;&#30053;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#23376;&#21160;&#24577;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02741v1 Announce Type: cross  Abstract: We study zero-sum differential games with state constraints and one-sided information, where the informed player (Player 1) has a categorical payoff type unknown to the uninformed player (Player 2). The goal of Player 1 is to minimize his payoff without violating the constraints, while that of Player 2 is to either violate the state constraints, or otherwise, to maximize the payoff. One example of the game is a man-to-man matchup in football. Without state constraints, Cardaliaguet (2007) showed that the value of such a game exists and is convex to the common belief of players. Our theoretical contribution is an extension of this result to differential games with state constraints and the derivation of the primal and dual subdynamic principles necessary for computing the behavioral strategies. Compared with existing works on imperfect-information dynamic games that focus on scalability and generalization, our focus is instead on reveal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02639</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
False Positive Sampling-based Data Augmentation for Enhanced 3D Object Detection Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#65292;&#20197;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#21319;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#19978;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#26377;&#38480;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#26159;&#20854;&#20542;&#21521;&#22686;&#21152;&#34394;&#20551;&#38451;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20811;&#26381;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#37319;&#26679;&#30340;&#23616;&#38480;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21517;&#20026;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#26032;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#28041;&#21450;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#22312;&#27169;&#22411;&#39044;&#27979;&#20013;&#34987;&#35782;&#21035;&#20026;&#34394;&#20551;&#38451;&#24615;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21516;&#26102;&#21033;&#29992;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#21644;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#24314;&#31435;&#34394;&#20551;&#38451;&#24615;&#26679;&#26412;&#25968;&#25454;&#24211;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30001;&#20110;&#34394;&#20551;&#38451;&#24615;&#37319;&#26679;&#23548;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02639v1 Announce Type: cross  Abstract: Recent studies have focused on enhancing the performance of 3D object detection models. Among various approaches, ground-truth sampling has been proposed as an augmentation technique to address the challenges posed by limited ground-truth data. However, an inherent issue with ground-truth sampling is its tendency to increase false positives. Therefore, this study aims to overcome the limitations of ground-truth sampling and improve the performance of 3D object detection models by developing a new augmentation technique called false-positive sampling. False-positive sampling involves retraining the model using point clouds that are identified as false positives in the model's predictions. We propose an algorithm that utilizes both ground-truth and false-positive sampling and an algorithm for building the false-positive sample database. Additionally, we analyze the principles behind the performance enhancement due to false-positive sampl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#21033;&#29992;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#23398;&#20064;&#65292;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01112</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01112
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#21033;&#29992;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#23398;&#20064;&#65292;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#33021;&#20307;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#27604;&#22914;&#20987;&#36133;&#25932;&#20154;&#25110;&#36827;&#29699;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#23398;&#20064;&#26102;&#38388;&#65292;&#36890;&#24120;&#20250;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#38543;&#21518;&#26410;&#33021;&#21457;&#29616;&#36798;&#25104;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;MARL&#30340;&#39640;&#25928;&#24773;&#33410;&#35760;&#24518;&#21033;&#29992;&#65288;EMU&#65289;&#65292;&#20854;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#65306;&#65288;a&#65289;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#24773;&#33410;&#32531;&#20914;&#21306;&#30340;&#35821;&#20041;&#19968;&#33268;&#20869;&#23384;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#21450;&#65288;b&#65289;&#26377;&#36873;&#25321;&#22320;&#20419;&#36827;&#29702;&#24819;&#30340;&#36716;&#25442;&#20197;&#38450;&#27490;&#23616;&#37096;&#25910;&#25947;&#12290;&#20026;&#23454;&#29616;&#65288;a&#65289;&#65292;EMU&#22312;MARL&#26049;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#21019;&#24314;&#20102;&#26377;&#21161;&#20110;&#25506;&#32034;&#24615;&#20869;&#23384;&#22238;&#24518;&#30340;&#36830;&#36143;&#35760;&#24518;&#23884;&#20837;&#12290;&#20026;&#23454;&#29616;&#65288;b&#65289;&#65292;EMU&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#24895;&#26395;&#24615;&#30340;&#26032;&#39062;&#22870;&#21169;&#32467;&#26500;&#65292;&#31216;&#20026;&#24773;&#33410;&#28608;&#21169;&#12290;&#36825;&#31181;&#22870;&#21169;&#25913;&#21892;&#20102;TD
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01081</link><description>&lt;p&gt;
LAB&#65306;&#38024;&#23545;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
LAB: Large-Scale Alignment for ChatBots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01081
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAB&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#36890;&#36807;&#20998;&#31867;&#27861;&#25351;&#23548;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#26114;&#36149;&#20154;&#24037;&#26631;&#27880;&#21644;GPT-4&#31561;&#19987;&#26377;&#27169;&#22411;&#20381;&#36182;&#36739;&#23569;&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;LLM&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;LAB&#65288;ChatBots&#30340;&#22823;&#35268;&#27169;&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#20013;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#22810;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#65292;LAB&#26174;&#33879;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#31867;&#27880;&#37322;&#21644;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#19987;&#26377;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LAB&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#21487;&#20197;&#19982;&#20351;&#29992;&#20256;&#32479;&#20154;&#31867;&#27880;&#37322;&#25110;GPT-4&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#21644;&#25351;&#20196;&#36981;&#24490;&#34892;&#20026;&#65292;&#26631;&#24535;&#30528;&#22312;&#39640;&#25928;&#35757;&#32451;&#21508;&#31181;&#24212;&#29992;&#30340;LLM&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00877</link><description>&lt;p&gt;
Disaggregated Multi-Tower: &#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#25512;&#33616;&#24314;&#27169;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00877
&lt;/p&gt;
&lt;p&gt;
Disaggregated Multi-Tower&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;SPTT&#12289;TM&#21644;TP&#19977;&#20010;&#32452;&#20214;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#65292;&#21152;&#36895;&#24615;&#33021;&#25552;&#21319;&#20102;1.9&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#30340;&#25153;&#24179;&#26550;&#26500;&#12289;&#24120;&#35265;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#27169;&#24335;&#21644;&#20998;&#23618;&#25968;&#25454;&#20013;&#24515;&#25299;&#25169;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#30456;&#20851;&#30340;&#20302;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disaggregated Multi-Tower&#65288;DMT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24314;&#27169;&#25216;&#26415;&#65292;&#21253;&#25324;&#65288;1&#65289;&#35821;&#20041;&#20445;&#30041;&#30340;Tower Transform&#65288;SPTT&#65289;&#65292;&#19968;&#20010;&#23558;&#21333;&#29255;&#20840;&#23616;&#23884;&#20837;&#26597;&#25214;&#36807;&#31243;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#22612;&#20197;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#20301;&#32622;&#20851;&#31995;&#30340;&#26032;&#22411;&#35757;&#32451;&#27169;&#24335;&#65307;&#65288;2&#65289;Tower Module&#65288;TM&#65289;&#65292;&#19968;&#20010;&#38468;&#21152;&#21040;&#27599;&#20010;&#22612;&#30340;&#21327;&#21516;&#31264;&#23494;&#32452;&#20214;&#65292;&#36890;&#36807;&#20998;&#23618;&#29305;&#24449;&#20132;&#20114;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36890;&#20449;&#37327;&#65307;&#21644;&#65288;3&#65289;Tower Partitioner&#65288;TP&#65289;&#65292;&#19968;&#20010;&#29305;&#24449;&#20998;&#21306;&#22120;&#65292;&#31995;&#32479;&#22320;&#21019;&#24314;&#20855;&#26377;&#26377;&#24847;&#20041;&#29305;&#24449;&#20132;&#20114;&#21644;&#36127;&#36733;&#24179;&#34913;&#20998;&#37197;&#30340;&#22612;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#23884;&#20837;&#26469;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DMT&#30456;&#27604;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;1.9&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.19212</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning: A Convex Optimization Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#27599;&#38598;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;$Q$-&#20989;&#25968;&#65292;&#30830;&#20445;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#38598;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#27599;&#20010;&#38598;&#20013;&#20351;&#29992;&#20984;&#20248;&#21270;&#26469;&#25214;&#21040;&#26368;&#20248;$Q$-&#20989;&#25968;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#12290;&#20984;&#20248;&#21270;&#26041;&#27861;&#30830;&#20445;&#27599;&#20010;&#38598;&#21512;&#20013;&#35745;&#31639;&#30340;&#26435;&#37325;&#26159;&#26368;&#20248;&#30340;&#65292;&#20851;&#20110;&#24403;&#21069;&#38598;&#21512;&#30340;&#37319;&#26679;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#23545;&#20110;&#31283;&#23450;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#65292;&#24182;&#19988;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25910;&#25947;&#21442;&#25968;&#21487;&#20197;&#19982;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26080;&#38480;&#25509;&#36817;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#27491;&#21017;&#21270;&#21442;&#25968;&#20026;$\rho$&#65292;&#26102;&#38388;&#38271;&#24230;&#20026;$T$&#65292;&#37027;&#20040;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#25910;&#25947;&#21040;$w$&#65292;&#20854;&#20013;$w$&#19982;&#26368;&#20248;&#21442;&#25968;$w^\star$&#20043;&#38388;&#30340;&#36317;&#31163;&#21463;&#21040;$\mathcal{O}(\rho T^{-1})$&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19212v1 Announce Type: cross  Abstract: In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\star$ is bounded by $\mathcal{O}(\rho T^{-1})$. That is, when the nu
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18064</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#27979;&#35797;&#31354;&#38388;&#30456;&#20851;&#29615;&#22659;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18064
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#35780;&#20272;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#35268;&#21010;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#37319;&#26679;&#23545;&#25143;&#22806;&#20449;&#24687;&#25910;&#38598;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#39640;&#26114;&#30340;&#37319;&#26679;&#25104;&#26412;&#65292;&#22914;&#26102;&#38388;&#12289;&#33021;&#37327;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#29615;&#22659;&#30772;&#22351;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#20808;&#39564;&#25968;&#25454;&#21487;&#20197;&#26159;&#25552;&#39640;&#25928;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#19982;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36890;&#24120;&#20107;&#20808;&#26410;&#30693;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21033;&#29992;&#27492;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#35268;&#21010;&#25928;&#29575;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#21644;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#20989;&#25968;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#32452;&#21512;&#65292;&#23427;&#21487;&#20197;&#30740;&#31350;&#20551;&#35774;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#31354;&#38388;&#65292;&#24182;&#21363;&#26102;&#35780;&#20272;&#36825;&#20123;&#20551;&#35774;&#65292;&#20351;&#27492;&#26032;&#30693;&#35782;&#33021;&#22815;&#31435;&#21363;&#20026;&#26410;&#26469;&#35745;&#21010;&#25152;&#21033;&#29992;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18064v1 Announce Type: cross  Abstract: The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;</title><link>https://arxiv.org/abs/2402.17002</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21547;&#27491;&#20132;&#20559;&#32622;&#21457;&#29616;&#23545;&#31216;&#32676;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Symmetry Group Structures via Implicit Orthogonality Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17002
&lt;/p&gt;
&lt;p&gt;
HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HyperCube&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#20013;&#23545;&#31216;&#32676;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21521;&#23398;&#20064;&#27491;&#20132;&#34920;&#31034;&#28748;&#36755;&#20102;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#21033;&#29992;&#20102;&#34920;&#31034;&#29702;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#23450;&#29702;&#65292;&#21363;&#25152;&#26377;&#32039;&#33268;/&#26377;&#38480;&#32676;&#37117;&#21487;&#20197;&#30001;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#12290;HyperCube&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#32676;&#25805;&#20316;&#65292;&#25104;&#21151;&#24674;&#22797;&#23436;&#25972;&#30340;&#25805;&#20316;&#34920;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#23398;&#20064;&#20986;&#30340;&#22240;&#32032;&#30452;&#25509;&#23545;&#24212;&#20110;&#24213;&#23618;&#32676;&#30340;&#31934;&#30830;&#30697;&#38453;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22240;&#32032;&#25429;&#25417;&#21040;&#20102;&#32676;&#30340;&#23436;&#25972;&#19981;&#21487;&#32422;&#34920;&#31034;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#25191;&#34892;&#32676;&#21367;&#31215;&#30340;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#12290;&#22312;&#23545;&#32676;&#21644;&#38750;&#32676;&#31526;&#21495;&#25805;&#20316;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;HyperCube&#23637;&#31034;&#20102;10
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#26408;&#39532;&#31614;&#21517;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26408;&#39532;&#31614;&#21517;&#26080;&#27861;&#25512;&#24191;&#21040;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.16896</link><description>&lt;p&gt;
&#20851;&#20110;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26408;&#39532;&#31614;&#21517;
&lt;/p&gt;
&lt;p&gt;
On Trojan Signatures in Large Language Models of Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#26408;&#39532;&#31614;&#21517;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#26408;&#39532;&#31614;&#21517;&#26080;&#27861;&#25512;&#24191;&#21040;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26408;&#39532;&#31614;&#21517;&#26159;&#30001;Fields&#31561;&#20154;(2021)&#25551;&#36848;&#30340;&#65292;&#26159;&#34987;&#24863;&#26579;&#31867;&#21035;&#21442;&#25968;&#65288;&#26435;&#37325;&#65289;&#19982;&#26410;&#34987;&#24863;&#26579;&#31867;&#21035;&#21442;&#25968;&#20043;&#38388;&#20998;&#24067;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#34987;&#24863;&#26579;&#30340;&#27169;&#22411;&#12290;Fields&#31561;&#20154;(2021)&#21457;&#29616;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26408;&#39532;&#31614;&#21517;&#65292;&#27604;&#22914;Resnet&#12289;WideResnet&#12289;Densenet&#21644;VGG&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28304;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#31867;&#23618;&#21442;&#25968;&#30340;&#36825;&#31181;&#31614;&#21517;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26408;&#39532;&#31614;&#21517;&#19981;&#33021;&#27867;&#21270;&#21040;&#20195;&#30721;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#26356;&#26126;&#30830;&#30340;&#35774;&#32622;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#20013;&#27602;&#65288;&#29992;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20923;&#32467;&#36827;&#34892;&#24494;&#35843;&#65289;&#65292;&#34987;&#24863;&#26579;&#30340;&#20195;&#30721;&#27169;&#22411;&#20063;&#20173;&#28982;&#22266;&#25191;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20108;&#36827;&#21046;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#20061;&#20010;&#24863;&#26579;&#27169;&#22411;&#30340;&#20998;&#26512;&#65306;&#20811;&#38534;&#21644;&#32570;&#38519;&#26816;&#27979;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26816;&#26597;&#22522;&#20110;&#26435;&#37325;&#30340;&#26408;&#39532;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16896v1 Announce Type: cross  Abstract: Trojan signatures, as described by Fields et al. (2021), are noticeable differences in the distribution of the trojaned class parameters (weights) and the non-trojaned class parameters of the trojaned model, that can be used to detect the trojaned model. Fields et al. (2021) found trojan signatures in computer vision classification tasks with image models, such as, Resnet, WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in the classifier layer parameters of large language models of source code.   Our results suggest that trojan signatures could not generalize to LLMs of code. We found that trojaned code models are stubborn, even when the models were poisoned under more explicit settings (finetuned with pre-trained weights frozen). We analyzed nine trojaned models for two binary classification tasks: clone and defect detection. To the best of our knowledge, this is the first work to examine weight-based troj
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.16374</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#30740;&#31350;&#65306;&#39046;&#22495;&#33258;&#36866;&#24212;&#12289;&#22806;&#37096;&#20998;&#24067;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#19988;&#22240;&#20854;&#22312;&#24314;&#27169;&#30001;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#30340;&#22797;&#26434;&#25968;&#25454;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21040;&#25512;&#33616;&#31995;&#32479;&#12290;&#29616;&#23454;&#20013;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#65292;&#33410;&#28857;&#23646;&#24615;&#21644;&#36793;&#32467;&#26500;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#22270;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#21463;&#21040;&#20998;&#24067;&#36716;&#31227;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#36716;&#31227;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#22270;&#23398;&#20064;&#26041;&#27861;&#22312;&#38477;&#20302;&#27867;&#21270;&#21644;&#36866;&#24212;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32473;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#35299;&#20915;&#22270;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#30340;&#26368;&#26032;&#26041;&#27861;&#12289;&#31574;&#30053;&#21644;&#35265;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16374v1 Announce Type: new  Abstract: Graph learning plays a pivotal role and has gained significant attention in various application scenarios, from social network analysis to recommendation systems, for its effectiveness in modeling complex data relations represented by graph structural data. In reality, the real-world graph data typically show dynamics over time, with changing node attributes and edge structure, leading to the severe graph data distribution shift issue. This issue is compounded by the diverse and complex nature of distribution shifts, which can significantly impact the performance of graph learning methods in degraded generalization and adaptation capabilities, posing a substantial challenge to their effectiveness. In this survey, we provide a comprehensive review and summary of the latest approaches, strategies, and insights that address distribution shifts within the context of graph learning. Concretely, according to the observability of distributions 
&lt;/p&gt;</description></item><item><title>&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14860</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22522;&#20934;&#23454;&#20917;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking Large Language Models without Ground Truth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14860
&lt;/p&gt;
&lt;p&gt;
&#19981;&#38656;&#35201;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#26469;&#25490;&#21517;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25490;&#21517;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#24433;&#21709;&#21147;&#30340;&#22686;&#24378;&#65292;&#35780;&#20272;&#21644;&#25490;&#21517;LLMs&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#20154;&#31867;&#21709;&#24212;&#65292;&#35201;&#20040;&#20351;&#29992;LLMs&#25104;&#23545;&#22320;&#20114;&#30456;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#32473;&#23450;&#19968;&#32452;&#25552;&#31034;&#25968;&#25454;&#38598;&#65288;&#27604;&#22914;&#38382;&#39064;&#12289;&#35828;&#26126;&#31561;&#65289;&#21644;&#19968;&#32452;LLMs&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#22522;&#20934;&#23454;&#20917;&#25110;&#21442;&#32771;&#21709;&#24212;&#30340;&#24773;&#20917;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#12290;&#21463;&#21040;&#29616;&#23454;&#29983;&#27963;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#19987;&#23478;&#21644;&#26377;&#30693;&#35782;&#30340;&#20154;&#37117;&#33021;&#35782;&#21035;&#19968;&#20010;&#26032;&#25163;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#36335;&#26159;&#32771;&#34385;&#27169;&#22411;&#30340;&#19977;&#20803;&#32452;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#35780;&#20272;&#20854;&#20182;&#20004;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#27491;&#30830;&#35782;&#21035;&#26368;&#24046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#24182;&#25552;&#20379;&#20102;&#25104;&#21151;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#21453;&#22797;&#24212;&#29992;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23545;LLMs&#36827;&#34892;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14860v1 Announce Type: cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12694</link><description>&lt;p&gt;
&#22797;&#20852;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21487;&#23398;&#20064;&#20998;&#35299;&#19982;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12694
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35201;&#27714;&#31934;&#30830;&#24314;&#27169;&#38169;&#32508;&#22797;&#26434;&#27169;&#24335;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21160;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#36235;&#21183;&#29305;&#24449;&#24102;&#26469;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22522;&#26412;&#30340;&#31227;&#21160;&#24179;&#22343;&#26680;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#21644;&#22797;&#26434;&#36235;&#21183;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20998;&#35299;&#31574;&#30053;&#65292;&#26356;&#21512;&#29702;&#22320;&#25429;&#25417;&#21160;&#24577;&#36235;&#21183;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#19987;&#38376;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#36890;&#36807;&#36890;&#36947;&#33258;&#27880;&#24847;&#21147;&#21644;&#33258;&#22238;&#24402;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20843;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340; Leddam...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05956</link><description>&lt;p&gt;
Pathformer: &#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20174;&#26377;&#38480;&#25110;&#22266;&#23450;&#23610;&#24230;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#36328;&#22810;&#20010;&#23610;&#24230;&#30340;&#19981;&#21516;&#29305;&#24449;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#65288;Pathformer&#65289;&#30340;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#25972;&#21512;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#22810;&#23610;&#24230;&#21010;&#20998;&#36816;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22522;&#20110;&#27599;&#20010;&#23610;&#24230;&#30340;&#21010;&#20998;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#22359;&#36827;&#34892;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#24615;&#21644;&#23616;&#37096;&#32454;&#33410;&#20316;&#20026;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20016;&#23500;&#22810;&#23610;&#24230;Transformer&#65292;&#35813;&#36335;&#24452;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#21160;&#24577;&#35843;&#25972;&#22810;&#23610;&#24230;&#24314;&#27169;&#36807;&#31243;&#65292;&#25552;&#39640;Pathformer&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;11&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05443</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;Wasserstein&#28176;&#21464;&#27969;
&lt;/p&gt;
&lt;p&gt;
Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;Semi-dual JKO (S-JKO)&#65292;&#36890;&#36807;&#37319;&#29992;&#21322;&#23545;&#20598;&#24418;&#24335;&#30340;JKO&#27493;&#39588;&#65292;&#38477;&#20302;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;WGF&#19978;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#28176;&#21464;&#27969;&#65288;WGF&#65289;&#25551;&#36848;&#20102;Wasserstein&#31354;&#38388;&#20013;&#27010;&#29575;&#23494;&#24230;&#30340;&#26799;&#24230;&#21160;&#21147;&#23398;&#12290;WGF&#25552;&#20379;&#20102;&#22312;&#27010;&#29575;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#25968;&#20540;&#19978;&#36817;&#20284;&#36830;&#32493;&#30340;WGF&#38656;&#35201;&#26102;&#38388;&#31163;&#25955;&#21270;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#26159;JKO&#26041;&#26696;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;WGF&#27169;&#22411;&#37319;&#29992;JKO&#26041;&#26696;&#65292;&#24182;&#20026;&#27599;&#20010;JKO&#27493;&#39588;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#19982;JKO&#27493;&#39588;&#25968;&#37327;K&#25104;&#20108;&#27425;&#22686;&#38271;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;$O(K^2)$&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;WGF&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#31216;&#20026;&#21322;&#23545;&#20598;JKO&#65288;S-JKO&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;JKO&#27493;&#39588;&#30340;&#21322;&#23545;&#20598;&#24418;&#24335;&#65292;&#36890;&#36807;JKO&#27493;&#39588;&#19982;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#24471;&#21040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35757;&#32451;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;$O(K)$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;WGF&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.04830</link><description>&lt;p&gt;
&#32553;&#23567;SGP4&#21644;&#39640;&#31934;&#24230;&#20256;&#25773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#36890;&#36807;&#21487;&#24494;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21270;&#30340;&#31532;&#22235;&#32423;&#25668;&#21160;(SGP4)&#36712;&#36947;&#20256;&#25773;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#29699;&#36712;&#36947;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#25913;&#36827;&#65292;SGP&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25968;&#20540;&#20256;&#25773;&#22120;&#30340;&#31934;&#24230;&#65292;&#21518;&#32773;&#30340;&#35823;&#24046;&#26174;&#33879;&#36739;&#23567;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#26032;&#22411;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#20351;SGP4&#21487;&#24494;&#21270;&#65292;dSGP4&#20415;&#20110;&#36827;&#34892;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33322;&#22825;&#22120;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#36716;&#25442;&#12289;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#35745;&#31639;&#21644;&#21327;&#26041;&#24046;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;PyTorch&#23454;&#29616;&#20801;&#35768;&#22312;&#25209;&#37327;&#30340;TLE&#65288;&#20004;&#34892;&#21442;&#25968;&#65289;&#38598;&#19978;&#36827;&#34892;&#23604;&#23596;&#30340;&#24182;&#34892;&#36712;&#36947;&#20256;&#25773;&#65292;&#21033;&#29992;CPU&#12289;GPU&#21644;&#20998;&#24067;&#24335;&#39044;&#27979;&#21355;&#26143;&#20301;&#32622;&#30340;&#39640;&#32423;&#30828;&#20214;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;&#21487;&#24494;&#24615;&#20351;&#20854;&#33021;&#19982;&#27169;&#24335;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02746</link><description>&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#36275;&#20197;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02746
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20351;&#29992;&#26631;&#20934; Gaussian &#36807;&#31243;&#65288;GP&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#21363;&#26631;&#20934; BO&#65292;&#22312;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#31181;&#35266;&#24565;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110; Gaussian &#36807;&#31243;&#22312;&#21327;&#26041;&#24046;&#24314;&#27169;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#23545;&#39640;&#32500;&#36755;&#20837;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#20123;&#25285;&#24551;&#30475;&#36215;&#26469;&#21512;&#29702;&#65292;&#20294;&#32570;&#20047;&#25903;&#25345;&#36825;&#31181;&#35266;&#28857;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934; GP &#30340;&#34920;&#29616;&#22987;&#32456;&#20301;&#20110;&#26368;&#20339;&#33539;&#22260;&#20869;&#65292;&#24448;&#24448;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#29616;&#26377; BO &#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934; GP &#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#20989;&#25968;&#30340;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#24378;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#36827;&#34892; BO &#21487;&#20197;&#33719;&#24471;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.02686</link><description>&lt;p&gt;
&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65306;&#19968;&#31181;&#21457;&#29616;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#26041;&#21521;&#24615;&#36890;&#35759;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Region Markovian Gaussian Process: An Efficient Method to Discover Directional Communications Across Multiple Brain Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#30340;&#26041;&#21521;&#24615;&#36890;&#35759;&#12290;&#36890;&#36807;&#24314;&#31435;LDS&#19982;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#19981;&#21516;&#33041;&#21306;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#31070;&#32463;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#21508;&#31181;&#32479;&#35745;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#22810;&#20010;&#33041;&#21306;&#20043;&#38388;&#28508;&#22312;&#30340;&#36890;&#35759;&#12290;&#20004;&#20010;&#20027;&#35201;&#30340;&#31867;&#21035;&#26159;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21644;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#20102;&#28508;&#22312;&#21464;&#37327;&#65292;&#22914;&#39057;&#24102;&#21644;&#36890;&#35759;&#26041;&#21521;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;LDS&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#39640;&#65292;&#20294;&#22312;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#32570;&#20047;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#19982;&#22810;&#36755;&#20986;GP&#30456;&#23545;&#24212;&#30340;LDS&#65292;&#21363;&#22810;&#21306;&#22495;&#39532;&#23572;&#21487;&#22827;&#39640;&#26031;&#36807;&#31243;&#65288;MRM-GP&#65289;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#21512;&#20108;&#20026;&#19968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#24314;&#31435;&#20102;LDS&#21644;&#22810;&#36755;&#20986;GP&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#31070;&#32463;&#35760;&#24405;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26126;&#30830;&#24314;&#27169;&#20102;&#39057;&#29575;&#21644;&#30456;&#20301;&#24310;&#36831;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#22312;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#20102;&#32447;&#24615;&#25512;&#26029;&#25104;&#26412;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying the complex interactions between different brain regions is crucial in neuroscience. Various statistical methods have explored the latent communication across multiple brain regions. Two main categories are the Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique strengths. The GP-based approach effectively discovers latent variables such as frequency bands and communication directions. Conversely, the LDS-based approach is computationally efficient but lacks powerful expressiveness in latent representation. In this study, we merge both methodologies by creating an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian Process (MRM-GP). Our work is the first to establish a connection between an LDS and a multi-output GP that explicitly models frequencies and phase delays within the latent space of neural recordings. Consequently, the model achieves a linear inference cost over time points and provides an interpretable low-dimensional repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#21592;&#21387;&#21147;&#22320;&#22270;&#26469;&#25429;&#25417;&#36275;&#29699;&#27604;&#36187;&#20013;&#29699;&#25511;&#29699;&#38431;&#25152;&#32463;&#21382;&#30340;&#21387;&#21147;&#65292;&#20026;&#25945;&#32451;&#21644;&#20998;&#26512;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2401.16235</link><description>&lt;p&gt;
&#29699;&#21592;&#21387;&#21147;&#22320;&#22270;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#36275;&#29699;&#21387;&#21147;&#34920;&#24449;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27604;&#36187;&#29615;&#22659;&#19979;&#29699;&#21592;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Player Pressure Map -- A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#21592;&#21387;&#21147;&#22320;&#22270;&#26469;&#25429;&#25417;&#36275;&#29699;&#27604;&#36187;&#20013;&#29699;&#25511;&#29699;&#38431;&#25152;&#32463;&#21382;&#30340;&#21387;&#21147;&#65292;&#20026;&#25945;&#32451;&#21644;&#20998;&#26512;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36275;&#29699;&#20013;&#65292;&#19978;&#19979;&#25991;&#30340;&#29699;&#21592;&#34920;&#29616;&#24230;&#37327;&#23545;&#25945;&#32451;&#20204;&#38750;&#24120;&#23453;&#36149;&#12290;&#20363;&#22914;&#65292;&#33021;&#22815;&#22312;&#27604;&#36187;&#20013;&#25215;&#21463;&#21387;&#21147;&#30340;&#33021;&#21147;&#23558;&#31934;&#33521;&#21644;&#26222;&#36890;&#29699;&#21592;&#21306;&#20998;&#24320;&#26469;&#12290;&#27491;&#30830;&#30340;&#21387;&#21147;&#24230;&#37327;&#21487;&#20197;&#20351;&#29699;&#38431;&#20934;&#30830;&#35780;&#20272;&#29699;&#21592;&#22312;&#21387;&#21147;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#35774;&#35745;&#38024;&#23545;&#24615;&#30340;&#35757;&#32451;&#22330;&#26223;&#26469;&#24357;&#34917;&#20182;&#20204;&#30340;&#24369;&#28857;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#36861;&#36394;&#21644;&#20107;&#20214;&#25968;&#25454;&#20197;&#21450;&#27604;&#36187;&#24405;&#20687;&#26469;&#25429;&#25417;&#36275;&#29699;&#27604;&#36187;&#22330;&#26223;&#20013;&#29699;&#25511;&#29699;&#38431;&#25152;&#32463;&#21382;&#30340;&#21387;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29699;&#21592;&#21387;&#21147;&#22320;&#22270;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#27604;&#36187;&#22330;&#26223;&#65292;&#23427;&#38477;&#20302;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20173;&#21253;&#21547;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23427;&#19981;&#20165;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#35780;&#20272;&#29699;&#38431;&#21644;&#27599;&#20010;&#20010;&#20307;&#38754;&#20020;&#30340;&#21387;&#21147;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#35775;&#38382;&#29699;&#21592;&#34920;&#29616;&#30340;&#22522;&#30784;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#25945;&#32451;&#21644;&#20998;&#26512;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16235v2 Announce Type: replace  Abstract: In soccer, contextual player performance metrics are invaluable to coaches. For example, the ability to perform under pressure during matches distinguishes the elite from the average. Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses. The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene. We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information. Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance. Overall, our model provides coaches and analysts with a deeper u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#20248;&#21270;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.00230</link><description>&lt;p&gt;
Transformer&#22810;&#20803;&#39044;&#27979;&#65306;&#23569;&#21363;&#26159;&#22810;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformer Multivariate Forecasting: Less is More?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#36890;&#36807;&#24341;&#20837;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#20248;&#21270;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20803;&#39044;&#27979;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#33073;&#39062;&#32780;&#20986;&#65292;&#23637;&#29616;&#20986;&#22312;&#22788;&#29702;&#26469;&#33258;&#30495;&#23454;&#22330;&#26223;&#20013;&#26434;&#20081;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20197;&#20247;&#22810;&#21464;&#37327;&#21644;&#28459;&#38271;&#26102;&#38388;&#24207;&#21015;&#20026;&#29305;&#24449;&#65292;&#24102;&#26469;&#25361;&#25112;&#65292;&#21253;&#25324;&#22686;&#21152;&#30340;&#22122;&#38899;&#21644;&#24310;&#38271;&#30340;&#27169;&#22411;&#36816;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#20887;&#20313;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#21516;&#26102;&#20248;&#21270;&#36816;&#34892;&#26102;&#38388;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#25152;&#26377;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00230v2 Announce Type: replace-cross  Abstract: In the domain of multivariate forecasting, transformer models stand out as powerful apparatus, displaying exceptional capabilities in handling messy datasets from real-world contexts. However, the inherent complexity of these datasets, characterized by numerous variables and lengthy temporal sequences, poses challenges, including increased noise and extended model runtime. This paper focuses on reducing redundant information to elevate forecasting accuracy while optimizing runtime efficiency. We propose a novel transformer forecasting framework enhanced by Principal Component Analysis (PCA) to tackle this challenge. The framework is evaluated by five state-of-the-art (SOTA) models and four diverse real-world datasets. Our experimental results demonstrate the framework's ability to minimize prediction errors across all models and datasets while significantly reducing runtime. From the model perspective, one of the PCA-enhanced m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2312.14625</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35780;&#20272;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#23545;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#30340;&#26085;&#30410;&#20381;&#36182;&#20351;&#24471;&#20132;&#36890;&#32593;&#32476;&#26356;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25968;&#25454;&#31713;&#25913;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23545;&#25163;&#21487;&#33021;&#21033;&#29992;&#23548;&#33322;&#26381;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#25110;&#22788;&#29702;&#26041;&#38754;&#30340;&#28431;&#27934;&#26469;&#27880;&#20837;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#24178;&#25200;&#39550;&#39542;&#21592;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#36825;&#31181;&#25915;&#20987;&#21487;&#33021;&#26174;&#33879;&#22686;&#21152;&#20132;&#36890;&#25317;&#22581;&#65292;&#23548;&#33268;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#22823;&#37327;&#28010;&#36153;&#65292;&#29978;&#33267;&#21487;&#33021;&#30772;&#22351;&#20381;&#36182;&#36947;&#36335;&#32593;&#32476;&#30340;&#22522;&#26412;&#26381;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#27492;&#31867;&#25915;&#20987;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#26469;&#21457;&#29616;&#38024;&#23545;&#20132;&#36890;&#32593;&#32476;&#30340;&#26368;&#22351;&#24773;&#20917;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25317;&#26377;&#23041;&#32961;&#34892;&#20026;&#32773;&#30340;&#23545;&#25239;&#27169;&#22411;&#65292;&#35813;&#34892;&#20026;&#32773;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#39550;&#39542;&#21592;&#22312;&#26576;&#20123;&#36947;&#36335;&#19978;&#24863;&#30693;&#30340;&#34892;&#39542;&#26102;&#38388;&#26469;&#25805;&#32437;&#20182;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#25214;&#21040;&#19968;&#31181;&#36817;&#20284;&#26368;&#20339;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14625v2 Announce Type: replace  Abstract: The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers' route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversaria
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20215;&#20540;&#26174;&#24615;&#39044;&#35757;&#32451;&#65288;VEP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#23545;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#29366;&#24577;&#36827;&#34892;&#20851;&#32852;&#65292;&#23454;&#29616;&#22312;Atari&#21644;&#35270;&#35273;&#23548;&#33322;&#20013;&#33719;&#24471;&#22810;&#36798;2&#20493;&#22870;&#21169;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2312.12339</link><description>&lt;p&gt;
&#20026;&#23398;&#20064;&#21487;&#36801;&#31227;&#34920;&#31034;&#25552;&#20986;&#20215;&#20540;&#26174;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Value Explicit Pretraining for Learning Transferable Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20215;&#20540;&#26174;&#24615;&#39044;&#35757;&#32451;&#65288;VEP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#23545;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#29366;&#24577;&#36827;&#34892;&#20851;&#32852;&#65292;&#23454;&#29616;&#22312;Atari&#21644;&#35270;&#35273;&#23548;&#33322;&#20013;&#33719;&#24471;&#22810;&#36798;2&#20493;&#22870;&#21169;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#20215;&#20540;&#26174;&#24615;&#39044;&#35757;&#32451;&#65288;Value Explicit Pretraining&#65292;VEP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#65292;&#20197;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#36801;&#31227;&#12290;VEP&#36890;&#36807;&#23398;&#20064;&#20026;&#19982;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#20849;&#20139;&#31867;&#20284;&#30446;&#26631;&#30340;&#26032;&#20219;&#21153;&#23398;&#20064;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#65292;&#26080;&#35770;&#22806;&#35266;&#21464;&#21270;&#21644;&#29615;&#22659;&#21160;&#24577;&#22914;&#20309;&#65292;&#37117;&#33021;&#23398;&#20064;&#21040;&#30446;&#26631;&#26465;&#20214;&#34920;&#31034;&#12290;&#20026;&#20102;&#20174;&#19968;&#31995;&#21015;&#35266;&#23519;&#20013;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#23548;&#33268;&#23398;&#20064;&#21040;&#26102;&#38388;&#19978;&#24179;&#28369;&#30340;&#34920;&#31034;&#12290;VEP&#23398;&#20064;&#23558;&#22522;&#20110;&#21453;&#26144;&#20219;&#21153;&#36827;&#23637;&#30340;&#36125;&#23572;&#26364;&#22238;&#25253;&#20272;&#35745;&#26469;&#20851;&#32852;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#29366;&#24577;&#12290;&#22312;&#20351;&#29992;&#30495;&#23454;&#23548;&#33322;&#27169;&#25311;&#22120;&#21644;Atari&#22522;&#20934;&#36827;&#34892;&#23454;&#39564;&#21518;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#26041;&#27861;&#20135;&#29983;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#30340;&#33021;&#21147;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;VEP&#22312;Atari&#21644;&#35270;&#35273;&#23548;&#33322;&#19978;&#30340;&#22870;&#21169;&#19978;&#33719;&#24471;&#20102;&#22810;&#36798;2&#20493;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12339v2 Announce Type: replace  Abstract: We propose Value Explicit Pretraining (VEP), a method that learns generalizable representations for transfer reinforcement learning. VEP enables learning of new tasks that share similar objectives as previously learned tasks, by learning an encoder for objective-conditioned representations, irrespective of appearance changes and environment dynamics. To pre-train the encoder from a sequence of observations, we use a self-supervised contrastive loss that results in learning temporally smooth representations. VEP learns to relate states across different tasks based on the Bellman return estimate that is reflective of task progress. Experiments using a realistic navigation simulator and Atari benchmark show that the pretrained encoder produced by our method outperforms current SoTA pretraining methods on the ability to generalize to unseen tasks. VEP achieves up to a 2 times improvement in rewards on Atari and visual navigation, and up 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22266;&#26377;&#30340;&#37327;&#23376;&#22122;&#22768;&#26469;&#20445;&#25252;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#23545;&#37327;&#23376;&#30005;&#36335;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#25512;&#23548;&#20854;&#26041;&#24046;&#30340;&#19978;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2312.11126</link><description>&lt;p&gt;
&#21033;&#29992;&#22266;&#26377;&#22122;&#22768;&#20445;&#25252;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Harnessing Inherent Noises for Privacy Preservation in Quantum Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22266;&#26377;&#30340;&#37327;&#23376;&#22122;&#22768;&#26469;&#20445;&#25252;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#23545;&#37327;&#23376;&#30005;&#36335;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#25512;&#23548;&#20854;&#26041;&#24046;&#30340;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24443;&#24213;&#25913;&#21464;&#20102;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#21644;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#26174;&#31034;&#20986;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#21487;&#33021;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#12290;&#23613;&#31649;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#27880;&#20837;&#20154;&#24037;&#22122;&#22768;&#20445;&#25252;&#38544;&#31169;&#30340;&#25104;&#29087;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;QML&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22266;&#26377;&#30340;&#37327;&#23376;&#22122;&#22768;&#20445;&#25252;QML&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#23384;&#22312;&#22122;&#22768;&#30340;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#23556;&#32447;&#22122;&#22768;&#21644;&#38750;&#30456;&#24178;&#22122;&#22768;&#26469;&#20445;&#25252;&#20108;&#20803;&#20998;&#31867;&#30340;QML&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#20998;&#26512;&#20102;QML&#20013;&#37327;&#23376;&#30005;&#36335;&#21442;&#25968;&#30340;&#26799;&#24230;&#28385;&#36275;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#26041;&#24046;&#30340;&#19978;&#19979;&#30028;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11126v2 Announce Type: replace-cross  Abstract: Quantum computing revolutionizes the way of solving complex problems and handling vast datasets, which shows great potential to accelerate the machine learning process. However, data leakage in quantum machine learning (QML) may present privacy risks. Although differential privacy (DP), which protects privacy through the injection of artificial noise, is a well-established approach, its application in the QML domain remains under-explored. In this paper, we propose to harness inherent quantum noises to protect data privacy in QML. Especially, considering the Noisy Intermediate-Scale Quantum (NISQ) devices, we leverage the unavoidable shot noise and incoherent noise in quantum computing to preserve the privacy of QML models for binary classification. We mathematically analyze that the gradient of quantum circuit parameters in QML satisfies a Gaussian distribution, and derive the upper and lower bounds on its variance, which can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DTP-Net&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#29305;&#24449;&#37325;&#29992;&#23398;&#20064;&#22312;&#26102;&#39057;&#22495;&#20013;&#37325;&#24314;&#33041;&#30005;&#22270;&#20449;&#21495;&#24182;&#21435;&#38500;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2312.09417</link><description>&lt;p&gt;
DTP-Net&#65306;&#36890;&#36807;&#22810;&#23610;&#24230;&#29305;&#24449;&#37325;&#29992;&#23398;&#20064;&#22312;&#26102;&#39057;&#22495;&#20013;&#37325;&#24314;&#33041;&#30005;&#22270;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
DTP-Net: Learning to Reconstruct EEG signals in Time-Frequency Domain by Multi-scale Feature Reuse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09417
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DTP-Net&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#29305;&#24449;&#37325;&#29992;&#23398;&#20064;&#22312;&#26102;&#39057;&#22495;&#20013;&#37325;&#24314;&#33041;&#30005;&#22270;&#20449;&#21495;&#24182;&#21435;&#38500;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#24456;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#20266;&#36857;&#21435;&#38500;&#23545;&#20110;&#25913;&#21892;&#20449;&#21495;&#36136;&#37327;&#22312;&#30142;&#30149;&#35786;&#26029;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#31561;&#22330;&#26223;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#21367;&#31215;&#31070;&#32463;&#26550;&#26500;&#65292;&#31216;&#20026;DTP-Net&#65292;&#23427;&#30001;&#19968;&#20010;&#31264;&#23494;&#36830;&#25509;&#30340;&#26102;&#38388;&#37329;&#23383;&#22612;&#65288;DTP&#65289;&#21644;&#19968;&#23545;&#21487;&#23398;&#20064;&#30340;&#26102;&#39057;&#21464;&#25442;&#32452;&#25104;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#21435;&#22122;&#12290;&#25152;&#25552;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#32534;&#30721;&#22120;&#23618;&#23558;&#20219;&#24847;&#38271;&#24230;&#30340;&#21333;&#36890;&#36947;EEG&#20449;&#21495;&#36716;&#25442;&#25104;&#26102;&#39057;&#22495;&#12290;&#28982;&#21518;&#65292;DTP&#20197;&#22810;&#23610;&#24230;&#30340;&#26041;&#24335;&#25552;&#21462;&#21644;&#20943;&#23569;&#22122;&#22768;&#65292;&#22914;&#30524;&#37096;&#21644;&#32908;&#32905;&#20266;&#36857;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#26469;&#37325;&#24314;&#21435;&#22122;&#21518;&#30340;EEG&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;DTP-Net&#20013;&#27599;&#20010;&#27169;&#22359;&#30340;&#34920;&#31034;&#23398;&#20064;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09417v2 Announce Type: replace-cross  Abstract: Electroencephalography (EEG) signals are easily corrupted by various artifacts, making artifact removal crucial for improving signal quality in scenarios such as disease diagnosis and brain-computer interface (BCI). In this paper, we present a fully convolutional neural architecture, called DTP-Net, which consists of a Densely Connected Temporal Pyramid (DTP) sandwiched between a pair of learnable time-frequency transformations for end-to-end electroencephalogram (EEG) denoising. The proposed method first transforms a single-channel EEG signal of arbitrary length into the time-frequency domain via an Encoder layer. Then, noises, such as ocular and muscle artifacts, are extracted by DTP in a multi-scale fashion and reduced. Finally, a Decoder layer is employed to reconstruct the artifact-reduced EEG signal. Additionally, we conduct an in-depth analysis of the representation learning behavior of each module in DTP-Net to substant
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17093</link><description>&lt;p&gt;
&#29992;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Out-of-Distribution Detection with Prototypical Semi-Supervised Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PAWS-VMK&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;Predicting View-Assignments With Support Samples&#65288;PAWS&#65289;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;(1) &#21442;&#25968;&#21270;von-Mises Fisher&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;vMF-SNE&#65289;&#26469;&#39044;&#35757;&#32451;&#25237;&#24433;&#22836;&#65292;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;;(2) &#21463;MixMatch&#21551;&#21457;&#30340;&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#22810;&#35270;&#22270;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#65292;&#25552;&#20379;&#27604;PAWS&#20013;&#20351;&#29992;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#26356;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;;&#21644;(3) &#31616;&#21333;k-Means&#21407;&#22411;&#36873;&#25321;&#65288;SKMPS&#65289;&#65292;&#19968;&#31181;&#27604;&#20854;&#20182;&#26080;&#30417;&#30563;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861;&#25552;&#20379;&#26356;&#20248;&#36234;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17093v2 Announce Type: replace-cross  Abstract: This paper describes PAWS-VMK, an improved approach to prototypical semi-supervised learning in the field of computer vision, specifically designed to utilize a frozen foundation model as the neural network backbone. This method outperforms previous results in semi-supervised learning and out-of-distribution (OOD) detection, improving upon the Predicting View-Assignments With Support Samples (PAWS) semi-supervised learning method. We introduce (1) parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to pretrain the projection head using the high-quality embeddings of the foundation model; (2) a MixMatch inspired loss, where predictions across multiple views are averaged to provide a more reliable supervision signal compared to the consistency loss used in PAWS and (3) simple $k$-Means prototype selection (SKMPS), a technique that provides superior performance to other unsupervised label selection approaches in t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.15502</link><description>&lt;p&gt;
&#36873;&#21462;&#23436;&#20840;&#38543;&#26426;&#30340;&#20114;&#34917;&#26631;&#31614;&#26159;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#23454;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20851;&#32852;&#30528;&#19968;&#20010;&#25110;&#22810;&#20010;&#20114;&#34917;&#26631;&#31614;&#65292;&#25351;&#31034;&#20854;&#19981;&#23646;&#20110;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#19968;&#33268;&#26041;&#27861;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#26469;&#27169;&#25311;&#20114;&#34917;&#26631;&#31614;&#30340;&#29983;&#25104;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#19968;&#20010;&#26222;&#36890;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#26469;&#20272;&#35745;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#33021;&#19981;&#20250;&#34987;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#36825;&#20123;&#26465;&#20214;&#12290;&#21463;&#21040;PU&#23398;&#20064;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#19968;&#32452;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of 
&lt;/p&gt;</description></item><item><title>InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12943</link><description>&lt;p&gt;
InteRACT&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#20154;&#31867;&#24847;&#22270;&#39044;&#27979;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12943
&lt;/p&gt;
&lt;p&gt;
InteRACT&#36890;&#36807;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20154;&#26426;&#25805;&#32437;&#20013;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#24182;&#30456;&#24212;&#35843;&#25972;&#20854;&#34892;&#21160;&#65292;&#20197;&#24179;&#31283;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24847;&#22270;&#21453;&#36807;&#26469;&#21448;&#21462;&#20915;&#20110;&#26426;&#22120;&#20154;&#37319;&#21462;&#30340;&#21160;&#20316;&#65292;&#36896;&#25104;&#20102;&#19968;&#20010;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26159;&#35757;&#32451;&#29420;&#31435;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#30340;&#36793;&#38469;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#32570;&#20047;&#37197;&#23545;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#26465;&#20214;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#33021;&#21542;&#36716;&#32780;&#21033;&#29992;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#65311;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#21160;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;-&#20154;&#31867;&#21040;&#20154;&#31867;-&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;InteRACT&#65292;&#35813;&#26550;&#26500;&#22312;&#22823;&#22411;&#20154;&#31867;-&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#26465;&#20214;&#24847;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#22312;&#23567;&#22411;&#20154;&#26426;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12943v2 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collabo
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25286;&#20998;&#32622;&#20449;&#39044;&#27979;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#31867;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20998;&#24067;&#22806;&#35206;&#30422;&#29575;&#22522;&#20110;&#27169;&#22411;&#22312;&#26657;&#20934;&#38598;&#19978;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2311.12688</link><description>&lt;p&gt;
&#20851;&#20110;&#32467;&#21512;&#25286;&#20998;&#32622;&#20449;&#39044;&#27979;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#24067;&#22806;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Out-of-Distribution Coverage of Combining Split Conformal Prediction and Bayesian Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12688
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25286;&#20998;&#32622;&#20449;&#39044;&#27979;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#31867;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20998;&#24067;&#22806;&#35206;&#30422;&#29575;&#22522;&#20110;&#27169;&#22411;&#22312;&#26657;&#20934;&#38598;&#19978;&#30340;&#20449;&#24515;&#27700;&#24179;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#21644;&#32622;&#20449;&#39044;&#27979;&#26159;&#20004;&#31181;&#29992;&#20110;&#20256;&#36798;&#19981;&#30830;&#23450;&#24615;&#24182;&#22686;&#21152;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#23558;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#19982;&#25286;&#20998;&#32622;&#20449;&#39044;&#27979;&#30456;&#32467;&#21512;&#23545;&#20998;&#24067;&#22806;&#35206;&#30422;&#29575;&#30340;&#24433;&#21709;&#65307;&#29305;&#21035;&#26159;&#22312;&#22810;&#31867;&#22270;&#20687;&#20998;&#31867;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#24314;&#35758;&#65292;&#22914;&#26524;&#27169;&#22411;&#22312;&#26657;&#20934;&#38598;&#19978;&#36890;&#24120;&#32570;&#20047;&#20449;&#24515;&#65292;&#37027;&#20040;&#24471;&#21040;&#30340;&#32622;&#20449;&#38598;&#21487;&#33021;&#19982;&#31616;&#21333;&#39044;&#27979;&#21487;&#20449;&#38598;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26356;&#31967;&#31957;&#30340;&#20998;&#24067;&#22806;&#35206;&#30422;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#27169;&#22411;&#22312;&#26657;&#20934;&#38598;&#19978;&#36807;&#20110;&#33258;&#20449;&#65292;&#20351;&#29992;&#32622;&#20449;&#39044;&#27979;&#21487;&#33021;&#20250;&#25913;&#21892;&#20998;&#24067;&#22806;&#35206;&#30422;&#12290;&#25105;&#20204;&#35780;&#20272;&#23558;&#25286;&#20998;&#32622;&#20449;&#26041;&#27861;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#12289;&#28145;&#24230;&#38598;&#21512;&#21644;&#22343;&#22330;&#21464;&#20998;&#25512;&#29702;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#38598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;B
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12688v2 Announce Type: replace  Abstract: Bayesian deep learning and conformal prediction are two methods that have been used to convey uncertainty and increase safety in machine learning systems. We focus on combining Bayesian deep learning with split conformal prediction and how this combination effects out-of-distribution coverage; particularly in the case of multiclass image classification. We suggest that if the model is generally underconfident on the calibration set, then the resultant conformal sets may exhibit worse out-of-distribution coverage compared to simple predictive credible sets. Conversely, if the model is overconfident on the calibration set, the use of conformal prediction may improve out-of-distribution coverage. We evaluate prediction sets as a result of combining split conformal methods and neural networks trained with (i) stochastic gradient descent, (ii) deep ensembles, and (iii) mean-field variational inference. Our results suggest that combining B
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#32423;&#26550;&#26500;SkelVit&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#24182;&#22312;&#27599;&#20010;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#26469;&#25214;&#21040;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.08094</link><description>&lt;p&gt;
SkelVIT&#65306;&#36731;&#37327;&#32423;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#32423;&#26550;&#26500;SkelVit&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#24182;&#22312;&#27599;&#20010;&#34920;&#31034;&#19978;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#26469;&#25214;&#21040;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39592;&#39612;&#22522;&#30784;&#21160;&#20316;&#35782;&#21035;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#35270;&#35282;&#21644;&#20809;&#29031;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20854;&#22788;&#29702;&#27604;&#35270;&#39057;&#24103;&#30340;&#22788;&#29702;&#25928;&#29575;&#39640;&#24471;&#22810;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20197;&#20266;&#22270;&#20687;&#24418;&#24335;&#34920;&#31034;&#39592;&#26550;&#25968;&#25454;&#24182;&#24212;&#29992;CNN&#36827;&#34892;&#21160;&#20316;&#35782;&#21035;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38598;&#20013;&#22312;&#23547;&#25214;&#24418;&#25104;&#20266;&#22270;&#20687;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#12290;&#26368;&#36817;&#65292;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#21464;&#21387;&#22120;&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;VIT&#23545;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#20266;&#22270;&#20687;&#34920;&#31034;&#26041;&#26696;&#19978;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#32423;&#26550;&#26500;&#65292;SkelVit&#65292;&#23427;&#24418;&#25104;&#19968;&#32452;&#20266;&#22270;&#20687;&#65292;&#23545;&#27599;&#20010;&#34920;&#31034;&#24212;&#29992;&#20998;&#31867;&#22120;&#65292;&#24182;&#32467;&#21512;&#23427;&#20204;&#30340;&#32467;&#26524;&#20197;&#25214;&#21040;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08094v2 Announce Type: replace-cross  Abstract: Skeleton-based action recognition receives the attention of many researchers as it is robust to viewpoint and illumination changes, and its processing is much more efficient than the processing of video frames. With the emergence of deep learning models, it has become very popular to represent the skeleton data in pseudo-image form and apply CNN for action recognition. Thereafter, studies concentrated on finding effective methods for forming pseudo-images. Recently, attention networks, more specifically transformers have provided promising results in various vision problems. In this study, the effectiveness of VIT for skeleton-based action recognition is examined and its robustness on the pseudo-image representation scheme is investigated. To this end, a three-level architecture, SkelVit is proposed, which forms a set of pseudo images, applies a classifier on each of the representations, and combines their results to find the f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.04235</link><description>&lt;p&gt;
LLM&#33021;&#36981;&#23432;&#31616;&#21333;&#35268;&#21017;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLMs Follow Simple Rules?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04235
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RuLES&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#36981;&#23432;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25215;&#25285;&#36234;&#26469;&#36234;&#22810;&#30340;&#36131;&#20219;&#65292;&#33021;&#22815;&#20197;&#21487;&#38752;&#30340;&#26041;&#24335;&#25351;&#23450;&#21644;&#32422;&#26463;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35268;&#21017;&#36981;&#24490;&#35821;&#35328;&#35780;&#20272;&#22330;&#26223;&#65288;RuLES&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27979;&#37327;LLMs&#36981;&#24490;&#35268;&#21017;&#33021;&#21147;&#30340;&#31243;&#24207;&#26694;&#26550;&#65292;&#21253;&#25324;14&#20010;&#31616;&#21333;&#30340;&#25991;&#26412;&#22330;&#26223;&#65292;&#27169;&#22411;&#22312;&#19982;&#29992;&#25143;&#20132;&#20114;&#26102;&#34987;&#25351;&#31034;&#36981;&#23432;&#21508;&#31181;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietar
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;AI&#21361;&#38505;&#31649;&#29702;&#26694;&#26550;&#65288;AIHM&#65289;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#22788;&#29702;AI&#21361;&#38505;&#65292;&#24182;&#22312;&#31995;&#32479;&#24320;&#21457;&#36807;&#31243;&#20013;&#36827;&#34892;&#65292;&#20197;&#30830;&#20445;&#23613;&#26089;&#21457;&#29616;&#20219;&#20309;AI&#21361;&#38505;&#12290;</title><link>https://arxiv.org/abs/2310.16727</link><description>&lt;p&gt;
AI&#39118;&#38505;&#31649;&#29702;: &#29992;&#20110;&#31995;&#32479;&#31649;&#29702;AI&#39118;&#38505;&#26681;&#26412;&#21407;&#22240;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AI Hazard Management: A framework for the systematic management of root causes for AI risks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.16727
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;AI&#21361;&#38505;&#31649;&#29702;&#26694;&#26550;&#65288;AIHM&#65289;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#22788;&#29702;AI&#21361;&#38505;&#65292;&#24182;&#22312;&#31995;&#32479;&#24320;&#21457;&#36807;&#31243;&#20013;&#36827;&#34892;&#65292;&#20197;&#30830;&#20445;&#23613;&#26089;&#21457;&#29616;&#20219;&#20309;AI&#21361;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#22880;&#23450;&#20102;&#35299;&#20915;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;AI&#30340;&#25972;&#21512;&#65292;&#26032;&#30340;&#39118;&#38505;&#20063;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20174;&#20854;&#20248;&#21183;&#20013;&#21463;&#30410;&#65292;&#24517;&#39035;&#20805;&#20998;&#22788;&#29702;&#19982;AI&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#29616;&#26377;&#39046;&#22495;&#20013;&#30340;&#39118;&#38505;&#31649;&#29702;&#27969;&#31243;&#65292;&#22914;&#36719;&#20214;&#31995;&#32479;&#65292;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;AI&#30340;&#29305;&#23450;&#38382;&#39064;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#31995;&#32479;&#22320;&#21644;&#36879;&#26126;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;AI&#39118;&#38505;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#20063;&#31216;&#20026;AI&#21361;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AI&#21361;&#38505;&#31649;&#29702;&#65288;AIHM&#65289;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#36807;&#31243;&#26469;&#31995;&#32479;&#22320;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#22788;&#29702;AI&#21361;&#38505;&#12290;&#25552;&#20986;&#30340;&#36807;&#31243;&#19982;&#24320;&#21457;&#21516;&#26102;&#36827;&#34892;&#65292;&#20197;&#30830;&#20445;&#20219;&#20309;AI&#21361;&#38505;&#22312;AI&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#30340;&#26368;&#26089;&#21487;&#33021;&#38454;&#27573;&#34987;&#25429;&#33719;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#21487;&#23457;&#35745;&#24615;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.16727v2 Announce Type: replace  Abstract: Recent advancements in the field of Artificial Intelligence (AI) establish the basis to address challenging tasks. However, with the integration of AI, new risks arise. Therefore, to benefit from its advantages, it is essential to adequately handle the risks associated with AI. Existing risk management processes in related fields, such as software systems, need to sufficiently consider the specifics of AI. A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards. This paper introduces the AI Hazard Management (AIHM) framework, which provides a structured process to systematically identify, assess, and treat AI hazards. The proposed process is conducted in parallel with the development to ensure that any AI hazard is captured at the earliest possible stage of the AI system's life cycle. In addition, to ensure the AI system's auditability, the proposed framework systemat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#22240;&#32032;&#26102;&#31354;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#22240;&#32032;&#19979;&#30340;&#37096;&#20998;&#26102;&#31354;&#25968;&#25454;&#28436;&#21464;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#21644;&#21487;&#31227;&#26893;&#30340;&#23454;&#20363;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2310.10374</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20998;&#35299;&#23398;&#20064;&#30340;&#22810;&#22240;&#32032;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#22240;&#32032;&#26102;&#31354;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#39044;&#27979;&#19981;&#21516;&#22240;&#32032;&#19979;&#30340;&#37096;&#20998;&#26102;&#31354;&#25968;&#25454;&#28436;&#21464;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#21644;&#21487;&#31227;&#26893;&#30340;&#23454;&#20363;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10374v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#26102;&#31354;&#65288;ST&#65289;&#39044;&#27979;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22478;&#24066;&#31995;&#32479;&#20013;&#30340;ST&#25968;&#25454;&#65292;&#22914;&#20132;&#36890;&#25968;&#25454;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;ST&#25968;&#25454;&#29983;&#25104;&#36890;&#24120;&#21463;&#21040;&#19982;&#33258;&#28982;&#29616;&#35937;&#25110;&#20154;&#31867;&#31038;&#20250;&#32463;&#27982;&#27963;&#21160;&#30456;&#20851;&#30340;&#21508;&#31181;&#28508;&#22312;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22240;&#32032;&#20250;&#36873;&#25321;&#24615;&#22320;&#24433;&#21709;&#29305;&#23450;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ST&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#19981;&#20250;&#32454;&#21270;&#19981;&#21516;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#32780;&#26159;&#30452;&#25509;&#27169;&#22411;&#21270;&#22810;&#20010;&#22240;&#32032;&#30340;&#20132;&#32455;&#24433;&#21709;&#12290;&#36825;&#22686;&#21152;&#20102;ST&#25968;&#25454;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#22240;&#32032;ST&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#19981;&#21516;&#22240;&#32032;&#19979;&#39044;&#27979;&#37096;&#20998;ST&#25968;&#25454;&#28436;&#21464;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;: &#19968;&#20010;&#26377;&#25928;&#30340;&#29702;&#35770;&#35299;&#20915;&#26041;&#26696;&#21644;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#23454;&#20363;&#21270;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#20915;&#26041;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10374v2 Announce Type: replace  Abstract: Spatio-temporal (ST) prediction is an important and widely used technique in data mining and analytics, especially for ST data in urban systems such as transportation data. In practice, the ST data generation is usually influenced by various latent factors tied to natural phenomena or human socioeconomic activities, impacting specific spatial areas selectively. However, existing ST prediction methods usually do not refine the impacts of different factors, but directly model the entangled impacts of multiple factors. This amplifies the modeling complexity of ST data and compromises model interpretability. To this end, we propose a multi-factor ST prediction task that predicts partial ST data evolution under different factors, and combines them for a final prediction. We make two contributions to this task: an effective theoretical solution and a portable instantiation framework. Specifically, we first propose a theoretical solution ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.07699</link><description>&lt;p&gt;
VeCLIP&#65306;&#36890;&#36807;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#25913;&#36827;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
VeCLIP: Improving CLIP Training via Visual-enriched Captions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.07699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#30340;&#26041;&#24335;&#26469;&#25913;&#36827;CLIP&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;VeCLIP&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#23545;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#29228;&#21462;&#30340;AltTexts&#23384;&#22312;&#22266;&#26377;&#30340;&#22122;&#38899;&#21644;&#28508;&#22312;&#30340;&#19981;&#30456;&#20851;&#24615;&#65292;&#36896;&#25104;&#20102;&#31934;&#30830;&#30340;&#22270;&#20687;-&#25991;&#23383;&#23545;&#40784;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22024;&#26434;&#26631;&#39064;&#37325;&#20889;&#30340;&#21487;&#25193;&#23637;&#27969;&#31243;&#12290;&#19982;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26631;&#39064;&#37325;&#20889;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#23567;&#22411;&#31574;&#21010;&#25968;&#25454;&#38598;&#65288;&#22914;CC3M&#21644;CC12M&#65289;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#35270;&#35273;&#27010;&#24565;&#34701;&#20837;&#26631;&#39064;&#20013;&#65292;&#31216;&#20026;&#23500;&#21547;&#35270;&#35273;&#20449;&#24687;&#30340;&#26631;&#39064;&#65288;VeCap&#65289;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;AltTexts&#19982;&#26032;&#29983;&#25104;&#30340;VeCap&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#29228;&#21462;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;CLIP&#30340;&#36866;&#24212;&#24615;&#65292;&#31216;&#20026;VeCLIP&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#25105;&#20204;&#36731;&#26494;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
&lt;/p&gt;</description></item><item><title>&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.05196</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#26159;&#21542;&#20250;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Writing with Language Models Reduce Content Diversity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05196
&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19982;&#27169;&#22411;&#36741;&#21161;&#21512;&#20316;&#20889;&#20316;&#30340;&#28608;&#22686;&#12290;&#24403;&#19981;&#21516;&#29992;&#25143;&#32435;&#20837;&#21516;&#19968;&#27169;&#22411;&#30340;&#24314;&#35758;&#26102;&#65292;&#20250;&#23384;&#22312;&#20869;&#23481;&#22810;&#26679;&#24615;&#20943;&#23569;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#38480;&#21046;&#20844;&#20849;&#35805;&#35821;&#20013;&#30340;&#22810;&#20803;&#35266;&#28857;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#27979;&#37327;&#20102;&#21327;&#21516;&#20889;&#20316;&#23545;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#22312;&#35813;&#23454;&#39564;&#20013;&#65292;&#29992;&#25143;&#20197;&#19977;&#31181;&#35774;&#32622;&#25776;&#20889;&#35758;&#35770;&#24615;&#25991;&#31456;--&#20351;&#29992;&#22522;&#26412;LLM&#65288;GPT3&#65289;&#12289;&#32463;&#36807;&#21453;&#39304;&#35843;&#25972;&#30340;LLM&#65288;InstructGPT&#65289;&#20197;&#21450;&#19981;&#20351;&#29992;&#27169;&#22411;&#24110;&#21161;&#20889;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;InstructGPT&#36827;&#34892;&#20889;&#20316;&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#23548;&#33268;&#22810;&#26679;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22686;&#21152;&#20102;&#19981;&#21516;&#20316;&#32773;&#30340;&#20889;&#20316;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20943;&#23569;&#20102;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#31181;&#24433;&#21709;&#20027;&#35201;&#26469;&#28304;&#20110;InstructGPT&#23545;&#20849;&#21516;&#25776;&#20889;&#30340;&#25991;&#26412;&#36129;&#29486;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
&lt;/p&gt;</description></item><item><title>&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#65288;DWA&#65289;&#20026;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#24341;&#20837;&#20102;&#26032;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#20102;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#20102;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#20102;&#24120;&#35265;&#22122;&#22768;&#65292;&#20174;&#32780;&#22312;&#32463;&#20856;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2307.04593</link><description>&lt;p&gt;
DWA&#65306;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;
&lt;/p&gt;
&lt;p&gt;
DWA: Differential Wavelet Amplifier for Image Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.04593
&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#65288;DWA&#65289;&#20026;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#24341;&#20837;&#20102;&#26032;&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#24046;&#24322;&#25913;&#36827;&#20102;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#20102;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#20102;&#24120;&#35265;&#22122;&#22768;&#65292;&#20174;&#32780;&#22312;&#32463;&#20856;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#24046;&#20998;&#23567;&#27874;&#25918;&#22823;&#22120;&#65288;Differential Wavelet Amplifier&#65292;DWA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#23567;&#27874;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#30340;&#25554;&#20837;&#27169;&#22359;&#12290;DWA&#28608;&#21457;&#20102;&#19968;&#31181;&#36817;&#26469;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#30340;&#26041;&#27861;&#65292;&#21363;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#12290;DWT&#23454;&#29616;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22270;&#20687;&#34920;&#31034;&#26041;&#24335;&#65292;&#29992;&#20110;SR&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#31354;&#38388;&#21306;&#22495;&#20943;&#23567;&#20102;4&#20493;&#65292;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#23558;&#20854;&#20316;&#20026;&#21487;&#25345;&#32493;ML&#30340;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DWA&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25913;&#36827;&#22522;&#20110;&#23567;&#27874;&#30340;SR&#27169;&#22411;&#65292;&#22312;&#23567;&#27874;&#22495;&#20013;&#31934;&#28860;&#30456;&#20851;&#29305;&#24449;&#25552;&#21462;&#65292;&#24378;&#35843;&#23616;&#37096;&#23545;&#27604;&#24230;&#24182;&#25233;&#21046;&#36755;&#20837;&#20449;&#21495;&#20013;&#30340;&#24120;&#35265;&#22122;&#22768;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#25972;&#21512;&#21040;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#20013;&#65288;&#22914;DWSR&#21644;MWCNN&#65289;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#32463;&#20856;SR&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;DWA&#20351;&#24471;DWSR&#21644;MWCNN&#30452;&#25509;&#24212;&#29992;&#20110;&#36755;&#20837;&#22270;&#20687;&#31354;&#38388;&#65292;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.04593v1 Announce Type: cross  Abstract: This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#31616;&#21333;&#24615;&#20559;&#35265;&#23545;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;SPARE&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2305.18761</link><description>&lt;p&gt;
&#36879;&#36807;&#31616;&#21333;&#24615;&#20559;&#35265;&#30340;&#35270;&#35282;&#26089;&#26399;&#35782;&#21035;&#35757;&#32451;&#20013;&#30340;&#34394;&#20551;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.18761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#31616;&#21333;&#24615;&#20559;&#35265;&#23545;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#35782;&#21035;&#21644;&#32531;&#35299;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;SPARE&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26397;&#21521;&#23398;&#20064;&#26356;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#26497;&#26131;&#20110;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#31616;&#21333;&#24615;&#20559;&#35265;&#23545;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#35757;&#32451;&#20013;&#22522;&#20110;&#27169;&#22411;&#36755;&#20986;&#21487;&#20197;&#26126;&#30830;&#20998;&#31163;&#20986;&#20855;&#26377;&#34394;&#20551;&#29305;&#24449;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#65292;&#22914;&#26524;&#34394;&#20551;&#29305;&#24449;&#30340;&#22122;&#22768;&#20449;&#21495;&#27604;&#36275;&#22815;&#23567;&#65292;&#32593;&#32476;&#22312;&#22823;&#22810;&#25968;&#31034;&#20363;&#19978;&#30340;&#36755;&#20986;&#20960;&#20046;&#23436;&#20840;&#30001;&#34394;&#20551;&#29305;&#24449;&#20915;&#23450;&#65292;&#23548;&#33268;&#36739;&#24046;&#30340;&#26368;&#22351;&#32452;&#27979;&#35797;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPARE&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#26089;&#26399;&#35782;&#21035;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#21033;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#26469;&#20943;&#36731;&#20854;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPARE&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.18761v2 Announce Type: replace  Abstract: Neural networks trained with (stochastic) gradient descent have an inductive bias towards learning simpler solutions. This makes them highly prone to learning spurious correlations in the training data, that may not hold at test time. In this work, we provide the first theoretical analysis of the effect of simplicity bias on learning spurious correlations. Notably, we show that examples with spurious features are provably separable based on the model's output early in training. We further illustrate that if spurious features have a small enough noise-to-signal ratio, the network's output on the majority of examples is almost exclusively determined by the spurious features, leading to poor worst-group test accuracy. Finally, we propose SPARE, which identifies spurious correlations early in training and utilizes importance sampling to alleviate their effect. Empirically, we demonstrate that SPARE outperforms state-of-the-art methods by
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#20013;&#20419;&#36827;&#31232;&#30095;&#23398;&#20064;&#35299;&#30340;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#34920;&#31034;&#23450;&#29702;&#21644;&#36716;&#25442;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2305.12584</link><description>&lt;p&gt;
&#31232;&#30095;&#34920;&#31034;&#23450;&#29702;&#29992;&#20110;&#22312;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sparse Representer Theorems for Learning in Reproducing Kernel Banach Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12584
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#20013;&#20419;&#36827;&#31232;&#30095;&#23398;&#20064;&#35299;&#30340;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#34920;&#31034;&#23450;&#29702;&#21644;&#36716;&#25442;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35299;&#30340;&#31232;&#30095;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#29702;&#24819;&#30340;&#29305;&#24449;&#12290;&#26576;&#20123;&#20877;&#29983;&#26680;Banach&#31354;&#38388;(RKBSs)&#26159;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#30340;&#36866;&#24403;&#20551;&#35774;&#31354;&#38388;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20160;&#20040;&#26679;&#30340;RKBS&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#35299;&#30340;&#31232;&#30095;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;RKBS&#20013;&#30340;&#20004;&#31181;&#20856;&#22411;&#23398;&#20064;&#27169;&#22411;&#65306;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;(MNI)&#38382;&#39064;&#21644;&#27491;&#21017;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#24314;&#31435;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#36890;&#36807;&#26497;&#20540;&#28857;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#34920;&#31034;&#35299;&#38598;&#30340;&#26497;&#20540;&#28857;&#65292;&#21363;&#35268;&#33539;&#20989;&#25968;&#30340;&#27425;&#26799;&#24230;&#38598;&#65292;&#36825;&#26159;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;RKBS&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21487;&#20197;&#23558;&#35299;&#30340;&#26174;&#24335;&#34920;&#31034;&#36716;&#25442;&#20026;&#20855;&#26377;&#27604;&#35266;&#27979;&#25968;&#25454;&#25968;&#37327;&#26356;&#23569;&#39033;&#30340;&#31232;&#30095;&#26680;&#34920;&#31034;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#20805;&#20998;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12584v2 Announce Type: replace-cross  Abstract: Sparsity of a learning solution is a desirable feature in machine learning. Certain reproducing kernel Banach spaces (RKBSs) are appropriate hypothesis spaces for sparse learning methods. The goal of this paper is to understand what kind of RKBSs can promote sparsity for learning solutions. We consider two typical learning models in an RKBS: the minimum norm interpolation (MNI) problem and the regularization problem. We first establish an explicit representer theorem for solutions of these problems, which represents the extreme points of the solution set by a linear combination of the extreme points of the subdifferential set, of the norm function, which is data-dependent. We then propose sufficient conditions on the RKBS that can transform the explicit representation of the solutions to a sparse kernel representation having fewer terms than the number of the observed data. Under the proposed sufficient conditions, we investiga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#31070;&#32463;&#20284;&#28982;&#26041;&#27861;&#65292;&#36890;&#36807;&#32435;&#20837;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#65292;&#20351;&#20854;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#23450;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2301.13368</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#35823;&#24046;&#40065;&#26834;&#39034;&#24207;&#31070;&#32463;&#20284;&#28982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Misspecification-robust Sequential Neural Likelihood for Simulation-based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#31070;&#32463;&#20284;&#28982;&#26041;&#27861;&#65292;&#36890;&#36807;&#32435;&#20837;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#65292;&#20351;&#20854;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#23450;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#25512;&#26029;&#25216;&#26415;&#23545;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#26426;&#26800;&#21644;&#21487;&#27169;&#25311;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20256;&#32479;&#32479;&#35745;&#26041;&#27861;(&#22914;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#21644;&#36125;&#21494;&#26031;&#21512;&#25104;&#20284;&#28982;)&#30340;&#30740;&#31350;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35268;&#23450;&#33391;&#22909;&#21644;&#38169;&#35823;&#35268;&#23450;&#30340;&#35774;&#32622;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#28010;&#36153;&#27169;&#22411;&#27169;&#25311;&#32780;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22914;&#39034;&#24207;&#31070;&#32463;&#20284;&#28982;(SNL)&#65292;&#36890;&#36807;&#21033;&#29992;&#25152;&#26377;&#27169;&#22411;&#27169;&#25311;&#26469;&#35757;&#32451;&#31070;&#32463;&#20195;&#29702;&#20284;&#28982;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#28010;&#36153;&#12290;&#28982;&#32780;&#65292;SNL&#22312;&#27169;&#22411;&#38169;&#35823;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#19981;&#21487;&#38752;&#65292;&#21487;&#33021;&#23548;&#33268;&#22260;&#32469;&#19981;&#20934;&#30830;&#21442;&#25968;&#20272;&#35745;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SNL&#26041;&#27861;&#65292;&#36890;&#36807;&#32435;&#20837;&#39069;&#22806;&#30340;&#35843;&#25972;&#21442;&#25968;&#65292;&#20351;&#20854;&#23545;&#27169;&#22411;&#38169;&#35823;&#35268;&#23450;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13368v2 Announce Type: replace-cross  Abstract: Simulation-based inference techniques are indispensable for parameter estimation of mechanistic and simulable models with intractable likelihoods. While traditional statistical approaches like approximate Bayesian computation and Bayesian synthetic likelihood have been studied under well-specified and misspecified settings, they often suffer from inefficiencies due to wasted model simulations. Neural approaches, such as sequential neural likelihood (SNL) avoid this wastage by utilising all model simulations to train a neural surrogate for the likelihood function. However, the performance of SNL under model misspecification is unreliable and can result in overconfident posteriors centred around an inaccurate parameter estimate. In this paper, we propose a novel SNL method, which through the incorporation of additional adjustment parameters, is robust to model misspecification and capable of identifying features of the data that 
&lt;/p&gt;</description></item><item><title>PDFormer&#26159;&#19968;&#31181;&#32771;&#34385;&#20256;&#25773;&#26102;&#24310;&#30340;&#21160;&#24577;&#36828;&#31243;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#38745;&#24577;&#31354;&#38388;&#20381;&#36182;&#12289;&#30701;&#31243;&#20449;&#24687;&#21644;&#26102;&#38388;&#24310;&#36831;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2301.07945</link><description>&lt;p&gt;
PDFormer&#65306;&#32771;&#34385;&#20256;&#25773;&#26102;&#24310;&#30340;&#21160;&#24577;&#36828;&#31243;Transformer&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for Traffic Flow Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.07945
&lt;/p&gt;
&lt;p&gt;
PDFormer&#26159;&#19968;&#31181;&#32771;&#34385;&#20256;&#25773;&#26102;&#24310;&#30340;&#21160;&#24577;&#36828;&#31243;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#38745;&#24577;&#31354;&#38388;&#20381;&#36182;&#12289;&#30701;&#31243;&#20449;&#24687;&#21644;&#26102;&#38388;&#24310;&#36831;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#20132;&#36890;&#27969;&#39044;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20132;&#36890;&#27969;&#39044;&#27979;&#38754;&#20020;&#30340;&#22522;&#26412;&#25361;&#25112;&#26159;&#26377;&#25928;&#22320;&#23545;&#20132;&#36890;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20855;&#21069;&#26223;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;i&#65289;&#22823;&#22810;&#25968;&#26041;&#27861;&#20197;&#38745;&#24577;&#26041;&#24335;&#24314;&#27169;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#38480;&#21046;&#20102;&#23398;&#20064;&#21160;&#24577;&#22478;&#24066;&#20132;&#36890;&#27169;&#24335;&#30340;&#33021;&#21147;&#65307;ii&#65289;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#32771;&#34385;&#30701;&#31243;&#31354;&#38388;&#20449;&#24687;&#65292;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65307;iii&#65289;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#20132;&#36890;&#29366;&#20917;&#22312;&#20301;&#32622;&#20043;&#38388;&#30340;&#20256;&#25773;&#23384;&#22312;&#26102;&#38388;&#24310;&#36831;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32771;&#34385;&#20256;&#25773;&#26102;&#24310;&#30340;&#21160;&#24577;&#36828;&#31243;Transformer&#65292;&#21363;PDFormer&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.07945v3 Announce Type: replace  Abstract: As a core technology of Intelligent Transportation System, traffic flow prediction has a wide range of applications. The fundamental challenge in traffic flow prediction is to effectively model the complex spatial-temporal dependencies in traffic data. Spatial-temporal Graph Neural Network (GNN) models have emerged as one of the most promising methods to solve this problem. However, GNN-based models have three major limitations for traffic prediction: i) Most methods model spatial dependencies in a static manner, which limits the ability to learn dynamic urban traffic patterns; ii) Most methods only consider short-range spatial information and are unable to capture long-range spatial dependencies; iii) These methods ignore the fact that the propagation of traffic conditions between locations has a time delay in traffic systems. To this end, we propose a novel Propagation Delay-aware dynamic long-range transFormer, namely PDFormer, fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#19968;&#38454;&#24809;&#32602;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#23547;&#25214;$\varepsilon$-KKT&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#30830;&#31435;&#20102;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2301.01716</link><description>&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#30340;&#19968;&#38454;&#24809;&#32602;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
First-order penalty methods for bilevel optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.01716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#19968;&#38454;&#24809;&#32602;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#23547;&#25214;$\varepsilon$-KKT&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#30830;&#31435;&#20102;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26080;&#32422;&#26463;&#21644;&#26377;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19979;&#23618;&#26159;&#21487;&#33021;&#26159;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#32780;&#19978;&#23618;&#26159;&#21487;&#33021;&#26159;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\varepsilon$-KKT&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;$\varepsilon$-KKT&#35299;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#20250;&#23548;&#33268;&#19968;&#20010;$O(\sqrt{\varepsilon})$&#25110;$O(\varepsilon)$-&#22522;&#20110;&#36229;&#26799;&#24230;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#38454;&#24809;&#32602;&#26041;&#27861;&#26469;&#23547;&#25214;&#23427;&#20204;&#30340;$\varepsilon$-KKT&#35299;&#65292;&#20854;&#23376;&#38382;&#39064;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#20316;&#32773;&#26368;&#36817;&#24320;&#21457;&#30340;&#19968;&#38454;&#26041;&#27861;&#36866;&#24403;&#22320;&#35299;&#20915;&#12290;&#22312;&#36866;&#24403;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#22522;&#26412;&#25805;&#20316;&#34913;&#37327;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#24809;&#32602;&#26041;&#27861;&#30340;&#8220;&#25805;&#20316;&#22797;&#26434;&#24230;&#8221;&#20998;&#21035;&#20026;$O(\varepsilon^{-4}\log\varepsilon^{-1})$&#21644;$O(\varepsilon^{-7}\log\varepsilon^{-1})$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.01716v2 Announce Type: replace-cross  Abstract: In this paper we study a class of unconstrained and constrained bilevel optimization problems in which the lower level is a possibly nonsmooth convex optimization problem, while the upper level is a possibly nonconvex optimization problem. We introduce a notion of $\varepsilon$-KKT solution for them and show that an $\varepsilon$-KKT solution leads to an $O(\sqrt{\varepsilon})$- or $O(\varepsilon)$-hypergradient based stionary point under suitable assumptions. We also propose first-order penalty methods for finding an $\varepsilon$-KKT solution of them, whose subproblems turn out to be a structured minimax problem and can be suitably solved by a first-order method recently developed by the authors. Under suitable assumptions, an \emph{operation complexity} of $O(\varepsilon^{-4}\log\varepsilon^{-1})$ and $O(\varepsilon^{-7}\log\varepsilon^{-1})$, measured by their fundamental operations, is established for the proposed penalty 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;START&#65292;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#35268;&#24459;&#21644;&#34892;&#31243;&#35821;&#20041;&#65292;&#36890;&#36807;&#36712;&#36857;&#27169;&#24335;&#22686;&#24378;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;(TPE-GAT)&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#34892;&#31243;&#35821;&#20041;&#36716;&#25442;&#20026;&#36947;&#36335;&#27573;&#30340;&#34920;&#31034;&#21521;&#37327;&#12290;</title><link>https://arxiv.org/abs/2211.09510</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#35268;&#24459;&#21644;&#34892;&#31243;&#35821;&#20041;&#30340;&#33258;&#30417;&#30563;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.09510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;START&#65292;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#35268;&#24459;&#21644;&#34892;&#31243;&#35821;&#20041;&#65292;&#36890;&#36807;&#36712;&#36857;&#27169;&#24335;&#22686;&#24378;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;(TPE-GAT)&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#34892;&#31243;&#35821;&#20041;&#36716;&#25442;&#20026;&#36947;&#36335;&#27573;&#30340;&#34920;&#31034;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#65288;TRL&#65289;&#26159;&#31354;&#38388; - &#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#21644;&#31649;&#29702;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;TRL&#30340;&#30446;&#26631;&#26159;&#23558;&#22797;&#26434;&#30340;&#21407;&#22987;&#36712;&#36857;&#36716;&#25442;&#20026;&#20302;&#32500;&#34920;&#31034;&#21521;&#37327;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#36712;&#36857;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#30456;&#20284;&#24615;&#35745;&#31639;&#12290;&#29616;&#26377;&#30340;TRL&#20316;&#21697;&#36890;&#24120;&#23558;&#36712;&#36857;&#35270;&#20026;&#26222;&#36890;&#30340;&#24207;&#21015;&#25968;&#25454;&#65292;&#32780;&#19968;&#20123;&#37325;&#35201;&#30340;&#31354;&#38388; - &#26102;&#38388;&#29305;&#24449;&#65292;&#22914;&#26102;&#38388;&#35268;&#24459;&#21644;&#34892;&#31243;&#35821;&#20041;&#65292;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#26102;&#38388;&#35268;&#24459;&#21644;&#34892;&#31243;&#35821;&#20041;&#30340;&#33258;&#30417;&#30563;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;START&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#36712;&#36857;&#27169;&#24335;&#22686;&#24378;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TPE-GAT&#65289;&#65292;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#34892;&#31243;&#35821;&#20041;&#36716;&#25442;&#20026;&#36947;&#36335;&#27573;&#30340;&#34920;&#31034;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.09510v4 Announce Type: replace  Abstract: Trajectory Representation Learning (TRL) is a powerful tool for spatial-temporal data analysis and management. TRL aims to convert complicated raw trajectories into low-dimensional representation vectors, which can be applied to various downstream tasks, such as trajectory classification, clustering, and similarity computation. Existing TRL works usually treat trajectories as ordinary sequence data, while some important spatial-temporal characteristics, such as temporal regularities and travel semantics, are not fully exploited. To fill this gap, we propose a novel Self-supervised trajectory representation learning framework with TemporAl Regularities and Travel semantics, namely START. The proposed method consists of two stages. The first stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT), which converts the road network features and travel semantics into representation vectors of road segments. The second stag
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;&#65292;&#20026;&#25856;&#23721;&#20581;&#36523;&#25151;&#25552;&#20379;&#20102;&#25913;&#21892;&#26381;&#21153;&#21644;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2211.02680</link><description>&lt;p&gt;
&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Climbing Routes Clustering Using Energy-Efficient Accelerometers Attached to the Quickdraws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.02680
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33410;&#33021;&#21152;&#36895;&#35745;&#23545;&#25856;&#23721;&#36335;&#32447;&#36827;&#34892;&#32858;&#31867;&#65292;&#20026;&#25856;&#23721;&#20581;&#36523;&#25151;&#25552;&#20379;&#20102;&#25913;&#21892;&#26381;&#21153;&#21644;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25856;&#23721;&#20581;&#36523;&#25151;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25214;&#20986;&#21463;&#27426;&#36814;&#30340;&#25856;&#30331;&#36335;&#32447;&#65292;&#20197;&#25913;&#21892;&#20182;&#20204;&#30340;&#26381;&#21153;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30828;&#20214;&#21407;&#22411;&#26469;&#25910;&#38598;&#25968;&#25454;&#65292;&#35813;&#21407;&#22411;&#20351;&#29992;&#38468;&#21152;&#21040;&#22681;&#22721;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#30340;&#21152;&#36895;&#35745;&#20256;&#24863;&#22120;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23558;&#25856;&#23721;&#32499;&#36830;&#25509;&#21040;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#65292;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#26102;&#38388;&#19978;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#22312;&#36229;&#20302;&#21151;&#32791;&#27169;&#24335;&#19979;&#27979;&#37327;&#30340;&#25968;&#25454;&#65292;&#26816;&#27979;&#20102;&#25856;&#30331;&#19981;&#21516;&#36335;&#32447;&#36807;&#31243;&#20013;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36335;&#32447;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.02680v2 Announce Type: replace-cross  Abstract: One of the challenges for climbing gyms is to find out popular routes for the climbers to improve their services and optimally use their infrastructure. This problem must be addressed preserving both the privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence becoming practical in terms of expenses and time consumption for replacement when used in large quantities in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in ultra-low power mode, detect patterns in data during climbing different routes, and develops an unsupervised approach for route clustering.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; FIRE&#65292;&#24341;&#20837;ImRE&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#65292;&#35299;&#20915;&#20102;RL&#26694;&#26550;&#22312;&#22788;&#29702;&#20598;&#21457;&#26381;&#21153;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2209.14399</link><description>&lt;p&gt;
FIRE&#65306;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge Computing Migrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.14399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; FIRE&#65292;&#24341;&#20837;ImRE&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#65292;&#35299;&#20915;&#20102;RL&#26694;&#26550;&#22312;&#22788;&#29702;&#20598;&#21457;&#26381;&#21153;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#65292;&#29992;&#25143;&#26381;&#21153;&#37197;&#32622;&#25991;&#20214;&#30001;&#20110;&#29992;&#25143;&#31227;&#21160;&#32780;&#36827;&#34892;&#36801;&#31227;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#36801;&#31227;&#65292;&#36890;&#24120;&#26159;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#24573;&#35270;&#20102;&#20598;&#21457;&#30340;&#26381;&#21153;&#22120;&#25925;&#38556;&#65292;&#23613;&#31649;&#32597;&#35265;&#65292;&#20294;&#20250;&#24433;&#21709;&#21040;&#20687;&#33258;&#21160;&#39550;&#39542;&#21644;&#23454;&#26102;&#38556;&#30861;&#26816;&#27979;&#31561;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#65288;&#32597;&#35265;&#20107;&#20214;&#65289;&#25925;&#38556;&#34429;&#28982;&#22312;&#21382;&#21490;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#20195;&#34920;&#65292;&#21364;&#23545;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;RL&#31639;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#30001;&#20110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35843;&#25972;&#25925;&#38556;&#39057;&#29575;&#36827;&#34892;&#35757;&#32451;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FIRE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ImRE&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;Q-learning&#31639;&#27861;&#65292;&#23427;&#26681;&#25454;&#32597;&#35265;&#20107;&#20214;&#23545;&#20540;&#20989;&#25968;&#30340;&#24433;&#21709;&#36827;&#34892;&#27604;&#20363;&#25277;&#26679;&#12290;FIRE&#32771;&#34385;&#20102;&#24310;&#36831;&#12289;&#36801;&#31227;&#12289;&#25925;&#38556;&#21644;&#22791;&#20221;pl
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.14399v2 Announce Type: replace-cross  Abstract: In edge computing, users' service profiles are migrated due to user mobility. Reinforcement learning (RL) frameworks have been proposed to do so, often trained on simulated data. However, existing RL frameworks overlook occasional server failures, which although rare, impact latency-sensitive applications like autonomous driving and real-time obstacle detection. Nevertheless, these failures (rare events), being not adequately represented in historical training data, pose a challenge for data-driven RL algorithms. As it is impractical to adjust failure frequency in real-world applications for training, we introduce FIRE, a framework that adapts to rare events by training a RL policy in an edge computing digital twin environment. We propose ImRE, an importance sampling-based Q-learning algorithm, which samples rare events proportionally to their impact on the value function. FIRE considers delay, migration, failure, and backup pl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26497;&#31471;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#37327;&#21270;&#32654;&#22269;&#26497;&#31471;&#26862;&#26519;&#28779;&#28798;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2208.07581</link><description>&lt;p&gt;
&#32654;&#22269;&#26497;&#31471;&#26862;&#26519;&#28779;&#28798;&#30340;&#26102;&#31354;&#22238;&#24402;&#24314;&#27169;&#65306;&#37096;&#20998;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Regression modelling of spatiotemporal extreme U.S. wildfires via partially-interpretable neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.07581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26497;&#31471;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#37327;&#21270;&#32654;&#22269;&#26497;&#31471;&#26862;&#26519;&#28779;&#28798;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29615;&#22659;&#32972;&#26223;&#19979;&#30340;&#39118;&#38505;&#31649;&#29702;&#38656;&#35201;&#29702;&#35299;&#39537;&#21160;&#26497;&#31471;&#20107;&#20214;&#30340;&#26426;&#21046;&#12290;&#29992;&#20110;&#37327;&#21270;&#36825;&#31181;&#39118;&#38505;&#30340;&#26377;&#29992;&#25351;&#26631;&#26159;&#21709;&#24212;&#21464;&#37327;&#30340;&#26497;&#31471;&#20998;&#20301;&#25968;&#65292;&#20854;&#26465;&#20214;&#26159;&#25551;&#36848;&#27668;&#20505;&#12289;&#29983;&#29289;&#22280;&#21644;&#29615;&#22659;&#29366;&#24577;&#31561;&#39044;&#27979;&#21464;&#37327;&#12290;&#20856;&#22411;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20998;&#20301;&#25968;&#20301;&#20110;&#21487;&#35266;&#27979;&#25968;&#25454;&#33539;&#22260;&#20043;&#22806;&#65292;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#34892;&#20272;&#35745;&#65292;&#38656;&#35201;&#22312;&#22238;&#24402;&#26694;&#26550;&#20869;&#25351;&#23450;&#21442;&#25968;&#26497;&#20540;&#27169;&#22411;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26497;&#31471;&#20998;&#20301;&#25968;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.07581v4 Announce Type: replace-cross  Abstract: Risk management in many environmental settings requires an understanding of the mechanisms that drive extreme events. Useful metrics for quantifying such risk are extreme quantiles of response variables conditioned on predictor variables that describe, e.g., climate, biosphere and environmental states. Typically these quantiles lie outside the range of observable data and so, for estimation, require specification of parametric extreme value models within a regression framework. Classical approaches in this context utilise linear or additive relationships between predictor and response variables and suffer in either their predictive capabilities or computational efficiency; moreover, their simplicity is unlikely to capture the truly complex structures that lead to the creation of extreme wildfires. In this paper, we propose a new methodological framework for performing extreme quantile regression using artificial neutral network
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2109.14200</link><description>&lt;p&gt;
&#25163;&#26426;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#30340;&#21103;&#20135;&#21697;&#32780;&#20986;&#29616;&#65311;-- &#19968;&#39033;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.14200
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#36328;&#24773;&#22659;&#35270;&#21548;&#23398;&#20064;&#65292;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#33021;&#21542;&#20316;&#20026;&#21103;&#20135;&#21697;&#20986;&#29616;&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#24418;&#24335;&#34920;&#24449;&#38388;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#24180;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#23398;&#20064;&#23156;&#20799;&#22914;&#20309;&#23398;&#20250;&#21306;&#20998;&#35821;&#38899;&#65292;&#20998;&#21106;&#21333;&#35789;&#65292;&#20197;&#21450;&#23558;&#21333;&#35789;&#19982;&#20854;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#30340;&#36880;&#28176;&#21457;&#23637;&#26159;&#27595;&#24248;&#32622;&#30097;&#30340;&#65292;&#20294;&#36825;&#20123;&#25216;&#33021;&#30340;&#30830;&#20999;&#24615;&#36136;&#21644;&#28508;&#22312;&#30340;&#24515;&#29702;&#34920;&#24449;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35745;&#31639;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#35821;&#38899;&#21644;&#21516;&#26102;&#20855;&#26377;&#25351;&#31216;&#19981;&#26126;&#30830;&#30340;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22522;&#26412;&#35821;&#38899;&#30340;&#29702;&#35299;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#27809;&#26377;&#35832;&#22914;&#35821;&#35328;&#21333;&#20301;&#30340;&#34920;&#24449;&#65292;&#20197;&#21450;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#21333;&#20301;&#30340;&#23398;&#20064;&#26426;&#21046;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#65292;&#31867;&#20284;&#38899;&#32032;&#12289;&#38899;&#33410;&#21644;&#21333;&#35789;&#30340;&#35821;&#35328;&#21333;&#20301;&#30340;&#30693;&#35782;&#23454;&#38469;&#19978;&#33021;&#22815;&#20316;&#20026;&#28508;&#22312;&#34920;&#24449;&#20986;&#29616;&#65292;&#25903;&#25345;&#35821;&#38899;&#19982;&#20854;&#20182;&#24418;&#24335;&#34920;&#24449;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#30340;&#23398;&#20064;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and withou
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#21452;Lipschitz&#27491;&#35268;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#38590;&#20197;&#36924;&#36817;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32473;&#20986;&#19979;&#30028;&#21051;&#30011;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#28508;&#22312;&#20998;&#24067;&#31561;&#28508;&#22312;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2107.07232</link><description>&lt;p&gt;
&#35770;&#21452;Lipschitz&#27491;&#35268;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the expressivity of bi-Lipschitz normalizing flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.07232
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#21452;Lipschitz&#27491;&#35268;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#38590;&#20197;&#36924;&#36817;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32473;&#20986;&#19979;&#30028;&#21051;&#30011;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#28508;&#22312;&#20998;&#24067;&#31561;&#28508;&#22312;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21487;&#36870;&#20989;&#25968;&#33509;&#20854;&#33258;&#36523;&#21644;&#20854;&#36870;&#20989;&#25968;&#22343;&#20855;&#26377;&#26377;&#30028;Lipschitz&#24120;&#25968;&#65292;&#21017;&#31216;&#20043;&#20026;&#21452;Lipschitz&#20989;&#25968;&#12290; &#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#27491;&#35268;&#21270;&#27969;&#36890;&#36807;&#35774;&#35745;&#25110;&#35757;&#32451;&#32780;&#25104;&#20026;&#21452;Lipschitz&#65292;&#20197;&#38480;&#21046;&#25968;&#20540;&#35823;&#24046; (&#31561;&#31561;)&#12290; &#26412;&#25991;&#35752;&#35770;&#20102;&#21452;Lipschitz&#27491;&#35268;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38590;&#20197;&#29992;&#36825;&#31181;&#27169;&#22411;&#36924;&#36817;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290; &#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32473;&#20986;&#33509;&#24178;&#29305;&#21035;&#19981;&#21033;&#20998;&#24067;&#19982;&#20854;&#26368;&#20339;&#36924;&#36817;&#20043;&#38388;&#30340;&#24635;&#21464;&#20998;&#36317;&#31163;&#30340;&#33509;&#24178;&#19979;&#30028;&#65292;&#21051;&#30011;&#20102;&#21452;Lipschitz&#27491;&#35268;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#28508;&#22312;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.07232v3 Announce Type: replace  Abstract: An invertible function is bi-Lipschitz if both the function and its inverse have bounded Lipschitz constants. Nowadays, most Normalizing Flows are bi-Lipschitz by design or by training to limit numerical errors (among other things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing Flows and identify several target distributions that are difficult to approximate using such models. Then, we characterize the expressivity of bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total Variation distance between these particularly unfavorable distributions and their best possible approximation. Finally, we discuss potential remedies which include using more complex latent distributions.
&lt;/p&gt;</description></item><item><title>&#20004;&#31181;&#26041;&#27861;&#22312;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#37325;&#35201;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/1902.06931</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the consistency of supervised learning with missing values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1902.06931
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#26041;&#27861;&#22312;&#24102;&#32570;&#22833;&#20540;&#30340;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#37325;&#35201;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#35774;&#32622;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#32570;&#22833;&#20540;&#65292;&#36825;&#20351;&#24471;&#20998;&#26512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20016;&#23500;&#30340;&#25991;&#29486;&#28041;&#21450;&#32570;&#22833;&#20540;&#22312;&#25512;&#26029;&#26694;&#26550;&#20013;&#30340;&#22788;&#29702;&#65306;&#20174;&#19981;&#23436;&#25972;&#30340;&#34920;&#20013;&#20272;&#35745;&#21442;&#25968;&#21450;&#20854;&#26041;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65306;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20013;&#20986;&#29616;&#32570;&#22833;&#20540;&#26102;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#20102;&#20004;&#31181;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#26159;&#65292;&#24403;&#32570;&#22833;&#20540;&#19981;&#20855;&#20449;&#24687;&#24615;&#26102;&#65292;&#20351;&#29992;&#24120;&#25968;&#36827;&#34892;&#25554;&#34917;&#65292;&#20363;&#22914;&#22312;&#23398;&#20064;&#20043;&#21069;&#20351;&#29992;&#22343;&#20540;&#65292;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#19982;&#25512;&#26029;&#35774;&#32622;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#25512;&#26029;&#35774;&#32622;&#20013;&#24120;&#29992;&#30340;&#22343;&#20540;&#25554;&#34917;&#26041;&#27861;&#34987;&#25351;&#36131;&#25197;&#26354;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#36825;&#26679;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#33021;&#22815;&#20445;&#25345;&#19968;&#33268;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36866;&#29992;&#20110;&#23436;&#25972;&#35266;&#27979;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#36890;&#36807;&#22810;&#37325;&#25554;&#34917;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#19978;&#36827;&#34892;&#26368;&#20339;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#27604;&#36739;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
arXiv:1902.06931v4 Announce Type: replace-cross  Abstract: In many application settings, the data have missing entries which make analysis challenging. An abundant literature addresses missing values in an inferential framework: estimating parameters and their variance from incomplete tables. Here, we consider supervised-learning settings: predicting a target when missing values appear in both training and testing data. We show the consistency of two approaches in prediction. A striking result is that the widely-used method of imputing with a constant, such as the mean prior to learning is consistent when missing values are not informative. This contrasts with inferential settings where mean imputation is pointed at for distorting the distribution of the data. That such a simple approach can be consistent is important in practice. We also show that a predictor suited for complete observations can predict optimally on incomplete data,through multiple imputation.Finally, to compare imput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.14512</link><description>&lt;p&gt;
&#25105;&#20204;&#38169;&#36807;&#20102;&#35841;&#65311;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#25581;&#31034;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#22312;&#29702;&#35299;&#22240;&#26524;&#25928;&#24212;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#23558;&#25512;&#35770;&#25193;&#23637;&#21040;&#30446;&#26631;&#20154;&#32676;&#26102;&#38754;&#20020;&#25928;&#24212;&#24322;&#36136;&#24615;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#35782;&#21035;&#21644;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30446;&#26631;&#20154;&#32676;&#20197;&#25552;&#21319;&#26222;&#36866;&#24615;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#8212;&#8212;Rashomon Set of Optimal Trees (ROOT)&#65292;&#26469;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#12290;ROOT&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#30830;&#20445;&#26356;&#31934;&#30830;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ROOT&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#27807;&#36890;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#23637;&#29616;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#22270;&#25193;&#22686;&#21644;&#35774;&#35745;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.12564</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30475;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Invariant Learning from the Causal Perspective. (arXiv:2401.12564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22270;&#23545;&#27604;&#19981;&#21464;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35889;&#22270;&#25193;&#22686;&#21644;&#35774;&#35745;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#25193;&#22686;&#22270;&#26469;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;GCL&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29702;&#35299;&#22312;&#23454;&#36341;&#20013;&#24635;&#26159;&#27491;&#30830;&#30340;&#21527;&#65311;&#26412;&#25991;&#39318;&#20808;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;GCL&#12290;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20998;&#26512;GCL&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;GCL&#30001;&#20110;&#22270;&#20013;&#21253;&#21547;&#30340;&#38750;&#22240;&#26524;&#20449;&#24687;&#65292;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;&#37027;&#20040;&#65292;&#25105;&#20204;&#22914;&#20309;&#20462;&#22797;&#24182;&#20419;&#20351;&#24403;&#21069;&#30340;GCL&#23398;&#20064;&#26356;&#22909;&#30340;&#19981;&#21464;&#34920;&#31034;&#21602;&#65311;SCM&#25552;&#20379;&#20102;&#20004;&#20010;&#35201;&#27714;&#24182;&#28608;&#21169;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GCL&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35889;&#22270;&#25193;&#22686;&#26469;&#27169;&#25311;&#23545;&#38750;&#22240;&#26524;&#22240;&#32032;&#30340;&#24178;&#39044;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21464;&#24615;&#30446;&#26631;&#21644;&#29420;&#31435;&#24615;&#30446;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#22240;&#26524;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;i&#65289;&#19981;&#21464;&#24615;&#30446;&#26631;&#40723;&#21169;e
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL), learning the node representation by contrasting two augmented graphs in a self-supervised way, has attracted considerable attention. GCL is usually believed to learn the invariant representation. However, does this understanding always hold in practice? In this paper, we first study GCL from the perspective of causality. By analyzing GCL with the structural causal model (SCM), we discover that traditional GCL may not well learn the invariant representations due to the non-causal information contained in the graph. How can we fix it and encourage the current GCL to learn better invariant representations? The SCM offers two requirements and motives us to propose a novel GCL method. Particularly, we introduce the spectral graph augmentation to simulate the intervention upon non-causal factors. Then we design the invariance objective and independence objective to better capture the causal factors. Specifically, (i) the invariance objective encourages the e
&lt;/p&gt;</description></item><item><title>AutoFT&#26159;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;OOD&#25968;&#25454;&#19978;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10220</link><description>&lt;p&gt;
AutoFT: &#36890;&#36807;&#20248;&#21270;OOD&#25968;&#25454;&#19978;&#30340;&#36229;&#21442;&#25968;&#23454;&#29616;&#40065;&#26834;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10220
&lt;/p&gt;
&lt;p&gt;
AutoFT&#26159;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#36229;&#21442;&#25968;&#22312;OOD&#25968;&#25454;&#19978;&#36827;&#34892;&#22522;&#30784;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#36866;&#24212;&#25152;&#38656;&#20219;&#21153;&#65292;&#20294;&#26159;&#22312;&#19968;&#31181;&#29305;&#23450;&#25968;&#25454;&#20998;&#24067;&#19978;&#24494;&#35843;&#27169;&#22411;&#24448;&#24448;&#20250;&#25439;&#23475;&#27169;&#22411;&#22312;&#20854;&#20182;&#20998;&#24067;&#19978;&#30340;&#21407;&#22987;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#21033;&#29992;&#25163;&#24037;&#21046;&#23450;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#32422;&#26463;&#24494;&#35843;&#36807;&#31243;&#65292;&#20197;&#20445;&#30041;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#20934;&#30830;&#25351;&#23450;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24212;&#20445;&#30041;&#21738;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#36825;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#24341;&#23548;&#22522;&#30784;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;AutoFT&#36890;&#36807;&#20248;&#21270;&#24494;&#35843;&#36229;&#21442;&#25968;&#26469;&#22312;&#23567;&#30340;ODD&#39564;&#35777;&#38598;&#19978;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#25351;&#23548;&#24494;&#35843;&#65292;AutoFT&#25628;&#32034;&#19968;&#20010;&#39640;&#24230;&#34920;&#36798;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.00110</link><description>&lt;p&gt;
&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#30495;&#23454;&#26679;&#26412;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#19981;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20381;&#38752;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26469;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#28982;&#32780;&#20854;&#24778;&#20154;&#30340;&#25928;&#26524;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#28304;&#33258;&#20854;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#24863;&#30693;&#25351;&#23548;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;&#25193;&#25955;&#35757;&#32451;&#20013;&#21152;&#20837;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#12290;&#30001;&#20110;&#25193;&#25955;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#20998;&#25968;&#21305;&#37197;&#30446;&#26631;&#19982;&#26080;&#30417;&#30563;&#35757;&#32451;&#24863;&#30693;&#32593;&#32476;&#26102;&#20351;&#29992;&#30340;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#30446;&#26631;&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#27492;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#24863;&#30693;&#32593;&#32476;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#24863;&#30693;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#24863;&#30693;&#30446;&#26631;&#65292;&#20854;&#32467;&#26524;&#26159;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#30495;&#23454;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#26465;&#20214;&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#25913;&#21892;&#26679;&#26412;&#36136;&#37327;&#65292;&#32780;&#19981;&#19982;&#26465;&#20214;&#32465;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
&lt;/p&gt;</description></item><item><title>VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>http://arxiv.org/abs/2310.18191</link><description>&lt;p&gt;
&#32553;&#25918;&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#21542;&#20540;&#24471;&#65311;&#35780;&#20272; VeLO &#30340; 4000 &#20010; TPU &#26376;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months. (arXiv:2310.18191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18191
&lt;/p&gt;
&lt;p&gt;
VeLO&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#35780;&#20272;&#21457;&#29616;&#23427;&#38656;&#35201;&#38382;&#39064;&#29305;&#23450;&#30340;&#35843;&#20248;&#65292;&#24182;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#35757;&#32451;&#35823;&#24046;&#38477;&#20302;&#36895;&#24230;&#65292;&#36825;&#23545;&#20110;VeLO&#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102; VeLO&#65288;&#19975;&#33021;&#23398;&#20064;&#20248;&#21270;&#22120;&#65289;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#35757;&#32451;&#36890;&#29992;&#8220;&#22522;&#30784;&#8221;&#20248;&#21270;&#22120;&#30340;&#23581;&#35797;&#12290;VeLO &#20351;&#29992;&#36229;&#36807; 4000 &#20010; TPU &#26376;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#30446;&#26631;&#26159;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#38382;&#39064;&#24182;&#19988;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#36229;&#36807; Adam &#31561;&#34892;&#19994;&#26631;&#20934;&#30340;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#23545; MLCommons &#20248;&#21270;&#22120;&#22522;&#20934;&#22871;&#20214;&#29420;&#31435;&#35780;&#20272;&#20102; VeLO&#12290;&#25105;&#20204;&#21457;&#29616;&#19982;&#21021;&#27493;&#22768;&#26126;&#30456;&#21453;&#65306;&#65288;1&#65289;VeLO&#26377;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#38382;&#39064;&#36827;&#34892;&#35843;&#25972;&#65292;&#65288;2&#65289;VeLO&#22312;&#25214;&#21040;&#30340;&#35299;&#30340;&#36136;&#37327;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#65292;&#65288;3&#65289;VeLO&#22312;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#19978;&#24182;&#19981;&#27604;&#31454;&#20105;&#20248;&#21270;&#22120;&#26356;&#24555;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#23545; VeLO &#30340;&#36890;&#29992;&#24615;&#21644;&#22521;&#35757;&#25237;&#36164;&#30340;&#20215;&#20540;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08775</link><description>&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27844;&#28431;&#65306;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#20010;&#20154;&#25110;&#23478;&#24237;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#26159;&#21542;&#20250;&#25644;&#36801;&#65292;&#21363;&#36801;&#31227;&#20542;&#21521;&#20998;&#31867;&#22120;&#12290;&#25915;&#20987;&#20551;&#35774;&#25915;&#20987;&#32773;&#21487;&#20197;&#26597;&#35810;&#27169;&#22411;&#20197;&#33719;&#21462;&#39044;&#27979;&#65292;&#24182;&#19988;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;&#25915;&#20987;&#36824;&#20551;&#35774;&#25915;&#20987;&#32773;&#24050;&#32463;&#33719;&#21462;&#20102;&#19968;&#23450;&#25968;&#37327;&#30446;&#26631;&#20010;&#20307;&#30340;&#38750;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25915;&#20987;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#36825;&#20123;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#23545;&#25915;&#20987;&#32773;&#25104;&#21151;&#25512;&#26029;&#25935;&#24863;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21644;&#21608;&#22260;&#23545;&#40784;&#30340;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2310.07229</link><description>&lt;p&gt;
&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21608;&#22260;&#23545;&#40784;&#23454;&#29616;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment. (arXiv:2310.07229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#34507;&#30333;&#36136;&#29255;&#27573;&#21644;&#21608;&#22260;&#23545;&#40784;&#30340;&#33258;&#30417;&#30563;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#34955;&#34920;&#31034;&#22312;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22914;&#33647;&#29289;&#21487;&#35064;&#38706;&#24615;&#20272;&#35745;&#12289;&#37197;&#20307;&#20146;&#21644;&#21147;&#39044;&#27979;&#21644;&#26032;&#33647;&#35774;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20960;&#20309;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36890;&#24120;&#23558;&#21475;&#34955;&#29420;&#31435;&#20110;&#37197;&#20307;&#22788;&#29702;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21475;&#34955;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20511;&#21161;&#39640;&#20998;&#36776;&#29575;&#21407;&#23376;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#65292;&#24182;&#36741;&#20197;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#23567;&#20998;&#23376;&#34920;&#31034;&#12290;&#23558;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#27573;&#20026;&#31867;&#20284;&#33647;&#29289;&#30340;&#29255;&#27573;&#21450;&#20854;&#30456;&#24212;&#30340;&#21475;&#34955;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#37197;&#20307;-&#21463;&#20307;&#30456;&#20114;&#20316;&#29992;&#27169;&#25311;&#65292;&#20174;&#32780;&#29983;&#25104;&#20102;&#36229;&#36807;500&#19975;&#20010;&#22797;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the PDB database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#20869;&#23481;&#23457;&#26597;&#30340;&#31169;&#19979;&#37096;&#32626;&#65292;&#35752;&#35770;&#20102;&#24341;&#20837;&#21407;&#22240;&#30340;&#24494;&#35843;&#36807;&#31243;&#21644;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#30340;&#21306;&#21035;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03400</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20869;&#23481;&#23457;&#26597;&#65306;&#25968;&#25454;&#24037;&#31243;&#21644;&#30417;&#30563;&#24494;&#35843;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#23454;&#29616;&#20869;&#23481;&#23457;&#26597;&#30340;&#31169;&#19979;&#37096;&#32626;&#65292;&#35752;&#35770;&#20102;&#24341;&#20837;&#21407;&#22240;&#30340;&#24494;&#35843;&#36807;&#31243;&#21644;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#30340;&#21306;&#21035;&#65292;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#24494;&#35843;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#25968;&#21313;&#20159;&#20154;&#27599;&#22825;&#22312;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#27807;&#36890;&#24182;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24182;&#38750;&#25152;&#26377;&#36825;&#20123;&#34920;&#36798;&#37117;&#21451;&#22909;&#25110;&#21512;&#35268;&#65292;&#36825;&#20351;&#24471;&#20869;&#23481;&#23457;&#26597;&#25104;&#20026;&#19968;&#39033;&#19981;&#21487;&#25110;&#32570;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#21457;&#23637;&#65292;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#24050;&#25104;&#20026;&#22788;&#29702;&#21508;&#20010;&#39046;&#22495;&#20219;&#21153;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20869;&#23481;&#23457;&#26597;&#39046;&#22495;&#65292;&#20173;&#32570;&#20047;&#35814;&#32454;&#30340;&#24037;&#20316;&#31995;&#32479;&#22320;&#20171;&#32461;&#23454;&#26045;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20415;&#21487;&#20197;&#31169;&#19979;&#37096;&#32626;&#29992;&#20110;&#20869;&#23481;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26159;&#21542;&#24212;&#35813;&#24341;&#20837;&#21407;&#22240;&#65292;&#20197;&#21450;&#23558;&#20854;&#35270;&#20026;&#30452;&#25509;&#20998;&#31867;&#20219;&#21153;&#26159;&#21542;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22240;&#23545;&#31169;&#19979;&#37096;&#32626;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#22312;&#22238;&#31572;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#21516;&#22788;&#29702;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#21644;&#25913;&#36827;&#20102;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;&#65292;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#21508;&#32452;&#20214;&#22312;&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#65292;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#20197;&#25913;&#36827;&#26368;&#20248;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;</title><link>http://arxiv.org/abs/2310.02611</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#25913;&#36827;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Analyzing and Improving OT-based Adversarial Networks. (arXiv:2310.02611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#21644;&#25913;&#36827;&#20102;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#32593;&#32476;&#65292;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#23637;&#31034;&#20102;&#21508;&#32452;&#20214;&#22312;&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#65292;&#26368;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#20197;&#25913;&#36827;&#26368;&#20248;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#36755;&#36816;&#26041;&#26696;&#65292;&#23427;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#30340;&#21516;&#26102;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#12290;OT&#29702;&#35770;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21021;&#65292;OT&#36317;&#31163;&#34987;&#29992;&#20316;&#35780;&#20272;&#25968;&#25454;&#21644;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#26368;&#36817;&#65292;OT&#20256;&#36755;&#26144;&#23556;&#22312;&#25968;&#25454;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#34987;&#29992;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#20123;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#30340;&#23545;&#25239;&#35757;&#32451;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#32479;&#19968;&#20102;&#36825;&#20123;&#22522;&#20110;OT&#30340;&#23545;&#25239;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#32452;&#20214;&#22312;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#22522;&#20110;OT&#30340;&#27169;&#22411;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#35843;&#25972;&#29983;&#25104;&#20998;&#24067;&#65292;&#36880;&#28176;&#20351;&#20854;&#19982;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#28176;&#36827;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.02396</link><description>&lt;p&gt;
&#29992;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization of multi-task learning and finetuning in overparameterized neural networks. (arXiv:2310.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#24102;&#26469;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#22312;&#31616;&#21270;&#30340;&#32447;&#24615;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24494;&#35843;&#25152;&#23545;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#29305;&#23450;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#40723;&#21169;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#24494;&#35843;&#36807;&#31243;&#21516;&#26102;&#20855;&#26377;&#20869;&#26680;&#21644;&#29305;&#24449;&#23398;&#20064;&#30340;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#34892;&#20026;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#35757;&#32451;&#36741;&#21161;&#20219;&#21153;&#30340;&#26041;&#27861;&#26469;&#26399;&#26395;&#23398;&#20064;&#21487;&#20197;&#37096;&#20998;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#36741;&#21161;&#20219;&#21153;&#25152;&#20135;&#29983;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#21253;&#25324;&#21516;&#26102;&#23398;&#20064;&#65288;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;MTL&#65289;&#21644;&#20381;&#24207;&#23398;&#20064;&#65288;&#39044;&#35757;&#32451;&#21644;&#38543;&#21518;&#24494;&#35843;&#65292;PT+FT&#65289;&#12290;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35757;&#32451;&#20004;&#23618;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19982;MTL&#21644;PT+FT&#30456;&#20851;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#24809;&#32602;&#65292;&#20004;&#32773;&#37117;&#40723;&#21169;&#20219;&#21153;&#20043;&#38388;&#30340;&#29305;&#24449;&#20849;&#20139;&#21644;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#31232;&#30095;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#32593;&#32476;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#30830;&#23450;&#30340;&#20869;&#26680;&#65288;&#25110;&#8220;&#24816;&#24615;&#8221;&#65289;&#29366;&#24577;&#21644;&#29305;&#24449;&#23398;&#20064;&#65288;&#8220;&#20016;&#23500;&#8221;&#65289;&#29366;&#24577;&#20043;&#38388;&#20855;&#26377;&#28151;&#21512;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;PT+FT&#36824;&#21487;&#20197;&#23637;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23884;&#22871;&#29305;&#24449;&#23398;&#20064;&#8221;&#34892;&#20026;&#65292;&#35813;&#34892;&#20026;&#26080;&#27861;&#34987;&#20219;&#20309;&#29366;&#24577;&#25152;&#25429;&#25417;&#65292;&#20351;&#20854;&#20559;&#21521;&#20110;&#25552;&#21462;&#19968;&#32452;&#31232;&#30095;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common in deep learning to train networks on auxiliary tasks with the expectation that the learning will transfer, at least partially, to another task of interest. In this work, we investigate the inductive biases that result from learning auxiliary tasks, either simultaneously (multi-task learning, MTL) or sequentially (pretraining and subsequent finetuning, PT+FT). In the simplified setting of two-layer diagonal linear networks trained with gradient descent, we identify implicit regularization penalties associated with MTL and PT+FT, both of which incentivize feature sharing between tasks and sparsity in learned task-specific features. Notably, our results imply that during finetuning, networks operate in a hybrid of the kernel (or "lazy") regime and the feature learning ("rich") regime identified in prior work. Moreover, PT+FT can exhibit a novel "nested feature learning" behavior not captured by either regime, which biases it to extract a sparse subset of the features learned
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01678</link><description>&lt;p&gt;
Score dynamics: &#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#30382;&#31186;&#26102;&#38388;&#27493;&#25552;&#39640;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Score dynamics (SD) &#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#28436;&#21270;&#31639;&#23376;&#65292;&#29992;&#20110;&#21407;&#23376;&#32423;&#21644;&#31895;&#31890;&#21270;&#21160;&#21147;&#23398;&#12290;SD&#20197;&#20998;&#25968;&#20026;&#20013;&#24515;&#65292;&#21363;&#19982;&#21160;&#24577;&#33258;&#30001;&#24230;&#30340;&#36716;&#25442;&#23545;&#25968;&#27010;&#29575;&#23548;&#25968;&#30456;&#20851;&#30340;&#37327;&#12290;&#21518;&#32773;&#22312;&#20998;&#25968;&#26102;&#38388;&#27493;&#20013;&#36215;&#21040;&#19982;MD&#20013;&#21147;&#22330;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#21464;&#37327;&#30340;&#31163;&#25955;&#36716;&#21464;&#12290;&#36825;&#31181;&#26102;&#38388;&#27493;&#38271;&#21487;&#20197;&#27604;&#20856;&#22411;&#30340;MD&#26102;&#38388;&#27493;&#38271;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#65292;&#29992;&#20110;&#28436;&#21270;&#20197;1~ps&#26102;&#38388;&#27493;&#38271;&#30340;&#29616;&#23454;&#20998;&#23376;&#20307;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#27700;&#28342;&#28082;&#20013;&#30340;&#30701;&#38142;&#28919;&#28867;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;Score dynamics&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#20174;&#26465;&#20214;&#27010;&#29575;&#30340;&#24179;&#31283;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#30340;&#24179;&#34913;&#39044;&#27979;&#21644;&#23545;&#36716;&#25442;&#36895;&#29575;&#21644;&#36716;&#25442;&#30340;&#21160;&#21147;&#23398;&#39044;&#27979;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
&lt;/p&gt;</description></item><item><title>RECOMBINER&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#12289;&#22686;&#21152;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#21106;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#26469;&#35299;&#20915;COMBINER&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17182</link><description>&lt;p&gt;
RECOMBINER&#65306;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17182
&lt;/p&gt;
&lt;p&gt;
RECOMBINER&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#21644;&#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#12289;&#22686;&#21152;&#20301;&#32622;&#32534;&#30721;&#21644;&#20998;&#21106;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#26469;&#35299;&#20915;COMBINER&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COMBINER&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#20808;&#21069;&#22522;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#30340;&#20851;&#38190;&#25928;&#29575;&#38382;&#39064;&#65306;&#36991;&#20813;&#20102;&#37327;&#21270;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#36895;&#29575;-&#22833;&#30495;&#24615;&#33021;&#30340;&#30452;&#25509;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;COMBINER&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#65306;1&#65289;&#20351;&#29992;&#22240;&#24335;&#21270;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#36924;&#36817;&#65292;&#32570;&#20047;&#28789;&#27963;&#24615;&#65307;2&#65289;&#19981;&#33021;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#20559;&#31163;&#20840;&#23616;&#27169;&#24335;&#65307;3&#65289;&#20854;&#24615;&#33021;&#26131;&#21463;&#24314;&#27169;&#36873;&#25321;&#21644;&#21464;&#20998;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#40065;&#26834;&#21644;&#22686;&#24378;&#30340;COMBINER(RECOMBINER)&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65306;1&#65289;&#36890;&#36807;&#23545;INR&#26435;&#37325;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#20016;&#23500;&#21464;&#20998;&#36924;&#36817;&#65292;&#24182;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#65307;2&#65289;&#36890;&#36807;&#22686;&#21152;&#21487;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;INR&#65292;&#20351;&#20854;&#36866;&#24212;&#23616;&#37096;&#32454;&#33410;&#65307;3&#65289;&#23558;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#20998;&#21106;&#25104;...
&lt;/p&gt;
&lt;p&gt;
COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16347</link><description>&lt;p&gt;
&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20869;&#22312;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#38754;&#20020;&#22256;&#22659;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#20247;&#22810;&#19981;&#21516;&#24207;&#21015;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;IGE-LLMs&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#25361;&#25112;&#30340;&#29615;&#22659;&#21644;&#19968;&#20010;&#21516;&#26102;&#38754;&#20020;&#25506;&#32034;&#21644;&#38271;&#35270;&#31243;&#25361;&#25112;&#30340;&#22797;&#26434;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#20869;&#22312;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;IGE-LLMs(i)&#22312;&#30456;&#20851;&#30340;&#20869;&#22312;&#26041;&#27861;&#21644;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#30340;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36739;&#39640;&#27700;&#24179;&#65292;(ii)&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#21644;&#20114;&#34917;&#65292;&#31361;&#20986;&#20854;&#27169;&#22359;&#21270;&#24615;&#33021;&#65292;(iii)&#23545;&#20110;&#19981;&#21516;&#30340;&#20869;&#22312;&#32553;&#25918;&#21442;&#25968;&#27604;&#36739;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
&lt;/p&gt;</description></item><item><title>PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12708</link><description>&lt;p&gt;
PointSSC&#65306;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion. (arXiv:2309.12708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12708
&lt;/p&gt;
&lt;p&gt;
PointSSC&#26159;&#31532;&#19968;&#20010;&#20026;&#20102;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#32780;&#24341;&#20837;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;&#65292;&#20855;&#22791;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#36974;&#25377;&#12290;&#36890;&#36807;&#20351;&#29992;Segment Anything&#36827;&#34892;&#33258;&#21160;&#21270;&#27880;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#30340;&#21512;&#20316;&#27169;&#22359;&#65292;&#26469;&#25512;&#21160;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#26088;&#22312;&#20026;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#29983;&#25104;&#31354;&#38388;&#21344;&#29992;&#21644;&#35821;&#20041;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#27169;&#22411;&#37117;&#38598;&#20013;&#22312;&#20307;&#32032;&#34920;&#31034;&#19978;&#65292;&#23545;&#20110;&#22823;&#22411;&#23460;&#22806;&#31354;&#38388;&#26469;&#35828;&#23384;&#22312;&#20869;&#23384;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#28857;&#20113;&#25552;&#20379;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#22522;&#20934;&#32570;&#20047;&#24102;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;&#23460;&#22806;&#28857;&#20113;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22330;&#26223;&#34917;&#20840;&#30340;&#36710;&#36742;&#22522;&#30784;&#35774;&#26045;&#28857;&#20113;&#21512;&#20316;&#22522;&#20934;PointSSC&#12290;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#38271;&#36317;&#31163;&#24863;&#30693;&#21644;&#26368;&#23567;&#30340;&#36974;&#25377;&#12290;&#25105;&#20204;&#21033;&#29992;Segment Anything&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#27880;&#37322;&#27969;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35780;&#20272;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#31354;&#38388;&#24863;&#30693;&#21464;&#25442;&#22120;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#21450;&#19968;&#20010;&#34917;&#20840;&#21644;&#20998;&#21106;&#21512;&#20316;&#27169;&#22359;&#29992;&#20110;&#32852;&#21512;&#34917;&#20840;&#21644;&#20998;&#21106;&#12290;PointSSC&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25512;&#21160;&#20102;&#35821;&#20041;&#28857;&#20113;&#34917;&#20840;&#22312;&#30495;&#23454;&#19990;&#30028;&#23548;&#33322;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06692</link><description>&lt;p&gt;
&#35299;&#20915;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#30340;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization. (arXiv:2309.06692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;FedGH&#65292;&#36890;&#36807;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#65292;FedGH&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#20174;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#21463;&#21040;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21644;&#35774;&#22791;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#30340;&#26799;&#24230;&#20914;&#31361;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26799;&#24230;&#20914;&#31361;&#29616;&#35937;&#65292;&#24182;&#25581;&#31034;&#20102;&#26356;&#24378;&#30340;&#24322;&#26500;&#24615;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#26799;&#24230;&#20914;&#31361;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGH&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#21327;&#35843;&#26469;&#20943;&#36731;&#26412;&#22320;&#28418;&#31227;&#12290;&#36825;&#31181;&#25216;&#26415;&#23558;&#19968;&#20010;&#26799;&#24230;&#21521;&#37327;&#25237;&#24433;&#21040;&#19982;&#20854;&#20182;&#20914;&#31361;&#23458;&#25143;&#31471;&#23545;&#20043;&#38388;&#30340;&#27491;&#20132;&#24179;&#38754;&#19978;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FedGH&#22312;&#19981;&#21516;&#22522;&#20934;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22987;&#32456;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FedGH&#22312;&#29305;&#23450;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#21033;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#23545;&#31216;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.04475</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#31561;&#21464;&#25193;&#25955;&#36827;&#34892;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crystal Structure Prediction by Joint Equivariant Diffusion. (arXiv:2309.04475v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#21033;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#23545;&#31216;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#22312;&#21508;&#20010;&#31185;&#23398;&#23398;&#31185;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#30446;&#21069;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#26230;&#20307;&#32467;&#26500;&#30340;&#23545;&#31216;&#20960;&#20309;&#29305;&#24615;&#65288;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21608;&#26399;&#24615;&#30340;&#19981;&#21464;&#24615;&#65289;&#65292;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#34701;&#20837;&#20197;&#19978;&#23545;&#31216;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#65292;&#32852;&#21512;&#29983;&#25104;&#27599;&#20010;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#26230;&#20307;&#20960;&#20309;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#30456;&#20851;&#30340;&#31561;&#21464;&#29983;&#25104;&#26041;&#27861;&#19981;&#21516;&#65292;DiffCSP&#21033;&#29992;&#20998;&#25968;&#22352;&#26631;&#32780;&#19981;&#26159;&#31515;&#21345;&#23572;&#22352;&#26631;&#26469;&#34920;&#31034;&#26230;&#20307;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#21407;&#23376;&#20301;&#32622;&#30340;&#25193;&#25955;&#21644;&#29983;&#25104;&#36807;&#31243;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;DiffCSP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (e.g. diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures -- the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP significantly outperforms existing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.14785</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#19982;&#27425;&#35201;&#36873;&#39033;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
A correlation-based fuzzy cluster validity index with secondary options detector. (arXiv:2308.14785v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#32771;&#34385;&#20102;&#22312;&#32858;&#31867;&#25968;&#37327;&#36873;&#25321;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#22810;&#20010;&#36873;&#39033;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#32858;&#31867;&#20998;&#26512;&#26102;&#65292;&#26368;&#20339;&#32858;&#31867;&#25968;&#37327;&#26159;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#24050;&#32463;&#24341;&#20837;&#20102;&#22810;&#20010;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#26377;&#22810;&#20010;&#36873;&#39033;&#21487;&#20197;&#20316;&#20026;&#26368;&#32456;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36825;&#20010;&#39046;&#22495;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#27169;&#31946;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631;&#65292;&#31216;&#20026;Wiroonsri-Preedasawakul&#65288;WP&#65289;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#26681;&#25454;&#19968;&#23545;&#25968;&#25454;&#28857;&#30340;&#23454;&#38469;&#36317;&#31163;&#19982;&#30456;&#24212;&#23545;&#30340;&#35843;&#25972;&#36136;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23450;&#20041;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;Xie-Beni&#65292;Pakhira-Bandyopadhyay-Maulik&#65292;Tang&#65292;Wu-Li&#65292;&#24191;&#20041;C&#21644;Kwon2&#31561;&#20960;&#20010;&#29616;&#26377;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#20154;&#24037;&#25968;&#25454;&#38598;&#65292;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#24102;&#26377;&#31561;&#32423;&#30340;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#27169;&#31946;c-mea&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal number of clusters is one of the main concerns when applying cluster analysis. Several cluster validity indexes have been introduced to address this problem. However, in some situations, there is more than one option that can be chosen as the final number of clusters. This aspect has been overlooked by most of the existing works in this area. In this study, we introduce a correlation-based fuzzy cluster validity index known as the Wiroonsri-Preedasawakul (WP) index. This index is defined based on the correlation between the actual distance between a pair of data points and the distance between adjusted centroids with respect to that pair. We evaluate and compare the performance of our index with several existing indexes, including Xie-Beni, Pakhira-Bandyopadhyay-Maulik, Tang, Wu-Li, generalized C, and Kwon2. We conduct this evaluation on four types of datasets: artificial datasets, real-world datasets, simulated datasets with ranks, and image datasets, using the fuzzy c-mea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#21516;&#26102;&#36866;&#24212;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.13662</link><description>&lt;p&gt;
&#24322;&#26500;&#21644;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments. (arXiv:2308.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#21516;&#26102;&#36866;&#24212;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#23376;&#39046;&#22495;&#65292;&#23427;&#23558;&#27169;&#22411;&#24102;&#21040;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;FL&#20013;&#30340;&#20854;&#20182;&#25361;&#25112;&#65292;&#22914;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RE-FL&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#36827;&#34892;&#21098;&#26525;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#21644;&#36890;&#20449;&#36718;&#25968;&#12290;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-enforcing sub-domain of machine learning that brings the model to the user's device for training, avoiding the need to share personal data with a central server. While existing works address data heterogeneity, they overlook other challenges in FL, such as device heterogeneity and communication efficiency. In this paper, we propose RE-FL, a novel approach that tackles computational and communication challenges in resource-constrained devices. Our variable pruning technique optimizes resource utilization by adapting pruning to each client's computational capabilities. We also employ knowledge distillation to reduce bandwidth consumption and communication rounds. Experimental results on image classification tasks demonstrate the effectiveness of our approach in resource-constrained environments, maintaining data privacy and performance while accommodating heterogeneous model architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.12899</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;[&#23454;&#39564;&#65292;&#20998;&#26512;&#21644;&#22522;&#20934;]&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]. (arXiv:2308.12899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#25968;&#25454;&#31649;&#29702;&#21644;&#32508;&#21512;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#20316;&#20026;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#25552;&#20379;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25216;&#26415;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#21644;&#37492;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#19981;&#21516;&#26469;&#28304;&#21644;&#20197;&#19981;&#21516;&#26684;&#24335;&#23384;&#20648;&#30340;&#22810;&#26679;&#21270;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#35775;&#38382;&#21644;&#21033;&#29992;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22823;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;&#26377;&#25928;&#30340;&#27169;&#22411;&#32467;&#26500;&#21644;&#32452;&#20214;&#20063;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#21407;&#23376;&#25991;&#20214;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#22823;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#23384;&#20648;&#26684;&#24335;&#65292;&#24182;&#22312;40&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#31649;&#29702;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#25216;&#26415;&#36827;&#23637;&#65292;&#25351;&#23548;&#20102;&#24378;&#22823;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#24615;&#33021;&#25490;&#34892;&#27036;&#24182;&#30830;&#23450;&#20102;&#26377;&#28508;&#21147;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06528</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65306;&#22522;&#20110;Raven&#28176;&#36827;&#30697;&#38453;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices. (arXiv:2308.06528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20219;&#21153;&#20998;&#35299;&#23398;&#20064;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#65292;&#36890;&#36807;&#22810;&#32500;&#39044;&#27979;&#26469;&#36873;&#25321;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#38382;&#39064;&#36890;&#24120;&#34987;&#25552;&#20986;&#20026;&#25972;&#20307;&#20219;&#21153;&#65292;&#27809;&#26377;&#20013;&#38388;&#30446;&#26631;&#12290;&#22312;Raven&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#20013;&#65292;&#20219;&#21153;&#26159;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#19968;&#20010;&#21487;&#29992;&#31572;&#26696;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#21644;&#31572;&#26696;&#37117;&#26159;&#22797;&#21512;&#22270;&#20687;&#65292;&#20855;&#26377;&#22810;&#20010;&#23545;&#35937;&#20197;&#21450;&#21508;&#31181;&#31354;&#38388;&#23433;&#25490;&#12290;&#30001;&#20110;&#21482;&#26377;&#36825;&#20010;&#39640;&#32423;&#30446;&#26631;&#20316;&#20026;&#25351;&#23548;&#65292;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#19981;&#36879;&#26126;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#34013;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#19981;&#30452;&#25509;&#36827;&#34892;&#19978;&#36848;&#36873;&#25321;&#65292;&#32780;&#26159;&#39044;&#27979;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#25490;&#21015;&#30340;&#35270;&#35273;&#29305;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#33719;&#24471;&#30340;&#22810;&#32500;&#39044;&#27979;&#30452;&#25509;&#24182;&#32622;&#20197;&#36873;&#25321;&#31572;&#26696;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#27169;&#22411;&#23558;&#35270;&#35273;&#36755;&#20837;&#35299;&#26512;&#20026;&#20196;&#29260;&#30340;&#20960;&#31181;&#26041;&#24335;&#65292;&#24182;&#37319;&#29992;&#20102;&#20960;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#20013;&#36755;&#20837;&#30340;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assess
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2308.05564</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#21246;&#32467;&#30340;&#39640;&#25928;&#21464;&#20998;&#25512;&#29702;&#21450;&#20854;&#22312;&#32929;&#31080;&#25910;&#30410;&#29575;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#23545;&#37329;&#34701;&#25968;&#25454;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#30340;&#23614;&#37096;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Azzalini&#21644;Capitanio&#65288;2003&#65289;&#25152;&#38544;&#21547;&#30340;&#20044;&#40486;&#21246;&#32467;&#22312;&#25104;&#23545;&#38750;&#23545;&#31216;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#20004;&#31181;&#27969;&#34892;&#30340;&#20044;&#40486;&#21246;&#32467;&#26356;&#39640;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23545;&#35813;&#20044;&#40486;&#21246;&#32467;&#30340;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#39640;&#26031;&#29983;&#25104;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#36817;&#20284;&#30340;&#38468;&#21152;&#21518;&#39564;&#12290;&#20351;&#29992;&#24555;&#36895;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;2017&#24180;&#33267;2021&#24180;&#38388;93&#20010;&#32654;&#22269;&#32929;&#31080;&#30340;&#32929;&#31080;&#25910;&#30410;&#29575;&#30340;&#21246;&#32467;&#27169;&#22411;&#12290;&#38500;&#20102;&#25104;&#23545;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#22806;&#65292;&#35813;&#21246;&#32467;&#36824;&#25429;&#25417;&#21040;&#20102;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20381;&#36182;&#30340;&#22823;&#37327;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#26579;&#33394;&#26631;&#20934;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.02851</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Stain Normalisation in Histopathology. (arXiv:2308.02851v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#26579;&#33394;&#26631;&#20934;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#25913;&#21892;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#39640;&#27700;&#24179;&#30340;&#35270;&#35273;&#21464;&#24322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#26579;&#33394;&#26631;&#20934;&#21270;&#26088;&#22312;&#22312;&#19981;&#25913;&#21464;&#22270;&#20687;&#32467;&#26500;&#20869;&#23481;&#30340;&#24773;&#20917;&#19979;&#26631;&#20934;&#21270;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#30340;&#19981;&#21516;&#25216;&#26415;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#20248;&#20110;&#38750;&#29983;&#25104;&#26041;&#27861;&#65292;&#20294;&#20195;&#20215;&#26159;&#26356;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#23545;&#26579;&#33394;&#26631;&#20934;&#21270;&#26356;&#22909;&#65292;&#19981;&#21516;&#30340;GAN&#21644;&#38750;GAN&#26041;&#27861;&#30456;&#20114;&#20043;&#38388;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each othe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05318</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#28342;&#35299;&#24230;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#20294;&#38590;&#20197;&#39044;&#27979;&#30340;&#24615;&#36136;&#12290;&#20351;&#29992;&#19968;&#32423;&#21407;&#29702;&#26041;&#27861;&#35745;&#31639;&#28342;&#35299;&#24230;&#38656;&#35201;&#32771;&#34385;&#29109;&#21644;&#28947;&#30340;&#31454;&#20105;&#25928;&#24212;&#65292;&#23548;&#33268;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#19988;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#20219;&#20309;&#35745;&#31639;&#25216;&#26415;&#30340;&#26131;&#29992;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#23548;&#33268;&#32676;&#20307;&#36129;&#29486;&#26041;&#27861;&#30340;&#25345;&#32493;&#27969;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20855;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#38745;&#24577;&#32593;&#31449;&#19978;&#36816;&#34892;&#65288;&#26080;&#38656;&#26381;&#21153;&#22120;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35745;&#31639;&#38656;&#27714;&#36716;&#31227;&#21040;&#32593;&#31449;&#35775;&#38382;&#32773;&#36523;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23433;&#35013;&#65292;&#28040;&#38500;&#20102;&#25903;&#20184;&#21644;&#32500;&#25252;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28342;&#35299;&#24230;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21019;&#24314;&#24179;&#34913;&#28342;&#35299;&#24230;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04001</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Polynomial Width is Sufficient for Set Representation with High-dimensional Features. (arXiv:2307.04001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#30340;&#25506;&#32034;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#23485;&#24230;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#30340;&#38598;&#21512;&#34920;&#31034;&#36275;&#22815;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#20998;&#26512;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#21512;&#34920;&#31034;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#29992;&#20110;&#24314;&#27169;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#39034;&#24207;&#19981;&#25935;&#24863;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;DeepSets&#26159;&#26368;&#24120;&#29992;&#30340;&#38598;&#21512;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#23558;&#27599;&#20010;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#21040;&#20855;&#26377;&#32500;&#24230;L&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27714;&#21644;&#27744;&#21270;&#20197;&#33719;&#24471;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#65292;&#26368;&#21518;&#23558;&#25972;&#20010;&#38598;&#21512;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32500;&#24230;L&#23545;DeepSets&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#20043;&#21069;&#30340;&#20998;&#26512;&#35201;&#20040;&#23558;&#39640;&#32500;&#29305;&#24449;&#36807;&#20110;&#31616;&#21270;&#20026;&#19968;&#32500;&#29305;&#24449;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#20998;&#26512;&#28608;&#27963;&#20989;&#25968;&#65292;&#20174;&#32780;&#33073;&#31163;&#23454;&#38469;&#24212;&#29992;&#25110;&#23548;&#33268;L&#38543;&#30528;&#38598;&#21512;&#22823;&#23567;N&#21644;&#29305;&#24449;&#32500;&#24230;D&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#30740;&#31350;&#36798;&#21040;&#36275;&#22815;&#34920;&#36798;&#33021;&#21147;&#30340;&#26368;&#23567;L&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38598;&#21512;&#20803;&#32032;&#23884;&#20837;&#23618;&#65306;&#65288;a&#65289;&#32447;&#24615;+&#24130;&#28608;&#27963;&#65288;LP&#65289;&#21644;&#65288;b&#65289;&#32447;&#24615;+&#25351;&#25968;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order. DeepSets is the most widely used neural network architecture for set representation. It involves embedding each set element into a latent space with dimension $L$, followed by a sum pooling to obtain a whole-set embedding, and finally mapping the whole-set embedding to the output. In this work, we investigate the impact of the dimension $L$ on the expressive power of DeepSets. Previous analyses either oversimplified high-dimensional features to be one-dimensional features or were limited to analytic activations, thereby diverging from practical use or resulting in $L$ that grows exponentially with the set size $N$ and feature dimension $D$. To investigate the minimal value of $L$ that achieves sufficient expressive power, we present two set-element embedding layers: (a) linear + power activation (LP) and (b) linear + exponential activatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16384</link><description>&lt;p&gt;
&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#65306;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses. (arXiv:2306.16384v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;GPU&#21457;&#36215;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26469;&#21152;&#36895;GNN&#26694;&#26550;&#20013;&#30340;&#37319;&#26679;&#21644;&#32858;&#21512;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#22270;&#19978;&#26102;CPU&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;GPU&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#27491;&#22312;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#21644;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#24378;&#22823;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;GNNs&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#19978;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#35775;&#38382;&#21644;&#25968;&#25454;&#31227;&#21160;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#29616;&#26377;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#20351;&#29992;CPU&#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#32780;&#27169;&#22411;&#26435;&#37325;&#30340;&#35757;&#32451;&#21644;&#26356;&#26032;&#21017;&#30001;GPU&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;CPU&#26080;&#27861;&#23454;&#29616;&#25152;&#38656;&#30340;&#21534;&#21520;&#37327;&#20197;&#20805;&#20998;&#21033;&#29992;&#26114;&#36149;&#30340;GPU&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24403;&#22270;&#21644;&#20854;&#23884;&#20837;&#19981;&#33021;&#36866;&#24212;CPU&#20869;&#23384;&#26102;&#65292;&#25805;&#20316;&#31995;&#32479;&#24341;&#20837;&#30340;&#24320;&#38144;&#65292;&#22914;&#22788;&#29702;&#39029;&#38754;&#38169;&#35823;&#65292;&#20250;&#25104;&#20026;&#20851;&#38190;&#36335;&#24452;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPU&#21457;&#36215;&#30340;&#30452;&#25509;&#23384;&#20648;&#35775;&#38382;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.  To address these issues, we propose the GPU Initiated Direct Storage Ac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.14131</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Safety-Critical Scenario Generation Via Reinforcement Learning Based Editing. (arXiv:2306.14131v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#23545;&#20110;&#27979;&#35797;&#21644;&#39564;&#35777;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#22312;&#32500;&#24230;&#35781;&#21650;&#21644;&#25628;&#32034;&#31354;&#38388;&#30340;&#38480;&#21046;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39034;&#24207;&#32534;&#36753;&#29983;&#25104;&#22330;&#26223;&#65292;&#20363;&#22914;&#28155;&#21152;&#26032;&#30340;&#20195;&#29702;&#25110;&#20462;&#25913;&#29616;&#26377;&#20195;&#29702;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#21253;&#21547;&#39118;&#38505;&#21644;&#21487;&#34892;&#24615;&#30446;&#26631;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#34892;&#24615;&#30446;&#26631;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#30340;&#21487;&#33021;&#24615;&#65307;&#23427;&#24809;&#32602;&#29983;&#25104;&#19981;&#22826;&#21487;&#33021;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#32500;&#24230;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#24191;&#27867;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#36136;&#37327;&#26356;&#39640;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating safety-critical scenarios is essential for testing and verifying the safety of autonomous vehicles. Traditional optimization techniques suffer from the curse of dimensionality and limit the search space to fixed parameter spaces. To address these challenges, we propose a deep reinforcement learning approach that generates scenarios by sequential editing, such as adding new agents or modifying the trajectories of the existing agents. Our framework employs a reward function consisting of both risk and plausibility objectives. The plausibility objective leverages generative models, such as a variational autoencoder, to learn the likelihood of the generated parameters from the training datasets; It penalizes the generation of unlikely scenarios. Our approach overcomes the dimensionality challenge and explores a wide range of safety-critical scenarios. Our evaluation demonstrates that the proposed method generates safety-critical scenarios of higher quality compared with previous
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.12059</link><description>&lt;p&gt;
EquiformerV2: &#25913;&#36827;&#30340;&#31561;&#21464;Transformer&#65292;&#29992;&#20110;&#25193;&#23637;&#21040;&#26356;&#39640;&#27425;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#21367;&#31215;&#31867;&#22411;&#21644;&#26550;&#26500;&#25913;&#36827;&#65292;&#25193;&#23637;&#20102;&#31561;&#21464;Transformer&#21040;&#26356;&#39640;&#30340;&#31561;&#21464;&#34920;&#31034;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#37327;&#21644;&#21147;&#30340;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#35745;&#31639;&#25928;&#29575;&#20063;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;Transformer&#65288;&#20363;&#22914;Equiformer&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;Transformer&#24212;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#39046;&#22495;&#30340;&#21151;&#25928;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23616;&#38480;&#20110;&#23567;&#25968;&#27425;&#31561;&#21464;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#26550;&#26500;&#26159;&#21542;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#26356;&#39640;&#30340;&#27425;&#25968;&#12290;&#20174;Equiformer&#24320;&#22987;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;eSCN&#21367;&#31215;&#26367;&#25442;&#20102;$SO(3)$&#21367;&#31215;&#65292;&#20197;&#26377;&#25928;&#22320;&#21512;&#24182;&#26356;&#39640;&#27425;&#30340;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#26356;&#39640;&#27425;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26550;&#26500;&#25913;&#36827;&#8212;&#8212;&#27880;&#24847;&#21147;&#37325;&#26631;&#20934;&#21270;&#12289;&#21487;&#20998;&#31163;&#30340;$S^2$&#28608;&#27963;&#21644;&#21487;&#20998;&#31163;&#23618;&#24402;&#19968;&#21270;&#12290;&#23558;&#36825;&#19968;&#20999;&#25918;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EquiformerV2&#65292;&#22312;&#22823;&#22411;OC20&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#21147;&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;$12\%$&#65292;&#33021;&#37327;&#19978;&#25552;&#39640;&#20102;$4\%$&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#21560;&#38468;&#33021;&#25152;&#38656;&#30340;DFT&#35745;&#31639;&#37327;&#26041;&#38754;&#32553;&#20943;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#20102;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#31181;&#20027;&#27969;&#37319;&#26679;&#22120;&#30340;&#26032;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#27493;&#25968;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#27604;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.09251</link><description>&lt;p&gt;
&#38754;&#21521;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#30340;&#38750;&#28176;&#36827;&#24555;&#36895;&#25910;&#25947;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models. (arXiv:2306.09251v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#20102;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#20004;&#31181;&#20027;&#27969;&#37319;&#26679;&#22120;&#30340;&#26032;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#39640;&#20102;&#24635;&#27493;&#25968;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21453;&#36716;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#36807;&#31243;&#23558;&#22122;&#38899;&#36716;&#21270;&#20026;&#26032;&#25968;&#25454;&#23454;&#20363;&#65292;&#22312;&#24403;&#20195;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#24050;&#25104;&#20026;&#22522;&#30707;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#29616;&#22312;&#24050;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#22815;&#25104;&#29087;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#38750;&#28176;&#36827;&#29702;&#35770;&#65292;&#20197;&#29702;&#35299;&#31163;&#25955;&#26102;&#38388;&#19979;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#65288;Stein&#65289;&#24471;&#20998;&#20989;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;&#12290;&#38024;&#23545;&#19968;&#31181;&#27969;&#34892;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#65288;&#22522;&#20110;&#27010;&#29575;&#27969;ODE&#65289;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982; $T$&#65288;&#24635;&#27493;&#25968;&#65289;&#25104;&#27604;&#20363;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25913;&#36827;&#20102;&#36807;&#21435;&#30340;&#32467;&#26524;&#65307;&#23545;&#20110;&#21478;&#19968;&#31181;&#20027;&#27969;&#30340;&#38543;&#26426;&#37319;&#26679;&#22120;&#65288;&#21363;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#65289;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#19982; $1/\sqrt{T}$ &#25104;&#27604;&#20363;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#29702;&#35770;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23545;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#21482;&#20316;&#20986;&#26368;&#23567;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#27809;&#26377;&#24179;&#28369;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to reliable estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model (DDPM)), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Our theory imposes only minimal assumptions on the target data distribution (e.g., no smoot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.06836</link><description>&lt;p&gt;
&#29992;&#20989;&#25968;&#36924;&#36817;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#23614;&#22870;&#21169;&#38382;&#39064;&#30340;&#26497;&#23567;&#26368;&#22823;&#21270;&#31639;&#27861;&#21644;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds. (arXiv:2306.06836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24403;&#22870;&#21169;&#21576;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#23454;&#20363;&#30456;&#20851;&#31639;&#27861;&#65292;&#24182;&#24471;&#21040;&#20102;&#26497;&#23567;&#26368;&#22823;&#21270;&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26377;&#35768;&#22810;&#24037;&#20316;&#37117;&#19987;&#27880;&#20110;&#20026;&#26377;&#30028;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#26377;&#25928;&#31639;&#27861;&#65292;&#20294;&#24403;&#22870;&#21169;&#21576;&#29616;&#8220;&#37325;&#23614;&#8221;&#20998;&#24067;&#26102;&#8212;&#8212;&#21363;&#23384;&#22312;&#26576;&#20010; $\epsilon\in(0,1]$ &#20351;&#24471;&#20165;&#26377;&#26377;&#38480;&#30340;$(1+\epsilon)$-&#38454;&#30697;&#8212;&#8212;&#26159;&#21542;&#23384;&#22312;&#23545;&#22823;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#25110;&#26102;&#25928;&#24615;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340; RL &#20013;&#30340;&#36825;&#31181;&#22870;&#21169;&#26426;&#21046;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;&#37325;&#23614;&#32447;&#24615;&#36172;&#33218;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;\textsc{Heavy-OFUL}&#65292;&#20854;&#23454;&#29616;&#20102;&#19968;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340; $T$-round &#36951;&#25022;&#24230;&#37327;&#65292;&#20026; $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$&#65292;&#36825;&#26159;&#36825;&#31181;&#31867;&#22411;&#30340;\emph{&#31532;&#19968;&#31687;}&#25991;&#31456;&#12290;$\nu_t^{1+\epsilon}$&#26159;&#31532; $t$ &#36718;&#22870;&#21169;&#30340; $(1+\epsilon)$-&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#22312;&#24212;&#29992;&#20110; st &#30340;&#26368;&#22351;&#24773;&#20917;&#26102;&#65292;&#19978;&#36848;&#30028;&#26159;&#26497;&#23567;&#20540;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \emph{heavy-tailed}, i.e., with only finite $(1+\epsilon)$-th moments for some $\epsilon\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \emph{instance-dependent} $T$-round regret of $\tilde{O}\big(d T^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum_{t=1}^T \nu_t^2} + d T^{\frac{1-\epsilon}{2(1+\epsilon)}}\big)$, the \emph{first} of this kind. Here, $d$ is the feature dimension, and $\nu_t^{1+\epsilon}$ is the $(1+\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in st
&lt;/p&gt;</description></item><item><title>DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05738</link><description>&lt;p&gt;
DOCTOR&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05738
&lt;/p&gt;
&lt;p&gt;
DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#65288;WMS&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#26234;&#33021;&#21307;&#30103;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#20026;&#27599;&#31181;&#30142;&#30149;&#21644;&#30456;&#24212;&#30340;WMS&#25968;&#25454;&#23450;&#21046;&#20010;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26032;&#20219;&#21153;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#27979;&#27599;&#20010;&#26032;&#30142;&#30149;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;WMS&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;DOCTOR&#12290;&#23427;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#19968;&#31181;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;CL&#31639;&#27861;&#20351;&#24471;&#26694;&#26550;&#33021;&#22815;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#20998;&#31867;&#31867;&#21035;&#21644;&#30142;&#30149;&#26816;&#27979;&#20219;&#21153;&#12290;DOCTOR&#22312;&#20351;&#29992;&#26469;&#33258;&#23454;&#38469;WMS&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#22235;&#31181;&#24120;&#35265;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;DOCTOR&#20063;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05666</link><description>&lt;p&gt;
&#23384;&#22312;&#23545;&#31216;&#24615;&#21644;&#29366;&#24577;&#25277;&#35937;&#30340;&#25919;&#31574;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#20381;&#38752;&#25277;&#35937;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#24182;&#23558;MDP&#21516;&#24577;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#38024;&#23545;&#25277;&#35937;MDP&#30340;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#23548;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26799;&#24230;&#32467;&#26524;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#22522;&#20110;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#65292;&#20351;&#29992;&#26494;&#25955;&#21452;&#20223;&#23556;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#29615;&#22659;&#65292;&#20197;&#36827;&#19968;&#27493;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#20316;&#25277;&#35937;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#29615;&#22659;&#20197;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.04288</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#30340;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#20316;&#26500;&#24314;&#20855;&#26377;&#25552;&#39640;&#25928;&#29992;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#20449;&#24687;&#12290;&#24517;&#39035;&#37319;&#29992;&#36866;&#24403;&#30340;&#20445;&#25252;&#26426;&#21046;&#26469;&#28385;&#36275;&#20445;&#25252;&#38544;&#31169;&#21644;&#32500;&#25252;&#39640;&#27169;&#22411;&#25928;&#29992;&#30340;&#35201;&#27714;&#12290;&#30446;&#21069;&#37319;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#26412;&#36136;&#65292;&#21253;&#25324;&#8220;&#38543;&#26426;&#21270;&#26426;&#21046;&#8221;&#21644;&#8220;&#21387;&#32553;&#26426;&#21046;&#8221;&#65292;&#26159;&#36890;&#36807;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#21644;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#34913;&#37327;&#25928;&#29992;&#12290;&#25105;&#20204;&#24819;&#35201;&#30830;&#23450;&#22312;&#20160;&#20040;&#26222;&#36941;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#36884;&#24452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25928;&#29992;&#25439;&#22833;&#30340;&#19978;&#38480;&#65292;&#29992;&#20004;&#20010;&#20027;&#35201;&#39033;&#31216;&#20026;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2304.05364</link><description>&lt;p&gt;
&#32422;&#26463;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Constrained Domains. (arXiv:2304.05364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#26032;&#36817;&#28044;&#29616;&#30340;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#35821;&#38899;&#29983;&#25104;&#31561;&#20247;&#22810;&#39046;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23427;&#20204;&#30001;&#30772;&#22351;&#25968;&#25454;&#30340;&#21152;&#22122;&#36807;&#31243;&#21644;&#23450;&#20041;&#20026;&#21152;&#22122;&#25193;&#25955;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#21518;&#21521;&#38454;&#27573;&#32452;&#25104;&#12290;&#20197;&#36825;&#20123;&#25104;&#21151;&#20026;&#22522;&#30784;&#65292;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#40654;&#26364;&#27969;&#24418;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#40654;&#26364;&#25193;&#25955;&#27169;&#22411;&#35201;&#27714;&#22312;&#25152;&#26377;&#26102;&#38388;&#19978;&#23450;&#20041;&#27979;&#22320;&#32447;&#12290;&#34429;&#28982;&#35813;&#35774;&#32622;&#21253;&#25324;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#65292;&#20294;&#19981;&#21253;&#25324;&#30001;&#19981;&#31561;&#24335;&#32422;&#26463;&#38598;&#23450;&#20041;&#30340;&#27969;&#24418;&#65292;&#36825;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#21644;&#34507;&#30333;&#35774;&#35745;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#35825;&#23548;&#30340;&#23545;&#25968;&#38556;&#30861;&#24230;&#37327;&#30340;&#21152;&#22122;&#36807;&#31243;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#30340;&#21152;&#22122;&#36807;&#31243;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#32422;&#26463;&#22495;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21019;&#24314;&#32422;&#26463;&#22495;&#30340;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a recent class of generative models which achieve state-of-the-art results in many domains such as unconditional image generation and text-to-speech tasks. They consist of a noising process destroying the data and a backward stage defined as the time-reversal of the noising diffusion. Building on their success, diffusion models have recently been extended to the Riemannian manifold setting. Yet, these Riemannian diffusion models require geodesics to be defined for all times. While this setting encompasses many important applications, it does not include manifolds defined via a set of inequality constraints, which are ubiquitous in many scientific domains such as robotics and protein design. In this work, we introduce two methods to bridge this gap. First, we design a noising process based on the logarithmic barrier metric induced by the inequality constraints. Second, we introduce a noising process based on the reflected Brownian motion. As existing diffu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#27431;&#27954;&#22320;&#21306;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.16198</link><description>&lt;p&gt;
&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#23616;&#37096;&#27668;&#35937;&#23545;&#26893;&#34987;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction. (arXiv:2303.16198v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#20174;&#21355;&#26143;&#35270;&#35282;&#39044;&#27979;&#27431;&#27954;&#22320;&#21306;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Sentinel 2&#21355;&#26143;&#27979;&#37327;&#27431;&#27954;&#22320;&#21306;&#30340;&#22825;&#27668;&#26469;&#24314;&#31435;&#26893;&#34987;&#23545;&#22825;&#27668;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#26041;&#27861;&#37325;&#28857;&#20851;&#27880;&#22810;&#20809;&#35889;&#22270;&#20687;&#30340;&#30495;&#23454;&#24863;&#65292;&#32780;&#34893;&#29983;&#30340;&#26893;&#34987;&#21160;&#24577;&#23578;&#26410;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27668;&#35937;&#25351;&#23548;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#39044;&#27979;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#23398;&#20064;&#30340;&#20113;&#23618;&#25513;&#27169;&#21644;&#36866;&#24403;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#25193;&#23637;&#20102;EarthNet2021&#25968;&#25454;&#38598;&#20197;&#36866;&#21512;&#26893;&#34987;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#39046;&#20808;&#30340;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#26041;&#27861;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#25105;&#20204;&#24314;&#31435;&#30340;&#26893;&#34987;&#21160;&#24577;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65306;&#29992;&#20110;&#30899;&#30417;&#27979;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#30340;&#25512;&#26029;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#24314;&#31435;&#22522;&#20110;&#27668;&#35937;&#24341;&#23548;&#30340;&#35270;&#39057;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for modeling vegetation response to weather in Europe as measured by the Sentinel 2 satellite. Existing satellite imagery forecasting approaches focus on photorealistic quality of the multispectral images, while derived vegetation dynamics have not yet received as much attention. We leverage both spatial and temporal context by extending state-of-the-art video prediction methods with weather guidance. We extend the EarthNet2021 dataset to be suitable for vegetation modeling by introducing a learned cloud mask and an appropriate evaluation scheme. Qualitative and quantitative experiments demonstrate superior performance of our approach over a wide variety of baseline methods, including leading approaches to satellite imagery forecasting. Additionally, we show how our modeled vegetation dynamics can be leveraged in a downstream task: inferring gross primary productivity for carbon monitoring. To the best of our knowledge, this work presents the first models fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.06965</link><description>&lt;p&gt;
Uni-RXN: &#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24357;&#21512;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Uni-RXN: A Unified Framework Bridging the Gap between Chemical Reaction Pretraining and Conditional Molecule Generation. (arXiv:2303.06965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Uni-RXN&#26694;&#26550;&#65292;&#22312;&#21270;&#23398;&#21453;&#24212;Pretraining&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#65292;&#29983;&#25104;&#36136;&#37327;&#39640;&#12289;&#21487;&#21512;&#25104;&#30340;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#26377;&#26426;&#21270;&#23398;&#30740;&#31350;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#19968;&#20010;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#21270;&#23398;&#21453;&#24212;&#22522;&#26412;&#35268;&#21017;&#30340;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#21453;&#24212;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#26356;&#25972;&#20307;&#30340;&#26041;&#27861;&#12290;&#21463;&#26377;&#26426;&#21270;&#23398;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#24402;&#32435;&#20559;&#35265;&#32435;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20855;&#22791;&#21270;&#23398;&#30693;&#35782;&#65292;&#35813;&#26694;&#26550;&#21487;&#24212;&#29992;&#20110;&#22522;&#20110;&#21453;&#24212;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#20165;&#20381;&#36182;&#23569;&#37327;&#21453;&#24212;&#27169;&#26495;&#30340;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#21487;&#21512;&#25104;&#33647;&#29289;&#31867;&#20998;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical reactions are the fundamental building blocks of drug design and organic chemistry research. In recent years, there has been a growing need for a large-scale deep-learning framework that can efficiently capture the basic rules of chemical reactions. In this paper, we have proposed a unified framework that addresses both the reaction representation learning and molecule generation tasks, which allows for a more holistic approach. Inspired by the organic chemistry mechanism, we develop a novel pretraining framework that enables us to incorporate inductive biases into the model. Our framework achieves state-of-the-art results on challenging downstream tasks. By possessing chemical knowledge, this framework can be applied to reaction-based generative models, overcoming the limitations of current molecule generation models that rely on a small number of reaction templates. In the extensive experiments, our model generates synthesizable drug-like structures of high quality. Overall,
&lt;/p&gt;</description></item><item><title>FOSI&#26159;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#25913;&#21892;&#19968;&#31867;&#20248;&#21270;&#22120;&#30340;&#26465;&#20214;&#25968;</title><link>http://arxiv.org/abs/2302.08484</link><description>&lt;p&gt;
FOSI&#65306;&#28151;&#21512;&#19968;&#38454;&#21644;&#20108;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FOSI: Hybrid First and Second Order Optimization. (arXiv:2302.08484v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08484
&lt;/p&gt;
&lt;p&gt;
FOSI&#26159;&#19968;&#31181;&#20803;&#31639;&#27861;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#25913;&#21892;&#19968;&#31867;&#20248;&#21270;&#22120;&#30340;&#26465;&#20214;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#35745;&#31639;&#26354;&#29575;&#30340;&#22256;&#38590;&#23548;&#33268;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#27969;&#34892;&#30340;&#26041;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#20165;&#20351;&#29992;&#19968;&#38454;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FOSI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#31639;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#21152;&#20837;&#20108;&#38454;&#20449;&#24687;&#20197;&#25552;&#39640;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;FOSI&#38544;&#21547;&#22320;&#23558;&#20989;&#25968;&#20998;&#20026;&#20004;&#20010;&#23450;&#20041;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#19978;&#30340;&#20108;&#27425;&#20989;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#20108;&#38454;&#26041;&#27861;&#26368;&#23567;&#21270;&#19968;&#20010;&#20989;&#25968;&#65292;&#20351;&#29992;&#22522;&#26412;&#20248;&#21270;&#22120;&#26368;&#23567;&#21270;&#21478;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;FOSI&#25910;&#25947;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#23427;&#22312;&#19968;&#31867;&#20248;&#21270;&#22120;&#20013;&#25913;&#21892;&#20102;&#26465;&#20214;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#23545;&#20110;&#38899;&#39057;&#20998;&#31867;&#65292;&#36801;&#31227;&#23398;&#20064;&#21644;&#29289;&#20307;&#20998;&#31867;&#31561;&#20960;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;FOSI&#24212;&#29992;&#20110;GD&#65292;Heavy-Ball&#21644;Adam&#31561;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though second-order optimization methods are highly effective, popular approaches in machine learning such as SGD and Adam use only first-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We prove FOSI converges and further show it improves the condition number for a large family of optimizers. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of GD, Heavy-Ball, and Adam when applied to several deep neural networks training tasks such as audio classification, transfer learning, and object classification, as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2301.10774</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#25581;&#31034;&#29983;&#29289;&#22823;&#20998;&#23376;&#30340;&#19968;&#32423;&#24207;&#21015;&#19982;&#19977;&#32423;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#29305;&#23450;&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;RNA&#24207;&#21015;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24443;&#24213;&#25506;&#32034;&#20102;&#34507;&#30333;&#36136;&#20013;&#32467;&#26500;&#21040;&#24207;&#21015;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;RNA&#35774;&#35745;&#20173;&#38754;&#20020;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#34429;&#28982;RNA&#19982;&#34507;&#30333;&#36136;&#20849;&#20139;&#31867;&#20284;&#30340;&#32467;&#26500;&#32452;&#20998;&#65292;&#20294;&#30452;&#25509;&#23558;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#27861;&#31227;&#26893;&#21040;RNA&#35774;&#35745;&#20013;&#21364;&#26080;&#27861;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32467;&#26500;&#34920;&#31034;&#30340;&#22810;&#20010;&#23618;&#27425;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.14960</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#30340;&#26631;&#31614;&#23545;&#40784;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#23545;&#40784;&#23646;&#24615;&#65288;LAP&#65289;&#65292;&#21363;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;&#21521;&#37327;&#22823;&#37096;&#20998;&#22312;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#12290;&#19982;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#19987;&#27880;&#20110;&#27491;&#21017;&#21270;&#34920;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30456;&#21453;&#65292;&#36890;&#36807;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;LAP&#65292;&#29992;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21069;&#20960;&#20010;&#21491;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#65292;&#24182;&#19982;&#26368;&#20248;&#35299;&#23545;&#40784;&#12290;&#36890;&#36807;&#28040;&#38500;&#32463;&#20856;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#20013;&#24120;&#35265;&#30340;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
&lt;/p&gt;</description></item></channel></rss>