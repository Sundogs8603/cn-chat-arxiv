<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25968;&#23383;&#38761;&#21629;&#23548;&#33268;&#20102;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#25968;&#23383;&#21270;&#65292;&#26032;&#20852;&#29616;&#35937;&#20351;&#28040;&#36153;&#32773;&#34892;&#20026;&#26356;&#21152;&#22797;&#26434;&#65292;&#20256;&#32479;&#30340;&#20998;&#26512;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#20102;&#26377;&#25928;&#35299;&#26512;&#21644;&#22788;&#29702;&#28040;&#36153;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14118</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28040;&#36153;&#32773;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Consumer Data. (arXiv:2306.14118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14118
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#38761;&#21629;&#23548;&#33268;&#20102;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#25968;&#23383;&#21270;&#65292;&#26032;&#20852;&#29616;&#35937;&#20351;&#28040;&#36153;&#32773;&#34892;&#20026;&#26356;&#21152;&#22797;&#26434;&#65292;&#20256;&#32479;&#30340;&#20998;&#26512;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#20102;&#26377;&#25928;&#35299;&#26512;&#21644;&#22788;&#29702;&#28040;&#36153;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#38761;&#21629;&#23548;&#33268;&#20154;&#31867;&#34892;&#20026;&#30340;&#25968;&#23383;&#21270;&#65292;&#21019;&#36896;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#20197;&#20102;&#35299;&#20197;&#21069;&#26410;&#26366;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#12290;&#26032;&#20852;&#29616;&#35937;&#65292;&#20363;&#22914;&#32676;&#31609;&#21644;&#20247;&#21253;&#65292;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#28040;&#36153;&#32773;&#34892;&#20026;&#65292;&#21516;&#26102;&#20063;&#24341;&#20837;&#20102;&#26032;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#32473;&#33829;&#38144;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20998;&#26512;&#28040;&#36153;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#26032;&#20852;&#25968;&#25454;&#26469;&#28304;&#30340;&#24191;&#24230;&#12289;&#31934;&#24230;&#21644;&#35268;&#27169;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#35745;&#31639;&#26041;&#27861;&#26469;&#22788;&#29702;&#19982;&#28040;&#36153;&#32773;&#34892;&#20026;&#30456;&#20851;&#30340;&#8220;&#22823;&#25968;&#25454;&#8221;&#65292;&#36825;&#36890;&#24120;&#21253;&#25324;&#32467;&#26500;&#21270;&#25968;&#25454;&#12289;&#25991;&#26412;&#25968;&#25454;&#12289;&#38899;&#39057;&#25968;&#25454;&#21644;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#26512;&#21644;&#22788;&#29702;&#22810;&#26041;&#38754;&#30340;&#25968;&#25454;&#12290;&#37492;&#20110;&#36825;&#20123;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#35753;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#29087;&#24713;&#26368;&#26032;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#28040;&#36153;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digital revolution has led to the digitization of human behavior, creating unprecedented opportunities to understand observable actions on an unmatched scale. Emerging phenomena such as crowdfunding and crowdsourcing have further illuminated consumer behavior while also introducing new behavioral patterns. However, the sheer volume and complexity of this data present significant challenges for marketing researchers and practitioners. Traditional methods used to analyze consumer data fall short in handling the breadth, precision, and scale of emerging data sources. To address this, computational methods have been developed to manage the "big data" associated with consumer behavior, which typically includes structured data, textual data, audial data, and visual data. These methods, particularly machine learning, allow for effective parsing and processing of multi-faceted data. Given these recent developments, this review article seeks to familiarize researchers and practitioners with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;</title><link>http://arxiv.org/abs/2306.14114</link><description>&lt;p&gt;
TNPAR: &#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20107;&#20214;&#24207;&#21015;Granger&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning Granger Causal Structure from Event Sequences. (arXiv:2306.14114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#26469;&#23398;&#20064;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26469;&#35299;&#20915;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38750; i.i.d. &#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;Granger&#22240;&#26524;&#20851;&#31995;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20107;&#20214;&#24207;&#21015;&#29420;&#31435;&#21516;&#20998;&#24067; (i.i.d.) &#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20107;&#20214;&#24207;&#21015;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#19968; i.i.d. &#20551;&#35774;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#34987;&#24314;&#27169;&#25104;&#19968;&#20010;&#25299;&#25169;&#32593;&#32476;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#24341;&#20837;Granger&#22240;&#26524;&#21457;&#29616;&#26469;&#35299;&#20915;&#38750; i.i.d. &#38382;&#39064;&#12290;&#36825;&#19968;&#21457;&#29616;&#20419;&#20351;&#25105;&#20204;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;1) &#22914;&#20309;&#22312;&#27169;&#22411;&#20107;&#20214;&#24207;&#21015;&#26102;&#21516;&#26102;&#32771;&#34385;&#20808;&#39564;&#25299;&#25169;&#32593;&#32476;&#21644;&#28508;&#22312;&#30340;Granger&#22240;&#26524;&#32467;&#26500;&#65307;2) &#22914;&#20309;&#23398;&#20064;Granger&#22240;&#26524;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#32479;&#19968;&#25299;&#25169;&#31070;&#32463;&#27850;&#26494;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#27850;&#26494;&#36807;&#31243;&#30340;&#19968;&#31181;&#21464;&#20307;&#26469;&#24314;&#27169;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#21051;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#20851;&#31995;&#21644;&#29616;&#26377;&#20107;&#20214;&#24207;&#21015;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Granger causality from event sequences is a challenging but essential task across various applications. Most existing methods rely on the assumption that event sequences are independent and identically distributed (i.i.d.). However, this i.i.d. assumption is often violated due to the inherent dependencies among the event sequences. Fortunately, in practice, we find these dependencies can be modeled by a topological network, suggesting a potential solution to the non-i.i.d. problem by introducing the prior topological network into Granger causal discovery. This observation prompts us to tackle two ensuing challenges: 1) how to model the event sequences while incorporating both the prior topological network and the latent Granger causal structure, and 2) how to learn the Granger causal structure. To this end, we devise a two-stage unified topological neural Poisson auto-regressive model. During the generation stage, we employ a variant of the neural Poisson process to model the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.14111</link><description>&lt;p&gt;
RLHF&#26159;&#21542;&#27604;&#26631;&#20934;RL&#26356;&#22256;&#38590;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#23398;&#20064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#20174;&#20559;&#22909;&#20449;&#21495;&#23398;&#20064;&#65292;&#32780;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#30452;&#25509;&#20174;&#22870;&#21169;&#20449;&#21495;&#23398;&#20064;&#12290;&#20559;&#22909;&#20449;&#21495;&#21487;&#33021;&#21253;&#21547;&#30340;&#20449;&#24687;&#27604;&#22870;&#21169;&#20449;&#21495;&#23569;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#20284;&#20046;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#31639;&#27861;&#21644;&#25216;&#26415;&#30452;&#25509;&#35299;&#20915;&#22522;&#20110;&#20559;&#22909;&#30340;RL&#38382;&#39064;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22870;&#21169;&#27010;&#29575;&#27169;&#22411;&#30340;&#20559;&#22909;&#65292;&#27492;&#26102;&#21487;&#20197;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#23481;&#24525;&#22870;&#21169;&#23567;&#35823;&#24046;&#30340;&#40065;&#26834;&#22870;&#21169;RL&#38382;&#39064;&#65307;&#65288;2&#65289;&#23545;&#20110;&#19968;&#33324;&#30340;&#20219;&#24847;&#20559;&#22909;&#19988;&#30446;&#26631;&#26159;&#25214;&#21040;von Neumann&#33719;&#32988;&#32773;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#22810;&#26234;&#33021;&#20307;&#22870;&#21169;RL&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#22312;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#19979;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22240;&#23376;&#32435;&#20160;&#24179;&#34913;&#35299;&#12290;&#21518;&#19968;&#31181;&#24773;&#20917;&#21487;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#25104;&#23545;&#20851;&#31995;&#30340;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;boosting&#31639;&#27861;&#20013;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14101</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#24369;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models are weak learners. (arXiv:2306.14101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;boosting&#31639;&#27861;&#20013;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#21644;&#29702;&#35770;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#27010;&#24565;&#26159;&#24369;&#23398;&#20064;&#22120;&#65292;&#21363;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#37117;&#33021;&#21462;&#24471;&#27604;&#38543;&#26426;&#26356;&#22909;&#30340;&#24615;&#33021;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#21482;&#26159;&#30053;&#24494;&#22909;&#19968;&#28857;&#12290;&#36825;&#26679;&#30340;&#24369;&#23398;&#20064;&#22120;&#26500;&#25104;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;boosting&#65289;&#30340;&#23454;&#29992;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20316;&#20026;&#19978;&#36848;&#24369;&#23398;&#20064;&#22120;&#36827;&#34892;&#25805;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;boosting&#31639;&#27861;&#20013;&#30340;&#24369;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20379;&#65288;&#26681;&#25454;&#24863;&#20852;&#36259;&#30340;&#20998;&#24067;&#36827;&#34892;&#36866;&#24403;&#37319;&#26679;&#30340;&#65289;&#34920;&#26684;&#25968;&#25454;&#26679;&#26412;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#21487;&#20197;&#20135;&#29983;&#26679;&#26412;&#30340;&#27719;&#24635;&#65292;&#20316;&#20026;&#20998;&#31867;&#30340;&#27169;&#26495;&#65292;&#24182;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20316;&#20026;&#24369;&#23398;&#20064;&#22120;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#32435;&#20837;boosting&#26041;&#27861;&#20013;&#65292;&#22312;&#26576;&#20123;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21033;&#29992;LLM&#20013;&#30340;&#30693;&#35782;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#26641;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central notion in practical and theoretical machine learning is that of a $\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14094</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Distributed Online Learning with Guaranteed Optimality. (arXiv:2306.14094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#30001;&#20110;&#20854;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27969;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20010;&#20154;&#31169;&#23494;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#24120;&#24120;&#38754;&#20020;&#20026;&#20102;&#38544;&#31169;&#20445;&#25252;&#32780;&#29306;&#29298;&#23398;&#20064;&#20934;&#30830;&#24615;&#30340;&#22256;&#22659;&#12290;&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#24182;&#30830;&#20445;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#22312;&#30830;&#20445;&#39044;&#26399;&#30636;&#26102;&#36951;&#25022;&#31243;&#24230;&#36880;&#28176;&#20943;&#23567;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20445;&#35777;&#26377;&#38480;&#30340;&#32047;&#31215;&#38544;&#31169;&#39044;&#31639;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#12290;&#20026;&#20102;&#24212;&#23545;&#23436;&#20840;&#20998;&#24067;&#24335;&#29615;&#22659;&#65292;&#25105;&#20204;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#23545;&#20840;&#23616;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed online learning is gaining increased traction due to its unique ability to process large-scale datasets and streaming data. To address the growing public awareness and concern on privacy protection, plenty of private distributed online learning algorithms have been proposed, mostly based on differential privacy which has emerged as the ``gold standard" for privacy protection. However, these algorithms often face the dilemma of trading learning accuracy for privacy. By exploiting the unique characteristics of online learning, this paper proposes an approach that tackles the dilemma and ensures both differential privacy and learning accuracy in distributed online learning. More specifically, while ensuring a diminishing expected instantaneous regret, the approach can simultaneously ensure a finite cumulative privacy budget, even on the infinite time horizon. To cater for the fully distributed setting, we adopt the local differential-privacy framework which avoids the reliance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14088</link><description>&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#38598;&#32676;&#19979;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31169;&#26377;&#25968;&#25454;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters. (arXiv:2306.14088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#36890;&#36807;&#22810;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#31169;&#26377;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#19968;&#31181;&#33879;&#21517;&#24182;&#24191;&#27867;&#20351;&#29992;&#30340;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#35745;&#31639;&#23616;&#37096;&#26799;&#24230;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#32852;&#21512;&#22120;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38544;&#31169;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#35266;&#23519;&#21040;&#23616;&#37096;&#26799;&#24230;&#23601;&#36275;&#20197;&#27844;&#38706;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;&#24050;&#30740;&#31350;&#20102;&#29992;&#20110;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#29992;&#25143;&#37117;&#24444;&#27492;&#36830;&#25509;&#24182;&#19982;&#32852;&#21512;&#22120;&#36830;&#25509;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#26550;&#26500;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20165;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#12290;&#24403;&#38656;&#35201;&#20449;&#24687;&#35770;&#38544;&#31169;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36890;&#20449;&#25104;&#26412;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#24182;&#24341;&#20837;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#37327;&#36523;&#23450;&#21046;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning collaboratively trains a neural network on privately owned data held by several participating clients. The gradient descent algorithm, a well-known and popular iterative optimization procedure, is run to train the neural network. Every client uses its local data to compute partial gradients and sends it to the federator which aggregates the results. Privacy of the clients' data is a major concern. In fact, observing the partial gradients can be enough to reveal the clients' data. Private aggregation schemes have been investigated to tackle the privacy problem in federated learning where all the users are connected to each other and to the federator. In this paper, we consider a wireless system architecture where clients are only connected to the federator via base stations. We derive fundamental limits on the communication cost when information-theoretic privacy is required, and introduce and analyze a private aggregation scheme tailored for this setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#23545;&#31616;&#21333;&#35299;&#37322;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.14087</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#20449;&#24687;&#35770;&#22797;&#26434;&#24615;&#34920;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Circuit Complexity Formulation of Algorithmic Information Theory. (arXiv:2306.14087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#23545;&#31616;&#21333;&#35299;&#37322;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Solomonoffs&#24402;&#32435;&#25512;&#29702;&#29702;&#35770;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#36335;&#22797;&#26434;&#24230;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#22797;&#26434;&#24230;&#34913;&#37327;&#26041;&#24335;&#65292;&#19981;&#20381;&#36182;&#20110;UTM&#30340;&#36873;&#25321;&#12290;&#30495;&#20540;&#34920;&#29616;&#20026;&#20803;&#20214;&#20043;&#38388;&#30340;&#36816;&#31639;&#32780;&#19981;&#26159;&#31243;&#24207;&#12290;&#20854;&#27425;&#65292;&#19981;&#23384;&#22312;&#20572;&#26426;&#38382;&#39064;&#12290;&#30005;&#36335;&#30340;&#36755;&#20986;&#20540;&#21487;&#20197;&#36890;&#36807;&#36882;&#24402;&#35745;&#31639;&#26426;&#26469;&#35745;&#31639;&#65292;&#26102;&#38388;&#19982;&#30005;&#36335;&#20869;&#38376;&#25968;&#25104;&#27491;&#27604;&#12290;&#25105;&#20204;&#30340;&#20808;&#39564;&#20551;&#35774;&#19968;&#20010;&#24067;&#23572;&#20989;&#25968;&#65292;&#25110;&#32773;&#31561;&#25928;&#30340;&#32534;&#30721;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#24067;&#23572;&#20018;&#65292;&#26159;&#30001;&#26576;&#20010;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#36825;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#20174;&#37096;&#20998;&#20449;&#24687;&#20013;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#65292;&#36890;&#24120;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#8220;&#20108;&#20803;&#20998;&#31867;&#8221;&#38382;&#39064;&#12290;&#20027;&#24352;&#23545;&#31616;&#21333;&#35299;&#37322;&#20855;&#26377;&#24402;&#32435;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by Solomonoffs theory of inductive inference, we propose a prior based on circuit complexity. There are several advantages to this approach. First, it relies on a complexity measure that does not depend on the choice of UTM. There is one universal definition for Boolean circuits involving an universal operation such as nand with simple conversions to alternative definitions such as and, or, and not. Second, there is no analogue of the halting problem. The output value of a circuit can be calculated recursively by computer in time proportional to the number of gates, while a short program may run for a very long time. Our prior assumes that a Boolean function, or equivalently, Boolean string of fixed length, is generated by some Bayesian mixture of circuits. This model is appropriate for learning Boolean functions from partial information, a problem often encountered within machine learning as "binary classification." We argue that an inductive bias towards simple explanations 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#33041;&#27169;&#22359;&#21270;&#20808;&#39564;&#30340;&#21160;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#29305;&#24449;&#34920;&#31034;&#65292;&#20026;&#38745;&#24687;&#29366;&#24577;&#19979;&#30340;&#21151;&#33021;&#24615;&#26680;&#30913;&#20849;&#25391;&#25104;&#20687;(rs-fMRI)&#30340;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#21644;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.14080</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#33041;&#27169;&#22359;&#21270;&#20808;&#39564;&#36827;&#34892;fMRI&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Brain Modularity Prior for Interpretable Representation Learning of fMRI. (arXiv:2306.14080v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14080
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#33041;&#27169;&#22359;&#21270;&#20808;&#39564;&#30340;&#21160;&#24577;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#29305;&#24449;&#34920;&#31034;&#65292;&#20026;&#38745;&#24687;&#29366;&#24577;&#19979;&#30340;&#21151;&#33021;&#24615;&#26680;&#30913;&#20849;&#25391;&#25104;&#20687;(rs-fMRI)&#30340;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#21644;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#29366;&#24577;&#19979;&#30340;&#21151;&#33021;&#24615;&#26680;&#30913;&#20849;&#25391;&#25104;&#20687;(rs-fMRI)&#21487;&#20197;&#21453;&#26144;&#22823;&#33041;&#30340;&#33258;&#21457;&#31070;&#32463;&#27963;&#21160;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#22823;&#33041;&#30142;&#30149;&#20998;&#26512;&#12290;&#20197;&#22270;&#35770;&#20026;&#35270;&#35282;&#65292;&#22823;&#33041;&#22312;&#33258;&#21457;&#33041;&#21151;&#33021;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#27599;&#20010;&#27169;&#22359;&#30001;&#21151;&#33021;&#30456;&#20851;&#30340;&#24863;&#20852;&#36259;&#33041;&#21306;&#22495;(ROIs)&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;fMRI&#20998;&#26512;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#22823;&#33041;&#27169;&#22359;&#21270;&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#33041;&#27169;&#22359;&#21270;&#32422;&#26463;&#30340;&#21160;&#24577;&#34920;&#31034;&#23398;&#20064;(BMR)&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;(1)&#21160;&#24577;&#22270;&#26500;&#24314;&#65292;(2)&#36890;&#36807;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#32422;&#26463;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#21160;&#24577;&#22270;&#23398;&#20064;&#21644;(3)&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#35782;&#21035;&#12290;&#20844;&#24320;&#30340; rs-fMRI &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340; BMR &#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#29305;&#24449;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026; rs-fMRI &#20998;&#26512;&#30340;&#26377;&#29992;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resting-state functional magnetic resonance imaging (rs-fMRI) can reflect spontaneous neural activities in brain and is widely used for brain disorder analysis.Previous studies propose to extract fMRI representations through diverse machine/deep learning methods for subsequent analysis. But the learned features typically lack biological interpretability, which limits their clinical utility. From the view of graph theory, the brain exhibits a remarkable modular structure in spontaneous brain functional networks, with each module comprised of functionally interconnected brain regions-of-interest (ROIs). However, most existing learning-based methods for fMRI analysis fail to adequately utilize such brain modularity prior. In this paper, we propose a Brain Modularity-constrained dynamic Representation learning (BMR) framework for interpretable fMRI analysis, consisting of three major components: (1) dynamic graph construction, (2) dynamic graph learning via a novel modularity-constrained g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14079</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65306;&#36890;&#36807;&#25193;&#25955;&#20998;&#25968;&#21305;&#37197;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20248;&#21270;&#33539;&#24335;&#65292;&#20363;&#22914;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#20801;&#35768;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#32771;&#34385;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#20026;&#20102;&#35753;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23427;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19981;&#20165;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;Lipschitz&#24120;&#25968;&#26469;&#20998;&#26512;&#27169;&#22411;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#21644;&#25968;&#25454;&#20284;&#28982;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#65292;&#20197;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14072</link><description>&lt;p&gt;
&#26080;&#24378;&#24230;&#21367;&#31215;&#26102;&#31354;&#28857;&#36807;&#31243;: &#34701;&#21512;&#23616;&#37096;&#19982;&#20840;&#23616;&#20107;&#20214;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Intensity-free Convolutional Temporal Point Process: Incorporating Local and Global Event Contexts. (arXiv:2306.14072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#65292;&#20197;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#30340;&#20107;&#20214;&#39044;&#27979;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#30456;&#24403;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#26102;&#38388;&#28857;&#36807;&#31243;(TPP)&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#29616;&#26377;&#30340;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#20687;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#25110;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#20043;&#31867;&#30340;&#25216;&#26415;&#26469;&#32534;&#30721;&#20107;&#20214;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#20294;&#26159;&#65292;&#23616;&#37096;&#20107;&#20214;&#19978;&#19979;&#25991;&#23545;&#20107;&#20214;&#30340;&#21457;&#29983;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#36825;&#26041;&#38754;&#21364;&#24456;&#23569;&#34987;&#20851;&#27880;&#12290;&#27969;&#34892;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19987;&#20026;&#25429;&#33719;&#23616;&#37096;&#19978;&#19979;&#25991;&#32780;&#35774;&#35745;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#22312;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#21270;&#65292;&#22240;&#27492;&#20174;&#26410;&#24212;&#29992;&#20110;TPP&#24314;&#27169;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;TPP&#24314;&#27169;&#26041;&#27861;&#65292;&#21363;&#23558;&#36830;&#32493;&#26102;&#38388;&#21367;&#31215;&#20107;&#20214;&#32534;&#30721;&#22120;&#19982;RNN&#38598;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#38271;&#24207;&#21015;&#21644;&#22797;&#26434;&#28508;&#22312;&#27169;&#24335;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event prediction in the continuous-time domain is a crucial but rather difficult task. Temporal point process (TPP) learning models have shown great advantages in this area. Existing models mainly focus on encoding global contexts of events using techniques like recurrent neural networks (RNNs) or self-attention mechanisms. However, local event contexts also play an important role in the occurrences of events, which has been largely ignored. Popular convolutional neural networks, which are designated for local context capturing, have never been applied to TPP modelling due to their incapability of modelling in continuous time. In this work, we propose a novel TPP modelling approach that combines local and global contexts by integrating a continuous-time convolutional event encoder with an RNN. The presented framework is flexible and scalable to handle large datasets with long sequences and complex latent patterns. The experimental result shows that the proposed model improves the perfo
&lt;/p&gt;</description></item><item><title>Waypoint Transformer&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#23588;&#20854;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#34920;&#29616;&#24471;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.14069</link><description>&lt;p&gt;
Waypoint Transformer: &#36890;&#36807;&#20013;&#38388;&#30446;&#26631;&#30340;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Waypoint Transformer: Reinforcement Learning via Supervised Learning with Intermediate Targets. (arXiv:2306.14069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14069
&lt;/p&gt;
&lt;p&gt;
Waypoint Transformer&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#23588;&#20854;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#34920;&#29616;&#24471;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20197;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#26550;&#26500;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#25104;&#21151;&#32780;&#35328;&#65292;&#20294;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DT&#36824;&#26159;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#20302;&#24615;&#33021;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#26080;&#27861;&#26080;&#32541;&#36830;&#25509;&#20122;&#20248;&#21270;&#36712;&#36857;&#30340;&#29255;&#27573;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;RvS&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25972;&#21512;&#20013;&#38388;&#30446;&#26631;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;Waypoint Transformer&#65288;WT&#65289;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;DT&#26694;&#26550;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36335;&#24452;&#28857;&#36827;&#34892;&#26465;&#20214;&#21270;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;RvS&#26041;&#27861;&#30456;&#27604;&#65292;&#26368;&#32456;&#22238;&#25253;&#26174;&#33879;&#22686;&#21152;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#37197;&#32622;&#20013;&#65292;&#21253;&#25324;AntMaze Large Play/Diverse&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advancements in offline reinforcement learning via supervised learning (RvS) and the success of the decision transformer (DT) architecture in various domains, DTs have fallen short in several challenging benchmarks. The root cause of this underperformance lies in their inability to seamlessly connect segments of suboptimal trajectories. To overcome this limitation, we present a novel approach to enhance RvS methods by integrating intermediate targets. We introduce the Waypoint Transformer (WT), using an architecture that builds upon the DT framework and conditioned on automatically-generated waypoints. The results show a significant increase in the final return compared to existing RvS methods, with performance on par or greater than existing state-of-the-art temporal difference learning-based methods. Additionally, the performance and stability improvements are largest in the most challenging environments and data configurations, including AntMaze Large Play/Diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14066</link><description>&lt;p&gt;
SEEDS&#65306;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20223;&#30495;&#22825;&#27668;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models. (arXiv:2306.14066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#20102;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#26410;&#26469;&#22825;&#27668;&#26102;&#65292;&#27010;&#29575;&#39044;&#27979;&#23545;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#39044;&#27979;&#38598;&#21512;&#26469;&#34920;&#31034;&#21644;&#37327;&#21270;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#38598;&#21512;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#26368;&#36817;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35268;&#27169;&#19978;&#29983;&#25104;&#38598;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;5&#25104;&#21592;&#38598;&#21512;GEFS&#37325;&#26032;&#39044;&#25253;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20135;&#29983;&#32852;&#21512;&#24773;&#20917;&#19979;&#30495;&#23454;&#30340;&#22825;&#27668;&#39044;&#27979;&#65292;&#36825;&#20123;&#24773;&#20917;&#21487;&#20197;&#22522;&#20110;&#25805;&#20316;GEFS&#39044;&#27979;&#31995;&#32479;&#30340;&#23569;&#25968;&#25104;&#21592;&#26465;&#20214;&#21270;&#12290;&#26681;&#25454;ERA5&#20998;&#26512;&#35780;&#20272;&#65292;&#29983;&#25104;&#30340;&#38598;&#21512;&#19982;&#23436;&#25972;&#30340;GEFS 31&#25104;&#21592;&#38598;&#21512;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#24456;&#22909;&#22320;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#38598;&#21512;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#30456;&#21516;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24320;&#21457;&#25193;&#25955;&#27169;&#22411;&#65292;&#36827;&#34892;&#29983;&#25104;&#21518;&#22788;&#29702;&#12290;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#23569;&#25968;&#39044;&#27979;&#25104;&#21592;&#26465;&#20214;&#21270;&#22320;&#29983;&#25104;&#31867;&#20284;&#20110;&#29289;&#29702;&#22823;&#27169;&#22411;&#38598;&#21512;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting is crucial to decision-making under uncertainty about future weather. The dominant approach is to use an ensemble of forecasts to represent and quantify uncertainty in operational numerical weather prediction. However, generating ensembles is computationally costly. In this paper, we propose to generate ensemble forecasts at scale by leveraging recent advances in generative artificial intelligence. Our approach learns a data-driven probabilistic diffusion model from the 5-member ensemble GEFS reforecast dataset. The model can then be sampled efficiently to produce realistic weather forecasts, conditioned on a few members of the operational GEFS forecasting system. The generated ensembles have similar predictive skill as the full GEFS 31-member ensemble, evaluated against ERA5 reanalysis, and emulate well the statistics of large physics-based ensembles. We also apply the same methodology to developing a diffusion model for generative post-processing: the model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14064</link><description>&lt;p&gt;
&#36229;&#36234;&#21452;&#26354;&#27169;&#22411;&#65306;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#20013;&#24314;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices. (arXiv:2306.14064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#25968;&#25454;&#30340;&#32467;&#26500;&#19982;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#23545;&#40784;&#23545;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#33267;&#20851;&#37325;&#35201;&#12290;&#27431;&#27663;&#31354;&#38388;&#21644;&#21452;&#26354;&#31354;&#38388;&#30340;&#22343;&#21248;&#20960;&#20309;&#20801;&#35768;&#20351;&#29992;&#26368;&#23567;&#30340;&#25197;&#26354;&#26469;&#34920;&#31034;&#20855;&#26377;&#22343;&#21248;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#22914;&#32593;&#26684;&#21644;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#25968;&#25454;&#20855;&#26377;&#22810;&#31181;&#31867;&#22411;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#29305;&#24449;&#65292;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#20960;&#20309;&#23884;&#20837;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65288;SPD&#65289;&#30340;&#40654;&#26364;&#23545;&#31216;&#31354;&#38388;&#26469;&#26500;&#24314;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21487;&#38752;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#24418;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#24211;&#65292;&#21033;&#29992;SPD gyrospace&#35745;&#31639;&#24037;&#20855;\cite{lopez2021gyroSPD}&#23454;&#29616;&#20102;&#22312;SPD&#20013;&#20116;&#20010;&#27969;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#24314;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;SPD&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that alignment between the structure of graph data and the geometry of an embedding space is crucial for learning high-quality representations of the data. The uniform geometry of Euclidean and hyperbolic spaces allows for representing graphs with uniform geometric and topological features, such as grids and hierarchies, with minimal distortion. However, real-world graph data is characterized by multiple types of geometric and topological features, necessitating more sophisticated geometric embedding spaces. In this work, we utilize the Riemannian symmetric space of symmetric positive definite matrices (SPD) to construct graph neural networks that can robustly handle complex graphs. To do this, we develop an innovative library that leverages the SPD gyrocalculus tools \cite{lopez2021gyroSPD} to implement the building blocks of five popular graph neural networks in SPD. Experimental results demonstrate that our graph neural networks in SPD substantially outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.14063</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Offline Policy Evaluation for Reinforcement Learning with Adaptively Collected Data. (arXiv:2306.14063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#38598;&#25968;&#25454;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20026;&#34920;&#26684;MDPs&#25512;&#23548;&#20986;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#31163;&#32447;RL&#26041;&#27861;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23454;&#29616;&#25968;&#25454;&#38656;&#27714;&#37327;&#36739;&#22823;&#30340;RL&#31639;&#27861;&#23454;&#38469;&#21487;&#34892;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#32467;&#26524;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#21363;&#21253;&#25324;&#19968;&#20010;&#30001;&#21333;&#19968;&#35760;&#24405;&#31574;&#30053;&#25910;&#38598;&#30340;i.i.d.&#36712;&#36857;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35774;&#32622;&#65292;&#21363;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#33258;&#36866;&#24212;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#20026;&#34920;&#26684;MDPs&#20013;&#30340;TMIS&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20272;&#35745;&#22120;&#22312;&#36825;&#20010;&#24191;&#20041;&#35774;&#32622;&#20013;&#24320;&#21457;&#29702;&#35770;&#65292;&#25512;&#23548;&#20854;&#20272;&#35745;&#35823;&#24046;&#30340;&#39640;&#27010;&#29575;&#12289;&#23454;&#20363;&#30456;&#20851;&#36793;&#30028;&#12290;&#25105;&#20204;&#36824;&#22238;&#25910;&#20102;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#30340;&#26497;&#23567;&#20540;&#26368;&#20248;&#31163;&#32447;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#25311;&#65292;&#20197;&#32463;&#39564;&#20998;&#26512;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#33258;&#36866;&#24212;&#21644;&#38750;&#33258;&#36866;&#24212;&#27169;&#24335;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Currently, most results hinge on unrealistic assumptions about the data distribution -- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We consider a more general setting where the dataset may have been gathered adaptively. We develop theory for the TMIS Offline Policy Evaluation (OPE) estimator in this generalized setting for tabular MDPs, deriving high-probability, instance-dependent bounds on its estimation error. We also recover minimax-optimal offline learning in the adaptive setting. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive regimes.
&lt;/p&gt;</description></item><item><title>DesCo&#26159;&#19968;&#31181;&#26032;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#39640;&#23545;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14060</link><description>&lt;p&gt;
DesCo: &#21033;&#29992;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#23398;&#20064;&#29289;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DesCo: Learning Object Recognition with Rich Language Descriptions. (arXiv:2306.14060v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14060
&lt;/p&gt;
&lt;p&gt;
DesCo&#26159;&#19968;&#31181;&#26032;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#35814;&#23613;&#30340;&#35821;&#35328;&#25551;&#36848;&#25552;&#39640;&#23545;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;&#35821;&#35328;&#26041;&#27861;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23398;&#20064;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#20174;&#35821;&#35328;&#30417;&#30563;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#23545;&#35937;&#19982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#24352;&#29483;&#30340;&#29031;&#29255;&#8221;&#65289;&#23545;&#40784;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#26032;&#23545;&#35937;&#21644;&#22495;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#20960;&#39033;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#21253;&#25324;&#23646;&#24615;&#65292;&#24418;&#29366;&#65292;&#32441;&#29702;&#21644;&#20851;&#31995;&#31561;&#32454;&#31890;&#24230;&#35821;&#20041;&#32454;&#33410;&#35268;&#33539;&#30340;&#22797;&#26434;&#35821;&#35328;&#34920;&#36798;&#24335;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#23558;&#35821;&#35328;&#25551;&#36848;&#20316;&#20026;&#26597;&#35810;&#21152;&#20837;&#24182;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#31934;&#30830;&#35299;&#37322;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;GLIP&#24120;&#24120;&#24573;&#30053;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#26159;&#36807;&#20110;&#20381;&#36182;&#20165;&#20973;&#21517;&#31216;&#26816;&#27979;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#25551;&#36848;&#26465;&#20214;&#65288;DesCo&#65289;&#8221;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent development in vision-language approaches has instigated a paradigm shift in learning visual recognition models from language supervision. These approaches align objects with language queries (e.g. "a photo of a cat") and improve the models' adaptability to identify novel objects and domains. Recently, several studies have attempted to query these models with complex language expressions that include specifications of fine-grained semantic details, such as attributes, shapes, textures, and relations. However, simply incorporating language descriptions as queries does not guarantee accurate interpretation by the models. In fact, our experiments show that GLIP, the state-of-the-art vision-language model for object detection, often disregards contextual information in the language descriptions and instead relies heavily on detecting objects solely by their names. To tackle the challenges, we propose a new description-conditioned (DesCo) paradigm of learning object recognition model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#31561;&#24335;&#32422;&#26463;&#19979;&#28145;&#24230;&#22768;&#26126;&#24335;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#36924;&#36817;&#31574;&#30053;&#65292;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20026;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.14054</link><description>&lt;p&gt;
&#25506;&#32034;&#31561;&#24335;&#32422;&#26463;&#19979;&#28145;&#24230;&#22768;&#26126;&#24335;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#36924;&#36817;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Gradient Approximation in Equality Constrained Deep Declarative Networks. (arXiv:2306.14054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#31561;&#24335;&#32422;&#26463;&#19979;&#28145;&#24230;&#22768;&#26126;&#24335;&#32593;&#32476;&#20013;&#30340;&#26799;&#24230;&#36924;&#36817;&#31574;&#30053;&#65292;&#20026;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20026;&#39640;&#25928;&#30340;&#35745;&#31639;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24573;&#30053;&#32422;&#26463;&#39033;&#26102;&#65292;&#28145;&#24230;&#22768;&#26126;&#24335;&#33410;&#28857;&#30340;&#26799;&#24230;&#26159;&#21542;&#21487;&#20197;&#36817;&#20284;&#65292;&#20174;&#32780;&#23548;&#33268;&#20840;&#23616;&#25439;&#22833;&#20989;&#25968;&#30340;&#19979;&#38477;&#26041;&#21521;&#12290;&#36825;&#23545;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#22240;&#20026;&#36924;&#36817;&#26041;&#27861;&#36890;&#24120;&#27604;&#30495;&#23454;&#26799;&#24230;&#35745;&#31639;&#26356;&#33410;&#30465;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#20855;&#26377;&#32447;&#24615;&#31561;&#24335;&#32422;&#26463;&#21644;&#24402;&#19968;&#21270;&#32422;&#26463;&#38382;&#39064;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#36924;&#36817;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#33391;&#22909;&#25928;&#26524;&#20197;&#21450;&#27880;&#24847;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore conditions for when the gradient of a deep declarative node can be approximated by ignoring constraint terms and still result in a descent direction for the global loss function. This has important practical application when training deep learning models since the approximation is often computationally much more efficient than the true gradient calculation. We provide theoretical analysis for problems with linear equality constraints and normalization constraints, and show examples where the approximation works well in practice as well as some cautionary tales for when it fails.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26234;&#33021;&#31639;&#27861;&#12289;&#39640;&#25928;&#31995;&#32479;&#21644;&#33258;&#23450;&#20041;&#30828;&#20214;&#31561;&#65292;&#25552;&#20986;&#20102;GNN&#21152;&#36895;&#30340;&#20998;&#31867;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.14052</link><description>&lt;p&gt;
&#19968;&#39033;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#32508;&#36848;&#65306;&#31639;&#27861;&#12289;&#31995;&#32479;&#21644;&#33258;&#23450;&#20041;&#30828;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Network Acceleration: Algorithms, Systems, and Customized Hardware. (arXiv:2306.14052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26234;&#33021;&#31639;&#27861;&#12289;&#39640;&#25928;&#31995;&#32479;&#21644;&#33258;&#23450;&#20041;&#30828;&#20214;&#31561;&#65292;&#25552;&#20986;&#20102;GNN&#21152;&#36895;&#30340;&#20998;&#31867;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27491;&#22312;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#23613;&#31649;GNN&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#28041;&#21450;&#35768;&#22810;&#25968;&#25454;&#21644;&#20005;&#26684;&#24310;&#36831;&#35201;&#27714;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#22312;&#22914;&#20309;&#21152;&#36895;GNN&#26041;&#38754;&#36827;&#34892;&#20102;&#12290;&#36825;&#20123;&#21152;&#36895;&#25216;&#26415;&#28041;&#21450;GNN&#27969;&#31243;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20174;&#26234;&#33021;&#35757;&#32451;&#21644;&#25512;&#29702;&#31639;&#27861;&#21040;&#39640;&#25928;&#30340;&#31995;&#32479;&#21644;&#33258;&#23450;&#20041;&#30828;&#20214;&#12290;&#38543;&#30528;&#23545;GNN&#21152;&#36895;&#30340;&#30740;&#31350;&#37327;&#24555;&#36895;&#22686;&#38271;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22788;&#29702;&#26469;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#22270;&#65292;&#24182;&#35299;&#20915;&#30456;&#20851;&#24037;&#20316;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;GNN&#21152;&#36895;&#30340;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23545;GNN&#21152;&#36895;&#30340;&#20998;&#31867;&#22788;&#29702;&#36830;&#25509;&#20102;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#24182;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are emerging for machine learning research on graph-structured data. GNNs achieve state-of-the-art performance on many tasks, but they face scalability challenges when it comes to real-world applications that have numerous data and strict latency requirements. Many studies have been conducted on how to accelerate GNNs in an effort to address these challenges. These acceleration techniques touch on various aspects of the GNN pipeline, from smart training and inference algorithms to efficient systems and customized hardware. As the amount of research on GNN acceleration has grown rapidly, there lacks a systematic treatment to provide a unified view and address the complexity of relevant works. In this survey, we provide a taxonomy of GNN acceleration, review the existing approaches, and suggest future research directions. Our taxonomic treatment of GNN acceleration connects the existing works and sets the stage for further development in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#40065;&#26834;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#32771;&#34385;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#21160;&#24577;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#20998;&#24067;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#21160;&#24577;&#27969;&#34892;&#30149;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14051</link><description>&lt;p&gt;
&#20915;&#31574;&#20381;&#36182;&#20998;&#24067;&#40065;&#26834;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#22312;&#21160;&#24577;&#27969;&#34892;&#30149;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decision-Dependent Distributionally Robust Markov Decision Process Method in Dynamic Epidemic Control. (arXiv:2306.14051v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#40065;&#26834;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#32771;&#34385;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#21160;&#24577;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#20998;&#24067;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;&#21160;&#24577;&#27969;&#34892;&#30149;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DRMDP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#27969;&#34892;&#30149;&#25511;&#21046;&#38382;&#39064;&#12290; Susceptible-Exposed-Infectious-Recovered&#65288;SEIR&#65289;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#20256;&#26579;&#24615;&#30142;&#30149;&#65288;&#22914;COVID-19&#65289;&#30340;&#38543;&#26426;&#20256;&#25773;&#12290;&#32780;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20026;&#35782;&#21035;&#26681;&#25454;SEIR&#27169;&#22411;&#23545;&#25239;&#30142;&#30149;&#20256;&#25773;&#30340;&#26368;&#20248;&#25514;&#26045;&#65288;&#22914;&#30123;&#33495;&#25509;&#31181;&#21644;&#38477;&#20302;&#20256;&#25773;&#24178;&#39044;&#65289;&#25552;&#20379;&#20102;&#25968;&#23398;&#26694;&#26550;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#24773;&#20917;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38656;&#35201;&#19968;&#31181;&#26356;&#24378;&#22823;&#12289;&#19981;&#37027;&#20040;&#20381;&#36182;&#20110;&#38169;&#35823;&#20551;&#35774;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;DRMDP&#26694;&#26550;&#65292;&#20801;&#35768;&#22312;&#19968;&#20010;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#27169;&#31946;&#38598;&#21512;&#20013;&#32771;&#34385;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#20915;&#31574;&#30456;&#20851;&#30340;&#27169;&#31946;&#38598;&#21512;&#20869;&#30340;&#36716;&#31227;&#27010;&#29575;&#30340;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#12290;&#20026;&#20102;&#20811;&#26381;&#19982;&#31574;&#30053;&#30830;&#23450;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Distributionally Robust Markov Decision Process (DRMDP) approach for addressing the dynamic epidemic control problem. The Susceptible-Exposed-Infectious-Recovered (SEIR) model is widely used to represent the stochastic spread of infectious diseases, such as COVID-19. While Markov Decision Processes (MDP) offers a mathematical framework for identifying optimal actions, such as vaccination and transmission-reducing intervention, to combat disease spreading according to the SEIR model. However, uncertainties in these scenarios demand a more robust approach that is less reliant on error-prone assumptions. The primary objective of our study is to introduce a new DRMDP framework that allows for an ambiguous distribution of transition dynamics. Specifically, we consider the worst-case distribution of these transition probabilities within a decision-dependent ambiguity set. To overcome the computational complexities associated with policy determination, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.14048</link><description>&lt;p&gt;
H$_2$O: &#39640;&#25928;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;, &#20294;&#26159;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23427;&#20204;&#29305;&#21035;&#38590;&#20197;&#29992;&#20110;&#23545;&#35805;&#31995;&#32479;&#21644;&#25925;&#20107;&#21019;&#20316;&#31561;&#38656;&#35201;&#29983;&#25104;&#38271;&#20869;&#23481;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#36890;&#24120;&#36824;&#38656;&#35201;&#22312;GPU&#20869;&#23384;&#20013;&#23384;&#20648;&#22823;&#37327;&#20020;&#26102;&#29366;&#24577;&#20449;&#24687;&#65292;&#31216;&#20026;KV cache&#65292;&#23427;&#19982;&#24207;&#21015;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#29616;KV cache&#30340;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#22320;&#20943;&#23569;&#20102;&#20854;&#20869;&#23384;&#21344;&#29992;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#21457;&#29616;&#65292;&#21363;&#22312;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#26102;&#65292;&#23567;&#37096;&#20998;&#26631;&#35760;&#36129;&#29486;&#26368;&#22823;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#26631;&#35760;&#20026;&#28909;&#38376;&#20803;&#32032;(H$_2$)&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;(i) H$_2$&#30340;&#20986;&#29616;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#65292;&#24182;&#19988;&#19982;&#25991;&#26412;&#20013;&#26631;&#35760;&#30340;&#39057;&#32321;&#20849;&#29616;&#24378;&#30456;&#20851;&#65307;(ii)&#21435;&#38500;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;H$_2$O&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;&#12290;H$_2$O&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;&#24207;&#21015;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#65292;&#24182;&#22240;&#27492;&#20445;&#25345;&#19968;&#20010;&#26356;&#23567;&#30340;KV cache,&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;50%&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;,H$_2$O&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#23436;&#25972;KV cache&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#31574;&#30053;&#34920;&#31034;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#30005;&#21147;&#36127;&#33655;&#39640;&#23792;&#21644;&#20302;&#23792;&#20043;&#38388;&#30340;&#21512;&#29702;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.14047</link><description>&lt;p&gt;
&#38754;&#21521;&#38656;&#27714;&#21709;&#24212;&#30340;&#26368;&#20248;&#23450;&#20215;--&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Pricing of Demand Response -- A Nonparametric Constrained Policy Optimization Approach. (arXiv:2306.14047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#31574;&#30053;&#34920;&#31034;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#30005;&#21147;&#36127;&#33655;&#39640;&#23792;&#21644;&#20302;&#23792;&#20043;&#38388;&#30340;&#21512;&#29702;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#38477;&#20302;&#30005;&#21147;&#24066;&#22330;&#20379;&#38656;&#20004;&#20391;&#19981;&#30830;&#23450;&#24615;&#21644;&#23792;&#20540;&#36127;&#33655;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DR&#30740;&#31350;&#32780;&#35328;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#36866;&#24403;&#22320;&#35843;&#25972;&#30005;&#21147;&#20215;&#26684;&#65292;&#20197;&#23558;&#30005;&#21147;&#36127;&#33655;&#20174;&#39640;&#23792;&#36716;&#31227;&#21040;&#20302;&#23792;&#26102;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38750;&#21442;&#25968;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#20248;&#21270;&#24230;&#24182;&#30830;&#20445;&#31574;&#30053;&#26356;&#26032;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demand response (DR) has been demonstrated to be an effective method for reducing peak load and mitigating uncertainties on both the supply and demand sides of the electricity market. One critical question for DR research is how to appropriately adjust electricity prices in order to shift electrical load from peak to off-peak hours. In recent years, reinforcement learning (RL) has been used to address the price-based DR problem because it is a model-free technique that does not necessitate the identification of models for end-use customers. However, the majority of RL methods cannot guarantee the stability and optimality of the learned pricing policy, which is undesirable in safety-critical power systems and may result in high customer bills. In this paper, we propose an innovative nonparametric constrained policy optimization approach that improves optimality while ensuring stability of the policy update, by removing the restrictive assumption on policy representation that the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.14043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#33258;&#24049;&#30340;&#38543;&#26426;&#26631;&#20934;&#65306;&#38543;&#26426;&#24179;&#28369;&#21644;&#22522;&#20110;PRNG&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24615;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#25968;&#25454;&#36873;&#25321;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23558;&#29983;&#25104;&#25110;&#25910;&#38598;&#38543;&#26426;&#24615;&#30340;&#20219;&#21153;&#22806;&#21253;&#32473;&#20102;&#32534;&#35793;&#22120;&#12289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25110;&#24037;&#20855;&#38142;&#20013;&#30340;&#20854;&#20182;&#22320;&#26041;&#12290;&#20294;&#26159;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19981;&#33391;&#38543;&#26426;&#24615;&#29978;&#33267;&#21019;&#24314;&#38543;&#26426;&#24615;&#30340;&#21382;&#21490;&#24736;&#20037;&#65292;&#23601;&#20687;NSA&#25918;&#32622;&#21518;&#38376;&#22312;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#20013;&#20197;&#30772;&#35299;&#21152;&#23494;&#19968;&#26679;&#12290;&#26412;&#25991;&#32771;&#34385;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#25915;&#20987;&#32773;&#36890;&#24120;&#20381;&#36182;&#30340;&#38543;&#26426;&#24615;&#26469;&#21361;&#23475;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#38543;&#26426;&#24179;&#28369;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#20219;&#24847;&#27169;&#22411;&#30340;&#29305;&#23450;&#36755;&#20837;&#25968;&#25454;&#28857;&#25552;&#20379;&#35748;&#35777;&#12290;&#25105;&#20204;&#36873;&#25321;&#38543;&#26426;&#24179;&#28369;&#26159;&#22240;&#20026;&#23427;&#29992;&#20110;&#23433;&#20840;&#21644;&#23433;&#20840;&#65288;&#29992;&#20110;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#65289;&#12290;&#22312;&#24149;&#21518;&#65292;&#23427;&#20381;&#36182;&#20110;&#37319;&#26679;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#22260;&#32469;&#25968;&#25454;&#28857;&#30340;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#22312;&#23454;&#29616;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#26102;&#23384;&#22312;&#38480;&#21046;&#21644;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#20294;&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21487;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#23454;&#29616;&#26368;&#32039;&#23494;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.14041</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65306;&#25351;&#25968;&#29575;&#25928;&#29575;&#21644;&#19981;&#24102;&#22797;&#26434;&#24615;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14041
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#22312;&#23454;&#29616;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#26102;&#23384;&#22312;&#38480;&#21046;&#21644;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#20294;&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21487;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#23454;&#29616;&#26368;&#32039;&#23494;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#20013;&#65292;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#24050;&#30693;&#23384;&#22312;&#19968;&#20010;&#25152;&#35859;&#30340;&#20248;&#21270;&#32773;&#35781;&#21650;&#65292;&#20250;&#23548;&#33268;&#22312;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#26102;&#20135;&#29983;&#20048;&#35266;&#20559;&#24046;&#12290;&#21487;&#20197;&#36890;&#36807;&#22312;&#20272;&#35745;&#30340;&#30446;&#26631;&#20540;&#20013;&#22686;&#21152;&#8220;&#20445;&#35777;&#31354;&#38388;&#8221;&#25110;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#24555;&#36895;&#22686;&#38271;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20026;&#33719;&#24471;&#30340;&#30446;&#26631;&#20215;&#20540;&#25552;&#20379;&#20102;&#20445;&#25252;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#23545;&#30495;&#23454;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#30340;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#35201;&#20040;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#22797;&#26434;&#24615;&#26377;&#38480;&#21046;&#24615;&#26465;&#20214;&#21644;&#30693;&#35782;&#65292;&#35201;&#20040;&#20250;&#34920;&#29616;&#20986;&#21462;&#20915;&#20110;&#20998;&#24067;&#32500;&#24230;&#30340;&#36807;&#20110;&#20445;&#23432;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#65292;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;DRO&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20248;&#21183;&#65306;&#23545;&#20110;&#19968;&#22823;&#31867;&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#33719;&#24471;&#20102;&#23545;&#30495;&#23454;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#36825;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#26159;&#21487;&#33021;&#30340;&#65292;&#23601;&#20854;&#32039;&#32553;&#31243;&#24230;&#32780;&#35328;&#65292;&#35201;&#32039;&#23494;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a "margin" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.14040</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#21152;&#26435;&#33258;&#21160;&#26426;&#25552;&#21462;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Weighted Automata Extraction and Explanation of Recurrent Neural Networks for Natural Language Tasks. (arXiv:2306.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29702;&#35299;&#21644;&#20998;&#26512;&#23427;&#20204;&#30340;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#35768;&#22810;&#20154;&#33268;&#21147;&#20110;&#20174;RNN&#20013;&#25552;&#21462;&#26377;&#38480;&#33258;&#21160;&#26426;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#21644;&#35299;&#37322;&#26356;&#26041;&#20415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#31934;&#30830;&#23398;&#20064;&#21644;&#32452;&#21512;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#25110;&#31934;&#24230;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;WFA&#65289;&#25552;&#21462;&#21644;&#35299;&#37322;&#26694;&#26550;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) have achieved tremendous success in processing sequential data, yet understanding and analyzing their behaviours remains a significant challenge. To this end, many efforts have been made to extract finite automata from RNNs, which are more amenable for analysis and explanation. However, existing approaches like exact learning and compositional approaches for model extraction have limitations in either scalability or precision. In this paper, we propose a novel framework of Weighted Finite Automata (WFA) extraction and explanation to tackle the limitations for natural language tasks. First, to address the transition sparsity and context loss problems we identified in WFA extraction for natural language tasks, we propose an empirical method to complement missing rules in the transition diagram, and adjust transition matrices to enhance the context-awareness of the WFA. We also propose two data augmentation tactics to track more dynamic behaviours of RNN, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PG k-means&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#21010;&#20998;&#26469;&#35299;&#20915;&#31354;&#31751;&#65292;&#25552;&#39640;&#20102;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14031</link><description>&lt;p&gt;
&#22522;&#20110;&#21010;&#20998;&#25351;&#23548;&#30340;k-means&#31639;&#27861;&#65306;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#19979;&#30340;&#26497;&#31471;&#31354;&#31751;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partitioning-Guided K-Means: Extreme Empty Cluster Resolution for Extreme Model Compression. (arXiv:2306.14031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PG k-means&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#21010;&#20998;&#26469;&#35299;&#20915;&#31354;&#31751;&#65292;&#25552;&#39640;&#20102;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#32039;&#20945;&#24615;&#23545;&#20110;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#37327;&#21270;&#12290;&#26412;&#25991;&#32771;&#34385;&#36845;&#20195;&#20056;&#31215;&#37327;&#21270;&#65288;iPQ&#65289;&#19982;&#37327;&#21270;&#22122;&#22768;&#26159;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20294;&#26159;&#36825;&#31181;&#37327;&#21270;&#26694;&#26550;&#30001;&#20110;&#23384;&#22312;&#31354;&#31751;&#32780;&#23548;&#33268;&#25512;&#29702;&#36136;&#37327;&#19979;&#38477;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#31354;&#31751;&#26469;&#25552;&#39640;iPQ&#19982;&#37327;&#21270;&#22122;&#22768;&#30340;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#34987;&#31216;&#20026;&#22522;&#20110;&#21010;&#20998;&#25351;&#23548;&#30340;k-means&#31639;&#27861;&#65288;PG k-means&#65289;&#65292;&#26159;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;&#39640;&#24230;&#22686;&#24378;&#30340;k-means&#23454;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21010;&#20998;&#30340;&#39044;&#20998;&#37197;&#31574;&#30053;&#65292;&#30830;&#20445;&#27809;&#26377;&#21021;&#22987;&#31354;&#31751;&#24182;&#40723;&#21169;&#22343;&#21248;&#30340;&#26435;&#37325;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#20248;&#36234;&#30340;&#31354;&#31751;&#35299;&#20915;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#35880;&#24910;&#22320;&#20998;&#21106;&#22823;&#31751;&#26469;&#25191;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38480;&#21046;&#20102;PG k-means&#30340;&#36845;&#20195;&#24635;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compactness in deep learning can be critical to a model's viability in low-resource applications, and a common approach to extreme model compression is quantization. We consider Iterative Product Quantization (iPQ) with Quant-Noise to be state-of-the-art in this area, but this quantization framework suffers from preventable inference quality degradation due to prevalent empty clusters. In this paper, we propose several novel enhancements aiming to improve the accuracy of iPQ with Quant-Noise by focusing on resolving empty clusters. Our contribution, which we call Partitioning-Guided k-means (PG k-means), is a heavily augmented k-means implementation composed of three main components. First, we propose a partitioning-based pre-assignment strategy that ensures no initial empty clusters and encourages an even weight-to-cluster distribution. Second, we propose an empirically superior empty cluster resolution heuristic executed via cautious partitioning of large clusters. Finally, we constr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14030</link><description>&lt;p&gt;
My Boli&#65306;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#30340;&#35821;&#26009;&#24211;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks. (arXiv:2306.14030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;&#28151;&#21512;BERT&#27169;&#22411;&#21644;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#35813;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#28151;&#21512;&#35821;&#26009;&#24211;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28151;&#21512;&#35821;&#35328;&#25968;&#25454;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36164;&#28304;&#21294;&#20047;&#30340;&#21360;&#24230;&#35821;&#35328;&#39532;&#25289;&#22320;&#35821;&#65292;&#36825;&#20010;&#35821;&#35328;&#20043;&#21069;&#27809;&#26377;&#20219;&#20309;&#28151;&#21512;&#35821;&#35328;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;L3Cube-MeCorpus&#65292;&#19968;&#20010;&#21253;&#21547;500&#19975;&#26465;&#25512;&#29305;&#30340;&#22823;&#22411;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;(Mr-En)&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;L3Cube-MeBERT&#21644;MeRoBERTa&#65292;&#22522;&#20110;BERT&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#22312;MeCorpus&#19978;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#26377;&#30417;&#30563;&#30340;&#25968;&#25454;&#38598;MeHate&#12289;MeSent&#21644;MeLID&#65292;&#29992;&#20110;&#28151;&#21512;Mr-En&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#35780;&#20272;&#25968;&#25454;&#38598;&#20998;&#21035;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;\url{~}12,000&#26465;&#39532;&#25289;&#22320;&#35821;-&#33521;&#35821;&#28151;&#21512;&#25512;&#29305;&#12290;&#21066;&#20943;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26032;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;BERT&#27169;&#22411;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#28151;&#21512;&#39532;&#25289;&#22320;&#35821;&#30340;&#20195;&#30721;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research on code-mixed data is limited due to the unavailability of dedicated code-mixed datasets and pre-trained language models. In this work, we focus on the low-resource Indian language Marathi which lacks any prior work in code-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English (Mr-En) corpus with 5 million tweets for pretraining. We also release L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models pre-trained on MeCorpus. Furthermore, for benchmarking, we present three supervised datasets MeHate, MeSent, and MeLID for downstream tasks like code-mixed Mr-En hate speech detection, sentiment analysis, and language identification respectively. These evaluation datasets individually consist of manually annotated \url{~}12,000 Marathi-English code-mixed tweets. Ablations show that the models trained on this novel corpus significantly outperform the existing state-of-the-art BERT models. This is the first work that presents artifacts for code-mix
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20004;&#31181;&#32467;&#26500;&#27169;&#22411;&#30340;&#20449;&#24687;&#20934;&#21017;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#24179;&#34913;&#20102;&#34394;&#20551;&#38451;&#24615;&#21644;&#34394;&#20551;&#38452;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14026</link><description>&lt;p&gt;
&#39640;&#32500;&#26641;&#21644;&#22270;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21442;&#25968;&#36873;&#25321;&#20449;&#24687;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Information criteria for structured parameter selection in high dimensional tree and graph models. (arXiv:2306.14026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20004;&#31181;&#32467;&#26500;&#27169;&#22411;&#30340;&#20449;&#24687;&#20934;&#21017;&#65292;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21442;&#25968;&#36873;&#25321;&#65292;&#24182;&#24179;&#34913;&#20102;&#34394;&#20551;&#38451;&#24615;&#21644;&#34394;&#20551;&#38452;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#36890;&#24120;&#20250;&#36890;&#36807;&#25511;&#21046;&#34394;&#20551;&#38451;&#24615;&#65288;&#30456;&#23545;&#65289;&#25968;&#37327;&#30340;&#26041;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#26159;&#22240;&#20026;&#21542;&#21017;&#65292;&#23569;&#25968;&#30340;&#30495;&#23454;&#38451;&#24615;&#21487;&#33021;&#20250;&#34987;&#20247;&#22810;&#30340;&#34394;&#20551;&#38451;&#24615;&#25152;&#20027;&#23548;&#12290;&#20363;&#22914;&#65292;&#24403;&#36873;&#25321;&#36981;&#24490;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#26420;&#32032;&#20248;&#21270;&#65288;&#20363;&#22914;AIC&#25110;Mallows&#30340;Cp&#65289;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#36825;&#31181;&#24773;&#20917;&#12290;&#21487;&#20197;&#35748;&#20026;&#65292;&#36873;&#25321;&#30340;&#36807;&#24230;&#20272;&#35745;&#26469;&#33258;&#20110;&#20248;&#21270;&#36807;&#31243;&#33258;&#36523;&#25913;&#21464;&#25152;&#36873;&#21464;&#37327;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#36825;&#26679;&#20449;&#24687;&#20934;&#21017;&#23601;&#19981;&#20877;&#21453;&#26144;&#36873;&#25321;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#30495;&#23454;&#24046;&#24322;&#12290;&#22312;lasso&#20013;&#65292;&#36807;&#24230;&#20272;&#35745;&#20063;&#21487;&#20197;&#19982;&#25910;&#32553;&#20272;&#35745;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#36825;&#20351;&#24471;&#36873;&#25321;&#23545;&#38169;&#35823;&#30340;&#38451;&#24615;&#36873;&#25321;&#36807;&#20110;&#23485;&#23481;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#31934;&#32454;&#30340;&#20449;&#24687;&#20934;&#21017;&#65292;&#20180;&#32454;&#24179;&#34913;&#34394;&#20551;&#38451;&#24615;&#21644;&#34394;&#20551;&#38452;&#24615;&#65292;&#29992;&#20110;&#27809;&#26377;&#25910;&#32553;&#30340;&#20272;&#35745;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26641;&#32467;&#26500;&#27169;&#22411;&#21644;&#19968;&#31181;&#22270;&#32467;&#26500;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#36825;&#20004;&#31181;&#27169;&#22411;&#35843;&#25972;&#20102;&#20004;&#20010;&#20449;&#24687;&#20934;&#21017;&#65288;&#38463;&#21345;&#36125;&#20811;&#20449;&#24687;&#20934;&#21017;&#21644;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65289;&#65292;&#32771;&#34385;&#20102;&#32467;&#26500;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter selection in high-dimensional models is typically finetuned in a way that keeps the (relative) number of false positives under control. This is because otherwise the few true positives may be dominated by the many possible false positives. This happens, for instance, when the selection follows from a naive optimisation of an information criterion, such as AIC or Mallows's Cp. It can be argued that the overestimation of the selection comes from the optimisation process itself changing the statistics of the selected variables, in a way that the information criterion no longer reflects the true divergence between the selection and the data generating process. In lasso, the overestimation can also be linked to the shrinkage estimator, which makes the selection too tolerant of false positive selections. For these reasons, this paper works on refined information criteria, carefully balancing false positives and false negatives, for use with estimators without shrinkage. In particul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20010;&#24615;&#21270;&#21307;&#30103;&#32473;&#33647;&#27169;&#22411;&#65292;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#21487;&#35843;&#12289;&#24555;&#36895;&#12289;&#36830;&#32493;&#12289;&#38381;&#21512;&#30340;&#39044;&#27979;&#31561;&#29305;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14020</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#20010;&#24615;&#21270;&#32473;&#33647;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Individualized Dosing Dynamics via Neural Eigen Decomposition. (arXiv:2306.14020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20010;&#24615;&#21270;&#21307;&#30103;&#32473;&#33647;&#27169;&#22411;&#65292;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#21487;&#35843;&#12289;&#24555;&#36895;&#12289;&#36830;&#32493;&#12289;&#38381;&#21512;&#30340;&#39044;&#27979;&#31561;&#29305;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#33647;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#24494;&#20998;&#26041;&#31243;&#26469;&#24314;&#27169;&#29983;&#29289;&#21160;&#21147;&#23398;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#36807;&#31243;&#30340;&#23548;&#25968;&#65292;&#20174;&#32780;&#22312;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26102;&#38388;&#19978;&#30340;&#28789;&#27963;&#24615;&#24120;&#24120;&#20276;&#38543;&#30528;&#23545;&#22122;&#22768;&#30340;&#39640;&#25935;&#24863;&#24615;&#65292;&#32780;&#21307;&#30103;&#38382;&#39064;&#24448;&#24448;&#23384;&#22312;&#39640;&#22122;&#22768;&#21644;&#26377;&#38480;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#21307;&#30103;&#32473;&#33647;&#27169;&#22411;&#24517;&#39035;&#21487;&#38752;&#22320;&#27867;&#21270;&#21040;&#20010;&#20307;&#24739;&#32773;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#27835;&#30103;&#25919;&#31574;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#29305;&#24449;&#20540;&#20998;&#35299;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#31639;&#27861;&#65288;NESDE&#65289;&#12290;NESDE&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#27169;&#65288;&#20351;&#29992;&#36229;&#32593;&#32476;&#23545;&#24739;&#32773;&#27700;&#24179;&#21442;&#25968;&#65289;&#65307;&#23545;&#26032;&#27835;&#30103;&#25919;&#31574;&#30340;&#27867;&#21270;&#65288;&#20351;&#29992;&#35299;&#32806;&#25511;&#21046;&#65289;&#65307;&#26681;&#25454;&#22122;&#22768;&#27700;&#24179;&#21487;&#35843;&#30340;&#34920;&#29616;&#65288;&#20351;&#29992;&#20998;&#27573;&#32447;&#24615;&#65289;&#65307;&#20197;&#21450;&#24555;&#36895;&#12289;&#36830;&#32493;&#12289;&#38381;&#21512;&#30340;&#39044;&#27979;&#65288;&#20351;&#29992;&#35889;&#34920;&#31034;&#65289;&#12290;&#26412;&#25991;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;NESDE&#30340;&#40065;&#26834;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dosing models often use differential equations to model biological dynamics. Neural differential equations in particular can learn to predict the derivative of a process, which permits predictions at irregular points of time. However, this temporal flexibility often comes with a high sensitivity to noise, whereas medical problems often present high noise and limited data. Moreover, medical dosing models must generalize reliably over individual patients and changing treatment policies. To address these challenges, we introduce the Neural Eigen Stochastic Differential Equation algorithm (NESDE). NESDE provides individualized modeling (using a hypernetwork over patient-level parameters); generalization to new treatment policies (using decoupled control); tunable expressiveness according to the noise level (using piecewise linearity); and fast, continuous, closed-form prediction (using spectral representation). We demonstrate the robustness of NESDE in both synthetic and real medical probl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;N-BEATS&#65292;&#39044;&#27979;&#24182;&#35299;&#35835;ICU&#24863;&#26579;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14016</link><description>&lt;p&gt;
&#20351;&#29992;N-BEATS&#27169;&#22411;&#35299;&#35835;&#24863;&#26579;&#24739;&#32773;&#39044;&#27979;&#30340;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Interpreting Forecasted Vital Signs Using N-BEATS in Sepsis Patients. (arXiv:2306.14016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;N-BEATS&#65292;&#39044;&#27979;&#24182;&#35299;&#35835;ICU&#24863;&#26579;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#21644;&#39044;&#27979;&#24863;&#26579;&#24615;&#20241;&#20811;&#23545;&#24739;&#32773;&#30340;&#26368;&#20339;&#22788;&#29702;&#32467;&#26524;&#26497;&#20026;&#37325;&#35201;&#12290;&#20934;&#30830;&#22320;&#39044;&#27979;&#24863;&#26579;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#21487;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20197;&#36827;&#34892;&#21450;&#26102;&#30340;&#24178;&#39044;&#65292;&#22914;&#32473;&#20104;&#31283;&#23450;&#33647;&#29289;&#25110;&#20248;&#21270;&#36755;&#28082;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;N-BEATS&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#24863;&#26579;&#24739;&#32773;&#30340;3&#23567;&#26102;&#29983;&#21629;&#20307;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;N-BEATS&#21487;&#35299;&#37322;&#30340;&#37197;&#32622;&#39044;&#27979;&#29983;&#21629;&#20307;&#24449;&#36235;&#21183;&#65292;&#24182;&#19982;&#23454;&#38469;&#36235;&#21183;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#24739;&#32773;&#30340;&#30149;&#24773;&#21464;&#21270;&#21644;&#33647;&#29289;&#23545;&#20854;&#29983;&#21629;&#20307;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;eICU Collaborative Research Database&#25968;&#25454;&#38598;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#26679;&#22806;&#35780;&#20272;&#26631;&#20934;&#23545;&#29983;&#21629;&#20307;&#24449;&#39044;&#27979;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#35823;&#24046;&#24230;&#37327;&#26469;&#23637;&#31034;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12289;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#65288;MAPE&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;N-BEATS&#22312;&#39044;&#27979;&#24863;&#26579;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting and predicting septic shock early is crucial for the best possible outcome for patients. Accurately forecasting the vital signs of patients with sepsis provides valuable insights to clinicians for timely interventions, such as administering stabilizing drugs or optimizing infusion strategies. Our research examines N-BEATS, an interpretable deep-learning forecasting model that can forecast 3 hours of vital signs for sepsis patients in intensive care units (ICUs). In this work, we use the N-BEATS interpretable configuration to forecast the vital sign trends and compare them with the actual trend to understand better the patient's changing condition and the effects of infused drugs on their vital signs. We evaluate our approach using the publicly available eICU Collaborative Research Database dataset and rigorously evaluate the vital sign forecasts using out-of-sample evaluation criteria. We present the performance of our model using error metrics, including mean squared error (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14009</link><description>&lt;p&gt;
&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#21152;&#24378;&#22270;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#22270;&#39044;&#27979;&#12290;&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27492;&#38382;&#39064;&#65292;&#32771;&#34385;&#21516;&#26102;&#22312;&#22270;&#19978;&#39044;&#27979;&#22810;&#20010;&#33410;&#28857;&#26631;&#31614;&#20989;&#25968;&#12290;&#20026;&#20102;&#20855;&#20307;&#35828;&#26126;&#65292;&#32771;&#34385;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#65306;&#27599;&#20010;&#31038;&#21306;&#25104;&#21592;&#36523;&#20221;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#37325;&#21472;&#27169;&#24335;&#65292;&#24403;&#25105;&#20204;&#23558;&#22810;&#20010;&#31038;&#21306;&#26816;&#27979;&#24212;&#29992;&#21040;naive&#22810;&#20219;&#21153;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36127;&#36801;&#31227;&#24456;&#26222;&#36941;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#20219;&#21153;&#20851;&#31995;&#39640;&#24230;&#38750;&#32447;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22522;&#20110;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27599;&#20010;&#20219;&#21153;&#32452;&#19978;&#25311;&#21512;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#20135;&#29983;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#30340;&#22686;&#24378;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#20272;&#35745;&#20026;&#39044;&#27979;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#20013;&#21457;&#29616;&#20102;&#20855;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;</title><link>http://arxiv.org/abs/2306.13995</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A clustering and graph deep learning-based framework for COVID-19 drug repurposing. (arXiv:2306.13995v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#22270;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312; COVID-19 &#33647;&#29289;&#20877;&#21033;&#29992;&#20013;&#21457;&#29616;&#20102;&#20855;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#30830;&#23450;&#20102;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#21033;&#29992; (&#25110;&#37325;&#26032;&#23450;&#20301;) &#26159;&#23547;&#25214;&#24050;&#32463;&#33719;&#24471;&#33647;&#29289;&#30417;&#31649;&#26426;&#26500; (&#20363;&#22914;&#32654;&#22269;&#39135;&#21697;&#33647;&#21697;&#30417;&#30563;&#31649;&#29702;&#23616;&#21644;&#27835;&#30103;&#21830;&#21697;&#31649;&#29702;&#23616;) &#25209;&#20934;&#29992;&#20110;&#20854;&#20182;&#30142;&#30149;&#30340;&#33647;&#29289;&#30340;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#36807;&#31243;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#19981;&#21516;&#29983;&#29289;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#33647;&#29289;&#38774;&#28857; (&#22522;&#22240;/&#34507;&#30333;&#36136;&#21644;&#29983;&#29289;&#36890;&#36335;) &#21644;&#33647;&#29289;&#23646;&#24615;&#65292;&#20197;&#21457;&#29616;&#26032;&#30340;&#33647;&#29289;&#38774;&#28857;&#25110;&#33647;&#29289;-&#30142;&#30149;&#20851;&#31995;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#22320;&#20998;&#26512;&#20102;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#24322;&#36136;&#25968;&#25454;&#65292;&#24182;&#19988;&#24050;&#34987;&#29992;&#20110;&#33647;&#29289;&#20877;&#21033;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#36136;&#33647;&#29289;&#25968;&#25454;&#19978;&#36827;&#34892;&#22810;&#29305;&#24449;&#31867;&#22411;&#32858;&#31867;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324; 438 &#31181;&#33647;&#29289;&#65292;&#20854;&#20013; 224 &#31181;&#27491;&#22312;&#25509;&#21463; COVID-19 &#20020;&#24202;&#35797;&#39564; (&#31867;&#21035; A)&#12290;&#20854;&#20313;&#33647;&#29289;&#32463;&#36807;&#31995;&#32479;&#36807;&#28388;&#20197;&#30830;&#20445;&#20877;&#21033;&#29992;&#28508;&#22312;&#33647;&#29289;&#20505;&#36873;&#29289;&#30340;&#23433;&#20840;&#24615; (&#31867;&#21035; B)&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312; COVID-19 &#29305;&#23450;&#33647;&#29289;&#20877;&#21033;&#29992;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#33021;&#22815;&#25581;&#31034;&#26377;&#35265;&#22320;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#24182;&#30830;&#23450;&#28508;&#22312;&#26377;&#25928;&#30340;&#33647;&#29289;&#20877;&#21033;&#29992;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repurposing (or repositioning) is the process of finding new therapeutic uses for drugs already approved by drug regulatory authorities (e.g., the Food and Drug Administration (FDA) and Therapeutic Goods Administration (TGA)) for other diseases. This involves analyzing the interactions between different biological entities, such as drug targets (genes/proteins and biological pathways) and drug properties, to discover novel drug-target or drug-disease relations. Artificial intelligence methods such as machine learning and deep learning have successfully analyzed complex heterogeneous data in the biomedical domain and have also been used for drug repurposing. This study presents a novel unsupervised machine learning framework that utilizes a graph-based autoencoder for multi-feature type clustering on heterogeneous drug data. The dataset consists of 438 drugs, of which 224 are under clinical trials for COVID-19 (category A). The rest are systematically filtered to ensure the safety 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#37319;&#29992;$\ell_0$-&#33539;&#25968;hinge loss&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;ADMM&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13991</link><description>&lt;p&gt;
&#24102;&#26377;$\ell_0$-&#33539;&#25968;hinge loss&#30340;&#26680;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Kernel Support Vector Machine Classifiers with the $\ell_0$-Norm Hinge Loss. (arXiv:2306.13991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#37319;&#29992;$\ell_0$-&#33539;&#25968;hinge loss&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;ADMM&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#24050;&#25104;&#20026;&#26368;&#25104;&#21151;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20043;&#19968;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#20445;&#35777;&#35757;&#32451;&#26679;&#26412;&#27491;&#30830;&#20998;&#31867;&#30340;&#26465;&#20214;&#19979;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#21040;&#36229;&#24179;&#38754;&#30340;&#38388;&#38548;&#12290;&#24120;&#29992;&#30340;hinge loss&#21450;&#20854;&#21464;&#20307;&#23545;&#26631;&#31614;&#22122;&#22768;&#25935;&#24863;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#26080;&#30028;&#24615;&#32780;&#23545;&#37325;&#37319;&#26679;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#24102;&#26377;$\ell_0$-norm hinge loss&#65288;&#31216;&#20026;$\ell_0$-KSVM&#65289;&#30340;&#26680;SVM&#65292;&#35813;&#26041;&#27861;&#26159;hinge loss &#21644; $\ell_0$-norm &#30340;&#22797;&#21512;&#20989;&#25968;&#65292;&#21487;&#20197;&#20811;&#26381;&#19978;&#36848;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machine (SVM) has been one of the most successful machine learning techniques for binary classification problems. The key idea is to maximize the margin from the data to the hyperplane subject to correct classification on training samples. The commonly used hinge loss and its variations are sensitive to label noise, and unstable for resampling due to its unboundedness. This paper is concentrated on the kernel SVM with the $\ell_0$-norm hinge loss (referred as $\ell_0$-KSVM), which is a composite function of hinge loss and $\ell_0$-norm and then could overcome the difficulties mentioned above. In consideration of the nonconvexity and nonsmoothness of $\ell_0$-norm hinge loss, we first characterize the limiting subdifferential of the $\ell_0$-norm hinge loss and then derive the equivalent relationship among the proximal stationary point, the Karush-Kuhn-Tucker point, and the local optimal solution of $\ell_0$-KSVM. Secondly, we develop an ADMM algorithm for $\ell_0$-KSVM, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13990</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#65306;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation. (arXiv:2306.13990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#37492;&#23450;&#21644;&#28040;&#38500;&#26631;&#31614;&#22122;&#22768;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#22823;&#24133;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#26041;&#27861;&#37117;&#26159;&#20026;&#20998;&#31867;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#32780;&#22522;&#20110;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#30340;&#25968;&#25454;&#28165;&#29702;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#21463;&#21040;&#20132;&#21449;&#39564;&#35777;&#20013;&#19981;&#21516;&#25240;&#30340;&#24615;&#33021;&#27874;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#20272;&#35745;&#30340;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;ReCoV&#65289;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;ReCoV&#36890;&#36807;&#35760;&#24405;&#27599;&#20010;&#26368;&#24046;&#34920;&#29616;&#25240;&#20013;&#30340;&#26679;&#26412;ID&#26469;&#26500;&#24314;&#19968;&#20010;&#22122;&#22768;&#30452;&#26041;&#22270;&#65292;&#20197;&#27492;&#26469;&#25490;&#21517;&#26679;&#26412;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22122;&#22768;&#30452;&#26041;&#22270;&#26469;&#37492;&#21035;&#22024;&#26434;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ReCoV&#22312;&#20998;&#31867;&#20219;&#21153;&#22522;&#20934;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26631;&#31614;&#28165;&#29702;&#31639;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we 
&lt;/p&gt;</description></item><item><title>SAM++&#26159;&#19968;&#20010;&#21307;&#23398;&#24433;&#20687;&#35299;&#21078;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22806;&#35266;&#21644;&#35821;&#20041;&#23884;&#20837;&#65292;&#24182;&#19988;&#36890;&#36807;&#22266;&#23450;&#28857;&#21305;&#37197;&#26426;&#21046;&#21516;&#26102;&#35299;&#20915;&#22806;&#35266;&#30456;&#20284;&#20294;&#35821;&#20041;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#32467;&#26500;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13988</link><description>&lt;p&gt;
SAM++: &#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#21644;&#32467;&#26500;&#25512;&#29702;&#22686;&#24378;&#35299;&#21078;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
SAM++: Enhancing Anatomic Matching using Semantic Information and Structural Inference. (arXiv:2306.13988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13988
&lt;/p&gt;
&lt;p&gt;
SAM++&#26159;&#19968;&#20010;&#21307;&#23398;&#24433;&#20687;&#35299;&#21078;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22806;&#35266;&#21644;&#35821;&#20041;&#23884;&#20837;&#65292;&#24182;&#19988;&#36890;&#36807;&#22266;&#23450;&#28857;&#21305;&#37197;&#26426;&#21046;&#21516;&#26102;&#35299;&#20915;&#22806;&#35266;&#30456;&#20284;&#20294;&#35821;&#20041;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#32467;&#26500;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20363;&#22914; CT &#21644; MRI &#25552;&#20379;&#20102;&#26377;&#20851;&#36523;&#20307;&#20869;&#37096;&#32467;&#26500;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#22270;&#20687;&#20013;&#35782;&#21035;&#20851;&#38190;&#35299;&#21078;&#32467;&#26500;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23558;&#27492;&#35270;&#20026;&#37197;&#20934;&#25110;&#20851;&#38190;&#28857;&#22238;&#24402;&#20219;&#21153;&#65292;&#20855;&#26377;&#20934;&#30830;&#21305;&#37197;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#21482;&#33021;&#22788;&#29702;&#39044;&#23450;&#20041;&#30340;&#22320;&#26631;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#26041;&#27861;&#24050;&#34987;&#24341;&#20837;&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#20854;&#20013;&#20043;&#19968;&#31216;&#20026; SAM&#65292;&#25552;&#20986;&#20351;&#29992;&#23494;&#38598;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064; CT &#22270;&#20687;&#19978;&#27599;&#20010;&#28857;&#30340;&#29420;&#29305;&#23884;&#20837;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;SAM &#22312;&#22788;&#29702;&#22806;&#35266;&#30456;&#20284;&#20294;&#35821;&#20041;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#20294;&#22806;&#35266;&#19981;&#21516;&#30340;&#32467;&#26500;&#26102;&#20173;&#21487;&#33021;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SAM++&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22266;&#23450;&#28857;&#21305;&#37197;&#26426;&#21046;&#21516;&#26102;&#23398;&#20064;&#22806;&#35266;&#21644;&#35821;&#20041;&#23884;&#20837;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102; SAM++ &#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical images like CT and MRI provide detailed information about the internal structure of the body, and identifying key anatomical structures from these images plays a crucial role in clinical workflows. Current methods treat it as a registration or key-point regression task, which has limitations in accurate matching and can only handle predefined landmarks. Recently, some methods have been introduced to address these limitations. One such method, called SAM, proposes using a dense self-supervised approach to learn a distinct embedding for each point on the CT image and achieving promising results. Nonetheless, SAM may still face difficulties when dealing with structures that have similar appearances but different semantic meanings or similar semantic meanings but different appearances. To overcome these limitations, we propose SAM++, a framework that simultaneously learns appearance and semantic embeddings with a novel fixed-points matching mechanism. We tested the SAM++ framework 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#25968;&#25454;&#20998;&#31867;&#30340;&#31283;&#20581;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#26080;&#38656;&#35843;&#21442;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#65292;&#24050;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#35777;&#26126;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2306.13985</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#30340;&#39640;&#32500;&#25968;&#25454;&#31283;&#20581;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance. (arXiv:2306.13985v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#25968;&#25454;&#20998;&#31867;&#30340;&#31283;&#20581;&#30340;&#25968;&#25454;&#33258;&#36866;&#24212;&#33021;&#37327;&#36317;&#31163;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#26080;&#38656;&#35843;&#21442;&#19988;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#65292;&#24050;&#22312;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#24471;&#21040;&#35777;&#26126;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#39640;&#32500;&#20302;&#26679;&#26412;&#37327;&#65288;HDLSS&#65289;&#25968;&#25454;&#30340;&#20998;&#31867;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#22522;&#22240;&#34920;&#36798;&#30740;&#31350;&#12289;&#30284;&#30151;&#30740;&#31350;&#21644;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#19987;&#38376;&#20026;HDLSS&#25968;&#25454;&#35774;&#35745;&#30340;&#20998;&#31867;&#22120;&#30340;&#24320;&#21457;&#21644;&#20998;&#26512;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#27809;&#26377;&#35843;&#33410;&#21442;&#25968;&#65292;&#24182;&#19988;&#26159;&#31283;&#20581;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#21463;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20219;&#20309;&#30697;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#19968;&#20123;&#30456;&#24403;&#26222;&#36941;&#30340;&#26465;&#20214;&#19979;&#65292;&#23427;&#20204;&#22312;HDLSS&#28176;&#36817;&#21306;&#22495;&#20869;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31867;&#12290;&#36824;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#20998;&#31867;&#25216;&#26415;&#20248;&#20110;&#20960;&#31181;&#24191;&#27867;&#35748;&#21487;&#30340;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of high-dimensional low sample size (HDLSS) data poses a challenge in a variety of real-world situations, such as gene expression studies, cancer research, and medical imaging. This article presents the development and analysis of some classifiers that are specifically designed for HDLSS data. These classifiers are free of tuning parameters and are robust, in the sense that they are devoid of any moment conditions of the underlying data distributions. It is shown that they yield perfect classification in the HDLSS asymptotic regime, under some fairly general conditions. The comparative performance of the proposed classifiers is also investigated. Our theoretical results are supported by extensive simulation studies and real data analysis, which demonstrate promising advantages of the proposed classification techniques over several widely recognized methods.
&lt;/p&gt;</description></item><item><title>&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#20998;&#25512;&#26029;&#65292;&#29983;&#25104;&#32039;&#20945;&#30340;&#29305;&#24449;&#24352;&#37327;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25512;&#26029;&#65292;&#20197;&#38477;&#20302;&#25512;&#26029;&#26102;&#24310;&#21644;&#38477;&#20302;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.13982</link><description>&lt;p&gt;
&#31227;&#21160;&#20113;&#21327;&#21516;&#26234;&#33021;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Mobile-Cloud Inference for Collaborative Intelligence. (arXiv:2306.13982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13982
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#37096;&#20998;&#25512;&#26029;&#65292;&#29983;&#25104;&#32039;&#20945;&#30340;&#29305;&#24449;&#24352;&#37327;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25512;&#26029;&#65292;&#20197;&#38477;&#20302;&#25512;&#26029;&#26102;&#24310;&#21644;&#38477;&#20302;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31227;&#21160;&#35774;&#22791;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25512;&#26029;&#30340;&#26356;&#24555;&#25191;&#34892;&#21644;&#26356;&#20302;&#30340;&#33021;&#37327;&#28040;&#32791;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#21382;&#21490;&#19978;&#65292;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#27169;&#22411;&#36739;&#22823;&#22411;&#26368;&#26032;&#30740;&#31350;&#27169;&#22411;&#36739;&#23567;&#19988;&#31616;&#21333;&#65292;&#21482;&#33021;&#22312;&#20113;&#31471;&#36816;&#34892;&#12290;&#28982;&#32780;&#65292;&#21482;&#22312;&#20113;&#19978;&#36827;&#34892;&#25512;&#26029;&#23384;&#22312;&#24102;&#23485;&#28040;&#32791;&#22686;&#21152;&#21644;&#26356;&#39640;&#24310;&#36831;&#30340;&#32570;&#38519;&#12290;&#27492;&#22806;&#65292;&#21482;&#22312;&#20113;&#19978;&#36827;&#34892;&#25512;&#26029;&#38656;&#35201;&#23558;&#36755;&#20837;&#25968;&#25454;&#65288;&#22270;&#20687;&#12289;&#38899;&#39057;&#65289;&#20840;&#37096;&#20256;&#36755;&#21040;&#20113;&#19978;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#21478;&#26377;&#19968;&#31181;&#36873;&#25321;&#65306;&#20849;&#20139;&#31227;&#21160;&#20113;&#25512;&#26029;&#12290;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#37096;&#20998;&#25512;&#26029;&#20197;&#32553;&#20943;&#36755;&#20837;&#25968;&#25454;&#30340;&#32500;&#24230;&#24182;&#29983;&#25104;&#19968;&#20010;&#32039;&#20945;&#30340;&#29305;&#24449;&#24352;&#37327;&#65292;&#36825;&#26159;&#36755;&#20837;&#20449;&#21495;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#23558;&#29305;&#24449;&#24352;&#37327;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#36827;&#34892;&#36827;&#19968;&#27493;&#25512;&#26029;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#38477;&#20302;&#25512;&#26029;&#26102;&#24310;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI applications for mobile devices become more prevalent, there is an increasing need for faster execution and lower energy consumption for deep learning model inference. Historically, the models run on mobile devices have been smaller and simpler in comparison to large state-of-the-art research models, which can only run on the cloud. However, cloud-only inference has drawbacks such as increased network bandwidth consumption and higher latency. In addition, cloud-only inference requires the input data (images, audio) to be fully transferred to the cloud, creating concerns about potential privacy breaches.  There is an alternative approach: shared mobile-cloud inference. Partial inference is performed on the mobile in order to reduce the dimensionality of the input data and arrive at a compact feature tensor, which is a latent space representation of the input signal. The feature tensor is then transmitted to the server for further inference. This strategy can reduce inference laten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.13960</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis. (arXiv:2306.13960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27491;&#21017;&#21270;SE(3)&#32676;&#21367;&#31215;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#36830;&#32493;SO(3)&#26680;&#21644;&#31354;&#38388;&#26680;&#20197;&#23454;&#29616;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#27491;&#21017;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(G-CNN)&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#25552;&#39640;&#23545;&#19981;&#21516;&#20960;&#20309;&#23545;&#31216;&#24615;&#30340;&#31561;&#21464;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;SE(3)&#38382;&#39064;&#65292;&#21363;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#22312;&#20307;&#31215;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#12290;&#20307;&#31215;&#22270;&#20687;&#25968;&#25454;&#22312;&#35768;&#22810;&#21307;&#30103;&#35774;&#32622;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#21463;&#21487;&#20998;&#31163;&#32452;&#21367;&#31215;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;SE(3)&#32676;&#21367;&#31215;&#26680;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#36830;&#32493;&#30340;SO(3)&#65288;&#26059;&#36716;&#65289;&#26680;&#21644;&#31354;&#38388;&#26680;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22343;&#21248;&#30340;SO(3)&#32593;&#26684;&#26469;&#36817;&#20284;&#36830;&#32493;&#35774;&#23450;&#19979;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;SO(3)&#26680;&#26159;&#36890;&#36807;&#31867;&#20284;&#22343;&#21248;&#32593;&#26684;&#30340;RBF&#25554;&#20540;&#21442;&#25968;&#21270;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;SE(3)&#31561;&#21464;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#20998;&#31867;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;CNN&#21644;&#24120;&#35268;&#31163;&#25955;G-CNN&#65292;&#24182;&#26174;&#31034;&#20986;&#26174;&#30528;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22122;&#22768;&#25968;&#25454;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#36798;&#21040;16.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular group convolutional neural networks (G-CNNs) have been shown to increase model performance and improve equivariance to different geometrical symmetries. This work addresses the problem of SE(3), i.e., roto-translation equivariance, on volumetric data. Volumetric image data is prevalent in many medical settings. Motivated by the recent work on separable group convolutions, we devise a SE(3) group convolution kernel separated into a continuous SO(3) (rotation) kernel and a spatial kernel. We approximate equivariance to the continuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernel is parameterized via RBF interpolation on similarly uniform grids. We demonstrate the advantages of our approach in volumetric medical image analysis. Our SE(3) equivariant models consistently outperform CNNs and regular discrete G-CNNs on challenging medical classification tasks and show significantly improved generalization capabilities. Our approach achieves up to a 16.5% gain in
&lt;/p&gt;</description></item><item><title>DiffDTM&#26159;&#19968;&#20010;&#21435;&#38500;&#26465;&#20214;&#32467;&#26500;&#38480;&#21046;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#38024;&#23545;&#21452;&#37325;&#34507;&#30333;&#36136;&#38774;&#28857;&#30340;&#29983;&#29289;&#27963;&#24615;&#20998;&#23376;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#27425;&#24615;&#26377;&#26465;&#20214;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#12289;&#21487;&#21512;&#25104;&#21644;&#26032;&#39062;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#65292;&#24182;&#32988;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.13957</link><description>&lt;p&gt;
DiffDTM: &#38024;&#23545;&#21452;&#37325;&#34507;&#30333;&#36136;&#38774;&#28857;&#30340;&#29983;&#29289;&#27963;&#24615;&#20998;&#23376;&#29983;&#25104;&#30340;&#26080;&#26465;&#20214;&#32467;&#26500;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffDTM: A conditional structure-free framework for bioactive molecules generation targeted for dual proteins. (arXiv:2306.13957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13957
&lt;/p&gt;
&lt;p&gt;
DiffDTM&#26159;&#19968;&#20010;&#21435;&#38500;&#26465;&#20214;&#32467;&#26500;&#38480;&#21046;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#38024;&#23545;&#21452;&#37325;&#34507;&#30333;&#36136;&#38774;&#28857;&#30340;&#29983;&#29289;&#27963;&#24615;&#20998;&#23376;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#27425;&#24615;&#26377;&#26465;&#20214;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#12289;&#21487;&#21512;&#25104;&#21644;&#26032;&#39062;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#65292;&#24182;&#32988;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20026;&#24102;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;de novo&#20998;&#23376;&#29983;&#25104;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#21452;&#37325;&#34507;&#30333;&#36136;&#38774;&#28857;&#30340;&#20998;&#23376;&#29983;&#25104;&#20173;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#34507;&#30333;&#36136;3D&#32467;&#26500;&#25968;&#25454;&#30340;&#33719;&#21462;&#65292;&#33258;&#22238;&#24402;&#37319;&#26679;&#20197;&#21450;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;DiffDTM&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26465;&#20214;&#32467;&#26500;&#20813;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#21452;&#37325;&#38774;&#28857;&#20998;&#23376;&#29983;&#25104;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DiffDTM&#25509;&#25910;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#20998;&#23376;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#38750;&#34507;&#30333;&#36136;&#21644;&#20998;&#23376;&#26500;&#35937;&#65292;&#24182;&#24341;&#20837;&#20449;&#24687;&#34701;&#21512;&#27169;&#22359;&#20197;&#23454;&#29616;&#19968;&#27425;&#24615;&#26377;&#26465;&#20214;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22810;&#35270;&#22270;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;DiffDTM&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#21452;&#37325;&#34507;&#30333;&#36136;&#30340;&#33647;&#29289;&#26679;&#65292;&#21487;&#21512;&#25104;&#65292;&#26032;&#39062;&#19988;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#20998;&#23376;&#65292;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep generative models shed light on de novo molecule generation with desired properties. However, molecule generation targeted for dual protein targets still faces formidable challenges including protein 3D structure data requisition for model training, auto-regressive sampling, and model generalization for unseen targets. Here, we proposed DiffDTM, a novel conditional structure-free deep generative model based on a diffusion model for dual targets based molecule generation to address the above issues. Specifically, DiffDTM receives protein sequences and molecular graphs as inputs instead of protein and molecular conformations and incorporates an information fusion module to achieve conditional generation in a one-shot manner. We have conducted comprehensive multi-view experiments to demonstrate that DiffDTM can generate drug-like, synthesis-accessible, novel, and high-binding affinity molecules targeting specific dual proteins, outperforming the state-of-the-art (SOTA) mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13948</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#65306;&#20171;&#32461;&#26131;&#20110;&#20351;&#29992;&#30340;PurpleAirSF&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#32473;&#30740;&#31350;&#20154;&#21592;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20840;&#38754;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;PurpleAir&#32593;&#32476;&#20013;&#25910;&#38598;&#32780;&#26469;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#21508;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26032;&#22411;&#39044;&#27979;&#27169;&#22411;&#12289;&#30740;&#31350;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#20197;&#21450;&#35843;&#26597;&#20854;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#30340;&#24433;&#21709;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;PurpleAirSF&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#32463;&#20856;&#21644;&#29616;&#20195;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#21046;&#23450;&#24314;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#12289;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13947</link><description>&lt;p&gt;
&#38754;&#21521;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison of Pre-trained Language Models for Turkish Address Parsing. (arXiv:2306.13947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#12289;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#27169;&#22411;&#22312;&#22303;&#32819;&#20854;&#22320;&#22336;&#35299;&#26512;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#21457;&#29616;&#38024;&#23545;&#22303;&#32819;&#20854;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#21450;&#20854;&#21464;&#31181;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#22823;&#22810;&#25968;&#23398;&#26415;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#33521;&#35821;&#36827;&#34892;&#30340;;&#28982;&#32780;&#65292;&#22810;&#35821;&#35328;&#21644;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#25968;&#37327;&#27491;&#22312;&#31283;&#27493;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30740;&#31350;&#22768;&#31216;&#65292;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#20542;&#21521;&#20110;&#38024;&#23545;&#20854;&#26696;&#20363;&#30740;&#31350;&#30340;&#35821;&#35328;&#26469;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#12290;&#26412;&#25991;&#38024;&#23545;&#22303;&#32819;&#20854;&#22320;&#22270;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#22810;&#35821;&#35328;&#21644;&#22303;&#32819;&#20854;BERT&#12289;DistilBERT&#12289;ELECTRA&#21644;RoBERTa&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#29992;&#20110;&#24494;&#35843;BERT&#65292;&#20197;&#21450;&#26631;&#20934;&#30340;&#19968;&#23618;&#24494;&#35843;&#26041;&#27861;&#12290;&#23545;&#20110;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36136;&#37327;&#30456;&#23545;&#36739;&#39640;&#30340;&#20013;&#31561;&#35268;&#27169;&#22320;&#22336;&#35299;&#26512;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Transformer based pre-trained models such as BERT and its variants, which are trained on large corpora, have demonstrated tremendous success for natural language processing (NLP) tasks. Most of academic works are based on the English language; however, the number of multilingual and language specific studies increase steadily. Furthermore, several studies claimed that language specific models outperform multilingual models in various tasks. Therefore, the community tends to train or fine-tune the models for the language of their case study, specifically. In this paper, we focus on Turkish maps data and thoroughly evaluate both multilingual and Turkish based BERT, DistilBERT, ELECTRA and RoBERTa. Besides, we also propose a MultiLayer Perceptron (MLP) for fine-tuning BERT in addition to the standard approach of one-layer fine-tuning. For the dataset, a mid-sized Address Parsing corpus taken with a relatively high quality is constructed. Conducted experiments on this dataset indicate that
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#20351;&#29992;Transformer&#31561;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25353;&#29031;&#22788;&#29702;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.13945</link><description>&lt;p&gt;
&#22823;&#22411;&#24207;&#21015;&#27169;&#22411;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Sequence Models for Sequential Decision-Making: A Survey. (arXiv:2306.13945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#20351;&#29992;Transformer&#31561;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#26368;&#36817;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25353;&#29031;&#22788;&#29702;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#39044;&#27979;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#36890;&#29992;&#24207;&#21015;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;GPT-3&#21644;Swin Transformer&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#65292;&#20294;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#35810;&#38382;&#23427;&#20204;&#26159;&#21542;&#36866;&#29992;&#20110;&#36890;&#24120;&#23384;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#20449;&#29992;&#20998;&#37197;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#38382;&#39064;&#30340;&#39034;&#24207;&#20915;&#31574;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#21560;&#24341;&#20102;RL&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#20855;&#26377;&#26174;&#30528;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#36890;&#36807;&#35752;&#35770;&#39034;&#24207;&#20915;&#31574;&#21644;&#24207;&#21015;&#24314;&#27169;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22788;&#29702;&#21069;&#36848;&#38382;&#39064;&#30340;&#26041;&#24335;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#65292;&#35299;&#20915;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;Transformer&#65289;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27515;&#23616;&#30340;&#36793;&#30028;&#26469;&#36776;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#29366;&#24577;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21516;&#26102;&#20943;&#23569;&#23545;&#25506;&#32034;&#30340;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20102;&#20004;&#20010;&#31574;&#30053;&#65306;&#19968;&#20010;&#20219;&#21153;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#21450;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13944</link><description>&lt;p&gt;
&#20855;&#26377;&#36991;&#24320;&#27515;&#23616;&#21644;&#24674;&#22797;&#33021;&#21147;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning with Dead-Ends Avoidance and Recovery. (arXiv:2306.13944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27515;&#23616;&#30340;&#36793;&#30028;&#26469;&#36776;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#29366;&#24577;&#65292;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21516;&#26102;&#20943;&#23569;&#23545;&#25506;&#32034;&#30340;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20102;&#20004;&#20010;&#31574;&#30053;&#65306;&#19968;&#20010;&#20219;&#21153;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#21450;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24615;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#29616;&#23454;&#29615;&#22659;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#30340;&#23433;&#20840;&#24615;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#20197;&#36991;&#20813;&#19981;&#23433;&#20840;&#30340;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#20005;&#37325;&#38459;&#30861;&#20102;&#25506;&#32034;&#65292;&#20351;&#31639;&#27861;&#30340;&#22238;&#25253;&#22823;&#22823;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#36793;&#30028;&#65292;&#21306;&#20998;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36793;&#30028;&#31561;&#20215;&#20110;&#21306;&#20998;&#27515;&#23616;&#29366;&#24577;&#65292;&#34920;&#26126;&#23433;&#20840;&#25506;&#32034;&#30340;&#26368;&#22823;&#31243;&#24230;&#65292;&#22240;&#27492;&#22312;&#25506;&#32034;&#26041;&#38754;&#30340;&#38480;&#21046;&#26368;&#23567;&#12290;&#31867;&#20284;&#20110;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#20998;&#31163;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#20004;&#20010;&#31574;&#30053;&#65292;(1) &#21482;&#32771;&#34385;&#25913;&#21892;&#20219;&#21153;&#34920;&#29616;&#30340;&#20219;&#21153;&#31574;&#30053;&#65292;&#20197;&#21450; (2) &#26368;&#22823;&#21270;&#23433;&#20840;&#24615;&#30340;&#24674;&#22797;&#31574;&#30053;&#12290;&#24674;&#22797;&#31574;&#30053;&#21644;&#30456;&#24212;&#30340;&#23433;&#20840;&#24615;&#25209;&#21028;&#23478;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#23433;&#20840;&#25209;&#21028;&#23478;&#20250;&#21306;&#20998;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#29366;&#24577;&#65292;&#32780;&#24674;&#22797;&#31574;&#30053;&#20250;&#37319;&#21462;&#25514;&#26045;&#20197;&#20174;&#19981;&#23433;&#20840;&#29366;&#24577;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is one of the main challenges in applying reinforcement learning to realistic environmental tasks. To ensure safety during and after training process, existing methods tend to adopt overly conservative policy to avoid unsafe situations. However, overly conservative policy severely hinders the exploration, and makes the algorithms substantially less rewarding. In this paper, we propose a method to construct a boundary that discriminates safe and unsafe states. The boundary we construct is equivalent to distinguishing dead-end states, indicating the maximum extent to which safe exploration is guaranteed, and thus has minimum limitation on exploration. Similar to Recovery Reinforcement Learning, we utilize a decoupled RL framework to learn two policies, (1) a task policy that only considers improving the task performance, and (2) a recovery policy that maximizes safety. The recovery policy and a corresponding safety critic are pretrained on an offline dataset, in which the safety c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861; OTSL&#65292;&#23427;&#37319;&#29992;&#22806;&#26679;&#26412;&#21644;&#37325;&#25277;&#26679;&#31574;&#30053;&#26469;&#20272;&#31639;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#25552;&#39640;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#22270;&#24418;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13932</link><description>&lt;p&gt;
&#21033;&#29992;&#22806;&#26679;&#26412;&#21644;&#37325;&#25277;&#26679;&#31574;&#30053;&#20248;&#21270;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tuning structure learning algorithms with out-of-sample and resampling strategies. (arXiv:2306.13932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861; OTSL&#65292;&#23427;&#37319;&#29992;&#22806;&#26679;&#26412;&#21644;&#37325;&#25277;&#26679;&#31574;&#30053;&#26469;&#20272;&#31639;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#21487;&#25552;&#39640;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#22270;&#24418;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23454;&#36341;&#32773;&#23558;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#20854;&#25968;&#25454;&#26102;&#65292;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#23450;&#19968;&#32452;&#36229;&#21442;&#25968;&#65307;&#21542;&#21017;&#65292;&#20551;&#23450;&#19968;&#32452;&#36229;&#21442;&#25968;&#40664;&#35748;&#20540;&#12290;&#26368;&#20339;&#36229;&#21442;&#25968;&#37197;&#32622;&#24120;&#24120;&#21462;&#20915;&#20110;&#22810;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#36890;&#24120;&#26410;&#30693;&#30340;&#30495;&#23454;&#24213;&#23618;&#22270;&#30340;&#22823;&#23567;&#21644;&#23494;&#24230;&#12289;&#36755;&#20837;&#25968;&#25454;&#30340;&#26679;&#26412;&#22823;&#23567;&#21644;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#21517;&#20026;Out-of-sample Tuning for Structure Learning&#65288;OTSL&#65289;&#65292;&#23427;&#37319;&#29992;&#22806;&#26679;&#26412;&#21644;&#37325;&#25277;&#26679;&#31574;&#30053;&#26469;&#20272;&#31639;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#38598;&#21644;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#37197;&#32622;&#12290;&#21512;&#25104;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;OTSL&#20316;&#20026;&#28151;&#21512;&#21644;&#22522;&#20110;&#20998;&#25968;&#30340;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#25163;&#27573;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#22270;&#24418;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges practitioners face when applying structure learning algorithms to their data involves determining a set of hyperparameters; otherwise, a set of hyperparameter defaults is assumed. The optimal hyperparameter configuration often depends on multiple factors, including the size and density of the usually unknown underlying true graph, the sample size of the input data, and the structure learning algorithm. We propose a novel hyperparameter tuning method, called the Out-of-sample Tuning for Structure Learning (OTSL), that employs out-of-sample and resampling strategies to estimate the optimal hyperparameter configuration for structure learning, given the input data set and structure learning algorithm. Synthetic experiments show that employing OTSL as a means to tune the hyperparameters of hybrid and score-based structure learning algorithms leads to improvements in graphical accuracy compared to the state-of-the-art. We also illustrate the applicability of this approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;Deep AR&#22312;&#32929;&#31080;&#25351;&#25968;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13931</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#32929;&#31080;&#25351;&#25968;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Predicting Stock Index Using Deep Learning Models. (arXiv:2306.13931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20256;&#32479;&#39044;&#27979;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;Deep AR&#22312;&#32929;&#31080;&#25351;&#25968;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#29616;&#24050;&#23581;&#35797;&#20102;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#25216;&#26415;&#20998;&#26512;&#12289;&#31639;&#27861;&#32479;&#35745;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#22330;&#26223;&#20013;&#65292;&#22914;LSTM&#21644;&#24120;&#35268;RNN&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#30701;&#26399;&#21644;&#38271;&#26399;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;ARIMA&#12289;SARIMA&#21644;SARIMAX&#65292;&#20197;&#21450;&#20351;&#29992;RNN&#26500;&#24314;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#22914;DF-RNN&#12289;DSSM&#21644;Deep AR&#12290;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#26631;&#20934;NIFTY-50&#25968;&#25454;&#38598;&#20351;&#29992;MSE&#12289;RMSE&#12289;MAPE&#12289;POCID&#21644;Theil's U&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Deep AR&#22312;&#25152;&#26377;&#20854;&#20182;&#24120;&#35268;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26041;&#27861;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#20854;MAPE&#20026;0.01&#65292;RMSE&#20026;189&#12290;&#27492;&#22806;&#65292;&#24403;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#26102;&#65292;Deep AR&#21644;GRU&#30340;&#24615;&#33021;&#19981;&#20250;&#38477;&#20302;&#65292;&#36825;&#34920;&#26126;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has seen many methods attempted over the past few decades, including traditional technical analysis, algorithmic statistical models, and more recent machine learning and artificial intelligence approaches. Recently, neural networks have been incorporated into the forecasting scenario, such as the LSTM and conventional RNN approaches, which utilize short-term and long-term dependencies. This study evaluates traditional forecasting methods, such as ARIMA, SARIMA, and SARIMAX, and newer neural network approaches, such as DF-RNN, DSSM, and Deep AR, built using RNNs. The standard NIFTY-50 dataset from Kaggle is used to assess these models using metrics such as MSE, RMSE, MAPE, POCID, and Theil's U. Results show that Deep AR outperformed all other conventional deep learning and traditional approaches, with the lowest MAPE of 0.01 and RMSE of 189. Additionally, the performance of Deep AR and GRU did not degrade when the amount of training data was reduced, suggesting t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.13929</link><description>&lt;p&gt;
&#35780;&#20272;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29992;&#20110;&#31867;&#21035;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of GAN Generated Synthetic Tabular Data for Class Balancing and Low Resource Settings. (arXiv:2306.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#21644;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;SMOTE&#12289;ADASYN&#21644;GAN&#25216;&#26415;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#24182;&#25913;&#21892;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#31867;&#21035;&#24179;&#34913;&#23454;&#39564;&#65292;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#36827;&#34892;&#20302;&#36164;&#28304;&#35774;&#32622;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25152;&#26377;&#20998;&#31867;&#27169;&#22411;&#30340;&#20027;&#35201;&#35780;&#20272;&#25351;&#26631;&#26159;&#21484;&#22238;&#29575;&#12290;&#31867;&#21035;&#24179;&#34913;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GAN&#24179;&#34913;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;GLM&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#21516;&#26679;&#22312;&#20302;&#36164;&#28304;&#23454;&#39564;&#20013;&#65292;&#35757;&#32451;&#22312;GAN&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#27604;&#21407;&#22987;&#25968;&#25454;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;&#36825;&#20123;&#21457;&#29616;&#23637;&#31034;&#20102;GAN&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#22312;&#35299;&#20915;&#20998;&#31867;&#20219;&#21153;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#25361;&#25112;&#21644;&#25913;&#21892;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#27169;&#22411;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study aimed to address the issue of imbalanced data in classification tasks and evaluated the suitability of SMOTE, ADASYN, and GAN techniques in generating synthetic data to address the class imbalance and improve the performance of classification models in low-resource settings. The study employed the Generalised Linear Model (GLM) algorithm for class balancing experiments and the Random Forest (RF) algorithm for low-resource setting experiments to assess model performance under varying training data. The recall metric was the primary evaluation metric for all classification models. The results of the class balancing experiments showed that the GLM model trained on GAN-balanced data achieved the highest recall value. Similarly, in low-resource experiments, models trained on data enhanced with GAN-synthesized data exhibited better recall values than original data. These findings demonstrate the potential of GAN-generated synthetic data for addressing the challenge of imbal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13928</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#30340;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems. (arXiv:2306.13928v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#25968;&#25454;&#39537;&#21160;&#36870;&#26368;&#20248;&#25511;&#21046;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#31995;&#32479;&#19979;&#30340;&#25104;&#26412;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#35770;&#36848;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#22495;&#30340;&#36870;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#30446;&#30340;&#26159;&#20174;&#35266;&#27979;&#20540;&#20013;&#25512;&#26029;&#20986;&#39537;&#21160;&#26234;&#33021;&#20307;&#34892;&#21160;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#36825;&#20010;&#25104;&#26412;&#26159;&#38750;&#20984;&#21644;&#38750;&#24179;&#31283;&#30340;&#65292;&#21516;&#26102;&#21463;&#21040;&#38750;&#32447;&#24615;&#12289;&#38750;&#24179;&#31283;&#21644;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#23454;&#29616;&#25104;&#26412;&#20272;&#35745;&#65292;&#21363;&#20351;&#20195;&#29702;&#25104;&#26412;&#19981;&#26159;&#20984;&#30340;&#65292;&#26412;&#25991;&#20063;&#33021;&#22815;&#29983;&#25104;&#20984;&#38382;&#39064;&#12290;&#20026;&#20102;&#24471;&#20986;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#20197;&#38543;&#26426;&#31574;&#30053;&#20026;&#20915;&#31574;&#21464;&#37327;&#30340;&#26377;&#38480;&#26102;&#22495;&#21069;&#21521;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#26368;&#20248;&#35299;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#36716;&#21270;&#20026;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#36890;&#36807;&#34394;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#26377;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with a finite-horizon inverse control problem, which has the goal of inferring, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result that enables cost estimation by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. For this problem, we give an explicit expression for the optimal solution. Moreover, we turn our findings into algorithmic procedures and we show the effectiveness of our approach via both in-silico and experimental validations with real hardware. All the experiments confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#12290; &#21457;&#29616;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#12290;</title><link>http://arxiv.org/abs/2306.13926</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#33719;&#30410;&#30340;&#35777;&#26126;&#65306;&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks Provably Benefit from Structural Information: A Feature Learning Perspective. (arXiv:2306.13926v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;GNN&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#12290; &#21457;&#29616;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#20808;&#39537;&#24615;&#36827;&#23637;&#65292;&#22312;&#22788;&#29702;&#22270;&#36755;&#20837;&#26102;&#34920;&#29616;&#20986;&#27604;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#26356;&#20248;&#36234;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;GNN&#30340;&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#20173;&#22788;&#20110;&#21021;&#22987;&#38454;&#27573;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30740;&#31350;&#22270;&#21367;&#31215;&#22312;&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#20316;&#29992;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20004;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;(GCNs)&#20013;&#20449;&#21495;&#23398;&#20064;&#21644;&#22122;&#22768;&#35760;&#24518;&#30340;&#19981;&#21516;&#21051;&#30011;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#20004;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#23545;&#24212;&#30340;CNNs&#30456;&#27604;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#26174;&#33879;&#22686;&#24378;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#21306;&#22495;&#65292;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#20449;&#21495;&#23398;&#20064;&#36229;&#36234;&#20102;&#22122;&#22768;&#35760;&#24518;&#65292;&#24182;&#19988;&#36817;&#20284;&#20110;&#22240;&#23376;$\sqrt{D}^{q-2}$&#65292;&#20854;&#20013;$D$&#34920;&#31034;&#33410;&#28857;&#30340;&#26399;&#26395;&#24230;&#25968;&#65292;$q$&#34920;&#31034;ReLU&#28608;&#27963;&#21151;&#33021;&#30340;&#24130;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have pioneered advancements in graph representation learning, exhibiting superior feature learning and performance over multilayer perceptrons (MLPs) when handling graph inputs. However, understanding the feature learning aspect of GNNs is still in its initial stage. This study aims to bridge this gap by investigating the role of graph convolution within the context of feature learning theory in neural networks using gradient descent training. We provide a distinct characterization of signal learning and noise memorization in two-layer graph convolutional networks (GCNs), contrasting them with two-layer convolutional neural networks (CNNs). Our findings reveal that graph convolution significantly augments the benign overfitting regime over the counterpart CNNs, where signal learning surpasses noise memorization, by approximately factor $\sqrt{D}^{q-2}$, with $D$ denoting a node's expected degree and $q$ being the power of the ReLU activation function where 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31561;&#21464;&#24615;&#30446;&#26631;&#24182;&#29702;&#35770;&#35777;&#26126;&#20102;&#26368;&#20248;&#35299;&#20250;&#23558;&#36755;&#20837;&#31354;&#38388;&#30340;&#22686;&#24378;&#21464;&#25442;&#23545;&#24212;&#20110;&#29699;&#24418;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#26059;&#36716;&#21464;&#25442;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;CARE&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13924</link><description>&lt;p&gt;
&#20351;&#29992;&#26059;&#36716;&#31561;&#21464;&#23545;&#27604;&#23398;&#20064;&#26500;&#24314;&#34920;&#31034;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning. (arXiv:2306.13924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31561;&#21464;&#24615;&#30446;&#26631;&#24182;&#29702;&#35770;&#35777;&#26126;&#20102;&#26368;&#20248;&#35299;&#20250;&#23558;&#36755;&#20837;&#31354;&#38388;&#30340;&#22686;&#24378;&#21464;&#25442;&#23545;&#24212;&#20110;&#29699;&#24418;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#26059;&#36716;&#21464;&#25442;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;CARE&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#23558;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#36716;&#25442;&#20026;&#19968;&#20010;&#32039;&#20945;&#30340;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#31616;&#21333;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21487;&#20197;&#34913;&#37327;&#25968;&#25454;&#30340;&#26377;&#24847;&#20041;&#21464;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#36755;&#20837;&#31354;&#38388;&#30340;&#21464;&#25442;&#23545;&#24212;&#20110;&#23884;&#20837;&#31354;&#38388;&#30340;&#31616;&#21333;&#65288;&#21363;&#32447;&#24615;&#65289;&#21464;&#25442;&#65292;&#26469;&#22686;&#21152;&#23884;&#20837;&#31354;&#38388;&#30340;&#39069;&#22806;&#20960;&#20309;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#23545;&#27604;&#23398;&#20064;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#31561;&#21464;&#24615;&#30446;&#26631;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26368;&#23567;&#20540;&#24378;&#21046;&#36755;&#20837;&#31354;&#38388;&#30340;&#22686;&#24378;&#21464;&#25442;&#23545;&#24212;&#20110;&#29699;&#24418;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#26059;&#36716;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#23558;&#31561;&#21464;&#25439;&#22833;&#19982;&#38750;&#25240;&#21472;&#39033;&#30456;&#32467;&#21512;&#21487;&#20197;&#23548;&#33268;&#38750;&#24179;&#20961;&#30340;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#23545;&#25968;&#25454;&#22686;&#24378;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#40723;&#21169;&#36817;&#20284;&#19981;&#21464;&#24615;&#65292;&#21363;&#36755;&#20837;&#22686;&#24378;&#23545;&#24212;&#20110;&#23567;&#30340;&#26059;&#36716;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CARE&#65306;&#36890;&#36807;&#23545;&#27604;&#22686;&#24378;&#35825;&#23548;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#23884;&#20837;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#22823;&#37327;&#20887;&#20313;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13923</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#30340;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
Active Data Acquisition in Autonomous Driving Simulation. (arXiv:2306.13923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#26469;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#20013;&#22823;&#37327;&#20887;&#20313;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#39640;&#24230;&#20381;&#36182;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#20887;&#20313;&#20449;&#24687;&#65292;&#32780;&#19988;&#25910;&#38598;&#21644;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#21644;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#31574;&#30053;&#30340;&#27010;&#24565;&#12290;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#22686;&#21152;&#37319;&#38598;&#23494;&#24230;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#26368;&#32456;&#23454;&#29616;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39564;&#35777;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#28436;&#31034;&#35813;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#29616;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Th1nkMore&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving algorithms rely heavily on learning-based models, which require large datasets for training. However, there is often a large amount of redundant information in these datasets, while collecting and processing these datasets can be time-consuming and expensive. To address this issue, this paper proposes the concept of an active data-collecting strategy. For high-quality data, increasing the collection density can improve the overall quality of the dataset, ultimately achieving similar or even better results than the original dataset with lower labeling costs and smaller dataset sizes. In this paper, we design experiments to verify the quality of the collected dataset and to demonstrate this strategy can significantly reduce labeling costs and dataset size while improving the overall quality of the dataset, leading to better performance of autonomous driving systems. The source code implementing the proposed approach is publicly available on https://github.com/Th1nkMore
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#22788;&#29702;&#22810;&#31449;&#22320;&#38663;&#25968;&#25454;&#65292;&#23454;&#29616;&#21516;&#26102;&#36827;&#34892;&#30456;&#20301;&#25342;&#21462;&#12289;&#20851;&#32852;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31449;&#20043;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#21518;&#35777;&#26126;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13918</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#22810;&#31449;&#22320;&#38663;&#30417;&#27979;&#65306;&#19968;&#31181;&#20840;&#33021;&#30340;&#22320;&#38663;&#30456;&#20301;&#25342;&#21462;&#12289;&#23450;&#20301;&#21644;&#20851;&#32852;&#32593;&#32476;&#65288;PLAN&#65289;
&lt;/p&gt;
&lt;p&gt;
Multi-task multi-station earthquake monitoring: An all-in-one seismic Phase picking, Location, and Association Network (PLAN). (arXiv:2306.13918v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13918
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#22788;&#29702;&#22810;&#31449;&#22320;&#38663;&#25968;&#25454;&#65292;&#23454;&#29616;&#21516;&#26102;&#36827;&#34892;&#30456;&#20301;&#25342;&#21462;&#12289;&#20851;&#32852;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31449;&#20043;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#65292;&#22312;&#23545;&#27604;&#25968;&#25454;&#21518;&#35777;&#26126;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#30417;&#27979;&#23545;&#20110;&#29702;&#35299;&#22320;&#38663;&#29289;&#29702;&#21644;&#35780;&#20272;&#22320;&#38663;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26631;&#20934;&#30340;&#30417;&#27979;&#24037;&#20316;&#27969;&#21253;&#25324;&#30456;&#20301;&#25342;&#21462;&#12289;&#20851;&#32852;&#21644;&#23450;&#20301;&#36825;&#20123;&#30456;&#20114;&#20851;&#32852;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22320;&#38663;&#30417;&#27979;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#20998;&#21035;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#24573;&#30053;&#20102;&#31449;&#20043;&#38388;&#30340;&#22320;&#29702;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22312;&#22810;&#31449;&#22320;&#38663;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#30456;&#20301;&#25342;&#21462;&#12289;&#20851;&#32852;&#21644;&#23450;&#20301;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20010;&#32593;&#32476;&#32467;&#26500;&#20013;&#32771;&#34385;&#20102;&#31449;&#28857;&#20043;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#65292;&#20197;&#20419;&#36827;&#20132;&#21449;&#31449;&#28857;&#21644;&#20132;&#21449;&#20219;&#21153;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;&#22312;&#23545;Ridgecrest&#21306;&#22495;&#21644;&#26085;&#26412;&#22320;&#21306;&#30340;&#25968;&#25454;&#24212;&#29992;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#20808;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30456;&#20301;&#25342;&#21462;&#21644;&#23450;&#20301;&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22320;&#38663;&#30417;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earthquake monitoring is vital for understanding the physics of earthquakes and assessing seismic hazards. A standard monitoring workflow includes the interrelated and interdependent tasks of phase picking, association, and location. Although deep learning methods have been successfully applied to earthquake monitoring, they mostly address the tasks separately and ignore the geographic relationships among stations. Here, we propose a graph neural network that operates directly on multi-station seismic data and achieves simultaneous phase picking, association, and location. Particularly, the inter-station and inter-task physical relationships are informed in the network architecture to promote accuracy, interpretability, and physical consistency among cross-station and cross-task predictions. When applied to data from the Ridgecrest region and Japan regions, this method showed superior performance over previous deep learning-based phase-picking and localization methods. Overall, our stu
&lt;/p&gt;</description></item><item><title>G-TRACER&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20248;&#21270;&#26041;&#26696;&#65292;&#37325;&#28857;&#35299;&#20915;&#20302;&#20449;&#22122;&#27604;&#38382;&#39064;&#65292;&#33021;&#26377;&#25928;&#20419;&#36827;&#27867;&#21270;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13914</link><description>&lt;p&gt;
G-TRACER: &#39044;&#26399;&#28165;&#26224;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
G-TRACER: Expected Sharpness Optimization. (arXiv:2306.13914v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13914
&lt;/p&gt;
&lt;p&gt;
G-TRACER&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20248;&#21270;&#26041;&#26696;&#65292;&#37325;&#28857;&#35299;&#20915;&#20302;&#20449;&#22122;&#27604;&#38382;&#39064;&#65292;&#33021;&#26377;&#25928;&#20419;&#36827;&#27867;&#21270;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20248;&#21270;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;G-TRACER&#65288;"Geometric TRACE Ratio"&#65289;&#65292;&#36890;&#36807;&#23547;&#27714;&#24179;&#22374;&#26368;&#23567;&#20540;&#20419;&#36827;&#27867;&#21270;&#65292;&#24182;&#20855;&#26377;&#20197;&#24191;&#20041;Bayes&#30446;&#26631;&#30340;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#20026;&#22522;&#30784;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#36890;&#36807;&#20351;&#29992;TRACER&#22686;&#21152;&#25439;&#22833;&#20989;&#25968;&#65292;&#26354;&#29575;&#27491;&#21017;&#21270;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;SGD-TRACER&#21644;Adam-TRACER&#65289;&#21487;&#20197;&#20316;&#20026;&#29616;&#26377;&#20248;&#21270;&#22120;&#30340;&#20462;&#25913;&#31616;&#21333;&#22320;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25910;&#25947;&#20110;&#26410;&#27491;&#21017;&#21270;&#30446;&#26631;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38468;&#36817;&#65288;&#21462;&#20915;&#20110;&#27491;&#21017;&#21270;&#24378;&#24230;&#30340;&#37051;&#22495;&#33539;&#22260;&#65289;&#65292;&#24182;&#22312;&#35768;&#22810;&#22522;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;NLP&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#29305;&#21035;&#20851;&#27880;&#25361;&#25112;&#24615;&#30340;&#20302;&#20449;&#22122;&#27604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new regularization scheme for the optimization of deep learning architectures, G-TRACER ("Geometric TRACE Ratio"), which promotes generalization by seeking flat minima, and has a sound theoretical basis as an approximation to a natural-gradient descent based optimization of a generalized Bayes objective. By augmenting the loss function with a TRACER, curvature-regularized optimizers (eg SGD-TRACER and Adam-TRACER) are simple to implement as modifications to existing optimizers and don't require extensive tuning. We show that the method converges to a neighborhood (depending on the regularization strength) of a local minimum of the unregularized objective, and demonstrate competitive performance on a number of benchmark computer vision and NLP datasets, with a particular focus on challenging low signal-to-noise ratio problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.13905</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#36712;&#36857;&#20998;&#26512;&#30340;&#26102;&#31354;&#21465;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis. (arXiv:2306.13905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#20041;&#36712;&#36857;&#36319;&#36394;&#36827;&#34892;&#20998;&#26512;&#21644;&#29983;&#25104;&#21512;&#25104;&#35821;&#20041;&#36712;&#36857;&#25968;&#25454;&#65288;SST&#65289;&#30340;&#24895;&#26223;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#26234;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#36712;&#36857;&#65292;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#65292;&#22686;&#24378;&#26426;&#22120;&#23545;&#21160;&#29289;&#12289;&#20154;&#31867;&#12289;&#36135;&#29289;&#31561;&#31227;&#21160;&#24773;&#20917;&#30340;&#29702;&#35299;&#65292;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#65292;&#24182;&#20026;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#24341;&#25806;&#21644;&#21830;&#19994;&#31574;&#30053;&#31561;&#19968;&#31995;&#21015;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we lay out a vision for analysing semantic trajectory traces and generating synthetic semantic trajectory data (SSTs) using generative language model. Leveraging the advancements in deep learning, as evident by progress in the field of natural language processing (NLP), computer vision, etc. we intend to create intelligent models that can study the semantic trajectories in various contexts, predicting future trends, increasing machine understanding of the movement of animals, humans, goods, etc. enhancing human-computer interactions, and contributing to an array of applications ranging from urban-planning to personalized recommendation engines and business strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#65292;&#38450;&#27490;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#26102;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.13892</link><description>&lt;p&gt;
&#20855;&#26377;&#20849;&#35782;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decentralized Deep Learning with Consensus Algorithms. (arXiv:2306.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#65292;&#38450;&#27490;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#26102;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#20998;&#25955;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#36890;&#20449;&#20195;&#29702;&#20043;&#38388;&#30340;&#30452;&#25509;&#20449;&#24687;&#20132;&#25442;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#21487;&#20197;&#35775;&#38382;&#24212;&#35813;&#20445;&#25345;&#31169;&#26377;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#30446;&#26631;&#26159;&#22312;&#35757;&#32451;&#21518;&#20351;&#24471;&#25152;&#26377;&#20195;&#29702;&#22312;&#27169;&#22411;&#21442;&#25968;&#19978;&#36798;&#25104;&#20849;&#35782;&#12290;&#28982;&#32780;&#65292;&#19982;&#19981;&#21487;&#20449;&#30340;&#37051;&#23621;&#20195;&#29702;&#20849;&#20139;&#21442;&#25968;&#21487;&#33021;&#20250;&#27844;&#38706;&#26377;&#20851;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#20998;&#25955;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22312;&#21512;&#20316;&#35757;&#32451;&#26399;&#38388;&#21644;&#20043;&#21518;&#20445;&#25252;&#27599;&#20010;&#20195;&#29702;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#24120;&#29992;&#20110;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#27867;&#21270;&#21040;&#23454;&#29992;&#30340;&#22522;&#20110;&#23376;&#26799;&#24230;&#21644;ADMM&#30340;&#20998;&#25955;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#36866;&#29992;&#20110;&#20219;&#24847;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative decentralized deep learning relies on direct information exchange between communicating agents, each with access to a local dataset which should be kept private. The goal is for all agents to achieve consensus on model parameters after training. However, sharing parameters with untrustworthy neighboring agents could leak exploitable information about local datasets. To combat this, we introduce differentially private decentralized learning that secures each agent's local dataset during and after cooperative training. In our approach, we generalize Differentially Private Stochastic Gradient Descent (DP-SGD) -- a popular differentially private training method for centralized deep learning -- to practical subgradient- and ADMM-based decentralized learning methods. Our algorithms' differential privacy guarantee holds for arbitrary deep learning objective functions, and we analyze the convergence properties for strongly convex objective functions. We compare our algorithms again
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; L3Cube-MahaSent-MD&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#30340; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#65292;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#30340;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.13888</link><description>&lt;p&gt;
L3Cube-MahaSent-MD&#65306;&#19968;&#31181;&#22810;&#22495; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#21644; Transformer &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset and Transformer Models. (arXiv:2306.13888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102; L3Cube-MahaSent-MD&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495;&#30340; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#22312;&#20854;&#20013;&#65292;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#30340;&#23376;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20302;&#36164;&#28304;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914; Marathi&#65289;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#19968;&#30452;&#21463;&#21040;&#30456;&#24212;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; L3Cube-MahaSent-MD&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#39046;&#22495; Marathi &#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#30005;&#24433;&#35780;&#35770;&#12289;&#26222;&#36890;&#25512;&#25991;&#12289;&#30005;&#35270;&#33410;&#30446;&#23383;&#24149;&#21644;&#25919;&#27835;&#25512;&#25991;&#31561;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422; 6 &#19975;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#35206;&#30422;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#24863; - &#27491;&#38754;&#12289;&#36127;&#38754;&#21644;&#20013;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#39046;&#22495;&#21019;&#24314;&#20102;&#19968;&#20010;&#23376;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23376;&#25968;&#25454;&#38598;&#21253;&#21547; 1.5 &#19975;&#20010;&#26679;&#26412;&#12290;&#36825;&#26159; Indic &#24773;&#24863;&#39046;&#22495;&#20869;&#30340;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#24494;&#35843;&#20102;&#19981;&#21516;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821; BERT &#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102; MahaBERT &#27169;&#22411;&#30340;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20302;&#36164;&#28304;&#22810;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22343;&#21487;&#22312; https://github.com/l3cube-pune/MarathiN &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of sentiment analysis in low-resource languages, such as Marathi, has been limited due to the availability of suitable datasets. In this work, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis dataset, with four different domains - movie reviews, general tweets, TV show subtitles, and political tweets. The dataset consists of around 60,000 manually tagged samples covering 3 distinct sentiments - positive, negative, and neutral. We create a sub-dataset for each domain comprising 15k samples. The MahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset within the Indic sentiment landscape. We fine-tune different monolingual and multilingual BERT models on these datasets and report the best accuracy with the MahaBERT model. We also present an extensive in-domain and cross-domain analysis thus highlighting the need for low-resource multi-domain datasets. The data and models are available at https://github.com/l3cube-pune/MarathiN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PINNs&#27714;&#35299;CDII&#30340;&#35745;&#31639;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#36896;&#29289;&#29702;&#21551;&#21457;&#24335;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#36755;&#20986;&#20989;&#25968;&#19982;&#22522;&#26412;&#24494;&#20998;&#26041;&#31243;&#30340;&#32806;&#21512;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13881</link><description>&lt;p&gt;
&#20351;&#29992;PINNs&#36827;&#34892;&#30005;&#27969;&#23494;&#24230;&#38459;&#25239;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Current density impedance imaging with PINNs. (arXiv:2306.13881v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;PINNs&#27714;&#35299;CDII&#30340;&#35745;&#31639;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#36896;&#29289;&#29702;&#21551;&#21457;&#24335;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#36755;&#20986;&#20989;&#25968;&#19982;&#22522;&#26412;&#24494;&#20998;&#26041;&#31243;&#30340;&#32806;&#21512;&#12290;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CDII-PINNs&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22312;Tikhonov&#27491;&#21017;&#21270;&#26694;&#26550;&#19979;&#20351;&#29992;PINNs&#27714;&#35299;CDII&#30340;&#35745;&#31639;&#39640;&#25928;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#30340;&#26368;&#23567;&#20108;&#20056;&#36755;&#20986;&#20989;&#25968;&#19982;&#25551;&#36848;&#30005;&#23548;&#21644;&#30005;&#21387;&#20043;&#38388;&#20851;&#31995;&#30340;&#22522;&#26412;&#24494;&#20998;&#26041;&#31243;&#30456;&#32467;&#21512;&#65292;&#26500;&#36896;&#20102;&#19968;&#20010;&#29289;&#29702;&#21551;&#21457;&#24335;&#25439;&#22833;&#20989;&#25968;&#12290;&#34920;&#31034;&#30005;&#23548;&#21644;&#30005;&#21387;&#30340;&#19968;&#23545;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#35813;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32806;&#21512;&#12290;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#37325;&#24314;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#32593;&#32476;&#21442;&#25968;&#30340;&#20808;&#39564;&#36873;&#25321;&#65292;&#32473;&#20986;&#20102;CDII-PINNs&#30340;&#35823;&#24046;&#20998;&#26512;&#21644;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#26159;&#20381;&#25454;&#26679;&#26412;&#25968;&#37327;&#32780;&#23450;&#30340;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;CDII-PINNs&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;$1\%$&#21040;$20\%$&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce CDII-PINNs, a computationally efficient method for solving CDII using PINNs in the framework of Tikhonov regularization. This method constructs a physics-informed loss function by merging the regularized least-squares output functional with an underlying differential equation, which describes the relationship between the conductivity and voltage. A pair of neural networks representing the conductivity and voltage, respectively, are coupled by this loss function. Then, minimizing the loss function provides a reconstruction. A rigorous theoretical guarantee is provided. We give an error analysis for CDII-PINNs and establish a convergence rate, based on prior selected neural network parameters in terms of the number of samples. The numerical simulations demonstrate that CDII-PINNs are efficient, accurate and robust to noise levels ranging from $1\%$ to $20\%$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#26597;&#35810;&#30340;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#26041;&#27861;&#65288;AQT&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#20855;&#20307;&#25551;&#36848;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.13879</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#20026;&#26597;&#35810;&#30340;Transformer&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Action Q-Transformer: Visual Explanation in Deep Reinforcement Learning with Encoder-Decoder Model using Action Query. (arXiv:2306.13879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#26597;&#35810;&#30340;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#26041;&#27861;&#65288;AQT&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#20855;&#20307;&#25551;&#36848;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#20013;&#24212;&#29992;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#20197;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;DRL&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#24212;&#29992;&#26234;&#33021;&#20307;&#21040;&#23454;&#38469;&#38382;&#39064;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Action Q-Transformer (AQT)&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#21040;&#22522;&#20110;Q-&#23398;&#20064;&#30340;DRL&#26041;&#27861;&#20013;&#12290;&#22312;AQT&#20013;&#65292;&#32534;&#30721;&#22120;&#35745;&#31639;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#35299;&#30721;&#22120;&#35745;&#31639;&#20248;&#21183;&#20989;&#25968;&#65292;&#20197;&#20419;&#36827;&#23398;&#20064;&#29305;&#23450;&#34892;&#20026;&#30340;&#19981;&#21516;&#27880;&#24847;&#21147;&#30340;&#33719;&#21462;&#12290;AQT&#20013;&#30340;&#35299;&#30721;&#22120;&#20351;&#29992;&#34892;&#20026;&#26597;&#35810;&#20316;&#20026;&#26597;&#35810;&#65292;&#20195;&#34920;&#27599;&#20010;&#34892;&#20026;&#30340;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#34920;&#31034;&#29366;&#24577;&#20540;&#21644;&#27599;&#20010;&#34892;&#20026;&#30340;&#27880;&#24847;&#21147;&#12290;&#36890;&#36807;&#33719;&#21462;&#21644;&#21487;&#35270;&#21270;&#36825;&#20123;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#30340;DRL&#27169;&#22411;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent performance of Transformer in supervised learning has led to growing interest in its potential application to deep reinforcement learning (DRL) to achieve high performance on a wide variety of problems. However, the decision making of a DRL agent is a black box, which greatly hinders the application of the agent to real-world problems. To address this problem, we propose the Action Q-Transformer (AQT), which introduces a transformer encoder-decoder structure to Q-learning based DRL methods. In AQT, the encoder calculates the state value function and the decoder calculates the advantage function to promote the acquisition of different attentions indicating the agent's decision-making. The decoder in AQT utilizes action queries, which represent the information of each action, as queries. This enables us to obtain the attentions for the state value and for each action. By acquiring and visualizing these attentions that detail the agent's decision-making, we achieve a DRL mod
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#29289;&#29702;&#32422;&#26463;&#21644;&#25277;&#35937;&#25968;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#39069;&#22806;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#20197;&#33719;&#21462;&#26356;&#26377;&#25928;&#12289;&#29289;&#29702;&#19978;&#19968;&#33268;&#12289;&#25968;&#25454;&#25928;&#29575;&#26356;&#39640;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#22312;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13867</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Machine Learning for Modeling and Control of Dynamical Systems. (arXiv:2306.13867v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13867
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#26159;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#29289;&#29702;&#32422;&#26463;&#21644;&#25277;&#35937;&#25968;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#39069;&#22806;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#20197;&#33719;&#21462;&#26356;&#26377;&#25928;&#12289;&#29289;&#29702;&#19978;&#19968;&#33268;&#12289;&#25968;&#25454;&#25928;&#29575;&#26356;&#39640;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#22312;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#35752;&#35770;&#20102;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;(PIML)&#26159;&#19968;&#32452;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#31995;&#32479;&#22320;&#23558;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#19982;&#29289;&#29702;&#32422;&#26463;&#21644;&#31185;&#23398;&#24037;&#31243;&#39046;&#22495;&#20013;&#24320;&#21457;&#30340;&#25277;&#35937;&#25968;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#19982;&#32431;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#21453;&#65292;PIML&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#25191;&#34892;&#33021;&#37327;&#21644;&#36136;&#37327;&#23432;&#24658;&#31561;&#29289;&#29702;&#23450;&#24459;&#33719;&#24471;&#39069;&#22806;&#30340;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#24191;&#20041;&#19978;&#35762;&#65292;PIML&#27169;&#22411;&#21487;&#20197;&#21253;&#25324;&#31283;&#23450;&#24615;&#12289;&#20984;&#24615;&#25110;&#19981;&#21464;&#24615;&#31561;&#25277;&#35937;&#23646;&#24615;&#21644;&#26465;&#20214;&#12290;PIML&#30340;&#22522;&#26412;&#21069;&#25552;&#26159;&#23558;ML&#19982;&#29289;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#12289;&#29289;&#29702;&#19978;&#19968;&#33268;&#19988;&#25968;&#25454;&#25928;&#29575;&#26356;&#39640;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#21644;&#25511;&#21046;&#20013;&#26368;&#26032;&#30340;PIML&#36827;&#23637;&#25552;&#20379;&#31867;&#20284;&#20110;&#25945;&#31243;&#24335;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#28085;&#30422;&#20197;&#19979;&#20027;&#39064;&#30340;&#29702;&#35770;&#12289;&#22522;&#26412;&#27010;&#24565;&#21644;&#26041;&#27861;&#12289;&#24037;&#20855;&#21644;&#24212;&#29992;&#30340;&#27010;&#36848;&#65306;1&#65289;&#29992;&#20110;&#31995;&#32479;&#35782;&#21035;&#30340;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#65307;2&#65289;&#29992;&#20110;&#39044;&#27979;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#65307;3&#65289;ML&#31639;&#27861;&#30340;&#29289;&#29702;&#24341;&#23548;&#35774;&#35745;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;PIML&#39046;&#22495;&#30340;&#37325;&#35201;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed machine learning (PIML) is a set of methods and tools that systematically integrate machine learning (ML) algorithms with physical constraints and abstract mathematical models developed in scientific and engineering domains. As opposed to purely data-driven methods, PIML models can be trained from additional information obtained by enforcing physical laws such as energy and mass conservation. More broadly, PIML models can include abstract properties and conditions such as stability, convexity, or invariance. The basic premise of PIML is that the integration of ML and physics can yield more effective, physically consistent, and data-efficient models. This paper aims to provide a tutorial-like overview of the recent advances in PIML for dynamical system modeling and control. Specifically, the paper covers an overview of the theory, fundamental concepts and methods, tools, and applications on topics of: 1) physics-informed learning for system identification; 2) physics-in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#25239;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13854</link><description>&lt;p&gt;
&#30456;&#20284;&#24615;&#20445;&#25345;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Similarity Preserving Adversarial Graph Contrastive Learning. (arXiv:2306.13854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#25239;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#23545;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#12290;&#22312;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#30340;&#26041;&#27861;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22266;&#26377;&#35774;&#35745;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#21407;&#22987;&#22270;&#27966;&#29983;&#20986;&#30340;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#28982;&#32780;&#24403;&#22270;&#21463;&#21040;&#25915;&#20987;&#26102;&#65292;&#21407;&#22987;&#22270;&#20013;&#24050;&#32463;&#21253;&#21547;&#20102;&#22122;&#22768;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#24212;&#29992;&#20110;GCL&#26694;&#26550;&#65292;&#23558;&#25915;&#20987;&#30340;&#22270;&#20316;&#20026;GCL&#26694;&#26550;&#19979;&#30340;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;GCL&#26041;&#27861;&#22312;&#20445;&#25345;&#33410;&#28857;&#29305;&#24449;&#30456;&#20284;&#24615;&#26041;&#38754;&#20184;&#20986;&#20102;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24615;&#20445;&#25345;&#30340;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;SP-AGCL&#65289;&#26694;&#26550;&#65292;&#23558;&#24178;&#20928;&#30340;&#22270;&#19982;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#30340;&#36741;&#21161;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works demonstrate that GNN models are vulnerable to adversarial attacks, which refer to imperceptible perturbation on the graph structure and node features. Among various GNN models, graph contrastive learning (GCL) based methods specifically suffer from adversarial attacks due to their inherent design that highly depends on the self-supervision signals derived from the original graph, which however already contains noise when the graph is attacked. To achieve adversarial robustness against such attacks, existing methods adopt adversarial training (AT) to the GCL framework, which considers the attacked graph as an augmentation under the GCL framework. However, we find that existing adversarially trained GCL methods achieve robustness at the expense of not being able to preserve the node feature similarity. In this paper, we propose a similarity-preserving adversarial graph contrastive learning (SP-AGCL) framework that contrasts the clean graph with two auxiliary views of differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#26041;&#27861;&#26469;&#32479;&#19968;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#29702;&#35770;&#20013;&#35768;&#22810;&#29305;&#27530;&#31867;&#30340;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13853</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#38236;&#38754;&#19979;&#38477;&#25511;&#21046;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Controlling Implicit Regularization via Mirror Descent. (arXiv:2306.13853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#26041;&#27861;&#26469;&#32479;&#19968;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;&#29702;&#35770;&#20013;&#35768;&#22810;&#29305;&#27530;&#31867;&#30340;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#21551;&#21457;&#65292;&#20154;&#20204;&#23545;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20154;&#20204;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#26469;&#30830;&#23450;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#20854;&#8220;&#39318;&#36873;&#8221;&#35299;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#24050;&#32463;&#26377;&#20154;&#35770;&#35777;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#20013;&#20250;&#24341;&#36215;&#38544;&#24335;&#30340;$\ell_2$ -&#33539;&#25968;&#27491;&#21017;&#21270;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#31639;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#20960;&#20309;&#25110;&#29305;&#23450;&#31867;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#34920;&#26126;&#38656;&#35201;&#19968;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38236;&#38754;&#19979;&#38477;&#65288;MD&#65289;&#26469;&#25511;&#21046;&#22238;&#24402;&#21644;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;MD&#19982;&#36890;&#29992;&#30340;&#19979;&#38477;&#26041;&#21521;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#22312;&#25152;&#26377;&#26631;&#20934;&#20960;&#20309;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;$\ell_p$&#65288;$p\in[1,\infty]$&#65289;&#33539;&#24335;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#35768;&#22810;&#29305;&#27530;&#31867;&#20013;&#20063;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#30340;&#27867;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the remarkable success of deep neural networks, there has been significant interest in understanding the generalization performance of overparameterized models. Substantial efforts have been invested in characterizing how optimization algorithms impact generalization through their "preferred" solutions, a phenomenon commonly referred to as implicit regularization. In particular, it has been argued that gradient descent (GD) induces an implicit $\ell_2$-norm regularization in regression and classification problems. However, the implicit regularization of different algorithms are confined to either a specific geometry or a particular class of learning problems, indicating a gap in a general approach for controlling the implicit regularization. To address this, we present a unified approach using mirror descent (MD), a notable generalization of GD, to control implicit regularization in both regression and classification settings. More specifically, we show that MD with the gen
&lt;/p&gt;</description></item><item><title>&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.13841</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30495;&#30340;&#27604;&#20803;&#23398;&#20064;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Pre-training Truly Better Than Meta-Learning?. (arXiv:2306.13841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13841
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#32988;&#36807;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#30446;&#21069;&#26222;&#36941;&#35748;&#20026;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PT&#65289;&#21152;&#19978;&#22312;&#35780;&#20215;&#26102;&#24494;&#35843;&#26368;&#21518;&#19968;&#23618;&#65292;&#32988;&#36807;&#26631;&#20934;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#27604;&#36739;PT&#21644;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#36825;&#20123;&#35828;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#20351;&#29992;&#30456;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#30456;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#20197;&#21450;&#25152;&#26377;&#27169;&#22411;&#37117;&#35757;&#32451;&#21040;&#25910;&#25947;&#12290;&#20851;&#38190;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#32479;&#35745;&#24037;&#20855;&#8212;&#8212;&#25928;&#24212;&#37327;&#65288;Cohen's d&#65289;&#8212;&#8212;&#26469;&#30830;&#23450;&#20351;&#29992;PT&#19982;&#20351;&#29992;MAML&#20043;&#38388;&#30340;&#27169;&#22411;&#24046;&#24322;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#25552;&#20986;&#30340;&#24230;&#37327;&#8212;&#8212;&#22810;&#26679;&#24615;&#31995;&#25968;&#8212;&#8212;&#26469;&#35745;&#31639;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#27491;&#24335;&#22810;&#26679;&#24615;&#12290;&#20351;&#29992;&#36825;&#31181;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20197;&#19979;&#20107;&#23454;&#65306;1. &#24403;&#25968;&#25454;&#38598;&#30340;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#20302;&#26102;&#65292;PT&#22312;&#24179;&#22343;&#24847;&#20041;&#19978;&#32988;&#36807;MAML&#65307;2. &#24403;&#27491;&#24335;&#22810;&#26679;&#24615;&#36739;&#39640;&#26102;&#65292;MAML&#32988;&#36807;PT&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of few-shot learning, it is currently believed that a fixed pre-trained (PT) model, along with fine-tuning the final layer during evaluation, outperforms standard meta-learning algorithms. We re-evaluate these claims under an in-depth empirical examination of an extensive set of formally diverse datasets and compare PT to Model Agnostic Meta-Learning (MAML). Unlike previous work, we emphasize a fair comparison by using: the same architecture, the same optimizer, and all models trained to convergence. Crucially, we use a more rigorous statistical tool -- the effect size (Cohen's d) -- to determine the practical significance of the difference between a model trained with PT vs. a MAML. We then use a previously proposed metric -- the diversity coefficient -- to compute the average formal diversity of a dataset. Using this analysis, we demonstrate the following: 1. when the formal diversity of a data set is low, PT beats MAML on average and 2. when the formal diversity is hi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Computron&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20869;&#23384;&#20132;&#25442;&#23454;&#29616;&#27169;&#22411;&#24182;&#34892;&#20132;&#25442;&#35774;&#35745;&#26469;&#26381;&#21153;&#20110;&#22810;&#20010;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#20219;&#21153;&#26102;&#21487;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#20132;&#25442;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.13835</link><description>&lt;p&gt;
Computron: &#21033;&#29992;&#27169;&#22411;&#24182;&#34892;&#20132;&#25442;&#26381;&#21153;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computron: Serving Distributed Deep Learning Models with Model Parallel Swapping. (arXiv:2306.13835v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Computron&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20869;&#23384;&#20132;&#25442;&#23454;&#29616;&#27169;&#22411;&#24182;&#34892;&#20132;&#25442;&#35774;&#35745;&#26469;&#26381;&#21153;&#20110;&#22810;&#20010;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#20219;&#21153;&#26102;&#21487;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#20132;&#25442;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#35768;&#22810;&#20197;&#35821;&#35328;&#21644;&#22270;&#20687;&#20026;&#20195;&#34920;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37117;&#26159;&#21253;&#21547;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#35843;&#25972;&#22411;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#38656;&#35201;&#23558;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#26381;&#21153;&#20110;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Computron&#65292;&#36825;&#20010;&#31995;&#32479;&#20351;&#29992;&#20869;&#23384;&#20132;&#25442;&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;GPU&#38598;&#32676;&#19978;&#26381;&#21153;&#20110;&#22810;&#20010;&#20998;&#24067;&#24335;&#27169;&#22411;&#12290;Computron &#23454;&#29616;&#20102;&#19968;&#20010;&#27169;&#22411;&#24182;&#34892;&#20132;&#25442;&#35774;&#35745;&#65292;&#21033;&#29992;&#38598;&#32676;&#30340;&#24635;CPU-GPU&#38142;&#25509;&#24102;&#23485;&#21152;&#24555;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#36895;&#24230;&#12290;&#35813;&#35774;&#35745;&#20351;&#20132;&#25442;&#22823;&#22411;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Computron&#25104;&#21151;&#22320;&#22312;&#22810;&#20010;GPU&#19978;&#24182;&#34892;&#21270;&#20102;&#27169;&#22411;&#20132;&#25442;&#65292;&#24182;&#23545;&#38543;&#26426;&#36127;&#36733;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23481;&#24525;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21464;&#21270;&#22240;&#32032;&#65292;&#20363;&#22914;&#31361;&#21457;&#24615;&#21644;&#20559;&#26012;&#30340;&#35831;&#27714;&#36895;&#29575;&#12290;Computron&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/dlzou/computron&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the most performant deep learning models today in fields like language and image understanding are fine-tuned models that contain billions of parameters. In anticipation of workloads that involve serving many of such large models to handle different tasks, we develop Computron, a system that uses memory swapping to serve multiple distributed models on a shared GPU cluster. Computron implements a model parallel swapping design that takes advantage of the aggregate CPU-GPU link bandwidth of a cluster to speed up model parameter transfers. This design makes swapping large models feasible and can improve resource utilization. We demonstrate that Computron successfully parallelizes model swapping on multiple GPUs, and we test it on randomized workloads to show how it can tolerate real world variability factors like burstiness and skewed request rates. Computron's source code is available at https://github.com/dlzou/computron.
&lt;/p&gt;</description></item><item><title>Minigrid &amp; Miniworld&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#38754;&#21521;&#30446;&#26631;&#30340;2D&#21644;3D&#29615;&#22659;&#65292;&#36890;&#36807;&#37319;&#29992;&#26497;&#31616;&#20027;&#20041;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#20351;&#20854;&#26131;&#20110;&#23450;&#21046;&#21644;&#24320;&#21457;&#65292;&#36825;&#24050;&#32463;&#34987;RL&#31038;&#21306;&#24191;&#27867;&#37319;&#29992;&#12290;&#21478;&#22806;&#65292;&#32479;&#19968;&#30340;API&#36824;&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.13831</link><description>&lt;p&gt;
Minigrid &amp; Miniworld: &#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#29992;&#20110;&#30446;&#26631;&#23548;&#21521;&#22411;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Minigrid &amp; Miniworld: Modular &amp; Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. (arXiv:2306.13831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13831
&lt;/p&gt;
&lt;p&gt;
Minigrid &amp; Miniworld&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#38754;&#21521;&#30446;&#26631;&#30340;2D&#21644;3D&#29615;&#22659;&#65292;&#36890;&#36807;&#37319;&#29992;&#26497;&#31616;&#20027;&#20041;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#20351;&#20854;&#26131;&#20110;&#23450;&#21046;&#21644;&#24320;&#21457;&#65292;&#36825;&#24050;&#32463;&#34987;RL&#31038;&#21306;&#24191;&#27867;&#37319;&#29992;&#12290;&#21478;&#22806;&#65292;&#32479;&#19968;&#30340;API&#36824;&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Minigrid&#21644;Miniworld&#24211;&#65292;&#36825;&#20004;&#20010;&#24211;&#25552;&#20379;&#20102;&#19968;&#22871;&#38754;&#21521;&#30446;&#26631;&#30340;2D&#21644;3D&#29615;&#22659;&#12290;&#36825;&#20123;&#24211;&#26159;&#26126;&#30830;&#37319;&#29992;&#26497;&#31616;&#20027;&#20041;&#35774;&#35745;&#33539;&#24335;&#32780;&#21019;&#24314;&#30340;&#65292;&#20197;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#20026;&#21508;&#31181;&#30740;&#31350;&#29305;&#23450;&#38656;&#27714;&#24320;&#21457;&#26032;&#29615;&#22659;&#65292;&#22240;&#27492;&#23427;&#20204;&#24050;&#34987;RL&#31038;&#21306;&#24191;&#27867;&#37319;&#29992;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#35774;&#35745;&#29702;&#24565;&#12289;&#29615;&#22659;&#32454;&#33410;&#20197;&#21450;&#23427;&#20204;&#30340;&#19990;&#30028;&#29983;&#25104;API&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;Minigrid&#21644;Miniworld&#20043;&#38388;&#32479;&#19968;API&#24102;&#26469;&#30340;&#38468;&#21152;&#21151;&#33021;&#65292;&#21253;&#25324;&#22312;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#65288;&#38024;&#23545;RL&#20195;&#29702;&#21644;&#20154;&#31867;&#65289;&#12290;Minigrid&#21644;Miniworld&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Farama-Foundation/{Minigrid, Miniworld}&#25214;&#21040;&#65292;&#25991;&#26723;&#21487;&#20197;&#22312;https://{minigrid, miniworld}.farama.org/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API. We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https://github.com/Farama-Foundation/{Minigrid, Miniworld} along with their documentation at https://{minigrid, miniworld}.farama.org/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#26395;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.13830</link><description>&lt;p&gt;
&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#30340;&#24230;&#37327;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aircraft Environmental Impact Segmentation via Metric Learning. (arXiv:2306.13830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#26395;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#26159;&#25351;&#20026;&#29305;&#23450;&#20219;&#21153;&#23398;&#20064;&#23450;&#21046;&#36317;&#31163;&#24230;&#37327;&#30340;&#36807;&#31243;&#12290;&#36825;&#19968;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#23545;&#20110;&#20381;&#38752;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25110;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#30340;&#20219;&#20309;&#24212;&#29992;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#12289;&#25552;&#21462;&#27169;&#24335;&#12289;&#21457;&#29616;&#30693;&#35782;&#31561;&#12290;&#28982;&#32780;&#65292;&#24230;&#37327;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#30340;&#20803;&#32032;&#65292;&#36804;&#20170;&#22312;&#30456;&#20851;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;&#32463;&#20856;&#30340;&#24230;&#37327;&#23398;&#20064;&#20844;&#24335;&#19982;&#26032;&#39062;&#30340;&#20803;&#32032;&#24212;&#29992;&#20110;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#26032;&#20986;&#29616;&#30340;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#25551;&#36848;&#21644;&#21010;&#20998;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#19968;&#32467;&#26524;&#23558;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#65292;&#23545;&#20110;&#33322;&#31354;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft envir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32858;&#21512;&#31639;&#23376;&#65292;GenAgg&#65292;&#23427;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.13826</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;$f$-&#22343;&#20540;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generalised $f$-Mean Aggregation for Graph Neural Networks. (arXiv:2306.13826v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#32858;&#21512;&#31639;&#23376;&#65292;GenAgg&#65292;&#23427;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26550;&#26500;&#30001;&#20854;&#26356;&#26032;&#21644;&#32858;&#21512;&#27169;&#22359;&#30340;&#23454;&#29616;&#26041;&#24335;&#23450;&#20041;&#12290;&#35768;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26032;&#22411;&#21442;&#25968;&#21270;&#26356;&#26032;&#27169;&#22359;&#30340;&#26041;&#27861;&#19978;&#65292;&#32780;&#32858;&#21512;&#27169;&#22359;&#30456;&#23545;&#36739;&#23569;&#21463;&#21040;&#20851;&#27880;&#12290;&#30001;&#20110;&#32858;&#21512;&#20989;&#25968;&#24456;&#38590;&#21442;&#25968;&#21270;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#36873;&#25321;&#8220;&#26631;&#20934;&#32858;&#21512;&#22120;&#8221;&#65292;&#22914;$\mathrm{mean}$&#12289;$\mathrm{sum}$&#25110;$\mathrm{max}$&#12290;&#23613;&#31649;&#36825;&#31181;&#36873;&#25321;&#36890;&#24120;&#27809;&#26377;&#20219;&#20309;&#29702;&#30001;&#65292;&#20294;&#24050;&#32463;&#34920;&#26126;&#32858;&#21512;&#22120;&#30340;&#36873;&#25321;&#23545;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#26368;&#20339;&#32858;&#21512;&#22120;&#30340;&#36873;&#25321;&#21462;&#20915;&#20110;&#38382;&#39064;&#12290;&#30001;&#20110;&#32858;&#21512;&#26159;&#19968;&#31181;&#26377;&#25439;&#25805;&#20316;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#20449;&#24687;&#20002;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GenAgg&#65292;&#19968;&#31181;&#24191;&#20041;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23427;&#21442;&#25968;&#21270;&#20102;&#19968;&#20010;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#32858;&#21512;&#22120;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GenAgg&#33021;&#22815;&#34920;&#31034;&#26631;&#20934;&#32858;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) architectures are defined by their implementations of update and aggregation modules. While many works focus on new ways to parametrise the update modules, the aggregation modules receive comparatively little attention. Because it is difficult to parametrise aggregation functions, currently most methods select a "standard aggregator" such as $\mathrm{mean}$, $\mathrm{sum}$, or $\mathrm{max}$. While this selection is often made without any reasoning, it has been shown that the choice in aggregator has a significant impact on performance, and the best choice in aggregator is problem-dependent. Since aggregation is a lossy operation, it is crucial to select the most appropriate aggregator in order to minimise information loss. In this paper, we present GenAgg, a generalised aggregation operator, which parametrises a function space that includes all standard aggregators. In our experiments, we show that GenAgg is able to represent the standard aggregators with mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#34701;&#21512;&#21464;&#25442;&#22120;&#65288;TFT&#65289;&#35299;&#20915;&#20102;&#36807;&#21435; GPP &#26102;&#38388;&#24207;&#21015;&#19981;&#36275;&#30340;&#32570;&#38519;&#65292;&#23454;&#29616;&#20102;&#38750;&#26893;&#34987;&#29305;&#24449;&#22312;&#21319;&#23610;&#24230; GPP &#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13815</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#34701;&#21512;&#21464;&#25442;&#22120;&#65288;TFT&#65289;&#21319;&#23610;&#24230;&#20840;&#29699;&#23567;&#26102;&#32423;GPP&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Upscaling Global Hourly GPP with Temporal Fusion Transformer (TFT). (arXiv:2306.13815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26102;&#38388;&#34701;&#21512;&#21464;&#25442;&#22120;&#65288;TFT&#65289;&#35299;&#20915;&#20102;&#36807;&#21435; GPP &#26102;&#38388;&#24207;&#21015;&#19981;&#36275;&#30340;&#32570;&#38519;&#65292;&#23454;&#29616;&#20102;&#38750;&#26893;&#34987;&#29305;&#24449;&#22312;&#21319;&#23610;&#24230; GPP &#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#24635;&#21021;&#32423;&#29983;&#20135;&#21147;&#65288;GPP&#65289;&#20272;&#35745;&#23545;&#20110;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#20513;&#35758;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#20165;&#26469;&#33258;&#20998;&#24067;&#31232;&#30095;&#30340;&#28065;&#24230;&#21327;&#26041;&#24046;&#22612;&#31449;&#28857;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;&#22312;&#21306;&#22495;&#21644;&#20840;&#29699;&#23610;&#24230;&#19978;&#33719;&#24471;&#21487;&#38752;&#30340;GPP&#37327;&#21270;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;Temporal Fusion Transformer&#65288;TFT&#65289;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21319;&#23610;&#24230;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;GPP&#26102;&#38388;&#24207;&#21015;&#12290;&#27169;&#22411;&#24320;&#21457;&#36741;&#21161;&#20351;&#29992;&#20102;Random Forest Regressor &#65288;RFR&#65289;&#21644;XGBoost&#65292;&#38543;&#21518;&#26159;TFT&#21644;&#26641;&#31639;&#27861;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#20135;&#29983;&#20102;0.704 NSE&#21644;3.54 RMSE&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#26681;&#25454;&#26102;&#38388;&#21644;&#36890;&#37327;&#22612;&#31449;&#28857;&#23545;&#32534;&#30721;&#22120;&#29305;&#24449;&#37325;&#35201;&#24615;&#36827;&#34892;&#30340;&#20998;&#35299;&#20998;&#26512;&#12290;&#36825;&#31181;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#26893;&#34987;&#30456;&#20851;&#29305;&#24449;&#65288;&#21253;&#25324;&#27668;&#35937;&#21464;&#37327;&#65289;&#22312;&#21319;&#23610;&#24230;GPP&#20272;&#35745;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable estimates of Gross Primary Productivity (GPP), crucial for evaluating climate change initiatives, are currently only available from sparsely distributed eddy covariance tower sites. This limitation hampers access to reliable GPP quantification at regional to global scales. Prior machine learning studies on upscaling \textit{in situ} GPP to global wall-to-wall maps at sub-daily time steps faced limitations such as lack of input features at higher temporal resolutions and significant missing values. This research explored a novel upscaling solution using Temporal Fusion Transformer (TFT) without relying on past GPP time series. Model development was supplemented by Random Forest Regressor (RFR) and XGBoost, followed by the hybrid model of TFT and tree algorithms. The best preforming model yielded to model performance of 0.704 NSE and 3.54 RMSE. Another contribution of the study was the breakdown analysis of encoder feature importance based on time and flux tower sites. Such anal
&lt;/p&gt;</description></item><item><title>BatchGNN&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;CPU&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36229;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;GNN&#27169;&#22411;&#65292;&#36890;&#36807;&#23439;&#25209;&#22788;&#29702;&#12289;&#38598;&#25104;&#30340;&#22270;&#20998;&#21306;&#21644;&#21407;&#29983;&#30340;GNN&#23618;&#23454;&#29616;&#20197;&#21450;&#32531;&#23384;&#32858;&#21512;&#30340;&#36755;&#20837;&#29305;&#24449;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#20998;&#24067;&#24335;GPU&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.13814</link><description>&lt;p&gt;
BatchGNN&#65306;&#39640;&#25928;&#22320;&#22312;&#36229;&#22823;&#35268;&#27169;&#22270;&#19978;&#36827;&#34892;&#22522;&#20110;CPU&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BatchGNN: Efficient CPU-Based Distributed GNN Training on Very Large Graphs. (arXiv:2306.13814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13814
&lt;/p&gt;
&lt;p&gt;
BatchGNN&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;CPU&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#36229;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;GNN&#27169;&#22411;&#65292;&#36890;&#36807;&#23439;&#25209;&#22788;&#29702;&#12289;&#38598;&#25104;&#30340;&#22270;&#20998;&#21306;&#21644;&#21407;&#29983;&#30340;GNN&#23618;&#23454;&#29616;&#20197;&#21450;&#32531;&#23384;&#32858;&#21512;&#30340;&#36755;&#20837;&#29305;&#24449;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#20998;&#24067;&#24335;GPU&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BatchGNN&#65292;&#19968;&#20010;&#20998;&#24067;&#24335;CPU&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;GNN&#12290;&#36890;&#36807;&#23439;&#25209;&#22788;&#29702;&#65288;macrobatching&#65289;&#65292;&#23558;&#22810;&#20010;&#23567;&#25209;&#37327;&#30340;&#23376;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#25552;&#21462;&#25209;&#22788;&#29702;&#25104;&#19968;&#20010;&#36890;&#20449;&#20013;&#32487;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#36755;&#20837;&#29305;&#24449;&#38745;&#24577;&#26102;&#30340;&#20887;&#20313;&#29305;&#24449;&#25552;&#21462;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;BatchGNN&#25552;&#20379;&#38598;&#25104;&#30340;&#22270;&#20998;&#21306;&#21644;&#21407;&#29983;&#30340;GNN&#23618;&#23454;&#29616;&#65292;&#20197;&#25552;&#39640;&#36816;&#34892;&#25928;&#29575;&#65292;&#24182;&#21487;&#20197;&#32531;&#23384;&#32858;&#21512;&#30340;&#36755;&#20837;&#29305;&#24449;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#37319;&#26679;&#24320;&#38144;&#12290;BatchGNN&#22312;OGBN&#22270;&#19978;&#35757;&#32451;&#30340;&#19977;&#20010;GNN&#27169;&#22411;&#20013;&#65292;&#24179;&#22343;&#36895;&#24230;&#27604;DistDGL&#24555;&#20102;$3\times$&#65292;&#36229;&#36807;&#20102;&#20998;&#24067;&#24335;GPU&#31995;&#32479;$P^3$&#21644;DistDGLv2&#25253;&#21578;&#30340;&#36816;&#34892;&#26102;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#36229;&#22823;&#35268;&#27169;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BatchGNN, a distributed CPU system that showcases techniques that can be used to efficiently train GNNs on terabyte-sized graphs. It reduces communication overhead with macrobatching in which multiple minibatches' subgraph sampling and feature fetching are batched into one communication relay to reduce redundant feature fetches when input features are static. BatchGNN provides integrated graph partitioning and native GNN layer implementations to improve runtime, and it can cache aggregated input features to further reduce sampling overhead. BatchGNN achieves an average $3\times$ speedup over DistDGL on three GNN models trained on OGBN graphs, outperforms the runtimes reported by distributed GPU systems $P^3$ and DistDGLv2, and scales to a terabyte-sized graph.
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13812</link><description>&lt;p&gt;
&#25345;&#32493;&#24615;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity in Deep Continual Learning. (arXiv:2306.13812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13812
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20250;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#19987;&#38376;&#29992;&#20110;&#19968;&#27425;&#24615;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#25345;&#32493;&#24615;&#23398;&#20064;&#65292;&#22914;&#26524;&#23558;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#24212;&#29992;&#20110;&#25345;&#32493;&#24615;&#23398;&#20064;&#20013;&#65292;&#21017;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#21487;&#33021;&#22312;&#35760;&#20303;&#26089;&#26399;&#30340;&#20363;&#23376;&#26041;&#38754;&#36973;&#36935;&#22833;&#36133;&#12290;&#26356;&#20026;&#22522;&#26412;&#20294;&#19981;&#20026;&#20154;&#30693;&#30340;&#26159;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#22833;&#21435;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#21487;&#22609;&#24615;&#20007;&#22833;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#37325;&#26500;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#12290;&#22312;ImageNet&#20013;&#65292;&#20108;&#20803;&#20998;&#31867;&#30340;&#24615;&#33021;&#20174;&#19968;&#20010;&#26089;&#26399;&#20219;&#21153;&#30340;89&#65285;&#27491;&#30830;&#19979;&#38477;&#21040;77&#65285;&#65292;&#25110;&#32773;&#22823;&#32422;&#31561;&#20110;&#32447;&#24615;&#32593;&#32476;&#30340;&#27700;&#24179;&#12290;&#36825;&#31181;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#21457;&#29983;&#22312;&#21508;&#31181;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#20248;&#21270;&#22120;&#21644;&#28608;&#27963;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#19981;&#20250;&#22240;&#25209;&#37327;&#24402;&#19968;&#21270;&#25110;&#25918;&#24323;&#32780;&#24471;&#21040;&#32531;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Contrastive Plasticity&#65292;&#21487;&#20197;&#32531;&#35299;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#21516;&#26102;&#20445;&#30041;&#35760;&#20303;&#26087;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;Contrastive Plasticity&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#24102;&#26469;&#38750;&#24120;&#23569;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep-learning systems are specialized to problem settings in which training occurs once and then never again, as opposed to continual-learning settings in which training occurs continually. If deep-learning systems are applied in a continual learning setting, then it is well known that they may fail catastrophically to remember earlier examples. More fundamental, but less well known, is that they may also lose their ability to adapt to new data, a phenomenon called \textit{loss of plasticity}. We show loss of plasticity using the MNIST and ImageNet datasets repurposed for continual learning as sequences of tasks. In ImageNet, binary classification performance dropped from 89% correct on an early task down to 77%, or to about the level of a linear network, on the 2000th task. Such loss of plasticity occurred with a wide range of deep network architectures, optimizers, and activation functions, and was not eased by batch normalization or dropout. In our experiments, loss of plasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.13803</link><description>&lt;p&gt;
&#22823;&#35937;&#19982;&#31639;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#22823;&#35937;&#30417;&#27979;&#20013;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#20316;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#20445;&#25252;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#20197;&#22788;&#29702;&#24182;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#12290;&#22312;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#19979;&#65292;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#65292;&#20026;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20026;&#22686;&#36827;&#23545;&#21160;&#29289;&#34892;&#20026;&#21644;&#20445;&#25252;&#31574;&#30053;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#26426;&#20250;&#12290;&#20197;&#38750;&#27954;&#20445;&#25252;&#21306;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#35937;&#20026;&#28966;&#28857;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#21644;ML&#22312;&#23427;&#20204;&#20445;&#25252;&#20013;&#30340;&#20316;&#29992;&#12290;&#32473;&#23450;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#65288;&#22914;&#25668;&#20687;&#22836;&#12289;&#40614;&#20811;&#39118;&#12289;&#22320;&#38663;&#20202;&#12289;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#65289;&#25910;&#38598;&#21040;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#65292;&#25361;&#25112;&#22312;&#20110;&#31649;&#29702;&#21644;&#35299;&#35835;&#36825;&#20123;&#24222;&#22823;&#30340;&#25968;&#25454;&#12290;&#26032;&#30340;AI&#21644;ML&#25216;&#26415;&#25552;&#20379;&#20102;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24110;&#21161;&#25105;&#20204;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;AI&#39537;&#21160;&#30417;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#25913;&#21892;&#22823;&#35937;&#20445;&#25252;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;AI&#19987;&#23478;&#21644;&#29983;&#24577;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#21327;&#20316;&#26159;&#21033;&#29992;&#36825;&#20123;&#21019;&#26032;&#25216;&#26415;&#20197;&#22686;&#24378;&#37326;&#29983;&#21160;&#29289;&#20445;&#25252;&#30340;&#20851;&#38190;&#25152;&#22312;&#65292;&#20026;&#35768;&#22810;&#20854;&#20182;&#29289;&#31181;&#35774;&#23450;&#20102;&#20808;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and machine learning (ML) present revolutionary opportunities to enhance our understanding of animal behavior and conservation strategies. Using elephants, a crucial species in Africa's protected areas, as our focal point, we delve into the role of AI and ML in their conservation. Given the increasing amounts of data gathered from a variety of sensors like cameras, microphones, geophones, drones, and satellites, the challenge lies in managing and interpreting this vast data. New AI and ML techniques offer solutions to streamline this process, helping us extract vital information that might otherwise be overlooked. This paper focuses on the different AI-driven monitoring methods and their potential for improving elephant conservation. Collaborative efforts between AI experts and ecological researchers are essential in leveraging these innovative technologies for enhanced wildlife conservation, setting a precedent for numerous other species.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65288;Tensor-DPMM&#65289;&#65292;&#36890;&#36807;&#24352;&#37327;&#20445;&#30041;&#20102;&#22810;&#32500;&#34892;&#31243;&#20449;&#24687;&#30340;&#22810;&#27169;&#24335;&#21644;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#20197;&#32479;&#19968;&#30340;&#19968;&#27493;&#26041;&#24335;&#36827;&#34892;&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;&#65292;&#22312;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13794</link><description>&lt;p&gt;
&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;&#30340;&#24352;&#37327;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tensor Dirichlet Process Multinomial Mixture Model for Passenger Trajectory Clustering. (arXiv:2306.13794v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13794
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65288;Tensor-DPMM&#65289;&#65292;&#36890;&#36807;&#24352;&#37327;&#20445;&#30041;&#20102;&#22810;&#32500;&#34892;&#31243;&#20449;&#24687;&#30340;&#22810;&#27169;&#24335;&#21644;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#20197;&#32479;&#19968;&#30340;&#19968;&#27493;&#26041;&#24335;&#36827;&#34892;&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;&#65292;&#22312;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20986;&#34892;&#35760;&#24405;&#30340;&#20056;&#23458;&#32858;&#31867;&#23545;&#20110;&#36816;&#36755;&#36816;&#33829;&#21830;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#20056;&#23458;&#34892;&#31243;&#20449;&#24687;&#30340;&#20998;&#23618;&#32467;&#26500;&#32780;&#38590;&#20197;&#36731;&#26494;&#36827;&#34892;&#20056;&#23458;&#32858;&#31867;&#65292;&#21363;&#65306;&#27599;&#20010;&#20056;&#23458;&#26377;&#22810;&#27425;&#20986;&#34892;&#65292;&#27599;&#27425;&#20986;&#34892;&#21253;&#21547;&#22810;&#32500;&#22810;&#27169;&#24335;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#30830;&#25351;&#23450;&#32858;&#31867;&#25968;&#37327;&#24320;&#22987;&#65292;&#32780;&#27599;&#22825;&#26377;&#25968;&#30334;&#19975;&#36890;&#21220;&#32773;&#20351;&#29992;&#20132;&#36890;&#31995;&#32479;&#26102;&#36825;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#22810;&#39033;&#24335;&#28151;&#21512;&#27169;&#22411;&#65288;Tensor-DPMM&#65289;&#65292;&#36890;&#36807;&#24352;&#37327;&#20445;&#30041;&#20102;&#22810;&#32500;&#34892;&#31243;&#20449;&#24687;&#30340;&#22810;&#27169;&#24335;&#21644;&#20998;&#23618;&#32467;&#26500;&#65292;&#24182;&#20197;&#32479;&#19968;&#30340;&#19968;&#27493;&#26041;&#24335;&#23545;&#23427;&#20204;&#36827;&#34892;&#32858;&#31867;&#12290;&#35813;&#27169;&#22411;&#36824;&#36890;&#36807;&#20351;&#29992;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#20915;&#23450;&#20056;&#23458;&#20998;&#37197;&#33267;&#29616;&#26377;&#32858;&#31867;&#36824;&#26159;&#24418;&#25104;&#26032;&#32858;&#31867;&#30340;&#27010;&#29575;&#65292;&#33258;&#21160;&#30830;&#23450;&#32858;&#31867;&#25968;&#30340;&#33021;&#21147;&#12290;&#22312;&#30495;&#23454;&#30340;&#20132;&#36890;&#36816;&#36755;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Tensor-DPMM&#27169;&#22411;&#22312;&#20056;&#23458;&#36712;&#36857;&#32858;&#31867;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Passenger clustering based on travel records is essential for transportation operators. However, existing methods cannot easily cluster the passengers due to the hierarchical structure of the passenger trip information, namely: each passenger has multiple trips, and each trip contains multi-dimensional multi-mode information. Furthermore, existing approaches rely on an accurate specification of the clustering number to start, which is difficult when millions of commuters are using the transport systems on a daily basis. In this paper, we propose a novel Tensor Dirichlet Process Multinomial Mixture model (Tensor-DPMM), which is designed to preserve the multi-mode and hierarchical structure of the multi-dimensional trip information via tensor, and cluster them in a unified one-step manner. The model also has the ability to determine the number of clusters automatically by using the Dirichlet Process to decide the probabilities for a passenger to be either assigned in an existing cluster 
&lt;/p&gt;</description></item><item><title>QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13793</link><description>&lt;p&gt;
QNNRepair&#65306;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13793
&lt;/p&gt;
&lt;p&gt;
QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QNNRepair&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#20462;&#27491;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#20854;&#26088;&#22312;&#25552;&#39640;&#22312;&#37327;&#21270;&#20043;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#25509;&#21463;&#20840;&#31934;&#24230;&#21644;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#19968;&#20010;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;QNNRepair&#23558;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#35268;&#21010;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#29305;&#28857;&#65292;&#35813;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#38543;&#26426;&#21644;&#26377;&#26426;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.13789</link><description>&lt;p&gt;
&#35299;&#26500;&#20998;&#31867;&#22120;&#65306;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models. (arXiv:2306.13789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#65292;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#29305;&#28857;&#65292;&#35813;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#22312;&#38543;&#26426;&#21644;&#26377;&#26426;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38738;&#30544;&#65292;&#22914;&#25991;&#26412;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#38544;&#31169;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#65292;&#21253;&#25324;&#26088;&#22312;&#25552;&#21462;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#25454;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#12290;&#22823;&#22810;&#25968;&#20197;&#21069;&#20851;&#20110;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;LLM&#19978;&#65292;&#32780;&#20998;&#31867;&#27169;&#22411;&#34987;&#35748;&#20026;&#26356;&#23433;&#20840;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#31216;&#20026;Mix And Match&#25915;&#20987;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#22810;&#25968;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;LLM&#30340;&#20107;&#23454;&#12290;Mix And Match&#25915;&#20987;&#20351;&#29992;&#30446;&#26631;&#27169;&#22411;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#20505;&#36873;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#20998;&#31867;&#22836;&#20462;&#21098;&#23427;&#20204;&#12290;&#25105;&#20204;&#24191;&#27867;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#19982;&#26377;&#26426;&#30340;&#37329;&#19997;&#38592;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#20998;&#31867;&#27169;&#22411;&#20013;&#32771;&#34385;&#19982;&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#30456;&#20851;&#30340;&#38544;&#31169;&#39118;&#38505;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) models have become increasingly popular in real-world applications, such as text classification. However, they are vulnerable to privacy attacks, including data reconstruction attacks that aim to extract the data used to train the model. Most previous studies on data reconstruction attacks have focused on LLM, while classification models were assumed to be more secure. In this work, we propose a new targeted data reconstruction attack called the Mix And Match attack, which takes advantage of the fact that most classification models are based on LLM. The Mix And Match attack uses the base model of the target model to generate candidate tokens and then prunes them using the classification head. We extensively demonstrate the effectiveness of the attack using both random and organic canaries. This work highlights the importance of considering the privacy risks associated with data reconstruction attacks in classification models and offers insights into po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#26041;&#27861;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#20309;&#20551;&#35774;&#19979;&#23545;&#35823;&#24046;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21482;&#35201;&#27714;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36866;&#24403;&#30340;&#36924;&#36817;&#33021;&#21147;&#23601;&#21487;&#20197;&#23558;&#36817;&#20284;&#36716;&#21270;&#20026;&#30446;&#26631;&#20989;&#25968;f&#12290;</title><link>http://arxiv.org/abs/2306.13784</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#26041;&#27861;&#65306;&#20272;&#35745;&#21644;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
A new approach to generalisation error of machine learning algorithms: Estimates and convergence. (arXiv:2306.13784v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20272;&#35745;&#26041;&#27861;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#20309;&#20551;&#35774;&#19979;&#23545;&#35823;&#24046;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21482;&#35201;&#27714;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36866;&#24403;&#30340;&#36924;&#36817;&#33021;&#21147;&#23601;&#21487;&#20197;&#23558;&#36817;&#20284;&#36716;&#21270;&#20026;&#30446;&#26631;&#20989;&#25968;f&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28145;&#24230;&#31070;&#32463;&#23398;&#20064;&#30340;&#19968;&#20010;&#27169;&#22411;&#38382;&#39064;&#65292;&#21363;&#22312;&#26377;&#38480;&#30340;&#28857;&#38598;&#19978;&#24050;&#30693;&#19968;&#20010;&#20989;&#25968;&#30340;&#28857;&#20540;&#65292;&#23398;&#20064;&#35813;&#20989;&#25968;&#12290;&#36890;&#36807;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21253;&#25324;&#32473;&#23450;&#30340;DNN&#20307;&#31995;&#32467;&#26500;&#21644;&#19968;&#20010;&#34987;&#35748;&#20026;&#21487;&#20197;&#23436;&#20840;&#35299;&#20915;&#30340;&#20248;&#21270;&#27493;&#39588;&#65292;&#24471;&#21040;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25554;&#20540;&#22120;&#65292;&#35813;&#25554;&#20540;&#22120;&#26159;f&#30340;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20272;&#35745;&#65288;&#27867;&#21270;&#65289;&#35823;&#24046;&#21644;&#25910;&#25947;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21253;&#25324;&#65306;&#65288;i&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#27809;&#26377;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24182;&#22312;&#23398;&#20064;&#20989;&#25968;f&#30340;&#28201;&#21644;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#20272;&#35745;&#35823;&#24046;&#65292;&#65288;ii&#65289;&#21482;&#35201;&#31070;&#32463;&#32593;&#32476;&#31354;&#38388;&#20855;&#26377;&#36866;&#24403;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#23601;&#21487;&#20197;&#23558;&#36817;&#20284;&#36716;&#21270;&#20026;&#30446;&#26631;&#20989;&#25968;f&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we consider a model problem of deep neural learning, namely the learning of a given function when it is assumed that we have access to its point values on a finite set of points. The deep neural network interpolant is the the resulting approximation of f, which is obtained by a typical machine learning algorithm involving a given DNN architecture and an optimisation step, which is assumed to be solved exactly. These are among the simplest regression algorithms based on neural networks. In this work we introduce a new approach to the estimation of the (generalisation) error and to convergence. Our results include (i) estimates of the error without any structural assumption on the neural networks and under mild regularity assumptions on the learning function f (ii) convergence of the approximations to the target function f by only requiring that the neural network spaces have appropriate approximation capability.
&lt;/p&gt;</description></item><item><title>Swin-Free&#26159;&#19968;&#20010;Transformer&#27169;&#22411;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#21464;&#21270;&#22823;&#23567;&#30340;&#31383;&#21475;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#31383;&#21475;&#27880;&#24847;&#21147;&#21644;&#25928;&#29575;&#12290;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#25512;&#29702;&#26102;&#36895;&#24230;&#26356;&#24555;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13776</link><description>&lt;p&gt;
Swin-Free: &#36890;&#36807;&#21464;&#21270;&#22823;&#23567;&#30340;&#31383;&#21475;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#31383;&#21475;&#27880;&#24847;&#21147;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window. (arXiv:2306.13776v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13776
&lt;/p&gt;
&lt;p&gt;
Swin-Free&#26159;&#19968;&#20010;Transformer&#27169;&#22411;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#21464;&#21270;&#22823;&#23567;&#30340;&#31383;&#21475;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#31383;&#21475;&#27880;&#24847;&#21147;&#21644;&#25928;&#29575;&#12290;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#25512;&#29702;&#26102;&#36895;&#24230;&#26356;&#24555;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21518;&#65292;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#20013;Swin Transformer&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#21367;&#31215;&#30340;&#26550;&#26500;&#65292;&#22312;&#19982;Vision Transformer&#21450;&#20854;&#21464;&#20307;&#30456;&#27604;&#30340;&#25928;&#29575;&#26041;&#38754;&#20063;&#26377;&#25152;&#25913;&#36827;&#65292;&#22240;&#20026;&#21518;&#32773;&#30456;&#23545;&#20110;&#36755;&#20837;&#22823;&#23567;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;Swin Transformer&#37319;&#29992;&#20102;&#31227;&#21160;&#31383;&#21475;&#30340;&#21151;&#33021;&#65292;&#20801;&#35768;&#36328;&#31383;&#21475;&#36830;&#25509;&#65292;&#21516;&#26102;&#23558;&#33258;&#25105;&#27880;&#24847;&#35745;&#31639;&#38480;&#21046;&#22312;&#19981;&#37325;&#21472;&#30340;&#23616;&#37096;&#31383;&#21475;&#20013;&#12290;&#28982;&#32780;&#65292;&#31227;&#21160;&#31383;&#21475;&#24341;&#20837;&#20102;&#20869;&#23384;&#22797;&#21046;&#25805;&#20316;&#65292;&#36825;&#21344;&#25454;&#20102;&#20854;&#36816;&#34892;&#26102;&#38388;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Swin-Free&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#21508;&#20010;&#38454;&#27573;&#24212;&#29992;&#21464;&#21270;&#22823;&#23567;&#30340;&#31383;&#21475;&#65292;&#32780;&#19981;&#26159;&#31227;&#21160;&#31383;&#21475;&#65292;&#20197;&#23454;&#29616;&#23616;&#37096;&#31383;&#21475;&#20043;&#38388;&#30340;&#20132;&#21449;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21333;&#30340;&#35774;&#35745;&#25913;&#21464;&#65292;Swin-Free&#22312;&#25512;&#29702;&#26102;&#27604;Swin Transformer&#36816;&#34892;&#26356;&#24555;&#65292;&#20934;&#30830;&#24230;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#27604;Swin-Free&#26356;&#24555;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have shown great potential in computer vision, following their success in language tasks. Swin Transformer is one of them that outperforms convolution-based architectures in terms of accuracy, while improving efficiency when compared to Vision Transformer (ViT) and its variants, which have quadratic complexity with respect to the input size. Swin Transformer features shifting windows that allows cross-window connection while limiting self-attention computation to non-overlapping local windows. However, shifting windows introduces memory copy operations, which account for a significant portion of its runtime. To mitigate this issue, we propose Swin-Free in which we apply size-varying windows across stages, instead of shifting windows, to achieve cross-connection among local windows. With this simple design change, Swin-Free runs faster than the Swin Transformer at inference with better accuracy. Furthermore, we also propose a few of Swin-Free variants that are faster 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.13773</link><description>&lt;p&gt;
&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour with Bandit Feedback. (arXiv:2306.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#24182;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#20855;&#26377;&#39640;&#25928;&#36816;&#34892;&#12289;&#24555;&#36895;&#25628;&#32034;&#21644;&#20934;&#32447;&#24615;&#31354;&#38388;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#36817;&#37051;&#31639;&#27861;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22788;&#29702;&#20102;&#23436;&#20840;&#23545;&#25239;&#30340;&#35774;&#32622;&#65292;&#21363;&#19981;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#24403;&#19982;&#24555;&#36895;&#25968;&#25454;&#32467;&#26500;&#65288;&#21487;&#33021;&#26159;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26368;&#36817;&#37051;&#25628;&#32034;&#65292;&#22914;&#23548;&#33322;&#32593;&#32476;&#65289;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#39640;&#25928;-&#27599;&#27425;&#35797;&#39564;&#30340;&#36816;&#34892;&#26102;&#38388;&#23545;&#21160;&#20316;&#25968;&#21644;&#35797;&#39564;&#25968;&#21576;&#23545;&#25968;&#22810;&#39033;&#24335;&#22686;&#38271;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#20934;&#32447;&#24615;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adapt the nearest neighbour rule to the contextual bandit problem. Our algorithm handles the fully adversarial setting in which no assumptions at all are made about the data-generation process. When combined with a sufficiently fast data-structure for (perhaps approximate) adaptive nearest neighbour search, such as a navigating net, our algorithm is extremely efficient - having a per trial running time polylogarithmic in both the number of trials and actions, and taking only quasi-linear space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#36335;&#24452;&#21644;&#27010;&#29575;&#36719;&#36923;&#36753;&#30340;&#33647;&#29289;-&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21508;&#31867;&#30456;&#20284;&#24615;&#20016;&#23500;&#20449;&#24687;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#36739;&#24555;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.13770</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#27010;&#29575;&#36719;&#36923;&#36753;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Meta-Path-based Probabilistic Soft Logic for Drug-Target Interaction Prediction. (arXiv:2306.13770v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#36335;&#24452;&#21644;&#27010;&#29575;&#36719;&#36923;&#36753;&#30340;&#33647;&#29289;-&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#21508;&#31867;&#30456;&#20284;&#24615;&#20016;&#23500;&#20449;&#24687;&#21644;&#25299;&#25169;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#36739;&#24555;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#39044;&#27979;&#33647;&#29289;&#26159;&#21542;&#19982;&#38774;&#28857;&#32467;&#21512;&#65292;&#20197;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#33647;&#29289;&#35774;&#35745;&#30340;&#25104;&#26412;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20284;&#24615;&#21644;&#38774;&#28857;-&#38774;&#28857;&#30456;&#20284;&#24615;&#20449;&#24687;&#36827;&#34892;DTI&#39044;&#27979;&#65292;&#26080;&#27861;&#21033;&#29992;&#21508;&#31181;&#30456;&#20284;&#24615;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#21033;&#29992;&#22810;&#30456;&#20284;&#24615;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#20173;&#32570;&#20047;&#32771;&#34385;&#33647;&#29289;&#21644;&#38774;&#28857;&#25152;&#39547;&#30041;&#30340;&#21508;&#31181;&#30693;&#35782;&#24211;&#30340;&#20016;&#23500;&#25299;&#25169;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26102;&#38388;&#28040;&#32791;&#38750;&#24120;&#39640;&#65292;&#38459;&#27490;&#20102;&#22823;&#35268;&#27169;&#32593;&#32476;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#33647;&#29289;-&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27010;&#29575;&#36719;&#36923;&#36753;&#65288;PSL&#65289;&#24212;&#29992;&#20110;&#20803;&#36335;&#24452;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug-target interaction (DTI) prediction, which aims at predicting whether a drug will be bounded to a target, have received wide attention recently, with the goal to automate and accelerate the costly process of drug design. Most of the recently proposed methods use single drug-drug similarity and target-target similarity information for DTI prediction, which are unable to take advantage of the abundant information regarding various types of similarities between them. Very recently, some methods are proposed to leverage multi-similarity information, however, they still lack the ability to take into consideration the rich topological information of all sorts of knowledge bases where the drugs and targets reside in. More importantly, the time consumption of these approaches is very high, which prevents the usage of large-scale network information. We thus propose a network-based drug-target interaction prediction approach, which applies probabilistic soft logic (PSL) to meta-paths on a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.13769</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration. (arXiv:2306.13769v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;D3FG&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;&#22522;&#20110;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;AI&#36741;&#21161;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#32473;&#23450;&#30446;&#26631;&#34507;&#30333;&#36136;&#30340;&#21475;&#34955;&#32467;&#26500;&#30340;&#20998;&#23376;&#12290;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#21407;&#23376;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#23376;&#35270;&#20026;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#29983;&#25104;&#21407;&#23376;&#20301;&#32622;&#21644;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#30340;&#29616;&#23454;&#29255;&#27573;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D3FG&#65292;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#22242;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#21475;&#34955;&#29305;&#24322;&#24615;&#20998;&#23376;&#29983;&#25104;&#21644;&#25193;&#23637;&#12290;D3FG&#23558;&#20998;&#23376;&#20998;&#35299;&#20026;&#20004;&#31867;&#32452;&#20214;&#65306;&#23450;&#20041;&#20026;&#21018;&#24615;&#20307;&#30340;&#21151;&#33021;&#22242;&#21644;&#36136;&#28857;&#30340;&#36830;&#25509;&#22120;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;&#32452;&#20214;&#21487;&#20197;&#20849;&#21516;&#24418;&#25104;&#22686;&#24378;&#37197;&#20307;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#29255;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;D3FG&#23558;&#32452;&#20214;&#30340;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#31867;&#22411;&#30340;&#25968;&#25454;&#20998;&#24067;&#25193;&#25955;&#21040;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#65307;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#21435;&#22122;&#22120;&#36880;&#28176;&#21435;&#38500;&#19977;&#20010;&#21464;&#37327;&#30340;&#22122;&#22768;&#65292;&#20197;&#33719;&#24471;&#29616;&#23454;&#20998;&#23376;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#38024;&#23545;&#20843;&#20010;&#30446;&#26631;&#36827;&#34892;&#24211;&#29983;&#25104;&#65292;&#38024;&#23545;&#20116;&#31181;&#29616;&#26377;&#33647;&#29289;&#36827;&#34892;&#29255;&#27573;&#23436;&#21892;&#65292;&#20197;&#21450;&#38024;&#23545;&#20004;&#20010;&#26410;&#30693;&#21442;&#32771;&#20998;&#23376;&#30340;&#30446;&#26631;&#36827;&#34892;&#21435;&#26032;&#35774;&#35745;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#21697;&#36136;&#30340;&#20998;&#23376;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, AI-assisted drug design methods have been proposed to generate molecules given the pockets' structures of target proteins. Most of them are atom-level-based methods, which consider atoms as basic components and generate atom positions and types. In this way, however, it is hard to generate realistic fragments with complicated structures. To solve this, we propose D3FG, a functional-group-based diffusion model for pocket-specific molecule generation and elaboration. D3FG decomposes molecules into two categories of components: functional groups defined as rigid bodies and linkers as mass points. And the two kinds of components can together form complicated fragments that enhance ligand-protein interactions.  To be specific, in the diffusion process, D3FG diffuses the data distribution of the positions, orientations, and types of the components into a prior distribution; In the generative process, the noise is gradually removed from the three variables by denoisers parame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13761</link><description>&lt;p&gt;
CeBed: &#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20449;&#36947;&#20272;&#35745;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20844;&#27491;&#21644;&#29616;&#23454;&#30340;&#27604;&#36739;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#24037;&#20855;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#24211;&#65289;&#38459;&#30861;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20449;&#36947;&#20272;&#35745;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24314;&#31435;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#65292;&#32479;&#19968;&#20102;&#20960;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CeBed&#65288;&#20449;&#36947;&#20272;&#35745;&#27979;&#35797;&#24179;&#21488;&#65289;&#65292;&#21253;&#25324;&#28085;&#30422;&#21508;&#31181;&#31995;&#32479;&#27169;&#22411;&#21644;&#20256;&#25773;&#26465;&#20214;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21313;&#20010;&#28145;&#24230;&#21644;&#20256;&#32479;&#30340;&#22522;&#32447;&#23454;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#26041;&#27861;IPC&#65292;&#36890;&#36807;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#35299;&#20915;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#22686;&#20540;&#27169;&#22411;&#20013;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#25110;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#36716;&#25442;&#36807;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#25928;&#29575;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.13759</link><description>&lt;p&gt;
&#22686;&#37327;&#36716;&#21270;&#25910;&#30410;&#65306;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20419;&#38144;&#22686;&#20540;&#27169;&#22411;&#30340;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incremental Profit per Conversion: a Response Transformation for Uplift Modeling in E-Commerce Promotions. (arXiv:2306.13759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#26041;&#27861;IPC&#65292;&#36890;&#36807;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#35299;&#20915;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#22686;&#20540;&#27169;&#22411;&#20013;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#25110;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#36716;&#25442;&#36807;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#25928;&#29575;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20419;&#38144;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#37319;&#29992;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#26469;&#25512;&#21160;&#29992;&#25143;&#21442;&#19982;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#20855;&#26377;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#30340;&#20419;&#38144;&#65292;&#21482;&#26377;&#24403;&#36141;&#20080;&#21457;&#29983;&#26102;&#25165;&#20250;&#20135;&#29983;&#36153;&#29992;&#12290;&#36825;&#20123;&#20419;&#38144;&#21253;&#25324;&#25240;&#25187;&#21644;&#20248;&#24800;&#21048;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22686;&#20540;&#27169;&#22411;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#22914;&#20803;&#23398;&#20064;&#22120;&#65292;&#25110;&#32773;&#30001;&#20110;&#38750;&#36716;&#25442;&#20010;&#20307;&#30340;&#38646;&#25104;&#26412;&#21644;&#21033;&#28070;&#24341;&#36215;&#30340;&#38646;&#33192;&#32960;&#20540;&#32780;&#20272;&#31639;&#21033;&#28070;&#26102;&#36935;&#21040;&#22797;&#26434;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#37327;&#36716;&#21270;&#25910;&#30410;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20419;&#38144;&#27963;&#21160;&#21333;&#20301;&#32463;&#27982;&#25928;&#29575;&#30340;&#22686;&#20540;&#24230;&#37327;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21709;&#24212;&#36716;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#20165;&#38656;&#35201;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#12289;&#20854;&#20542;&#21521;&#24615;&#21644;&#19968;&#20010;&#35201;&#20272;&#35745;&#30340;&#27169;&#22411;&#21363;&#21487;&#12290;&#32467;&#26524;&#65292;IPC&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#36890;&#24120;&#19982;&#21709;&#24212;&#30456;&#20851;&#25104;&#26412;&#26377;&#20851;&#30340;&#22122;&#22768;&#12290;&#23545;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;IPC&#25552;&#20379;&#20102;&#23545;&#20419;&#38144;&#27963;&#21160;&#33719;&#21033;&#33021;&#21147;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#23450;&#30340;&#20272;&#35745;&#65292;&#26159;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20419;&#38144;&#23450;&#21521;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Promotions play a crucial role in e-commerce platforms, and various cost structures are employed to drive user engagement. This paper focuses on promotions with response-dependent costs, where expenses are incurred only when a purchase is made. Such promotions include discounts and coupons. While existing uplift model approaches aim to address this challenge, these approaches often necessitate training multiple models, like meta-learners, or encounter complications when estimating profit due to zero-inflated values stemming from non-converted individuals with zero cost and profit.  To address these challenges, we introduce Incremental Profit per Conversion (IPC), a novel uplift measure of promotional campaigns' efficiency in unit economics. Through a proposed response transformation, we demonstrate that IPC requires only converted data, its propensity, and a single model to be estimated. As a result, IPC resolves the issues mentioned above while mitigating the noise typically associate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#20844;&#29702;&#21270;&#34920;&#24449;&#65292;&#30830;&#23450;&#20102;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#22312;&#31526;&#21512;&#19981;&#21516;&#20844;&#29702;&#38598;&#30340;&#24402;&#22240;&#26041;&#27861;&#31867;&#21035;&#20013;&#26159;&#21807;&#19968;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.13753</link><description>&lt;p&gt;
&#38598;&#25104;&#26799;&#24230;&#24402;&#22240;&#26041;&#27861;&#30340;&#22235;&#20010;&#20844;&#29702;&#21270;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Four Axiomatic Characterizations of the Integrated Gradients Attribution Method. (arXiv:2306.13753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#20844;&#29702;&#21270;&#34920;&#24449;&#65292;&#30830;&#23450;&#20102;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#22312;&#31526;&#21512;&#19981;&#21516;&#20844;&#29702;&#38598;&#30340;&#24402;&#22240;&#26041;&#27861;&#31867;&#21035;&#20013;&#26159;&#21807;&#19968;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#31934;&#24230;&#21644;&#21151;&#33021;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20854;&#20869;&#37096;&#24037;&#20316;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#30693;&#12290;&#24402;&#22240;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25351;&#31034;&#27599;&#20010;&#36755;&#20837;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#36129;&#29486;&#26469;&#25581;&#31034;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#20844;&#29702;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#34987;&#35774;&#35745;&#20026;&#31526;&#21512;&#24402;&#22240;&#30340;&#29305;&#23450;&#21407;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IG&#30340;&#22235;&#20010;&#20844;&#29702;&#21270;&#34920;&#24449;&#65292;&#23558;IG&#30830;&#31435;&#20026;&#28385;&#36275;&#19968;&#31867;&#24402;&#22240;&#26041;&#27861;&#19981;&#21516;&#20844;&#29702;&#38598;&#30340;&#21807;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have produced significant progress among machine learning models in terms of accuracy and functionality, but their inner workings are still largely unknown. Attribution methods seek to shine a light on these "black box" models by indicating how much each input contributed to a model's outputs. The Integrated Gradients (IG) method is a state of the art baseline attribution method in the axiomatic vein, meaning it is designed to conform to particular principles of attributions. We present four axiomatic characterizations of IG, establishing IG as the unique method to satisfy different sets of axioms among a class of attribution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;CCP&#36741;&#21161;UMAP&#21644;t-SNE&#26041;&#27861;&#21487;&#35270;&#21270;scRNA-seq&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32454;&#32990;&#31867;&#22411;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20247;&#22810;scRNA-seq&#24212;&#29992;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.13750</link><description>&lt;p&gt;
&#21033;&#29992;CCP&#36741;&#21161;UMAP&#21644;t-SNE&#20998;&#26512;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing scRNA-seq data by CCP-assisted UMAP and t-SNE. (arXiv:2306.13750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;CCP&#36741;&#21161;UMAP&#21644;t-SNE&#26041;&#27861;&#21487;&#35270;&#21270;scRNA-seq&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32454;&#32990;&#31867;&#22411;&#32858;&#31867;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#65292;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20247;&#22810;scRNA-seq&#24212;&#29992;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#65288;scRNA-seq&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25581;&#31034;&#32454;&#32990;&#24322;&#36136;&#24615;&#65292;&#36825;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#32454;&#32990;&#38388;&#36890;&#20449;&#65292;&#32454;&#32990;&#20998;&#21270;&#21644;&#24046;&#24322;&#22522;&#22240;&#34920;&#36798;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#28041;&#21450;&#30340;&#22522;&#22240;&#25968;&#37327;&#24222;&#22823;&#65292;&#20998;&#26512;scRNA-seq&#25968;&#25454;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#38477;&#32500;&#21644;&#29305;&#24449;&#36873;&#25321;&#23545;&#20110;&#28040;&#38500;&#20551;&#20449;&#21495;&#24182;&#22686;&#24378;&#19979;&#28216;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#30456;&#20851;&#32858;&#31867;&#21644;&#25237;&#24433;&#65288;CCP&#65289;&#26159;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;scRNA-seq&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;CCP&#21033;&#29992;&#22522;&#22240;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23545;&#22522;&#22240;&#36827;&#34892;&#20998;&#32452;&#65292;&#24182;&#26681;&#25454;&#27492;&#20998;&#32452;&#20351;&#29992;&#32454;&#32990;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#33719;&#21462;&#36229;&#32423;&#22522;&#22240;&#12290;&#30001;&#20110;CCP&#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#30697;&#38453;&#23545;&#35282;&#21270;&#30340;&#25968;&#25454;&#22495;&#26041;&#27861;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;CCP&#20316;&#20026;&#22343;&#21248;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#21644;t&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#30340;&#21021;&#22987;&#21270;&#24037;&#20855;&#26469;&#21487;&#35270;&#21270;scRNA-seq&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;CCP&#36741;&#21161;UMAP&#21644;t-SNE&#22312;&#32454;&#32990;&#31867;&#22411;&#32858;&#31867;&#26041;&#38754;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#25968;&#30334;&#19975;&#20010;&#32454;&#32990;&#21644;&#25968;&#21315;&#20010;&#22522;&#22240;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;CCP&#36741;&#21161;UMAP&#21644;t-SNE&#21487;&#20197;&#20316;&#20026;&#35768;&#22810;scRNA-seq&#24212;&#29992;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell RNA sequencing (scRNA-seq) is widely used to reveal heterogeneity in cells, which has given us insights into cell-cell communication, cell differentiation, and differential gene expression. However, analyzing scRNA-seq data is a challenge due to sparsity and the large number of genes involved. Therefore, dimensionality reduction and feature selection are important for removing spurious signals and enhancing downstream analysis. Correlated clustering and projection (CCP) was recently introduced as an effective method for preprocessing scRNA-seq data. CCP utilizes gene-gene correlations to partition the genes and, based on the partition, employs cell-cell interactions to obtain super-genes. Because CCP is a data-domain approach that does not require matrix diagonalization, it can be used in many downstream machine learning tasks. In this work, we utilize CCP as an initialization tool for uniform manifold approximation and projection (UMAP) and t-distributed stochastic neighbo
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#27493;&#39588;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#25104;&#21151;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20854;&#23384;&#22312;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13746</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#20043;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Valid inference after prediction. (arXiv:2306.13746v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13746
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#27491;&#27493;&#39588;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#30340;&#26041;&#27861;&#25104;&#21151;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20854;&#23384;&#22312;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#22522;&#20110;&#39044;&#27979;&#30340;&#25512;&#26029;&#65292;&#21363;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#21464;&#37327;&#65292;&#28982;&#21518;&#23545;&#35813;&#39044;&#27979;&#21709;&#24212;&#19982;&#26576;&#20123;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#23558;&#26631;&#20934;&#25512;&#26029;&#26041;&#27861;&#24212;&#29992;&#20110;&#35813;&#36807;&#31243;&#24182;&#19981;&#33021;&#20934;&#30830;&#37327;&#21270;&#26410;&#35266;&#27979;&#21040;&#65288;&#32780;&#38750;&#39044;&#27979;&#21040;&#65289;&#21709;&#24212;&#19982;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;Wang&#31561;&#20154;&#65288;2020&#65289;&#21644;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#20102;&#20462;&#27491;&#65288;ii&#65289;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21040;&#21709;&#24212;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25512;&#26029;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;Angelopoulos&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25511;&#21046;&#20102;&#31532;&#19968;&#31867;&#38169;&#35823;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#27491;&#30830;&#21629;&#21517;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#26080;&#35770;&#39044;&#20808;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#30340;&#36136;&#37327;&#22914;&#20309;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has focused on the very common practice of prediction-based inference: that is, (i) using a pre-trained machine learning model to predict an unobserved response variable, and then (ii) conducting inference on the association between that predicted response and some covariates. As pointed out by Wang et al. [2020], applying a standard inferential approach in (ii) does not accurately quantify the association between the unobserved (as opposed to the predicted) response and the covariates. In recent work, Wang et al. [2020] and Angelopoulos et al. [2023] propose corrections to step (ii) in order to enable valid inference on the association between the unobserved response and the covariates. Here, we show that the method proposed by Angelopoulos et al. [2023] successfully controls the type 1 error rate and provides confidence intervals with correct nominal coverage, regardless of the quality of the pre-trained machine learning model used to predict the unobserved response. Howe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#22810;&#30446;&#26631;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#36873;&#25321;&#23545;&#36807;&#31243;&#20013;&#28789;&#27963;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#24179;&#34913;&#28789;&#27963;&#24615;&#21644;&#20844;&#27491;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.13738</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#22810;&#26679;&#24615;&#65306;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#28789;&#27963;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#30446;&#26631;&#35268;&#23450;
&lt;/p&gt;
&lt;p&gt;
Multi-Target Multiplicity: Flexibility and Fairness in Target Specification under Resource Constraints. (arXiv:2306.13738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13738
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#22810;&#30446;&#26631;&#22810;&#26679;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#36873;&#25321;&#23545;&#36807;&#31243;&#20013;&#28789;&#27963;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#24179;&#34913;&#28789;&#27963;&#24615;&#21644;&#20844;&#27491;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#22914;&#23601;&#19994;&#12289;&#25945;&#32946;&#12289;&#36151;&#27454;&#21644;&#20581;&#24247;&#12290;&#28982;&#32780;&#65292;&#23569;&#26377;&#30495;&#27491;&#30340;&#23454;&#38469;&#38382;&#39064;&#21487;&#20197;&#34987;&#31934;&#30830;&#23450;&#20041;&#20026;&#39044;&#27979;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#24120;&#26377;&#35768;&#22810;&#21512;&#29702;&#30340;&#30446;&#26631;&#21464;&#37327;&#36873;&#39033;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#35748;&#20026;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32780;&#19988;&#26377;&#26102;&#34987;&#20302;&#20272;&#30340;&#36873;&#25321;&#65292;&#20063;&#34920;&#26126;&#20102;&#30446;&#26631;&#36873;&#25321;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#20844;&#27491;&#24615;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#24182;&#27809;&#26377;&#25552;&#20379;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26694;&#26550;&#26469;&#25551;&#36848;&#30446;&#26631;&#36873;&#25321;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#30446;&#26631;&#36873;&#25321;&#38382;&#39064;&#21644;&#26368;&#36817;&#20851;&#20110;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#24037;&#20316;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#30446;&#26631;&#36873;&#25321;&#22312;&#22810;&#26679;&#24615;&#21644;&#32676;&#20307;&#36873;&#25321;&#29575;&#19981;&#24179;&#31561;&#26041;&#38754;&#24433;&#21709;&#20010;&#20154;&#32467;&#26524;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22810;&#30446;&#26631;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35782;&#21035;&#24179;&#34913;&#28789;&#27963;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#26368;&#20248;&#30446;&#26631;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20986;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#26126;&#30830;&#32771;&#34385;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#21435;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals' outcomes and selection rate disparities across groups. We call this multi-target mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#32452;&#21512;&#20844;&#20849;&#25968;&#25454;&#38598;&#20197;&#24212;&#23545;HAR&#39046;&#22495;&#30340;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;HAR&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;&#20180;&#32454;&#36873;&#25321;&#21644;&#32452;&#21512;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21487;&#36798;&#21040;&#19982;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13735</link><description>&lt;p&gt;
&#32452;&#21512;&#20844;&#20849;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#20197;&#20943;&#36731;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combining Public Human Activity Recognition Datasets to Mitigate Labeled Data Scarcity. (arXiv:2306.13735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#32452;&#21512;&#20844;&#20849;&#25968;&#25454;&#38598;&#20197;&#24212;&#23545;HAR&#39046;&#22495;&#30340;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#39640;HAR&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#37319;&#29992;&#20180;&#32454;&#36873;&#25321;&#21644;&#32452;&#21512;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21487;&#36798;&#21040;&#19982;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035; (HAR) &#21487;&#20197;&#24102;&#26469;&#24378;&#22823;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#24050;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#19981;&#20165;&#29992;&#20110;&#27169;&#22411;&#30340;&#21021;&#22987;&#35757;&#32451;&#65292;&#36824;&#29992;&#20110;&#22312;&#29305;&#23450;&#23458;&#25143;&#31471;&#19978;&#23545;&#20854;&#36827;&#34892;&#23450;&#21046;&#65288;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#36890;&#24120;&#19982;&#35757;&#32451;&#25968;&#25454;&#26377;&#24456;&#22823;&#21306;&#21035;&#65289;&#12290;&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#25104;&#26412;&#12289;&#24178;&#25200;&#24615;&#21644;&#32791;&#26102;&#30340;&#26412;&#36136;&#65292;&#36825;&#23454;&#38469;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#24050;&#26631;&#27880;&#30340;&#25968;&#25454;&#30340;&#24110;&#21161;&#65292;&#27169;&#22411;&#22312;&#24322;&#26500;&#23458;&#25143;&#31471;&#19978;&#30340;&#37096;&#32626;&#20063;&#38754;&#20020;&#30528;&#27010;&#25324;&#26410;&#35265;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20943;&#23569;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#24182;&#26356;&#22909;&#22320;&#31649;&#29702;&#24322;&#26500;&#24615;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#30001;&#20110;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#36824;&#27809;&#26377;&#22312;HAR&#39046;&#22495;&#20013;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32452;&#21512;&#20844;&#20849;&#25968;&#25454;&#38598;&#20197;&#20943;&#36731;HAR&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#22909;&#22788;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#22810;&#26679;&#24615;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#21644;&#32452;&#21512;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#26356;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#26159;&#25913;&#21892;HAR&#27169;&#22411;&#27010;&#25324;&#24615;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20943;&#36731;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of supervised learning for Human Activity Recognition (HAR) on mobile devices leads to strong classification performances. Such an approach, however, requires large amounts of labeled data, both for the initial training of the models and for their customization on specific clients (whose data often differ greatly from the training data). This is actually impractical to obtain due to the costs, intrusiveness, and time-consuming nature of data annotation. Moreover, even with the help of a significant amount of labeled data, model deployment on heterogeneous clients faces difficulties in generalizing well on unseen data. Other domains, like Computer Vision or Natural Language Processing, have proposed the notion of pre-trained models, leveraging large corpora, to reduce the need for annotated data and better manage heterogeneity. This promising approach has not been implemented in the HAR domain so far because of the lack of public datasets of sufficient size. In this paper, we pr
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13724</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#21450;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#19978;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13724
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32508;&#36848;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#22312;&#21387;&#32553;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22238;&#39038;&#20102;&#21487;&#35757;&#32451;&#30340;&#12289;&#21387;&#32553;&#30340;&#23884;&#20837;&#23618;&#30340;&#25991;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#21387;&#32553;&#24040;&#22411;&#31070;&#32463;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#21387;&#32553;&#23884;&#20837;&#23618;&#25152;&#27979;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#36341;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#23454;&#29616;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#31232;&#32570;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13700</link><description>&lt;p&gt;
&#25506;&#32034;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#65306;&#20197;ChatGPT&#20026;&#20363;&#30340;&#36828;&#31243;&#27979;&#25511;&#25968;&#25454;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of AI-Generated Synthetic Datasets: A Case Study on Telematics Data with ChatGPT. (arXiv:2306.13700v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;OpenAI&#30340;ChatGPT&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#36341;&#26696;&#20363;&#23637;&#31034;&#20102;&#20854;&#23454;&#29616;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#21644;&#31232;&#32570;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36965;&#24863;&#25216;&#26415;&#39046;&#22495;&#20869;&#26500;&#24314;&#21644;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;OpenAI&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;ChatGPT&#12290; &#21512;&#25104;&#25968;&#25454;&#38598;&#20026;&#35299;&#20915;&#26377;&#20851;&#25968;&#25454;&#38544;&#31169;&#12289;&#31232;&#32570;&#24615;&#21644;&#23545;&#21464;&#37327;&#30340;&#25511;&#21046;&#31561;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#29305;&#24615;&#20351;&#23427;&#20204;&#22312;&#30740;&#31350;&#20013;&#20855;&#26377;&#29305;&#21035;&#30340;&#20215;&#20540;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;&#22810;&#26679;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#36830;&#36143;&#24615;&#31561;&#26041;&#38754;&#26469;&#34913;&#37327;&#30340;&#12290; &#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#25968;&#25454;&#21019;&#24314;&#36807;&#31243;&#65292;&#23637;&#24320;&#20102;&#19968;&#39033;&#23454;&#36341;&#24615;&#26696;&#20363;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#29983;&#25104;&#21512;&#25104;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#12290; &#23454;&#39564;&#28041;&#21450;&#23545;ChatGPT&#36827;&#34892;&#36845;&#20195;&#25351;&#23548;&#65292;&#36880;&#27493;&#25552;&#28860;&#25552;&#31034;&#65292;&#24182;&#26368;&#32456;&#21019;&#24314;&#20102;&#21733;&#20262;&#24067;&#24066;&#21306;&#22495;&#35268;&#21010;&#26041;&#26696;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#21518;&#65292;&#35813;&#21512;&#25104;&#25968;&#25454;&#38598;&#25509;&#21463;&#20102;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#20808;&#21069;&#30830;&#23450;&#30340;&#36136;&#37327;&#21442;&#25968;&#12290;&#26368;&#32456;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#22312;&#21508;&#31181;&#30740;&#31350;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#25968;&#25454;&#26469;&#28304;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#23616;&#38480;&#21644;&#25361;&#25112;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research delves into the construction and utilization of synthetic datasets, specifically within the telematics sphere, leveraging OpenAI's powerful language model, ChatGPT. Synthetic datasets present an effective solution to challenges pertaining to data privacy, scarcity, and control over variables - characteristics that make them particularly valuable for research pursuits. The utility of these datasets, however, largely depends on their quality, measured through the lenses of diversity, relevance, and coherence. To illustrate this data creation process, a hands-on case study is conducted, focusing on the generation of a synthetic telematics dataset. The experiment involved an iterative guidance of ChatGPT, progressively refining prompts and culminating in the creation of a comprehensive dataset for a hypothetical urban planning scenario in Columbus, Ohio. Upon generation, the synthetic dataset was subjected to an evaluation, focusing on the previously identified quality parame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#22686;&#24378;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#29289;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;14&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#23545;&#23616;&#37096;&#32467;&#26500;&#24341;&#20837;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#20351;&#29992;Ollivier-Ricci&#26354;&#29575;&#65288;ORC&#65289;&#20316;&#20026;&#26435;&#37325;&#65292;&#35813;&#27169;&#22411;&#22312;13&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13699</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#22686;&#24378;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#29289;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Curvature-enhanced Graph Convolutional Network for Biomolecular Interaction Prediction. (arXiv:2306.13699v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#22686;&#24378;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#29983;&#29289;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#39044;&#27979;&#65292;&#24182;&#22312;14&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#12290;&#36890;&#36807;&#23545;&#23616;&#37096;&#32467;&#26500;&#24341;&#20837;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#20351;&#29992;Ollivier-Ricci&#26354;&#29575;&#65288;ORC&#65289;&#20316;&#20026;&#26435;&#37325;&#65292;&#35813;&#27169;&#22411;&#22312;13&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#22312;&#19981;&#35268;&#21017;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23558;&#20960;&#20309;&#20449;&#24687;&#32435;&#20837;&#23398;&#20064;&#26550;&#26500;&#23545;&#20854;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#22686;&#24378;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CGCN&#65289;&#29992;&#20110;&#29983;&#29289;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;CGCN&#37319;&#29992;&#20102;Ollivier-Ricci&#26354;&#29575;&#65288;ORC&#65289;&#26469;&#34920;&#24449;&#32593;&#32476;&#23616;&#37096;&#32467;&#26500;&#24182;&#22686;&#24378;GCN&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;ORCs&#26159;&#26681;&#25454;&#33410;&#28857;&#37051;&#22495;&#30340;&#23616;&#37096;&#25299;&#25169;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#24182;&#19988;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#29992;&#20316;&#29305;&#24449;&#32858;&#21512;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;CGCN&#27169;&#22411;&#22312;14&#20010;&#23454;&#38469;&#30340;&#29983;&#29289;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#21644;&#19968;&#31995;&#21015;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CGCN&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;14&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#65292;&#23427;&#27604;&#25152;&#26377;&#29616;&#26377;&#30340;&#27169;&#22411;&#34920;&#29616;&#37117;&#35201;&#22909;&#65292;&#22312;&#20854;&#20013;13&#20010;&#25968;&#25454;&#38598;&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#35813;&#35770;&#25991;&#26159;&#19968;&#31687;arXiv&#39044;&#21360;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning has demonstrated a great potential in non-Euclidean data analysis. The incorporation of geometric insights into learning architecture is vital to its success. Here we propose a curvature-enhanced graph convolutional network (CGCN) for biomolecular interaction prediction, for the first time. Our CGCN employs Ollivier-Ricci curvature (ORC) to characterize network local structures and to enhance the learning capability of GCNs. More specifically, ORCs are evaluated based on the local topology from node neighborhoods, and further used as weights for the feature aggregation in message-passing procedure. Our CGCN model is extensively validated on fourteen real-world bimolecular interaction networks and a series of simulated data. It has been found that our CGCN can achieve the state-of-the-art results. It outperforms all existing models, as far as we know, in thirteen out of the fourteen real-world datasets and ranks as the second in the rest one. The results from the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.13695</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#30456;&#20301;&#23637;&#24320;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24515;&#33039;&#34880;&#27969;&#30340;&#23454;&#26102;&#20449;&#24687;&#12290;&#22312;&#24038;&#24515;&#23460;&#38271;&#36724;&#35270;&#22270;&#20013;&#65292;&#24425;&#33394;&#22810;&#26222;&#21202;&#23481;&#26131;&#20986;&#29616;&#30456;&#20301;&#21253;&#35065;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#33039;&#25910;&#32553;&#21644;&#33298;&#24352;&#26399;&#12290;&#24403;&#22522;&#20110;&#24425;&#33394;&#22810;&#26222;&#21202;&#30340;&#23450;&#37327;&#26041;&#27861;&#26102;&#65292;&#24517;&#39035;&#32416;&#27491;&#36825;&#31181;&#21253;&#35065;&#20266;&#24433;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#21253;(&#21435;&#20266;&#24433;)&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65292;&#23558;&#20854;&#26377;&#25928;&#24615;&#19982;&#22522;&#20110;nnU-Net&#21644;Transformer&#27169;&#22411;&#30340;&#20004;&#31181;&#26368;&#26032;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#33258;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#27599;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;nnU-Net&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#21435;&#20266;&#24433;&#32467;&#26524;&#65292;&#20854;&#27425;&#26159;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21644;&#22522;&#20110;Transformer&#30340;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#25317;&#26377;&#26174;&#33879;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#24615;&#33021;&#20173;&#33021;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#28145;&#23618;&#20912;&#23618;&#21402;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#21152;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.13690</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#28145;&#23618;&#20912;&#23618;&#21402;&#24230;
&lt;/p&gt;
&lt;p&gt;
Prediction of Deep Ice Layer Thickness Using Adaptive Recurrent Graph Neural Networks. (arXiv:2306.13690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#24490;&#29615;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#28145;&#23618;&#20912;&#23618;&#21402;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#19988;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#22823;&#27668;&#28201;&#24230;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#36319;&#36394;&#21644;&#39044;&#27979;&#26497;&#22320;&#20912;&#23618;&#20013;&#30340;&#20912;&#23618;&#21402;&#24230;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#36825;&#20123;&#20912;&#23618;&#25581;&#31034;&#20102;&#27668;&#20505;&#36235;&#21183;&#65292;&#38477;&#38634;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#20197;&#21450;&#26410;&#26469;&#27668;&#20505;&#21644;&#38477;&#27700;&#30340;&#36712;&#36857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#12289;&#24490;&#29615;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#39134;&#34892;&#38647;&#36798;&#25968;&#25454;&#25910;&#38598;&#30340;&#36817;&#24180;&#31215;&#38634;&#37327;&#65292;&#39044;&#27979;&#21382;&#21490;&#31215;&#38634;&#37327;&#65292;&#36827;&#32780;&#39044;&#27979;&#28145;&#23618;&#20912;&#23618;&#30340;&#21402;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#21644;&#31561;&#25928;&#30340;&#38750;&#26102;&#38388;&#12289;&#38750;&#20960;&#20309;&#21644;&#38750;&#33258;&#36866;&#24212;&#27169;&#22411;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we deal with the effects of climate change and the increase of global atmospheric temperatures, the accurate tracking and prediction of ice layers within polar ice sheets grows in importance. Studying these ice layers reveals climate trends, how snowfall has changed over time, and the trajectory of future climate and precipitation. In this paper, we propose a machine learning model that uses adaptive, recurrent graph convolutional networks to, when given the amount of snow accumulation in recent years gathered through airborne radar data, predict historic snow accumulation by way of the thickness of deep ice layers. We found that our model performs better and with greater consistency than our previous model as well as equivalent non-temporal, non-geometric, and non-adaptive models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2306.13686</link><description>&lt;p&gt;
&#25299;&#23637;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35270;&#35282;&#65306; &#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#32508;&#21512;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Broadening the perspective for sustainable AI: Comprehensive sustainability criteria and indicators for AI systems. (arXiv:2306.13686v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65292;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#26088;&#22312;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#22810;&#26041;&#38754;&#30340;&#31038;&#20250;&#12289;&#29615;&#22659;&#21644;&#32463;&#27982;&#21518;&#26524;&#65292;&#21253;&#25324;&#38750;&#36879;&#26126;&#30340;&#20915;&#31574;&#36807;&#31243;&#12289;&#27495;&#35270;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#12289;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#20197;&#21450;&#32463;&#27982;&#23454;&#21147;&#30340;&#38598;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#20026;&#8220;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#8221;&#30340;&#29702;&#24565;&#25552;&#20379;&#20102;&#23454;&#36136;&#24615;&#30340;&#25903;&#25345;&#12290;&#25552;&#20986;&#20102;SCAIS&#26694;&#26550;&#65288;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;&#25351;&#26631;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;19&#20010;&#21487;&#25345;&#32493;&#24615;&#26631;&#20934;&#21644;67&#20010;&#25351;&#26631;&#65292;&#36825;&#20123;&#26631;&#20934;&#21644;&#25351;&#26631;&#22522;&#20110;&#25209;&#21028;&#24615;&#23457;&#26597;&#21644;&#19987;&#23478;&#30740;&#35752;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#36328;&#23398;&#31185;&#26041;&#27861;&#20026;&#20419;&#36827;&#21644;&#32467;&#26500;&#21270;&#20851;&#20110;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25972;&#20307;&#24615;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#26694;&#26550;&#65292;&#20026;AI&#31995;&#32479;&#30340;&#21518;&#32493;&#21457;&#23637;&#21644;&#35780;&#20272;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased use of AI systems is associated with multi-faceted societal, environmental, and economic consequences. These include non-transparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multi-dimensionality of sustainability, this paper takes steps towards substantiating the call for an overarching perspective on "sustainable AI". It presents the SCAIS Framework (Sustainability Criteria and Indicators for Artificial Intelligence Systems) which contains a set 19 sustainability criteria for sustainable AI and 67 indicators that is based on the results of a critical review and expert workshops. This interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete framework that lays the foundation for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#23545;&#8220;&#40657;&#21283;&#23376;&#8221;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#65292;&#24182;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13682</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the overall sensitivity of saliency-based explanation methods. (arXiv:2306.13682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#23545;&#8220;&#40657;&#21283;&#23376;&#8221;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#65292;&#24182;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26041;&#27861;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30528;&#30524;&#20110;&#29983;&#25104;&#23545;"&#40657;&#21283;&#23376;"&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24544;&#23454;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#27979;&#35797;&#26469;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#23454;&#31243;&#24230;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#36328;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#21644;&#20005;&#35880;&#30340;&#26041;&#27861;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#19981;&#20381;&#36182;&#29305;&#23450;&#27169;&#22411;&#65292;&#24182;&#19988;&#38750;&#24120;&#36866;&#21512;&#27604;&#36739;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#30340;&#24544;&#23454;&#31243;&#24230;&#30340;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#25351;&#23450;&#27491;&#24335;&#30340;&#38408;&#20540;&#21644;&#24314;&#31435;&#26631;&#20934;&#26469;&#25193;&#23637;&#23427;&#65292;&#20197;&#30830;&#23450;&#35299;&#37322;&#26041;&#27861;&#30340;&#24635;&#20307;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#25193;&#23637;&#26041;&#27861;&#26469;&#27604;&#36739;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25935;&#24863;&#24615;&#21644;&#24544;&#23454;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#32771;&#34385;&#22914;&#20309;&#35843;&#25972;&#27979;&#35797;&#26469;&#35780;&#20272;&#20854;&#20182;&#39046;&#22495;&#30340;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the need to generate faithful explanations of "black box" Deep Learning models. Several tests have been proposed to determine aspects of faithfulness of explanation methods, but they lack cross-domain applicability and a rigorous methodology. Hence, we select an existing test that is model agnostic and is well-suited for comparing one aspect of faithfulness (i.e., sensitivity) of multiple explanation methods, and extend it by specifying formal thresh-olds and building criteria to determine the over-all sensitivity of the explanation method. We present examples of how multiple explanation methods for Convolutional Neural Networks can be compared using this extended methodology. Finally, we discuss the relationship between sensitivity and faithfulness and consider how the test can be adapted to assess different explanation methods in other domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#31639;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2306.13681</link><description>&lt;p&gt;
&#20272;&#31639;&#22522;&#20110;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Estimating the Value of Evidence-Based Decision Making. (arXiv:2306.13681v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#31639;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;/&#25919;&#31574;&#20915;&#31574;&#36890;&#24120;&#22522;&#20110;&#38543;&#26426;&#23454;&#39564;&#21644;&#35266;&#23519;&#24615;&#30740;&#31350;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#26469;&#20272;&#31639;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#65288;EBDM&#65289;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business/policy decisions are often based on evidence from randomized experiments and observational studies. In this article we propose an empirical framework to estimate the value of evidence-based decision making (EBDM) and the return on the investment in statistical precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#20851;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#30340;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.13679</link><description>&lt;p&gt;
&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#36935;&#35265;&#20223;&#30495;&#65306;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20110;&#20223;&#30495;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks. (arXiv:2306.13679v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#30340;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#25110;GPT-4&#25152;&#25552;&#20379;&#30340;&#39072;&#35206;&#24615;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36890;&#24120;&#24378;&#35843;&#39640;&#27700;&#24179;&#30340;&#26426;&#20250;&#21644;&#25285;&#24551;&#12290;&#26412;&#25991;&#26159;&#20851;&#20110;LLMs&#22312;&#31185;&#23398;&#20223;&#30495;&#20013;&#24212;&#29992;&#30340;&#31532;&#19968;&#31687;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#22235;&#20010;&#24314;&#27169;&#21644;&#20223;&#30495;&#20219;&#21153;&#65292;&#27599;&#27425;&#35780;&#20272;LLMs&#30340;&#39044;&#26399;&#30410;&#22788;&#21644;&#38480;&#21046;&#65292;&#21516;&#26102;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#30340;&#32467;&#26500;&#65292;&#20197;&#20419;&#36827;&#21442;&#19982;&#32773;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#21442;&#19982;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#19987;&#27880;&#20110;&#27719;&#24635;&#20223;&#30495;&#36755;&#20986;&#65292;&#20197;&#20415;&#27169;&#22411;&#29992;&#25143;&#33021;&#22815;&#35782;&#21035;&#20986;&#20248;&#36873;&#22330;&#26223;&#12290;&#31532;&#19977;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#25991;&#26412;&#20256;&#36798;&#23545;&#20223;&#30495;&#21487;&#35270;&#21270;&#30340;&#35265;&#35299;&#65292;&#20197;&#25193;&#22823;&#20223;&#30495;&#24179;&#21488;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#21518;&#65292;&#26368;&#21518;&#19968;&#20010;&#20219;&#21153;&#24341;&#20986;&#20102;&#20351;&#29992;LLMs&#35299;&#37322;&#20223;&#30495;&#38169;&#35823;&#21644;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#20415;&#27169;&#22411;&#24320;&#21457;&#32773;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The disruptive technology provided by large-scale pre-trained language models (LLMs) such as ChatGPT or GPT-4 has received significant attention in several application domains, often with an emphasis on high-level opportunities and concerns. This paper is the first examination regarding the use of LLMs for scientific simulations. We focus on four modeling and simulation tasks, each time assessing the expected benefits and limitations of LLMs while providing practical guidance for modelers regarding the steps involved. The first task is devoted to explaining the structure of a conceptual model to promote the engagement of participants in the modeling process. The second task focuses on summarizing simulation outputs, so that model users can identify a preferred scenario. The third task seeks to broaden accessibility to simulation platforms by conveying the insights of simulation visualizations via text. Finally, the last task evokes the possibility of explaining simulation errors and pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20132;&#21449;&#24615;&#26469;&#26816;&#27979;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#35777;&#35328;&#19981;&#20844;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#37266;&#20154;&#20204;&#21333;&#19968;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26080;&#27861;&#20805;&#20998;&#28085;&#30422;&#30149;&#20154;&#32463;&#21382;&#30340;&#24494;&#22937;&#36523;&#20221;&#65292;&#32780;&#24573;&#30053;&#36825;&#20123;&#19981;&#20844;&#27491;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#36136;&#37327;&#24046;&#25110;&#21361;&#21450;&#29983;&#21629;&#12290;</title><link>http://arxiv.org/abs/2306.13675</link><description>&lt;p&gt;
&#20132;&#21449;&#24615;&#21644;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#35777;&#35328;&#19981;&#20844;&#27491;
&lt;/p&gt;
&lt;p&gt;
Intersectionality and Testimonial Injustice in Medical Records. (arXiv:2306.13675v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20132;&#21449;&#24615;&#26469;&#26816;&#27979;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#35777;&#35328;&#19981;&#20844;&#27491;&#65292;&#36827;&#19968;&#27493;&#25552;&#37266;&#20154;&#20204;&#21333;&#19968;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26080;&#27861;&#20805;&#20998;&#28085;&#30422;&#30149;&#20154;&#32463;&#21382;&#30340;&#24494;&#22937;&#36523;&#20221;&#65292;&#32780;&#24573;&#30053;&#36825;&#20123;&#19981;&#20844;&#27491;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#36136;&#37327;&#24046;&#25110;&#21361;&#21450;&#29983;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#35777;&#35328;&#19981;&#20844;&#27491;&#26159;&#35299;&#20915;&#19981;&#20844;&#24179;&#21644;&#20419;&#36827;&#21253;&#23481;&#24615;&#21307;&#30103;&#23454;&#36341;&#30340;&#22522;&#26412;&#20803;&#32032;&#20043;&#19968;&#65292;&#20854;&#20013;&#35768;&#22810;&#26159;&#19982;&#29983;&#21629;&#23494;&#20999;&#30456;&#20851;&#30340;&#12290;&#28982;&#32780;&#65292;&#21482;&#20351;&#29992;&#21333;&#20010;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26469;&#26816;&#27979;&#35777;&#35328;&#19981;&#20844;&#27491;&#24182;&#19981;&#33021;&#20805;&#20998;&#28085;&#30422;&#20026;&#24739;&#32773;&#20307;&#39564;&#20570;&#20986;&#36129;&#29486;&#30340;&#24494;&#22937;&#36523;&#20221;&#12290;&#27492;&#22806;&#65292;&#26377;&#20123;&#19981;&#20844;&#27491;&#21482;&#26377;&#22312;&#36890;&#36807;&#20132;&#21449;&#24615;&#38236;&#22836;&#26816;&#26597;&#20135;&#29983;&#30340;&#32454;&#24494;&#24046;&#21035;&#26102;&#25165;&#33021;&#26174;&#29616;&#12290;&#24573;&#30053;&#36825;&#20123;&#19981;&#20844;&#27491;&#21487;&#33021;&#23548;&#33268;&#21307;&#30103;&#36136;&#37327;&#24046;&#25110;&#21361;&#21450;&#29983;&#21629;&#12290;&#22240;&#27492;&#65292;&#22312;&#32771;&#34385;&#20132;&#21449;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#21644;&#20844;&#27491;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#30340;&#21307;&#30103;&#25968;&#25454;&#26469;&#30830;&#23450;&#21307;&#30103;&#35760;&#24405;&#20013;&#26159;&#21542;&#23384;&#22312;&#21487;&#33021;&#23548;&#33268;&#35777;&#35328;&#19981;&#20844;&#27491;&#30340;&#21333;&#35789;&#65292;&#20351;&#29992;&#20844;&#24179;&#24230;&#37327;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#12289;&#24046;&#24322;&#20132;&#21449;&#20844;&#24179;&#24615;&#21644;&#23376;&#32452;&#20844;&#24179;&#24615;&#65289;&#26469;&#35780;&#20272;&#19981;&#21516;&#23376;&#32452;&#20307;&#39564;&#35777;&#35328;&#19981;&#20844;&#27491;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#24182;&#20998;&#26512;&#24739;&#32773;&#30340;&#20132;&#21449;&#36523;&#20221;&#22914;&#20309;&#23548;&#33268;&#36825;&#20123;&#19981;&#20844;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient's experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the inter
&lt;/p&gt;</description></item><item><title>MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.13674</link><description>&lt;p&gt;
MeciFace&#65306;&#22522;&#20110;&#32908;&#32905;&#30005;&#21644;&#24815;&#24615;&#34701;&#21512;&#30340;&#36793;&#32536;&#23454;&#26102;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#30524;&#38236;
&lt;/p&gt;
&lt;p&gt;
MeciFace: Mechanomyography and Inertial Fusion based Glasses for Edge Real-Time Recognition of Facial and Eating Activities. (arXiv:2306.13674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13674
&lt;/p&gt;
&lt;p&gt;
MeciFace&#26159;&#19968;&#27454;&#27880;&#37325;&#38544;&#31169;&#19988;&#20302;&#21151;&#32791;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#23427;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#65292;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;F1&#20998;&#25968;&#36798;&#21040;&#20102;86&#65285;&#65292;&#39278;&#39135;&#30417;&#27979;&#21017;&#36798;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MeciFace&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#21151;&#32791;&#65288;0.55&#29926;&#65289;&#65292;&#27880;&#37325;&#38544;&#31169;&#65292;&#23454;&#26102;&#36793;&#32536;&#30417;&#27979;&#65288;RTE&#65289;&#30340;&#21487;&#31359;&#25140;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#24494;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65288;11-19 KB&#65289;&#65292;&#26088;&#22312;&#30417;&#27979;&#38754;&#37096;&#34920;&#24773;&#21644;&#36827;&#39135;&#27963;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38754;&#37096;&#21644;&#36827;&#39135;&#22330;&#26223;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;&#35813;&#31995;&#32479;&#22312;&#38754;&#37096;&#34920;&#24773;&#26696;&#20363;&#30340;RTE&#35780;&#20272;&#20013;&#20135;&#29983;&#20102;86&#65285;&#30340;F1&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26410;&#30693;&#29992;&#25143;&#30340;RTE&#36827;&#34892;&#39278;&#39135;&#30417;&#27979;&#65292;&#24471;&#21040;&#20102;90&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MeciFace, a low-power (0.55 Watts), privacy-conscious, real-time on-the-edge (RTE) wearable solution with a tiny memory footprint (11-19 KB), designed to monitor facial expressions and eating activities. We employ lightweight convolutional neural networks as the backbone models for both facial and eating scenarios. The system yielded an F1-score of 86% for the RTE evaluation in the facial expression case. In addition, we obtained an F1-score of 90% for eating/drinking monitoring for the RTE of an unseen user.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CongestEXP&#30340;&#20998;&#25955;&#31639;&#27861;&#65292;&#21487;&#20197;&#32447;&#24615;&#32553;&#25918;&#35774;&#26045;&#25968;&#37327;&#65292;&#23454;&#29616;&#22312;&#32447;&#25317;&#22622;&#21338;&#24328;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#20197;&#19982;&#24050;&#30693;&#26368;&#20339;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#24555;&#36895;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.13673</link><description>&lt;p&gt;
&#39550;&#39533;&#25351;&#25968;&#32423;&#30340;&#21160;&#20316;&#38598;&#65306;&#22312;&#32447;&#25317;&#22622;&#21338;&#24328;&#20013;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#24555;&#36895;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Taming the Exponential Action Set: Sublinear Regret and Fast Convergence to Nash Equilibrium in Online Congestion Games. (arXiv:2306.13673v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CongestEXP&#30340;&#20998;&#25955;&#31639;&#27861;&#65292;&#21487;&#20197;&#32447;&#24615;&#32553;&#25918;&#35774;&#26045;&#25968;&#37327;&#65292;&#23454;&#29616;&#22312;&#32447;&#25317;&#22622;&#21338;&#24328;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#20197;&#19982;&#24050;&#30693;&#26368;&#20339;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#36895;&#24230;&#24555;&#36895;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#22622;&#21338;&#24328;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#20132;&#36890;&#32593;&#32476;&#21644;&#36164;&#28304;&#20998;&#37197;&#31561;&#19968;&#31995;&#21015;&#24037;&#31243;&#31995;&#32479;&#12290;&#26412;&#25991;&#30740;&#31350;&#25317;&#22622;&#21338;&#24328;&#30340;&#22312;&#32447;&#24418;&#24335;&#65292;&#20854;&#20013;&#20195;&#29702;&#37325;&#22797;&#21442;&#19982;&#21338;&#24328;&#65292;&#24182;&#35266;&#23519;&#21040;&#24102;&#26377;&#38543;&#26426;&#24615;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CongestEXP&#30340;&#20998;&#25955;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#32463;&#20856;&#30340;&#25351;&#25968;&#26435;&#37325;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35774;&#26045;&#32423;&#21035;&#19978;&#20445;&#25345;&#26435;&#37325;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#31639;&#27861;&#23545;&#21487;&#33021;&#35774;&#26045;&#38598;&#22823;&#23567;&#30340;&#25351;&#25968;&#20381;&#36182;&#65292;&#21363;$\binom{F}{k}\approx F^k$&#65292;&#24182;&#19988;&#20165;&#19982;$F$&#32447;&#24615;&#32553;&#25918;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#21333;&#29420;&#30340;&#29609;&#23478;&#65292;CongestEXP&#21487;&#20197;&#23454;&#29616;$O(kF\sqrt{T})$&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#20013;$T$&#26159;&#26102;&#38388;&#21608;&#26399;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21033;&#29992;&#26435;&#37325;&#30340;&#25351;&#25968;&#22686;&#38271;&#20351;CongestEXP&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21040;&#32435;&#20160;&#22343;&#34913;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\ln F/\sqrt{T})$&#65292;&#36825;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#30456;&#31526;&#65292;&#20165;&#30456;&#24046;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20063;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#28216;&#25103;&#31867;&#21035;&#65292;&#21253;&#25324;&#37027;&#20123;&#20855;&#26377;&#36830;&#32493;&#20998;&#24067;&#26435;&#37325;&#21644;&#37027;&#20123;&#20855;&#26377;&#20219;&#24847;&#29609;&#23478;&#34892;&#21160;&#30340;&#28216;&#25103;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;CongestEXP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The congestion game is a powerful model that encompasses a range of engineering systems such as traffic networks and resource allocation. It describes the behavior of a group of agents who share a common set of $F$ facilities and take actions as subsets with $k$ facilities. In this work, we study the online formulation of congestion games, where agents participate in the game repeatedly and observe feedback with randomness. We propose CongestEXP, a decentralized algorithm that applies the classic exponential weights method. By maintaining weights on the facility level, the regret bound of CongestEXP avoids the exponential dependence on the size of possible facility sets, i.e., $\binom{F}{k} \approx F^k$, and scales only linearly with $F$. Specifically, we show that CongestEXP attains a regret upper bound of $O(kF\sqrt{T})$ for every individual player, where $T$ is the time horizon. On the other hand, exploiting the exponential growth of weights enables CongestEXP to achieve a fast conv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26368;&#20339;&#23454;&#36341;&#30340;&#24037;&#19994;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20116;&#20010;&#27493;&#39588;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.13662</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26368;&#20339;&#23454;&#36341;&#65306;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#30340;&#24037;&#19994;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Best Practices for Machine Learning Systems: An Industrial Framework for Analysis and Optimization. (arXiv:2306.13662v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26368;&#20339;&#23454;&#36341;&#30340;&#24037;&#19994;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20116;&#20010;&#27493;&#39588;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#24320;&#22987;&#23545;ML&#31995;&#32479;&#30340;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20135;&#29983;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#65292;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#26088;&#22312;&#25913;&#21892;ML&#31995;&#32479;&#36719;&#20214;&#36136;&#37327;&#30340;&#26368;&#20339;&#23454;&#36341;&#12289;&#35268;&#21017;&#21644;&#25351;&#21335;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#23545;&#25972;&#20307;&#36136;&#37327;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#23454;&#36341;&#36890;&#24120;&#20197;&#25351;&#23548;&#24615;&#26041;&#24335;&#21576;&#29616;&#65292;&#27809;&#26377;&#26126;&#30830;&#22320;&#19982;&#23427;&#20204;&#23545;&#36719;&#20214;&#36136;&#37327;&#30340;&#25972;&#20307;&#36129;&#29486;&#24314;&#31435;&#32852;&#31995;&#12290;&#22522;&#20110;&#23545;&#19981;&#21516;&#23454;&#36341;&#24433;&#21709;&#36719;&#20214;&#36136;&#37327;&#19981;&#21516;&#30340;&#35266;&#23519;&#20197;&#21450;&#21333;&#19968;&#36136;&#37327;&#26041;&#38754;&#21487;&#33021;&#30001;&#22810;&#20010;&#23454;&#36341;&#26469;&#35299;&#20915;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#23454;&#29616;&#30340;&#36136;&#37327;&#24433;&#21709;&#21644;&#23454;&#29616;&#20248;&#20808;&#39034;&#24207;&#26041;&#38754;&#30340;&#26368;&#20339;&#23454;&#36341;&#38598;&#21512;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;ML&#31995;&#32479;&#37327;&#36523;&#23450;&#21046;&#30340;&#20998;&#23618;&#36719;&#20214;&#36136;&#37327;&#27169;&#22411;&#65288;SQM&#65289;&#12290;&#20511;&#21161;&#19987;&#23478;&#30693;&#35782;&#65292;&#24314;&#31435;&#20102;&#20010;&#20307;&#23454;&#36341;&#19982;SQM&#31867;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;&#19968;&#32452;&#26368;&#20339;&#23454;&#36341;&#30340;ML&#31995;&#32479;&#26368;&#20339;&#23454;&#36341;&#65288;MSBP&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30001;&#20116;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26631;&#35782;&#26368;&#20339;&#23454;&#36341;&#65292;&#65288;ii&#65289;&#26368;&#20339;&#23454;&#36341;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#65288;iii&#65289;&#35780;&#20272;&#26368;&#20339;&#23454;&#36341;&#30340;&#36136;&#37327;&#24433;&#21709;&#65292;&#65288;iv&#65289;&#26368;&#20339;&#23454;&#36341;&#30340;&#25805;&#20316;&#21270;&#21644;&#23454;&#26045;&#65292;&#20197;&#21450;&#65288;v&#65289;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#24037;&#19994;ML&#31995;&#32479;&#30340;&#30495;&#23454;&#26696;&#20363;&#26469;&#35828;&#26126;&#35813;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;MSBP&#26694;&#26550;&#21487;&#29992;&#20110;&#25913;&#21892;ML&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the Machine Learning (ML) and Artificial Intelligence community has developed an increasing interest in Software Engineering (SE) for ML Systems leading to a proliferation of best practices, rules, and guidelines aiming at improving the quality of the software of ML Systems. However, understanding their impact on the overall quality has received less attention. Practices are usually presented in a prescriptive manner, without an explicit connection to their overall contribution to software quality. Based on the observation that different practices influence different aspects of software-quality and that one single quality aspect might be addressed by several practices we propose a framework to analyse sets of best practices with focus on quality impact and prioritization of their implementation. We first introduce a hierarchical Software Quality Model (SQM) specifically tailored for ML Systems. Relying on expert knowledge, the connection between individual practi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;FPGA&#22120;&#20214;&#19978;&#23454;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#25163;&#20889;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#25552;&#39640;&#23545;&#25163;&#20889;&#23383;&#27597;&#21644;&#25968;&#23383;&#30340;&#35782;&#21035;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.13557</link><description>&lt;p&gt;
FPGA&#23454;&#29616;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#25163;&#20889;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FPGA Implementation of Convolutional Neural Network for Real-Time Handwriting Recognition. (arXiv:2306.13557v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;FPGA&#22120;&#20214;&#19978;&#23454;&#29616;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#25163;&#20889;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#25552;&#39640;&#23545;&#25163;&#20889;&#23383;&#27597;&#21644;&#25968;&#23383;&#30340;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#26368;&#36817;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#20316;&#20026;&#35745;&#31639;&#26426;&#30828;&#20214;&#24037;&#31243;&#24072;&#65292;&#25105;&#20204;&#23545;&#23558;&#27969;&#34892;&#30340;&#36719;&#20214;&#26426;&#22120;&#23398;&#20064;&#32467;&#26500;&#36827;&#34892;&#30828;&#20214;&#23454;&#29616;&#20197;&#20248;&#21270;&#20854;&#24615;&#33021;&#12289;&#21487;&#38752;&#24615;&#21644;&#36164;&#28304;&#20351;&#29992;&#29575;&#20805;&#28385;&#28909;&#24773;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Altera DE1 FPGA&#22871;&#20214;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#23454;&#26102;&#35774;&#22791;&#65292;&#29992;&#20110;&#35782;&#21035;&#25163;&#20889;&#23383;&#27597;&#21644;&#25968;&#23383;&#12290;&#25105;&#20204;&#36981;&#24490;&#20102;IEEE-754 32&#20301;&#28014;&#28857;&#26631;&#20934;&#12289;&#35270;&#39057;&#22270;&#24418;&#38453;&#21015;&#65288;VGA&#65289;&#26174;&#31034;&#21327;&#35758;&#12289;&#36890;&#29992;&#24322;&#27493;&#25910;&#21457;&#22120;&#65288;UART&#65289;&#21327;&#35758;&#21644;Inter-Integrated Circuit&#65288;I2C&#65289;&#21327;&#35758;&#31561;&#21508;&#31181;&#24037;&#31243;&#26631;&#20934;&#65292;&#20197;&#23454;&#29616;&#39033;&#30446;&#30446;&#26631;&#12290;&#36981;&#24490;&#36825;&#20123;&#26631;&#20934;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;32&#20301;&#28014;&#28857;&#65288;FP&#65289;&#25351;&#20196;&#38598;&#26550;&#26500;&#65288;ISA&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;System Verilog&#24320;&#21457;&#20102;&#19968;&#20010;5&#32423;RISC&#22788;&#29702;&#22120;&#65292;&#20197;&#31649;&#29702;&#22270;&#20687;&#22788;&#29702;&#12289;&#30697;&#38453;&#20056;&#27861;&#21644;&#21367;&#31215;&#36816;&#31639;&#31561;&#25805;&#20316;&#12290;&#35813;&#23454;&#29616;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#20197;&#25552;&#39640;&#23545;&#25163;&#20889;&#23383;&#27597;&#21644;&#25968;&#23383;&#30340;&#35782;&#21035;&#31934;&#24230;&#65292;&#24182;&#22312;FPGA&#22120;&#20214;&#19978;&#23454;&#29616;&#20102;&#23454;&#26102;&#25163;&#20889;&#20307;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has recently been a skyrocketing field in Computer Science. As computer hardware engineers, we are enthusiastic about hardware implementations of popular software ML architectures to optimize their performance, reliability, and resource usage. In this project, we designed a highly-configurable, real-time device for recognizing handwritten letters and digits using an Altera DE1 FPGA Kit. We followed various engineering standards, including IEEE-754 32-bit Floating-Point Standard, Video Graphics Array (VGA) display protocol, Universal Asynchronous Receiver-Transmitter (UART) protocol, and Inter-Integrated Circuit (I2C) protocols to achieve the project goals. These significantly improved our design in compatibility, reusability, and simplicity in verifications. Following these standards, we designed a 32-bit floating-point (FP) instruction set architecture (ISA). We developed a 5-stage RISC processor in System Verilog to manage image processing, matrix multiplication
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13237</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pruning for Better Domain Generalizability. (arXiv:2306.13237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21098;&#26525;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;DSS&#65292;&#35813;&#26041;&#27861;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#65292;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#21363;&#20415;&#21482;&#24341;&#20837;&#23569;&#37327;&#31232;&#30095;&#20063;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#22914;L2&#24050;&#32463;&#21487;&#20197;&#22312;&#30446;&#26631;&#22495;&#24615;&#33021;&#19978;&#25552;&#20379;&#23567;&#24133;&#24230;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#26525;&#35780;&#20998;&#26041;&#27861;&#65292;&#31216;&#20026;DSS&#65292;&#35774;&#35745;&#19981;&#26159;&#20026;&#20102;&#20445;&#25345;&#28304;&#20934;&#30830;&#24615;&#32780;&#26159;&#30452;&#25509;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#29978;&#33267;&#21487;&#20197;&#19982;MIRO(Cha&#31561;&#20154;&#65292;2022&#24180;)&#31561;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;MNIST&#21040;MNIST-M&#19978;&#65292;&#36890;&#36807;&#23558;60%&#36890;&#36947;&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;5&#20010;&#30334;&#20998;&#28857;&#20197;&#19978;&#12290;&#22312;DomainBed&#22522;&#20934;&#21644;&#26368;&#20808;&#36827;&#30340;MIRO&#19978;&#65292;&#20165;&#36890;&#36807;&#23558;10%&#31232;&#30095;&#24341;&#20837;&#27169;&#22411;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate whether we could use pruning as a reliable method to boost the generalization ability of the model. We found that existing pruning method like L2 can already offer small improvement on the target domain performance. We further propose a novel pruning scoring method, called DSS, designed not to maintain source accuracy as typical pruning work, but to directly enhance the robustness of the model. We conduct empirical experiments to validate our method and demonstrate that it can be even combined with state-of-the-art generalization work like MIRO(Cha et al., 2022) to further boost the performance. On MNIST to MNIST-M, we could improve the baseline performance by over 5 points by introducing 60% channel sparsity into the model. On DomainBed benchmark and state-of-the-art MIRO, we can further boost its performance by 1 point only by introducing 10% sparsity into the model. Code can be found at: https://github.com/AlexSunNik/Pruning-for-Better-Domain-Generaliza
&lt;/p&gt;</description></item><item><title>ovla&#26159;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#20351;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#34987;&#24212;&#29992;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#65292;&#35813;&#26041;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13215</link><description>&lt;p&gt;
ovla&#65306;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ovla: Neural Network Ownership Verification using Latent Watermarks. (arXiv:2306.13215v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13215
&lt;/p&gt;
&lt;p&gt;
ovla&#26159;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#27700;&#21360;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#20351;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#21482;&#26377;&#22312;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#34987;&#24212;&#29992;&#26102;&#25165;&#20250;&#34987;&#28608;&#27963;&#65292;&#35813;&#26041;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#23545;&#20110;&#20445;&#25252;&#36825;&#20123;&#27169;&#22411;&#20813;&#21463;&#38750;&#27861;&#22797;&#21046;&#12289;&#20813;&#36153;&#39569;&#36710;&#12289;&#37325;&#26032;&#20998;&#37197;&#21644;&#20854;&#20182;&#30693;&#35782;&#20135;&#26435;&#30340;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#27700;&#21360;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#26435;&#39564;&#35777;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#32593;&#32476;&#30340;&#27491;&#24120;&#25805;&#20316;&#21644;&#23545;&#24102;&#27700;&#21360;&#36755;&#20837;&#30340;&#21709;&#24212;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#35757;&#32451;&#32593;&#32476;&#65292;&#20351;&#24471;&#27700;&#21360;&#20445;&#25345;&#20241;&#30496;&#29366;&#24577;&#65292;&#38500;&#38750;&#24212;&#29992;&#25152;&#26377;&#32773;&#30340;&#31192;&#23494;&#23494;&#38053;&#26469;&#28608;&#27963;&#23427;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#29575;&#12289;&#35823;&#25253;&#29575;&#21644;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ownership verification for neural networks is important for protecting these models from illegal copying, free-riding, re-distribution and other intellectual property misuse. We present a novel methodology for neural network ownership verification based on the notion of latent watermarks. Existing ownership verification methods either modify or introduce constraints to the neural network parameters, which are accessible to an attacker in a white-box attack and can be harmful to the network's normal operation, or train the network to respond to specific watermarks in the inputs similar to data poisoning-based backdoor attacks, which are susceptible to backdoor removal techniques. In this paper, we address these problems by decoupling a network's normal operation from its responses to watermarked inputs during ownership verification. The key idea is to train the network such that the watermarks remain dormant unless the owner's secret key is applied to activate it. The secret key is real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12625</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#26159;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27604;&#29305;&#29575;-&#20934;&#30830;&#24615;&#25240;&#34935;&#8212;&#8212;&#20854;&#20013;&#23458;&#25143;&#31471;n&#21457;&#36865;&#26469;&#33258;&#20165;&#20026;&#35813;&#23458;&#25143;&#31471;&#30340;&#27010;&#29575;&#20998;&#24067;q&#966;&#65288;n&#65289;&#30340;&#26679;&#26412;&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#20272;&#35745;&#23458;&#25143;&#31471;&#20998;&#24067;&#30340;&#24179;&#22343;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;FL&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20855;&#26377;&#39044;&#25968;&#25454;&#20998;&#24067;p&#952;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#35813;&#20998;&#24067;&#19982;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n&#65289;&#22312;Kullback-Leibler&#65288;KL&#65289;&#21457;&#25955;&#26041;&#38754;&#25509;&#36817;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n)&#19982;&#38468;&#21152;&#20449;&#24687;p&#952;&#20043;&#38388;&#30340;&#36825;&#31181;&#25509;&#36817;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#22823;&#32422;Dkl&#65288;q&#966;&#65288;n&#65289;|| p&#952;&#65289;&#20301;&#30340;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;MGxTransformer&#65292;&#32467;&#21512;&#20102;VPTR&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.12545</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#37325;&#32593;&#26684;&#20869;&#23384;&#29992;&#20110;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Multigrid Memory For Computational Fluid Dynamics. (arXiv:2306.12545v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;MGxTransformer&#65292;&#32467;&#21512;&#20102;VPTR&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#30340;&#20248;&#28857;&#65292;&#20351;&#24471;&#27169;&#25311;&#32467;&#26524;&#26356;&#20026;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#21253;&#25324;&#39134;&#26426;&#21644;&#33337;&#33334;&#35774;&#35745;&#12289;&#24037;&#19994;&#27969;&#31243;&#20248;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#28237;&#27969;&#27969;&#21160;&#27169;&#25311;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#35270;&#39057;&#39044;&#27979;&#21464;&#25442;&#22120;&#65288;VPTR&#65289;&#65288;Ye &amp; Bilodeau, 2022&#65289;&#21644;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#65288;MgConv&#65292;MgResnet&#65289;&#65288;Ke&#31561;&#65292;2017&#65289;&#30340;&#20248;&#28857;&#12290;VPTR&#25797;&#38271;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22788;&#29702;&#22823;&#22411;&#36755;&#20837;&#25968;&#25454;&#65292;&#26159;&#28237;&#27969;&#27969;&#21160;&#39044;&#27979;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22810;&#37325;&#32593;&#26684;&#26550;&#26500;&#21033;&#29992;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#32593;&#26684;&#26469;&#25429;&#25417;&#28237;&#27969;&#27969;&#21160;&#30340;&#22810;&#23610;&#24230;&#29305;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27169;&#25311;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;MGxTransformer&#22312;&#20934;&#30830;&#39044;&#27979;&#36895;&#24230;&#12289;&#28201;&#24230;&#21644;&#28237;&#27969;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Turbulent flow simulation plays a crucial role in various applications, including aircraft and ship design, industrial process optimization, and weather prediction. In this paper, we propose an advanced data-driven method for simulating turbulent flow, representing a significant improvement over existing approaches.  Our methodology combines the strengths of Video Prediction Transformer (VPTR) (Ye &amp; Bilodeau, 2022) and Multigrid Architecture (MgConv, MgResnet) (Ke et al., 2017). VPTR excels in capturing complex spatiotemporal dependencies and handling large input data, making it a promising choice for turbulent flow prediction. Meanwhile, Multigrid Architecture utilizes multiple grids with different resolutions to capture the multiscale nature of turbulent flows, resulting in more accurate and efficient simulations.  Through our experiments, we demonstrate the effectiveness of our proposed approach, named MGxTransformer, in accurately predicting velocity, temperature, and turbulence in
&lt;/p&gt;</description></item><item><title>Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.</title><link>http://arxiv.org/abs/2306.12194</link><description>&lt;p&gt;
6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;Split Learning
&lt;/p&gt;
&lt;p&gt;
Split Learning in 6G Edge Networks. (arXiv:2306.12194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12194
&lt;/p&gt;
&lt;p&gt;
Split learning (SL) enables servers to handle the major training workload while still enhancing data privacy, which is an important approach in 6G edge networks. This article provides an overview of the tailored 6G architecture to support edge SL, the critical design issues for edge SL, and presents future research direction and exciting applications of SL in 6G edge networks.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20998;&#24067;&#24335;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#30340;&#26222;&#21450;&#65292;6G&#31227;&#21160;&#32593;&#32476;&#23558;&#21457;&#23637;&#25104;&#20026;&#19968;&#20010;&#36830;&#25509;&#26234;&#33021;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#26465;&#32447;&#36335;&#19978;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#32435;&#20837;&#31227;&#21160;&#36793;&#32536;&#30340;&#25552;&#35758;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24222;&#22823;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20960;&#20046;&#26080;&#27861;&#25903;&#25345;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#20102;Split Learning (SL)&#30340;&#20986;&#29616;&#65292;&#23427;&#20351;&#26381;&#21153;&#22120;&#22788;&#29702;&#20027;&#35201;&#30340;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;SL&#30340;&#20851;&#38190;&#21457;&#23637;&#65292;&#24182;&#38416;&#36848;&#20102;&#20854;&#19982;&#26080;&#32447;&#36793;&#32536;&#32593;&#32476;&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#39318;&#20808;&#35828;&#26126;&#20102;&#23450;&#21046;&#30340;6G&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#25903;&#25345;&#36793;&#32536;SL&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36793;&#32536;SL&#30340;&#20851;&#38190;&#35774;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#21019;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#21644;&#22312;&#21333;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#19979;&#30340;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Split Learning&#27169;&#22411;&#30340;&#22810;&#36793;&#32536;&#26381;&#21153;&#22120;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#19968;&#20123;&#21050;&#28608;&#20154;&#24515;&#30340;SL&#22312;6G&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of distributed edge computing resources, the 6G mobile network will evolve into a network for connected intelligence. Along this line, the proposal to incorporate federated learning into the mobile edge has gained considerable interest in recent years. However, the deployment of federated learning faces substantial challenges as massive resource-limited IoT devices can hardly support on-device model training. This leads to the emergence of split learning (SL) which enables servers to handle the major training workload while still enhancing data privacy. In this article, we offer a brief overview of key advancements in SL and articulate its seamless integration with wireless edge networks. We begin by illustrating the tailored 6G architecture to support edge SL. Then, we examine the critical design issues for edge SL, including innovative resource-efficient learning frameworks and resource management strategies under a single edge server. Additionally, we expand t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11915</link><description>&lt;p&gt;
&#22270;&#20998;&#31867;&#38382;&#39064;&#20013;&#32467;&#26500;&#24863;&#30693;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware Robustness Certificates for Graph Classification. (arXiv:2306.11915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11915
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#29983;&#25104;&#23545;&#24212;&#30340;&#40065;&#26834;&#24615;&#35777;&#20070;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#39046;&#20808;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#40065;&#26834;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20445;&#35777;&#23433;&#20840;&#24615;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#29992;&#20110;&#22270;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35777;&#26126;&#20445;&#35777;&#19982;&#33410;&#28857;&#23545;&#32763;&#36716;&#65288;&#28155;&#21152;&#25110;&#21024;&#38500;&#36793;&#32536;&#65289;&#30340;&#24635;&#25968;&#26377;&#20851;&#65292;&#36825;&#30456;&#24403;&#20110;&#20197;&#37051;&#25509;&#30697;&#38453;&#20026;&#20013;&#24515;&#30340;l0&#29699;&#12290;&#23613;&#31649;&#20174;&#29702;&#35770;&#19978;&#30475;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#36825;&#31181;&#21508;&#21521;&#21516;&#24615;&#30340;&#32467;&#26500;&#22122;&#22768;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#22240;&#20026;&#26377;&#20123;&#33410;&#28857;&#23545;&#20110;&#30830;&#23450;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#26356;&#20026;&#20851;&#38190;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35777;&#20070;&#32473;&#20986;&#20102;&#23545;&#22270;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24754;&#35266;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#21508;&#21521;&#21516;&#24615;&#30340;&#22122;&#22768;&#20998;&#24067;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36807;&#31243;&#20026;&#20998;&#31867;&#22120;&#29983;&#25104;&#20102;&#32467;&#26500;&#24863;&#30693;&#30340;&#35777;&#20070;&#65292;&#22240;&#27492;&#40065;&#26834;&#24615;&#35777;&#20070;&#30340;&#22823;&#23567;&#21487;&#20197;&#22312;&#22270;&#30340;&#19981;&#21516;&#39044;&#23450;&#20041;&#32467;&#26500;&#20043;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certifying the robustness of a graph-based machine learning model poses a critical challenge for safety. Current robustness certificates for graph classifiers guarantee output invariance with respect to the total number of node pair flips (edge addition or edge deletion), which amounts to an $l_{0}$ ball centred on the adjacency matrix. Although theoretically attractive, this type of isotropic structural noise can be too restrictive in practical scenarios where some node pairs are more critical than others in determining the classifier's output. The certificate, in this case, gives a pessimistic depiction of the robustness of the graph model. To tackle this issue, we develop a randomised smoothing method based on adding an anisotropic noise distribution to the input graph structure. We show that our process generates structural-aware certificates for our classifiers, whereby the magnitude of robustness certificates can vary across different pre-defined structures of the graph. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2306.11839</link><description>&lt;p&gt;
&#26159;&#21542;&#24212;&#35813;&#20572;&#27490;&#65306;&#20855;&#26377;&#24322;&#36136;&#31181;&#32676;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Should I Stop or Should I Go: Early Stopping with Heterogeneous Populations. (arXiv:2306.11839v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20110;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;CLASH&#65292;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#30001;&#20110;&#27835;&#30103;&#36896;&#25104;&#24847;&#22806;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#22240;&#27492;&#24448;&#24448;&#38656;&#35201;&#25552;&#21069;&#20572;&#27490;&#12290;&#30446;&#21069;&#30830;&#23450;&#20309;&#26102;&#25552;&#21069;&#32456;&#27490;&#23454;&#39564;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#20110;&#24635;&#20307;&#25968;&#25454;&#65292;&#19981;&#32771;&#34385;&#27835;&#30103;&#25928;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24322;&#36136;&#31181;&#32676;&#26377;&#23475;&#23454;&#39564;&#30340;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#29616;&#26377;&#26041;&#27861;&#22312;&#27835;&#30103;&#23545;&#23569;&#25968;&#21442;&#19982;&#32773;&#36896;&#25104;&#20260;&#23475;&#26102;&#24448;&#24448;&#26080;&#27861;&#20572;&#27490;&#23454;&#39564;&#12290;&#28982;&#21518;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20102;CLASH&#65292;&#36825;&#26159;&#39318;&#20010;&#24191;&#27867;&#36866;&#29992;&#20110;&#24322;&#36136;&#26089;&#26399;&#20572;&#27490;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;CLASH&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;A/B&#27979;&#35797;&#20013;&#37117;&#33021;&#26377;&#25928;&#25552;&#21069;&#20572;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experiments often need to be stopped prematurely due to the treatment having an unintended harmful effect. Existing methods that determine when to stop an experiment early are typically applied to the data in aggregate and do not account for treatment effect heterogeneity. In this paper, we study the early stopping of experiments for harm on heterogeneous populations. We first establish that current methods often fail to stop experiments when the treatment harms a minority group of participants. We then use causal machine learning to develop CLASH, the first broadly-applicable method for heterogeneous early stopping. We demonstrate CLASH's performance on simulated and real data and show that it yields effective early stopping for both clinical trials and A/B tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#26469;&#21033;&#29992;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.11488</link><description>&lt;p&gt;
&#36879;&#35270;&#39069;&#22806;&#20449;&#24687;&#30340;Informed POMDP: &#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Informed POMDP: Leveraging Additional Information in Model-Based RL. (arXiv:2306.11488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#26469;&#21033;&#29992;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#35757;&#32451;&#26102;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#25512;&#24191;&#20102;POMDP&#20013;&#30340;&#20132;&#20114;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Informed POMDP&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#28165;&#26224;&#21306;&#20998;&#20102;&#35757;&#32451;&#20449;&#24687;&#21644;&#25191;&#34892;&#35266;&#23519;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30446;&#26631;&#65292;&#29992;&#20110;&#20174;&#21382;&#21490;&#35760;&#24405;&#20013;&#23398;&#20064;&#20986;&#20805;&#20998;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#25511;&#21046;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;Informed&#30446;&#26631;&#30001;&#23398;&#20064;&#19968;&#20010;&#29615;&#22659;&#27169;&#22411;&#32452;&#25104;&#65292;&#20174;&#20854;&#20013;&#25105;&#20204;&#21487;&#20197;&#37319;&#26679;&#38544;&#24335;&#36712;&#36857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20960;&#20010;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#36825;&#20010;Informed&#29615;&#22659;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;Dreamer&#31639;&#27861;&#31574;&#30053;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#20197;&#21450;&#25552;&#35758;&#30340;&#31616;&#21333;&#36866;&#24212;&#24615;&#65292;&#20513;&#23548;&#22312;&#20351;&#29992;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;POMDP&#26102;&#65292;&#35201;&#31995;&#32479;&#22320;&#32771;&#34385;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we generalize the problem of learning through interaction in a POMDP by accounting for eventual additional information available at training time. First, we introduce the informed POMDP, a new learning paradigm offering a clear distinction between the training information and the execution observation. Next, we propose an objective for learning a sufficient statistic from the history for the optimal control that leverages this information. We then show that this informed objective consists of learning an environment model from which we can sample latent trajectories. Finally, we show for the Dreamer algorithm that the convergence speed of the policies is sometimes greatly improved on several environments by using this informed environment model. Those results and the simplicity of the proposed adaptation advocate for a systematic consideration of eventual additional information when learning in a POMDP using model-based RL.
&lt;/p&gt;</description></item><item><title>LoSparse &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#65292;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.11222</link><description>&lt;p&gt;
&#22522;&#20110;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#21270;&#21387;&#32553; LoSparse
&lt;/p&gt;
&lt;p&gt;
LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. (arXiv:2306.11222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11222
&lt;/p&gt;
&lt;p&gt;
LoSparse &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#65292;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#27169;&#22411;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#36807;&#20110;&#24222;&#22823;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoSparse&#65288;&#20302;&#31209;&#21644;&#31232;&#30095;&#36924;&#36817;&#65289;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#21644;&#31232;&#30095;&#30697;&#38453;&#20043;&#21644;&#36924;&#36817;&#26435;&#37325;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#20302;&#31209;&#36924;&#36817;&#21644;&#35009;&#21098;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#20302;&#31209;&#36924;&#36817;&#21387;&#32553;&#20102;&#31070;&#32463;&#20803;&#20013;&#30340;&#19968;&#33268;&#21644;&#34920;&#36798;&#21147;&#24378;&#30340;&#37096;&#20998;&#65292;&#32780;&#35009;&#21098;&#21017;&#28040;&#38500;&#20102;&#31070;&#32463;&#20803;&#20013;&#30340;&#19981;&#19968;&#33268;&#21644;&#34920;&#36798;&#21147;&#19981;&#24378;&#30340;&#37096;&#20998;&#12290;&#35009;&#21098;&#22686;&#24378;&#20102;&#20302;&#31209;&#36924;&#36817;&#30340;&#22810;&#26679;&#24615;&#65292;&#20302;&#31209;&#36924;&#36817;&#38450;&#27490;&#20102;&#35009;&#21098;&#20002;&#22833;&#34920;&#36798;&#21147;&#24378;&#30340;&#31070;&#32463;&#20803;&#36807;&#22810;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;All-Positive (AP)&#28608;&#27963;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#20013;&#30340;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#36798;&#36127;&#35777;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.11113</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#20013;&#32047;&#31215;&#35777;&#25454;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Learn to Accumulate Evidence from All Training Samples: Theory and Practice. (arXiv:2306.11113v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;All-Positive (AP)&#28608;&#27963;&#65292;&#36991;&#20813;&#20102;&#29616;&#26377;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#20013;&#30340;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#21516;&#26102;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#36798;&#36127;&#35777;&#25454;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#29702;&#35770;&#21644;&#20027;&#35266;&#36923;&#36753;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30830;&#23450;&#24615;&#31070;&#32463;&#32593;&#32476;&#21464;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#35777;&#25454;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#35777;&#25454;&#37327;&#21270;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#35777;&#25454;&#27169;&#22411;&#65292;&#35777;&#25454;&#38656;&#35201;&#26159;&#38750;&#36127;&#30340;&#65292;&#36825;&#38656;&#35201;&#20351;&#29992;&#29305;&#27530;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#36825;&#20010;&#38480;&#21046;&#36890;&#24120;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#19981;&#22914;&#26631;&#20934;&#30340;softmax&#27169;&#22411;&#65292;&#20351;&#24471;&#23558;&#23427;&#20204;&#25193;&#23637;&#21040;&#35768;&#22810;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#30340;&#30495;&#27491;&#21407;&#22240;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#35777;&#25454;&#27169;&#22411;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#26681;&#26412;&#38480;&#21046;&#65292;&#23427;&#35299;&#37322;&#20102;&#19981;&#33391;&#24615;&#33021;&#65306;&#29616;&#26377;&#30340;&#35777;&#26126;&#28608;&#27963;&#20989;&#25968;&#21019;&#24314;&#20102;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#36825;&#38459;&#27490;&#20102;&#27169;&#22411;&#20174;&#33853;&#20837;&#36825;&#20123;&#21306;&#22495;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#12290;&#23545;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#23558;&#21407;&#22987;&#36755;&#20837;&#36716;&#25442;&#20026;&#35777;&#25454;&#37327;&#65292;&#28982;&#21518;&#20351;&#29992;&#20449;&#24565;&#26356;&#26032;&#35268;&#21017;&#36827;&#34892;&#32858;&#21512;&#30340;&#36716;&#25442;&#22120;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#31216;&#20026;&#20840;&#27491;&#28608;&#27963;&#65288;All-Positive&#65292;AP&#65289;&#65292;&#23427;&#36890;&#36807;&#26500;&#36896;&#36991;&#20813;&#38646;&#35777;&#25454;&#21306;&#22495;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35777;&#25454;&#28608;&#27963;&#20989;&#25968;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;AP&#28608;&#27963;&#20943;&#23569;&#20102;ReLU&#28608;&#27963;&#65292;&#20174;&#32780;&#24674;&#22797;&#20102;ReLU&#30340;&#33391;&#22909;&#27491;&#23450;&#24615;&#36136;&#65292;&#24182;&#19988;&#23427;&#30340;&#27867;&#21270;&#20801;&#35768;&#36127;&#35777;&#25454;&#37327;&#65292;&#22240;&#27492;&#27604;&#29616;&#26377;&#30340;&#35777;&#25454;&#28608;&#27963;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive performance compared to standard softmax models, making it challenging to extend them to many large-scale datasets. To unveil the real cause of this undesired behavior, we theoretically investigate evidential models and identify a fundamental limitation that explains the inferior performance: existing evidential activation functions create zero evidence regions, which prevent the model to learn from training samples falling into such regions. A deeper analysis of evidential activation functions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10191</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#29992;&#20110;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#26410;&#32463;&#36807;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#32473;&#23450;&#31867;&#21517;&#25110;&#26080;&#26631;&#31614;&#27979;&#35797;&#26679;&#26412;&#26102;&#65292;&#31070;&#32463;&#21551;&#21160;&#21487;&#20197;&#20351;&#27169;&#22411;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#30456;&#20851;&#25968;&#25454;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#26465;&#20214;&#21270;&#20854;&#21442;&#25968;&#65292;&#20174;&#32780;&#20351;&#20854;&#38024;&#23545;&#27979;&#35797;&#20998;&#24067;&#20570;&#22909;&#20934;&#22791;&#12290;&#31070;&#32463;&#21551;&#21160;&#36824;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#65292;&#21363;&#20351;&#26159;&#38024;&#23545;&#22914;LAION-2B&#36825;&#26679;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#21508;&#31181;&#20998;&#24067;&#21464;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23545;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNet&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.45&#65285;&#65292;&#22312;&#26631;&#20934;&#30340;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;3.81&#65285;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#31070;&#32463;&#21551;&#21160;&#26469;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNetV2&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;1.41&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#31070;&#32463;&#21551;&#21160;&#22312;&#22788;&#29702;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.09850</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#19981;&#33021;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima. (arXiv:2306.09850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#29992;&#30340;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#31639;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#19981;&#33021;&#22815;&#20840;&#31243;&#21521;&#26368;&#20248;&#28857;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;(SAM)&#26159;&#19968;&#31181;&#20248;&#21270;&#22120;&#65292;&#23427;&#22522;&#20110;&#24403;&#21069;&#28857;$x_t$&#30340;&#26799;&#24230;&#65292;&#22312;&#25200;&#21160;$y_t=x_t+\rho\frac{\nabla f(x_t)}{\lVert\nabla f(x_t)\rVert}$&#22788;&#36827;&#34892;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#35777;&#26126;&#20102;SAM&#23545;&#20110;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#20551;&#35774;&#25200;&#21160;&#30340;&#22823;&#23567;$\rho$&#36880;&#28176;&#34928;&#20943;&#21644;/&#25110;&#22312;$y_t$&#20013;&#27809;&#26377;&#26799;&#24230;&#24402;&#19968;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#19981;&#31526;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#23454;&#29992;&#37197;&#32622;&#65288;&#21363;&#24120;&#25968;$\rho$&#21644;$y_t$&#20013;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#65289;&#30340;&#30830;&#23450;&#24615;/&#38543;&#26426;&#29256;&#26412;&#30340;SAM&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#65288;&#38750;&#65289;&#20984;&#24615;&#20551;&#35774;&#30340;&#24179;&#28369;&#20989;&#25968;&#19978;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;SAM&#22312;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#25110;&#31283;&#23450;&#28857;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#24179;&#28369;&#24378;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30830;&#23450;&#24615;SAM&#20855;&#26377;&#20005;&#26684;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20026;$\tilde\Theta(\frac{1}{T^2})$&#65292;&#32780;&#38543;&#26426;SAM&#30340;&#25910;&#25947;&#30028;&#21017;&#21463;&#21040;&#22122;&#22768;&#27700;&#24179;&#38477;&#20302;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#24179;&#38754;&#30446;&#26631;&#34920;&#38754;&#30340;&#23574;&#38160;&#24230;&#21644;&#24179;&#32531;&#24615;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is an optimizer that takes a descent step based on the gradient at a perturbation $y_t = x_t + \rho \frac{\nabla f(x_t)}{\lVert \nabla f(x_t) \rVert}$ of the current point $x_t$. Existing studies prove convergence of SAM for smooth functions, but they do so by assuming decaying perturbation size $\rho$ and/or no gradient normalization in $y_t$, which is detached from practice. To address this gap, we study deterministic/stochastic versions of SAM with practical configurations (i.e., constant $\rho$ and gradient normalization in $y_t$) and explore their convergence properties on smooth functions with (non)convexity assumptions. Perhaps surprisingly, in many scenarios, we find out that SAM has limited capability to converge to global minima or stationary points. For smooth strongly convex functions, we show that while deterministic SAM enjoys tight global convergence rates of $\tilde \Theta(\frac{1}{T^2})$, the convergence bound of stochastic SAM suffer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09382</link><description>&lt;p&gt;
2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#8212;&#8212;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Sound Demixing Challenge 2023 -- Music Demixing Track Technical Report. (arXiv:2306.09382v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#30340;&#20004;&#20010;&#26377;&#25928;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#21644;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;2023&#24180;&#22768;&#38899;&#20998;&#31163;&#25361;&#25112;&#36187;&#38899;&#20048;&#20998;&#31163;&#36187;&#36947;&#20013;&#33719;&#22870;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#22312;&#27492;&#25361;&#25112;&#20013;&#35774;&#35745;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#26102;&#25928;&#39640;&#30340;&#28304;&#20998;&#31163;&#32593;&#32476;&#65292;&#22312;MUSDB&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65307;&#19968;&#31181;&#36866;&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#24615;&#28304;&#20998;&#31163;&#30340;&#25439;&#22833;&#25513;&#27169;&#26041;&#27861;&#12290;&#22312;github.com/kuielab/sdx23&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#26368;&#32456;&#25552;&#20132;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we present our award-winning solutions for the Music Demixing Track of Sound Demixing Challenge 2023. We focus on two methods designed for this challenge: a time-efficient source separation network that achieves state-of-the-art results on the MUSDB benchmark and a loss masking method for noise-robust source separation. Code for reproducing model training and final submissions is available at github.com/kuielab/sdx23.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;DreamSim&#65292;&#35813;&#24230;&#37327;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09344</link><description>&lt;p&gt;
DreamSim&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. (arXiv:2306.09344v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;DreamSim&#65292;&#35813;&#24230;&#37327;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23398;&#20064;&#20154;&#31867;&#35270;&#35273;&#30456;&#20284;&#24615;&#30340;&#26032;&#32500;&#24230;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24863;&#30693;&#30456;&#20284;&#24615;&#24230;&#37327;&#26159;&#22312;&#20687;&#32032;&#21644;&#22270;&#20687;&#22359;&#30340;&#23618;&#38754;&#25805;&#20316;&#30340;&#12290;&#36825;&#20123;&#24230;&#37327;&#20351;&#29992;&#20302;&#23618;&#27425;&#30340;&#39068;&#33394;&#21644;&#32441;&#29702;&#26469;&#27604;&#36739;&#22270;&#20687;&#65292;&#20294;&#26410;&#33021;&#25429;&#25417;&#22270;&#20687;&#24067;&#23616;&#12289;&#23545;&#35937;&#23039;&#24577;&#21644;&#35821;&#20041;&#20869;&#23481;&#30340;&#20013;&#23618;&#27425;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;&#22270;&#20687;&#30340;&#24863;&#30693;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#27493;&#26159;&#25910;&#38598;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#30456;&#20284;&#32500;&#24230;&#22270;&#20687;&#23545;&#30340;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#26159;&#35780;&#21028;&#26159;&#20960;&#20046;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#30001;&#25152;&#26377;&#35266;&#23519;&#32773;&#20849;&#20139;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20123;&#27839;&#19981;&#21516;&#32500;&#24230;&#25200;&#21160;&#30340;&#21512;&#25104;&#22270;&#20687;&#23545;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;&#27969;&#34892;&#30340;&#24863;&#30693;&#24230;&#37327;&#26080;&#27861;&#35299;&#37322;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;DreamSim&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#35270;&#35273;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;&#24230;&#37327;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23427;&#20005;&#37325;&#20851;&#27880;&#21069;&#26223;&#29289;&#20307;&#21644;&#35821;&#20041;&#20869;&#23481;&#12290;DreamSim&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#30340;&#24230;&#37327;&#26356;&#20248;&#65292;&#21253;&#25324;&#39044;&#27979;&#34892;&#20026;&#23454;&#39564;&#32467;&#26524;&#12289;&#39044;&#27979;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic conte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.08997</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models. (arXiv:2306.08997v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08997
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32763;&#35793;&#20102;MIT&#25968;&#23398;&#21644;EECS&#35838;&#31243;&#20013;&#30340;4550&#20010;&#39064;&#30446;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20102;&#33719;&#21462;&#23398;&#20301;&#25152;&#38656;&#30340;&#25152;&#26377;MIT&#25968;&#23398;&#21644;&#30005;&#27668;&#24037;&#31243;&#21450;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;EECS&#65289;&#35838;&#31243;&#30340;&#39064;&#30446;&#38598;&#12289;&#26399;&#20013;&#32771;&#35797;&#21644;&#26399;&#26411;&#32771;&#35797;&#20013;&#30340;4550&#20010;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20219;&#20309;MIT&#25968;&#23398;&#21644;EECS&#19987;&#19994;&#27605;&#19994;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#25104;&#21151;&#35299;&#20915;&#20102;&#25972;&#20010;MIT&#35838;&#31243;&#30340;&#19977;&#20998;&#20043;&#19968;&#65292;&#32780;GPT-4&#22312;&#39064;&#30446;&#20013;&#19981;&#21253;&#21547;&#22270;&#20687;&#30340;&#27979;&#35797;&#38598;&#19978;&#32463;&#36807;&#25552;&#31034;&#24037;&#31243;&#21518;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#35299;&#20915;&#29575;&#12290;&#25105;&#20204;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;GPT-4&#33258;&#21160;&#35780;&#20998;&#65292;&#25552;&#20379;&#20102;&#35838;&#31243;&#12289;&#38382;&#39064;&#21644;&#31572;&#26696;&#31867;&#22411;&#30340;&#35814;&#32454;&#24615;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#23884;&#20837;&#20302;&#32500;&#31354;&#38388;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38382;&#39064;&#12289;&#20027;&#39064;&#21644;&#35838;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#21738;&#20123;&#38382;&#39064;&#21644;&#35838;&#31243;&#26159;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#21644;&#35838;&#31243;&#25152;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We curate a comprehensive dataset of 4,550 questions and solutions from problem sets, midterm exams, and final exams across all MIT Mathematics and Electrical Engineering and Computer Science (EECS) courses required for obtaining a degree. We evaluate the ability of large language models to fulfill the graduation requirements for any MIT major in Mathematics and EECS. Our results demonstrate that GPT-3.5 successfully solves a third of the entire MIT curriculum, while GPT-4, with prompt engineering, achieves a perfect solve rate on a test set excluding questions based on images. We fine-tune an open-source large language model on this dataset. We employ GPT-4 to automatically grade model responses, providing a detailed performance breakdown by course, question, and answer type. By embedding questions in a low-dimensional space, we explore the relationships between questions, topics, and classes and discover which questions and classes are required for solving other questions and classes
&lt;/p&gt;</description></item><item><title>MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07952</link><description>&lt;p&gt;
MOFI: &#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MOFI: Learning Image Representations from Noisy Entity Annotated Images. (arXiv:2306.07952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07952
&lt;/p&gt;
&lt;p&gt;
MOFI &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598; I2E&#65292;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#23398;&#20064;&#21040;&#20102;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411; MOFI&#65292;&#26088;&#22312;&#20174;&#21547;&#22122;&#23454;&#20307;&#26631;&#27880;&#30340;&#22270;&#20687;&#20013;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#12290;MOFI &#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#26377;&#20004;&#28857;&#19981;&#21516;&#65306;&#65288;i&#65289;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#37197;&#26041;&#12290;&#22312;&#25968;&#25454;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#20174;&#21547;&#22122;&#22270;&#20687;&#25991;&#26412;&#23545;&#20013;&#20026;&#22270;&#20687;&#25351;&#23450;&#23454;&#20307;&#26631;&#31614;&#12290;&#25105;&#20204;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20174; alt-text &#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#28982;&#21518;&#20351;&#29992; CLIP &#27169;&#22411;&#36873;&#25321;&#27491;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#22270;&#20687;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20174; web &#19978;&#25366;&#25496;&#30340;&#25968;&#21313;&#20159;&#20010;&#22270;&#20687;&#25991;&#26412;&#23545;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102; Image-to-Entities&#65288;I2E&#65289;&#36825;&#19968;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547; 10 &#20159;&#24352;&#22270;&#20687;&#21644; 200 &#19975;&#20010;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#28085;&#30422;&#20102;&#37326;&#22806;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#22522;&#20110; I2E &#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#26377;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#12289;&#23545;&#27604;&#24230;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.07566</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#26631;&#31614;&#19979;&#30340;&#24322;&#36136;&#20915;&#31574;&#32773;&#65306;&#19968;&#31181;&#24037;&#20855;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#38382;&#39064;&#22312;&#21382;&#21490;&#20915;&#31574;&#23548;&#33268;&#32467;&#26524;&#20165;&#37096;&#20998;&#26631;&#35760;&#26102;&#20986;&#29616;&#12290;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#25972;&#20307;&#20154;&#32676;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#24403;&#21382;&#21490;&#20915;&#31574;&#21644;&#30446;&#26631;&#32467;&#26524;&#21487;&#20197;&#21516;&#26102;&#21463;&#26576;&#20123;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#26102;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#25972;&#20307;&#20154;&#32676;&#20013;&#30340;&#20005;&#37325;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#24212;&#29992;&#20013;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#28385;&#36275;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26102;&#20219;&#20309;&#32473;&#23450;&#39044;&#27979;&#35268;&#21017;&#30340;&#20840;&#20307;&#39118;&#38505;&#30340;&#28857;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#22312;&#28857;&#35782;&#21035;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2306.07392</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#23398;&#20064;&#20219;&#24847;&#35270;&#35282;&#30340;6DoF&#26426;&#22120;&#20154;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural Surface Rendering. (arXiv:2306.07392v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07392
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#65292;NeuGraspNet&#33021;&#22815;&#22312;&#28151;&#20081;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#20174;&#20219;&#24847;&#35270;&#35282;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22312;&#26234;&#33021;&#36741;&#21161;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#20174;&#20219;&#20309;&#35270;&#35282;&#26377;&#25928;&#22320;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#22330;&#26223;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NeuGraspNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;6DoF&#25235;&#21462;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#31070;&#32463;&#20307;&#31215;&#34920;&#31034;&#21644;&#34920;&#38754;&#28210;&#26579;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#20840;&#23616;&#65288;&#22330;&#26223;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#25235;&#21462;&#32423;&#21035;&#65289;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#22330;&#26223;&#30340;&#26410;&#35265;&#37096;&#20998;&#65292;&#20063;&#33021;&#26377;&#25928;&#22320;&#39044;&#27979;6DoF&#25235;&#21462;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25235;&#21462;&#37325;&#26032;&#35299;&#37322;&#20026;&#19968;&#20010;&#23616;&#37096;&#30340;&#31070;&#32463;&#34920;&#38754;&#28210;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#32534;&#30721;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21644;&#23545;&#35937;&#34920;&#38754;&#20960;&#20309;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;NeuGraspNet&#22312;&#21333;&#20010;&#35270;&#35282;&#19978;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#37319;&#26679;&#25235;&#21462;&#20505;&#36873;&#39033;&#65292;&#34920;&#29616;&#20986;&#20248;&#20110;&#29616;&#26377;&#38544;&#24335;&#21644;&#21322;&#38544;&#24335;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulation is critical for admitting robotic agents to various application domains, like intelligent assistance. A major challenge therein is the effective 6DoF grasping of objects in cluttered environments from any viewpoint without requiring additional scene exploration. We introduce $\textit{NeuGraspNet}$, a novel method for 6DoF grasp detection that leverages recent advances in neural volumetric representations and surface rendering. Our approach learns both global (scene-level) and local (grasp-level) neural surface representations, enabling effective and fully implicit 6DoF grasp quality prediction, even in unseen parts of the scene. Further, we reinterpret grasping as a local neural surface rendering problem, allowing the model to encode the interaction between the robot's end-effector and the object's surface geometry. NeuGraspNet operates on single viewpoints and can sample grasp candidates in occluded scenes, outperforming existing implicit and semi-implicit baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06208</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#24040;&#22823;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;GPU&#21644;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24555;&#36895;&#12289;&#21450;&#26102;&#30340;&#22788;&#29702;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23376;&#20248;&#26144;&#23556;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#26102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#34892;&#20026;&#12290;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#26144;&#23556;&#26159;&#36890;&#36807;&#22810;&#20010;&#36719;&#20214;&#32452;&#20214;&#36827;&#34892;&#30340;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#12289;&#35774;&#22791;&#24211;&#31561;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35745;&#31639;&#29615;&#22659;&#12290;&#38543;&#30528;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#35774;&#22791;&#31561;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27491;&#30830;&#24615;&#30340;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26356;&#25913;&#36719;&#20214;&#32452;&#20214;&#26469;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#39044;&#27979;&#36755;&#20986;&#24046;&#24322;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#22270;&#20687;&#21644;&#25200;&#21160;&#22270;&#20687;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19977;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36125;&#21494;&#26031;&#31890;&#23376;&#27969;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#23545;&#20110;&#36890;&#36807;&#21487;&#38752;&#30340;&#29366;&#24577;&#25512;&#26029;&#23454;&#29616;&#31934;&#30830;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24320;&#21457;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36719;&#27979;&#37327;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031; (DPFB) &#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#30446;&#26631;&#29366;&#24577;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#65292;&#20197;&#25191;&#34892;&#28508;&#22312;&#30340;&#36328;&#39046;&#22495;&#36719;&#24863;&#30693;&#38382;&#39064;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#22312;&#26694;&#26550;&#26680;&#24515;&#65292;&#25105;&#20204;&#32467;&#21512;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#31890;&#23376;&#27969;&#65292;&#36890;&#36807;&#20248;&#21270;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#26469;&#25191;&#34892;&#27169;&#22411;&#25552;&#21462;&#30340;&#28508;&#22312;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#26356;&#26032;&#12290;&#30001;&#27492;&#65292;&#36825;&#20123;&#36129;&#29486;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#26377;&#26426;&#30340;&#36817;&#20284;&#21518;&#39564;&#29305;&#24449;&#34920;&#31034;&#65292;&#33021;&#22815;&#34920;&#24449;&#22797;&#26434;&#30340;&#36328;&#39046;&#22495;&#31995;&#32479;&#21160;&#21147;&#23398;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03763</link><description>&lt;p&gt;
ChatGPT&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Informed Graph Neural Network for Stock Movement Prediction. (arXiv:2306.03763v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#25216;&#26415;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#36130;&#32463;&#26032;&#38395;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#33719;&#24471;&#20102;&#36229;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#30340;&#34920;&#29616;&#65292;&#25552;&#31034;&#20102;ChatGPT&#22312;&#25991;&#26412;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24050;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20174;&#26102;&#38388;&#25991;&#26412;&#25968;&#25454;&#65288;&#23588;&#20854;&#26159;&#36130;&#32463;&#26032;&#38395;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#32467;&#26500;&#30340;&#28508;&#21147;&#20173;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;ChatGPT&#30340;&#22270;&#25512;&#26029;&#33021;&#21147;&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24039;&#22937;&#22320;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20986;&#19981;&#26029;&#21464;&#21270;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#23558;&#36825;&#20123;&#32593;&#32476;&#32467;&#26500;&#34701;&#21512;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36827;&#34892;&#21518;&#32493;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#27169;&#22411;&#30340;&#20135;&#20986;&#26500;&#24314;&#30340;&#32452;&#21512;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#24180;&#21270;&#32047;&#35745;&#22238;&#25253;&#12289;&#26356;&#20302;&#30340;&#27874;&#21160;&#24615;&#21644;&#26368;&#22823;&#22238;&#25764;&#12290;&#36825;&#31181;&#21331;&#36234;&#34920;&#29616;&#31361;&#26174;&#20102;ChatGPT&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#37329;&#34701;&#39044;&#27979;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2306.03013</link><description>&lt;p&gt;
&#26126;&#34255;&#26263;&#31363;&#65306;&#20266;&#35013;&#25968;&#25454;&#31363;&#21462;&#25915;&#20987;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning. (arXiv:2306.03013v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#26694;&#26550;SEER&#65292;&#21487;&#20197;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#34987;&#23458;&#25143;&#31471;&#20390;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#24050;&#32463;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#31363;&#21462;&#22312;&#22823;&#25209;&#37327;&#21644;&#23433;&#20840;&#32858;&#21512;&#31561;&#20043;&#21069;&#34987;&#35270;&#20026;&#31169;&#23494;&#30340;&#35774;&#32622;&#20013;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20851;&#20110;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#23458;&#25143;&#31471;&#20390;&#27979;&#24615;&#30340;&#30097;&#34385;&#34987;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#34987;&#20844;&#24320;&#30693;&#26195;&#21518;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#23458;&#25143;&#31471;&#20390;&#27979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35768;&#22810;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#21407;&#21017;&#30340;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#37117;&#21487;&#20197;&#36890;&#36807;&#21512;&#29702;&#30340;&#23458;&#25143;&#31471;&#26816;&#26597;&#26469;&#26816;&#27979;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#29702;&#24819;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;SEER&#25915;&#20987;&#26694;&#26550;&#65292;&#23427;&#28385;&#36275;&#25152;&#26377;&#29702;&#24819;&#35201;&#27714;&#65292;&#21487;&#20197;&#20174;&#29616;&#23454;&#32593;&#32476;&#30340;&#26799;&#24230;&#20013;&#31363;&#21462;&#29992;&#25143;&#25968;&#25454;&#65292;&#21363;&#20351;&#26159;&#22312;&#22823;&#25209;&#37327;(&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#22823;&#21487;&#36798;512)&#21644;&#23433;&#20840;&#32858;&#21512;&#30340;&#24773;&#20917;&#19979;&#12290;SEER&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#31192;&#23494;&#35299;&#30721;&#22120;&#65292;&#22312;&#20849;&#20139;&#27169;&#22411;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#36808;&#21521;&#23454;&#29992;&#24694;&#24847;&#26381;&#21153;&#22120;&#25915;&#20987;&#30340;&#26377;&#21069;&#36884;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious server (MS) attacks have enabled the scaling of data stealing in federated learning to large batch sizes and secure aggregation, settings previously considered private. However, many concerns regarding client-side detectability of MS attacks were raised, questioning their practicality once they are publicly known. In this work, for the first time, we thoroughly study the problem of client-side detectability.We demonstrate that most prior MS attacks, which fundamentally rely on one of two key principles, are detectable by principled client-side checks. Further, we formulate desiderata for practical MS attacks and propose SEER, a novel attack framework that satisfies all desiderata, while stealing user data from gradients of realistic networks, even for large batch sizes (up to 512 in our experiments) and under secure aggregation. The key insight of SEER is the use of a secret decoder, which is jointly trained with the shared model. Our work represents a promising first step to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#24433;&#21709;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#23398;&#20064;&#36229;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.01613</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#19979;&#30340;&#36229;&#21442;&#25968;&#23398;&#20064;&#65306;&#22522;&#20110;&#22810;&#30446;&#26631;&#20108;&#23618;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Learning under Data Poisoning: Analysis of the Influence of Regularization via Multiobjective Bilevel Optimization. (arXiv:2306.01613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#24433;&#21709;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#23398;&#20064;&#36229;&#21442;&#25968;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23481;&#26131;&#36973;&#21463;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#21363;&#36890;&#36807;&#25805;&#32437;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#26469;&#26377;&#24847;&#30772;&#22351;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#20248;&#25915;&#20987;&#21487;&#20197;&#34987;&#21046;&#23450;&#20026;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#26377;&#21161;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#31639;&#27861;&#30340;&#24378;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#36229;&#21442;&#25968;&#20445;&#25345;&#19981;&#21464;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#31639;&#27861;&#40065;&#26834;&#24615;&#21644;&#27491;&#21017;&#21270;&#24433;&#21709;&#30340;&#36807;&#20110;&#24754;&#35266;&#30340;&#35266;&#28857;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20248;&#25915;&#20987;&#20844;&#24335;&#65292;&#32771;&#34385;&#25915;&#20987;&#23545;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#25915;&#20987;&#24314;&#27169;&#20026;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20801;&#35768;&#21046;&#23450;&#26368;&#20248;&#25915;&#20987;&#12289;&#23398;&#20064;&#36229;&#21442;&#25968;&#24182;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#25915;&#20987;&#20844;&#24335;&#24212;&#29992;&#20110;&#20351;&#29992;$L_2$&#21644;$L_1$&#27491;&#21017;&#21270;&#30340;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19978;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#30830;&#35748;&#20102;&#20808;&#21069;&#31574;&#30053;&#30340;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#31934;&#30830;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#21644;&#22312;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26102;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#36229;&#21442;&#25968;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) algorithms are vulnerable to poisoning attacks, where a fraction of the training data is manipulated to deliberately degrade the algorithms' performance. Optimal attacks can be formulated as bilevel optimization problems and help to assess their robustness in worst-case scenarios. We show that current approaches, which typically assume that hyperparameters remain constant, lead to an overly pessimistic view of the algorithms' robustness and of the impact of regularization. We propose a novel optimal attack formulation that considers the effect of the attack on the hyperparameters and models the attack as a multiobjective bilevel optimization problem. This allows to formulate optimal attacks, learn hyperparameters and evaluate robustness under worst-case conditions. We apply this attack formulation to several ML classifiers using $L_2$ and $L_1$ regularization. Our evaluation on multiple datasets confirms the limitations of previous strategies and evidences the ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00684</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#27969;&#37319;&#26679;&#24179;&#34913;&#35757;&#32451;&#33021;&#37327;&#22522;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33021;&#37327;&#22522;&#27169;&#22411;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#21644;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411; (EBM) &#26159;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#26410;&#26631;&#20934;&#21270;&#23545;&#25968;&#23494;&#24230;&#30340;&#22810;&#21151;&#33021;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;EBM &#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#32570;&#20047;&#27169;&#22411;&#30340;&#35268;&#33539;&#21270;&#24120;&#37327;&#65292;&#20351;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#35745;&#31639;&#19981;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36817;&#20284;&#37319;&#26679;&#22120;&#21644;&#21464;&#20998;&#25512;&#29702;&#25216;&#26415;&#26469;&#20272;&#35745;&#20284;&#28982;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#20272;&#35745;&#23494;&#24230;&#30340;&#32479;&#35745;&#31934;&#24230;&#65292;&#20363;&#22914;&#30830;&#23450;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#31867;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#21364;&#20184;&#20986;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24402;&#19968;&#21270;&#27969; (NF)&#65292;&#36825;&#31181;&#27169;&#22411;&#26368;&#36817;&#34987;&#25552;&#20986;&#20197;&#20415;&#20110;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558; NF &#25311;&#21512;&#21040; EBM &#19978;&#65292;&#20197;&#20415; NF &#36741;&#21161;&#19979;&#30340;&#37319;&#26679;&#26041;&#26696;&#33021;&#22815;&#22987;&#32456;&#20026; EBM &#25552;&#20379;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#26368;&#32456;&#25552;&#39640;&#27169;&#22411;&#30340;&#32479;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479; EBM &#35757;&#32451;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#21644;&#26356;&#22909;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.00013</link><description>&lt;p&gt;
&#30284;&#30151;&#23454;&#20307;&#30340;&#20851;&#32852;&#21644;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approach for Cancer Entities Association and Classification. (arXiv:2306.00013v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#20174;&#22823;&#37327;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30284;&#30151;&#30456;&#20851;&#23454;&#20307;&#21644;&#23454;&#20307;&#38388;&#20851;&#31995;&#65292;&#20197;&#24110;&#21161;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#65288;WHO&#65289;&#30340;&#25968;&#25454;&#65292;&#30284;&#30151;&#26159;&#20840;&#29699;&#31532;&#20108;&#22823;&#27515;&#22240;&#12290;&#19981;&#21516;&#31867;&#22411;&#30284;&#30151;&#30340;&#31185;&#23398;&#30740;&#31350;&#20197;&#27599;&#24180;&#21457;&#24067;&#22823;&#37327;&#30340;&#30740;&#31350;&#25991;&#31456;&#30340;&#36895;&#24230;&#19981;&#26029;&#22686;&#38271;&#12290;&#19982;&#22522;&#22240;&#30456;&#20851;&#30340;&#33647;&#29289;&#12289;&#35786;&#26029;&#12289;&#39118;&#38505;&#12289;&#30151;&#29366;&#12289;&#27835;&#30103;&#31561;&#30340;&#20449;&#24687;&#21644;&#30693;&#35782;&#26159;&#24110;&#21161;&#25506;&#32034;&#21644;&#25512;&#36827;&#30284;&#30151;&#30740;&#31350;&#36827;&#23637;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25163;&#21160;&#31579;&#36873;&#36825;&#20040;&#22823;&#37327;&#30340;&#25991;&#31456;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#24456;&#38590;&#21046;&#23450;&#20219;&#20309;&#20551;&#35774;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20004;&#31181;&#26368;&#20026;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21151;&#33021;&#65292;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#21457;&#29616;&#30693;&#35782;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20511;&#21161;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#20869;&#32622;&#23383;&#20856;&#35782;&#21035;&#24182;&#25552;&#21462;&#19982;&#30284;&#30151;&#30456;&#20851;&#30340;&#39044;&#23450;&#20041;&#23454;&#20307;&#12290;&#25991;&#26412;&#20998;&#31867;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24110;&#21161;&#25506;&#31350;&#30284;&#30151;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
According to the World Health Organization (WHO), cancer is the second leading cause of death globally. Scientific research on different types of cancers grows at an ever-increasing rate, publishing large volumes of research articles every year. The insight information and the knowledge of the drug, diagnostics, risk, symptoms, treatments, etc., related to genes are significant factors that help explore and advance the cancer research progression. Manual screening of such a large volume of articles is very laborious and time-consuming to formulate any hypothesis. The study uses the two most non-trivial NLP, Natural Language Processing functions, Entity Recognition, and text classification to discover knowledge from biomedical literature. Named Entity Recognition (NER) recognizes and extracts the predefined entities related to cancer from unstructured text with the support of a user-friendly interface and built-in dictionaries. Text classification helps to explore the insights into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;</title><link>http://arxiv.org/abs/2305.19535</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32447;&#23398;&#20064;&#30340;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-rank extended Kalman filtering for online learning of neural networks from streaming data. (arXiv:2305.19535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#36817;&#20284;&#36125;&#21494;&#26031;&#25512;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21487;&#33021;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#20013;&#20272;&#35745;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EKF&#65289;&#65292;&#20294;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20302;&#31209;&#21152;&#23545;&#35282;&#32447;&#30340;&#21518;&#39564;&#31934;&#24230;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#27599;&#27493;&#30340;&#25104;&#26412;&#19982;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#21464;&#20998;&#25512;&#29702;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#30830;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27493;&#38271;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#23548;&#33268;&#26356;&#24555;&#65288;&#26356;&#39640;&#25928;&#65289;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#29992;&#20316;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#30340;&#19968;&#37096;&#20998;&#26102;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#24555;&#30340;&#22870;&#21169;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient online approximate Bayesian inference algorithm for estimating the parameters of a nonlinear function from a potentially non-stationary data stream. The method is based on the extended Kalman filter (EKF), but uses a novel low-rank plus diagonal decomposition of the posterior precision matrix, which gives a cost per step which is linear in the number of model parameters. In contrast to methods based on stochastic variational inference, our method is fully deterministic, and does not require step-size tuning. We show experimentally that this results in much faster (more sample efficient) learning, which results in more rapid adaptation to changing distributions, and faster accumulation of reward when used as part of a contextual bandit algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19185</link><description>&lt;p&gt;
Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#19979;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compression with Bayesian Implicit Neural Representations. (arXiv:2305.19185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;Bayesian&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#21387;&#32553;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24120;&#35265;&#31867;&#22411;&#30340;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#23558;&#22352;&#26631;&#26144;&#23556;&#21040;&#20449;&#21495;&#20540;&#30340;&#20989;&#25968;&#65292;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#20301;&#32622;&#21040;RGB&#20540;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21151;&#33021;&#34920;&#31034;&#36827;&#34892;&#36229;&#25311;&#21512;&#65292;&#28982;&#21518;&#32534;&#30721;&#32593;&#32476;&#26435;&#37325;&#26469;&#21387;&#32553;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23558;&#31934;&#24230;&#37327;&#21270;&#21040;&#20302;&#27604;&#29305;&#20250;&#22823;&#24133;&#38477;&#20302;&#37325;&#26500;&#36136;&#37327;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36807;&#24230;&#25311;&#21512;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#36817;&#20284;&#21518;&#39564;&#26435;&#37325;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#37327;&#21270;&#21644;&#29109;&#32534;&#30721;&#23427;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#26368;&#23567;&#21270; $\beta$-ELBO &#30452;&#25509;&#20248;&#21270;&#30721;-&#22833;&#30495;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972; $\beta$ &#26469;&#38024;&#23545;&#32473;&#23450;&#30340;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#19981;&#21516;&#30340;&#30721;-&#22833;&#30495;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#20808;&#39564;&#26435;&#37325;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#37319;&#29992;&#20027;&#21160;&#23610;&#23544;&#35843;&#25972;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $\beta$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $\beta$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#30697;&#38453;&#33609;&#22270;&#30340;&#24133;&#20540;&#21098;&#26525;&#30340;&#26032;&#39062;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#24378;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#20197;&#22312;&#21098;&#26525;&#21644;&#36229;&#21442;&#25968;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#36793;&#30028;&#30340;&#31639;&#27861;&#24320;&#21457;&#26041;&#38754;&#24320;&#36767;&#26032;&#30340;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.18789</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#30697;&#38453;&#33609;&#22270;&#30340;&#24133;&#20540;&#21098;&#26525;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching. (arXiv:2305.18789v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#30697;&#38453;&#33609;&#22270;&#30340;&#24133;&#20540;&#21098;&#26525;&#30340;&#26032;&#39062;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#24378;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#20197;&#22312;&#21098;&#26525;&#21644;&#36229;&#21442;&#25968;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#36793;&#30028;&#30340;&#31639;&#27861;&#24320;&#21457;&#26041;&#38754;&#24320;&#36767;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24133;&#20540;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#26412;&#24037;&#20316;&#22522;&#20110;Arora&#31561;&#20154;&#65288;2018&#65289;&#30340;&#36793;&#30028;&#65292;&#20854;&#20013;&#35823;&#24046;&#21462;&#20915;&#20110;&#21098;&#26525;&#24341;&#36215;&#30340;&#36924;&#36817;&#21644;&#21098;&#26525;&#27169;&#22411;&#20013;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25913;&#36827;&#20102;&#26631;&#20934;&#22522;&#20110;&#33539;&#25968;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#20351;&#29992;&#25105;&#20204;&#26032;&#30340;&#22522;&#20110;&#24133;&#20540;&#30340;&#21387;&#32553;&#31639;&#27861;&#24471;&#21040;&#30340;&#21098;&#26525;&#20272;&#35745;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25509;&#36817;&#26410;&#21098;&#26525;&#20989;&#25968;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#12290;&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#33609;&#22270;&#65292;&#21098;&#26525;&#30697;&#38453;&#30340;&#31354;&#38388;&#21487;&#20197;&#22312;&#36828;&#23567;&#20110;&#20854;&#32500;&#24230;&#30340;&#23494;&#38598;&#30697;&#38453;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#34920;&#31034;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#31532;&#20108;&#20010;&#26631;&#20934;&#12290;&#36825;&#23548;&#33268;&#20102;&#27604;&#35768;&#22810;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#24378;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#20174;&#32780;&#22312;&#21098;&#26525;&#21644;&#36229;&#21442;&#25968;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#36793;&#30028;&#30340;&#31639;&#27861;&#24320;&#21457;&#26041;&#38754;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#33719;&#24471;&#20102;&#32467;&#26500;&#21098;&#26525;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#20854;&#20013;&#36824;&#28041;&#21450;&#21040;&#22359;&#29366;&#26435;&#37325;&#30340;&#21098;&#26525;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#23545;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we derive a novel bound on the generalization error of Magnitude-Based pruning of overparameterized neural networks. Our work builds on the bounds in Arora et al. [2018] where the error depends on one, the approximation induced by pruning, and two, the number of parameters in the pruned model, and improves upon standard norm-based generalization bounds. The pruned estimates obtained using our new Magnitude-Based compression algorithm are close to the unpruned functions with high probability, which improves the first criteria. Using Sparse Matrix Sketching, the space of the pruned matrices can be efficiently represented in the space of dense matrices of much smaller dimensions, thereby lowering the second criterion. This leads to stronger generalization bound than many state-of-the-art methods, thereby breaking new ground in the algorithm development for pruning and bounding generalization error of overparameterized models. Beyond this, we extend our results to obtain gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#65292;&#24182;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.18738</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#34892;&#20026;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generating Behaviorally Diverse Policies with Latent Diffusion Models. (arXiv:2305.18738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#65292;&#24182;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#21697;&#36136;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;QD-RL&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20351;&#24471;&#23398;&#20064;&#34892;&#20026;&#22810;&#26679;&#21270;&#12289;&#39640;&#24615;&#33021;&#30340;&#31574;&#30053;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#23384;&#20648;&#25968;&#21315;&#20010;&#31574;&#30053;&#65292;&#23548;&#33268;&#31354;&#38388;&#22797;&#26434;&#24230;&#39640;&#19988;&#38590;&#20197;&#36866;&#24212;&#26356;&#22810;&#34892;&#20026;&#30340;&#25193;&#23637;&#12290;&#23558;&#26723;&#26696;&#21387;&#32553;&#20026;&#21333;&#20010;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#31574;&#30053;&#38598;&#30340;&#24615;&#33021;&#21644;&#35206;&#30422;&#29575;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23558;&#24402;&#26723;&#21387;&#32553;&#20026;&#21333;&#20010;&#23545;&#20110;&#31574;&#30053;&#21442;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;13&#20493;&#30340;&#21387;&#32553;&#27604;&#29575;&#65292;&#21516;&#26102;&#24674;&#22797;&#20102;98%&#30340;&#21407;&#22987;&#22238;&#25253;&#21644;89%&#30340;&#21407;&#22987;&#35206;&#30422;&#29575;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#35843;&#33410;&#26426;&#21046;&#36824;&#20801;&#35768;&#28789;&#27963;&#36873;&#25321;&#21644;&#25490;&#24207;&#34892;&#20026;&#65292;&#21253;&#25324;&#20351;&#29992;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98% of the original rewards and 89% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.18612</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
Networked Time Series Imputation via Position-aware Graph Enhanced Variational Autoencoders. (arXiv:2305.18612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20301;&#32622;&#24863;&#30693;&#22270;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#26469;&#22788;&#29702;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;NTS&#25968;&#25454;&#20013;&#30340;&#22270;&#21160;&#24577;&#21644;&#32570;&#22833;&#36793;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26159;&#36817;&#24180;&#26469;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#22823;&#31867;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#28145;&#24230;&#36882;&#24402;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;MTS&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20316;&#20026;&#25554;&#34917;&#30340;&#20851;&#31995;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#25299;&#25169;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#23450;&#22270;&#32467;&#26500;&#22266;&#23450;&#19988;&#20934;&#30830;&#24050;&#30693;&#12290;&#22240;&#27492;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#65288;NTS&#65289;&#25968;&#25454;&#20013;&#65292;&#23427;&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22270;&#21160;&#24577;&#36827;&#34892;&#31934;&#30830;&#30340;&#25554;&#34917;&#65292;&#20854;&#20013;&#24213;&#23618;&#22270;&#19981;&#26029;&#21464;&#21270;&#24182;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#36793;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21253;&#21547;&#33410;&#28857;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20013;&#32570;&#22833;&#20540;&#30340;NTS&#25554;&#34917;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PGE-VAE&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#23450;&#20301;&#32534;&#30721;&#25216;&#26415;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#21512;&#24182;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#22270;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22270;&#29983;&#25104;&#32593;&#32476;&#26469;&#23398;&#20064;&#22270;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#30340;&#36793;&#24182;&#36866;&#24212;&#22270;&#21160;&#24577;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series (MTS) imputation is a widely studied problem in recent years. Existing methods can be divided into two main groups, including (1) deep recurrent or generative models that primarily focus on time series features, and (2) graph neural networks (GNNs) based models that utilize the topological information from the inherent graph structure of MTS as relational inductive bias for imputation. Nevertheless, these methods either neglect topological information or assume the graph structure is fixed and accurately known. Thus, they fail to fully utilize the graph dynamics for precise imputation in more challenging MTS data such as networked time series (NTS), where the underlying graph is constantly changing and might have missing edges. In this paper, we propose a novel approach to overcome these limitations. First, we define the problem of imputation over NTS which contains missing values in both node time series features and graph structures. Then, we design a new mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.18391</link><description>&lt;p&gt;
MemeGraphs: &#23558;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#36830;
&lt;/p&gt;
&lt;p&gt;
MemeGraphs: Linking Memes to Knowledge Graphs. (arXiv:2305.18391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18391
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21270;&#34920;&#36798;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30456;&#27604;&#20351;&#29992;&#23398;&#20064;&#34920;&#36798;&#24335;&#30340;&#27169;&#22411;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#26159;&#19968;&#31181;&#22312;&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#19978;&#27969;&#34892;&#30340;&#20256;&#25773;&#36235;&#21183;&#21644;&#35266;&#28857;&#30340;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#24189;&#40664;&#21644;&#35773;&#21050;&#65292;&#20294;&#20063;&#21487;&#33021;&#21547;&#26377;&#20882;&#29359;&#24615;&#30340;&#20869;&#23481;&#12290;&#33258;&#21160;&#20998;&#26512;&#21644;&#20998;&#31867;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#20381;&#36182;&#20110;&#23545;&#35270;&#35273;&#20803;&#32032;&#12289;&#35821;&#35328;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#26377;&#24847;&#20041;&#22320;&#34920;&#31034;&#36825;&#20123;&#26469;&#28304;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#20415;&#23558;&#34920;&#24773;&#21253;&#20316;&#20026;&#25972;&#20307;&#20998;&#31867;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22330;&#26223;&#22270;&#20316;&#20026;&#34920;&#31034;&#22270;&#20687;&#20013;&#29289;&#20307;&#21450;&#20854;&#35270;&#35273;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#36798;&#26041;&#24335;&#65292;&#24182;&#23558;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#32593;&#32476;&#25991;&#21270;&#34920;&#24773;&#21253;&#20998;&#31867;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;ImgBERT&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#20351;&#29992;&#20165;&#23398;&#20064;&#65288;&#32780;&#19981;&#26159;&#32467;&#26500;&#21270;&#65289;&#30340;&#34920;&#36798;&#24335;&#36827;&#34892;&#22810;&#27169;&#24335;&#24314;&#27169;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22987;&#32456;&#26377;&#25152;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#22270;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20379;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a popular form of communicating trends and ideas in social media and on the internet in general, combining the modalities of images and text. They can express humor and sarcasm but can also have offensive content. Analyzing and classifying memes automatically is challenging since their interpretation relies on the understanding of visual elements, language, and background knowledge. Thus, it is important to meaningfully represent these sources and the interaction between them in order to classify a meme as a whole. In this work, we propose to use scene graphs, that express images in terms of objects and their visual relations, and knowledge graphs as structured representations for meme classification with a Transformer-based architecture. We compare our approach with ImgBERT, a multimodal model that uses only learned (instead of structured) representations of the meme, and observe consistent improvements. We further provide a dataset with human graph annotations that we compa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;</title><link>http://arxiv.org/abs/2305.18088</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#30340;&#33647;&#29289;&#37325;&#29992;&#20197;&#38774;&#21521;COVID-19 3CL Protease
&lt;/p&gt;
&lt;p&gt;
Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach. (arXiv:2305.18088v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23376;&#23545;&#25509;&#21644;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#65292;&#31579;&#36873;&#20986;&#38024;&#23545; SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#28508;&#22312;&#27835;&#30103;&#33647;&#29289;&#12290;&#20854;&#20013;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#20581;&#24247;&#21361;&#26426;&#65292;&#36843;&#20999;&#38656;&#35201;&#24555;&#36895;&#37492;&#23450;&#28508;&#22312;&#30340;&#27835;&#30103;&#33647;&#29289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#33647;&#29289;&#37325;&#29992;&#26159;&#30465;&#26102;&#30465;&#21147;&#30340;&#21807;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Zinc&#25968;&#25454;&#24211;&#23545;&#20840;&#29699;&#24050;&#25209;&#20934;&#65288;&#21253;&#25324;FDA&#25209;&#20934;&#65289;&#30340;5903&#31181;&#33647;&#29289;&#36827;&#34892;&#31579;&#36873;&#65292;&#20316;&#20026;&#28508;&#22312;&#30340;COVID-19&#27835;&#30103;&#33647;&#29289;&#65292;&#20197;&#38774;&#21521;SARS-CoV-2&#30340;&#20027;&#35201;&#34507;&#30333;&#37238;3CL&#12290;&#25105;&#20204;&#20351;&#29992;Autodock-Vina&#36827;&#34892;&#20998;&#23376;&#23545;&#25509;&#65292;&#26816;&#26597;&#33647;&#29289;&#20998;&#23376;&#30340;&#21151;&#25928;&#12290;&#20026;&#20102;&#25552;&#39640;&#33647;&#29289;&#37325;&#29992;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20915;&#31574;&#26641;&#12289;&#39069;&#22806;&#26641;&#12289;MLP&#12289;KNN&#12289;XGBoost&#21644;&#26799;&#24230;&#25552;&#21319;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#24314;&#27169;&#32467;&#21512;&#33647;&#29289;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#22238;&#24402;&#65288;DTR&#65289;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#32479;&#35745;&#25514;&#26045;R2&#21644;RMSE&#12290;&#36825;&#20123;&#27169;&#25311;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#20855;&#26377;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#26377;&#21033;&#30340;&#32467;&#21512;&#33021;&#30340;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has created a global health crisis, driving the need for the rapid identification of potential therapeutics. To meet this challenge, drug repurposing is the only solution with saving cost and time. In this study, we used the Zinc database to screen the world-approved including FDA-approved 5903 drugs for repurposing as potential COVID-19 treatments targeting the main protease 3CL of SARS-CoV-2. We performed molecular docking using Autodock-Vina to check the efficacy of drug molecules. To enhance the efficiency of drug repurposing approach, we modeled the binding affinities using several machine learning regression approaches for QSAR modeling such as decision tree, extra trees, MLP, KNN, XGBoost, and gradient boosting. The computational results demonstrated that Decision Tree Regression (DTR) model has improved statistical measures of R2 and RMSE. These simulated results helped to identify drugs with high binding affinity and favorable binding energies. From the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17535</link><description>&lt;p&gt;
PFN&#26159;&#36866;&#29992;&#20110;&#23454;&#38469;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PFNs Are Flexible Models for Real-World Bayesian Optimization. (arXiv:2305.17535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28789;&#27963;&#30340;PFN&#20316;&#20026;BO&#20195;&#29702;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20801;&#35768;&#36827;&#19968;&#27493;&#20449;&#24687;&#32435;&#20837;&#20197;&#36827;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;(PFNs)&#20316;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#30340;&#28789;&#27963;&#20195;&#29702;&#12290;PFN&#26159;&#19968;&#31181;&#31070;&#32463;&#36807;&#31243;&#65292;&#34987;&#35757;&#32451;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;(PPD)&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#21487;&#26377;&#25928;&#37319;&#26679;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#26469;&#36827;&#34892;BO&#30340;&#20195;&#29702;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;PFN&#26469;&#27169;&#25311;&#19968;&#20010;&#26420;&#32032;&#39640;&#26031;&#36807;&#31243;(GP)&#65292;&#19968;&#20010;&#20808;&#36827;&#30340;GP&#21644;&#19968;&#20010;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNN)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36827;&#19968;&#27493;&#30340;&#20449;&#24687;&#32435;&#20837;&#20808;&#39564;&#65292;&#20363;&#22914;&#20801;&#35768;&#26377;&#20851;&#26368;&#20248;&#20301;&#32622;&#30340;&#25552;&#31034;(&#29992;&#25143;&#20808;&#39564;)&#65292;&#24573;&#30053;&#19981;&#30456;&#20851;&#30340;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#33719;&#21462;&#20989;&#25968;&#26469;&#25191;&#34892;&#38750;&#36828;&#35270;BO&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#28789;&#27963;&#24615;&#20026;&#20351;&#29992;PFN&#36827;&#34892;BO&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#20154;&#24037;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#27979;&#35797;&#24179;&#21488;&#19978;&#23637;&#31034;&#20102;PFN&#23545;BO&#30340;&#26377;&#29992;&#24615;&#65306;HPO-B&#12289;Bayesmark&#21644;PD1&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible surrogate for Bayesian Optimization (BO). PFNs are neural processes that are trained to approximate the posterior predictive distribution (PPD) for any prior distribution that can be efficiently sampled from. We describe how this flexibility can be exploited for surrogate modeling in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and a Bayesian Neural Network (BNN). In addition, we show how to incorporate further information into the prior, such as allowing hints about the position of optima (user priors), ignoring irrelevant dimensions, and performing non-myopic BO by learning the acquisition function. The flexibility underlying these extensions opens up vast possibilities for using PFNs for BO. We demonstrate the usefulness of PFNs for BO in a large-scale evaluation on artificial GP samples and three different hyperparameter optimization testbeds: HPO-B, Bayesmark, and PD1. We publish code 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16440</link><description>&lt;p&gt;
&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#36801;&#31227;&#23398;&#20064;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Transfer Learning via Multiple Pre-trained models for Linear Regression. (arXiv:2305.16440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#24456;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#39046;&#22495;&#65288;&#30446;&#26631;&#65289;&#19978;&#23398;&#20064;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#32452;&#22312;&#21487;&#33021;&#19981;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#65288;&#26469;&#28304;&#65289;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#65292;&#26469;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;i&#65289;&#21033;&#29992;&#19981;&#21516;&#30340;&#28304;&#34920;&#31034;&#26469;&#26500;&#36896;&#36866;&#24212;&#30446;&#26631;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#23558;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#20540;&#65292;&#36890;&#36807;&#24494;&#35843;&#31243;&#24207;&#65292;&#22312;&#30446;&#26631;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#65288;&#36229;&#21442;&#25968;&#65289;&#22238;&#24402;&#27169;&#22411;&#12290;&#23545;&#20110;&#35757;&#32451;&#26041;&#27861;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#27169;&#22411;&#19982;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#36229;&#39069;&#39118;&#38505;&#38480;&#21046;&#12290;&#23548;&#20986;&#30340;&#38480;&#21046;&#26174;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning a linear regression model on a data domain of interest (target) given few samples. To aid learning, we are provided with a set of pre-trained regression models that are trained on potentially different data domains (sources). Assuming a representation structure for the data generating linear models at the sources and the target domains, we propose a representation transfer based learning method for constructing the target model. The proposed scheme is comprised of two phases: (i) utilizing the different source representations to construct a representation that is adapted to the target data, and (ii) using the obtained model as an initialization to a fine-tuning procedure that re-trains the entire (over-parameterized) regression model on the target data. For each phase of the training method, we provide excess risk bounds for the learned model compared to the true data generating target model. The derived bounds show a gain in sample co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13599</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#23398;&#20064;&#29575;&#30340;&#20998;&#31163;&#24335;&#29702;&#24615;&#21270;: &#19968;&#31181;&#28789;&#27963;&#30340;Lipschitz&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Decoupled Rationalization with Asymmetric Learning Rates: A Flexible Lipshitz Restraint. (arXiv:2305.13599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#26469;&#35299;&#20915;&#30001;&#21512;&#20316;&#21338;&#24328;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25913;&#21892;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#33258;&#35828;&#26126;&#29702;&#24615;&#21270;&#27169;&#22411;&#36890;&#36807;&#21512;&#20316;&#21338;&#24328;&#26500;&#24314;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#36873;&#25321;&#26368;&#26131;&#29702;&#35299;&#30340;&#37096;&#20998;&#20316;&#20026;&#21407;&#29702;&#65292;&#25509;&#30528;&#39044;&#27979;&#22120;&#22522;&#20110;&#25152;&#36873;&#25321;&#30340;&#21407;&#29702;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#21338;&#24328;&#21487;&#33021;&#20250;&#24341;&#21457;&#36864;&#21270;&#38382;&#39064;&#65292;&#39044;&#27979;&#22120;&#36807;&#24230;&#25311;&#21512;&#20110;&#30001;&#23578;&#26410;&#35757;&#32451;&#22909;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#37096;&#20998;&#65292;&#21453;&#36807;&#26469;&#23548;&#33268;&#29983;&#25104;&#22120;&#25910;&#25947;&#20110;&#36235;&#21521;&#20110;&#36873;&#25321;&#26080;&#24847;&#20041;&#30340;&#37096;&#20998;&#30340;&#27425;&#20248;&#27169;&#22411;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#36864;&#21270;&#38382;&#39064;&#19982;&#39044;&#27979;&#22120;&#30340;Lipschitz&#36830;&#32493;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#12289;&#28789;&#27963;&#22320;&#32422;&#26463;&#39044;&#27979;&#22120;&#30340;Lipschitz&#24120;&#25968;&#65292;&#24182;&#35299;&#20915;&#20102;&#36864;&#21270;&#38382;&#39064;&#12290;DR&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23558;&#29983;&#25104;&#22120;&#21644;&#39044;&#27979;&#22120;&#20998;&#31163;&#65292;&#20026;&#23427;&#20204;&#20998;&#37197;&#19981;&#23545;&#31216;&#30340;&#23398;&#20064;&#29575;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DR&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#29616;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A self-explaining rationalization model is generally constructed by a cooperative game where a generator selects the most human-intelligible pieces from the input text as rationales, followed by a predictor that makes predictions based on the selected rationales. However, such a cooperative game may incur the degeneration problem where the predictor overfits to the uninformative pieces generated by a not yet well-trained generator and in turn, leads the generator to converge to a sub-optimal model that tends to select senseless pieces. In this paper, we theoretically bridge degeneration with the predictor's Lipschitz continuity. Then, we empirically propose a simple but effective method named DR, which can naturally and flexibly restrain the Lipschitz constant of the predictor, to address the problem of degeneration. The main idea of DR is to decouple the generator and predictor to allocate them with asymmetric learning rates. A series of experiments conducted on two widely used benchm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#27450;&#35784;&#25915;&#20987;&#21644;&#37096;&#20998;&#20266;&#36896;&#35821;&#38899;&#31561;ASV&#25915;&#20987;&#31867;&#22411;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#32508;&#36848;&#20013;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.12804</link><description>&lt;p&gt;
&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#20013;&#30340;&#38450;&#24481;&#32773;&#35282;&#24230;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The defender's perspective on automatic speaker verification: An overview. (arXiv:2305.12804v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38024;&#23545;&#27450;&#35784;&#25915;&#20987;&#21644;&#37096;&#20998;&#20266;&#36896;&#35821;&#38899;&#31561;ASV&#25915;&#20987;&#31867;&#22411;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#32508;&#36848;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;(ASV)&#22312;&#23433;&#20840;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;ASV&#30340;&#21487;&#38752;&#24615;&#24050;&#32463;&#21463;&#21040;&#27450;&#35784;&#25915;&#20987;(&#22914;&#37325;&#25918;&#21644;&#21512;&#25104;&#35821;&#38899;)&#12289;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#23545;&#36739;&#26032;&#30340;&#37096;&#20998;&#20266;&#36896;&#35821;&#38899;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#26377;&#20960;&#31687;&#32508;&#36848;&#35770;&#25991;&#28085;&#30422;&#20102;&#37325;&#25918;&#21644;&#21512;&#25104;&#35821;&#38899;&#12289;&#23545;&#25239;&#25915;&#20987;&#65292;&#20294;&#32570;&#20047;&#19968;&#31687;&#20840;&#38754;&#35780;&#20272;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#31867;&#22411;&#36827;&#34892;&#38450;&#24481;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#23545;&#36825;&#20123;&#25915;&#20987;&#31867;&#22411;&#20351;&#29992;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speaker verification (ASV) plays a critical role in security-sensitive environments. Regrettably, the reliability of ASV has been undermined by the emergence of spoofing attacks, such as replay and synthetic speech, as well as adversarial attacks and the relatively new partially fake speech. While there are several review papers that cover replay and synthetic speech, and adversarial attacks, there is a notable gap in a comprehensive review that addresses defense against adversarial attacks and the recently emerged partially fake speech. Thus, the aim of this paper is to provide a thorough and systematic overview of the defense methods used against these types of attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#22797;&#26434;&#30340;&#26102;&#31354;&#32852;&#21512;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#31354;&#20849;&#21516;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#33258;&#36866;&#24212;&#25429;&#25417;&#20107;&#20214;&#26102;&#38388;&#21644;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#22312;&#26102;&#31354;&#28857;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#21644;&#20107;&#20214;&#39044;&#27979;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12403</link><description>&lt;p&gt;
&#26102;&#31354;&#25193;&#25955;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal Diffusion Point Processes. (arXiv:2305.12403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#22797;&#26434;&#30340;&#26102;&#31354;&#32852;&#21512;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#31354;&#20849;&#21516;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#33258;&#36866;&#24212;&#25429;&#25417;&#20107;&#20214;&#26102;&#38388;&#21644;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#22312;&#26102;&#31354;&#28857;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#21644;&#20107;&#20214;&#39044;&#27979;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#28857;&#36807;&#31243;&#26159;&#24102;&#26377;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#38543;&#26426;&#20107;&#20214;&#38598;&#21512;&#12290;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;&#26102;&#31354;&#28857;&#36807;&#31243;&#35299;&#20915;&#26041;&#26696;&#22312;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#24067;&#26102;&#65292;&#20250;&#22949;&#21327;&#20110;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#26410;&#33021;&#24456;&#22909;&#22320;&#24314;&#27169;&#32852;&#21512;&#20998;&#24067;&#12290;&#36825;&#23548;&#33268;&#22312;&#25551;&#36848;&#19982;&#36807;&#21435;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#26102;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#22797;&#26434;&#30340;&#26102;&#31354;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#32852;&#21512;&#20998;&#24067;&#30340;&#23398;&#20064;&#20998;&#35299;&#20026;&#22810;&#20010;&#27493;&#39588;&#65292;&#20854;&#20013;&#27599;&#20010;&#27493;&#39588;&#21487;&#20197;&#34987;&#19968;&#20010;&#39640;&#26031;&#20998;&#24067;&#24456;&#22909;&#22320;&#25551;&#36848;&#12290;&#20026;&#20102;&#22686;&#24378;&#27599;&#20010;&#27493;&#39588;&#30340;&#23398;&#20064;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26102;&#31354;&#20849;&#21516;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#25417;&#20107;&#20214;&#26102;&#38388;&#21644;&#31354;&#38388;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#31532;&#19968;&#27425;&#25171;&#30772;&#20102;&#23545;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#38480;&#21046;&#30340;&#35268;&#23450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#28789;&#27963;&#30340;&#26102;&#31354;&#28857;&#36807;&#31243;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26102;&#31354;&#28857;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#21644;&#20107;&#20214;&#39044;&#27979;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal point process (STPP) is a stochastic collection of events accompanied with time and space. Due to computational complexities, existing solutions for STPPs compromise with conditional independence between time and space, which consider the temporal and spatial distributions separately. The failure to model the joint distribution leads to limited capacities in characterizing the spatio-temporal entangled interactions given past events. In this work, we propose a novel parameterization framework for STPPs, which leverages diffusion models to learn complex spatio-temporal joint distributions. We decompose the learning of the target joint distribution into multiple steps, where each step can be faithfully described by a Gaussian distribution. To enhance the learning of each step, an elaborated spatio-temporal co-attention module is proposed to capture the interdependence between the event time and space adaptively. For the first time, we break the restrictions on spatio-temp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.06446</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;: &#24322;&#27493;&#36890;&#20449;&#21644;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Multi-agent Reinforcement Learning: Asynchronous Communication and Linear Function Approximation. (arXiv:2305.06446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#22312;&#20445;&#35777;&#21512;&#20316;&#20248;&#21183;&#30340;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#25552;&#20379;&#21644;&#35777;&#26126;&#30340;&#31639;&#27861;&#21644;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#29702;&#35770;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#24773;&#33410;&#24335;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#35774;&#32622;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#20197;&#21512;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#26126;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#24322;&#27493;&#36890;&#20449;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#20316;&#20248;&#21183;&#19988;&#36890;&#20449;&#24320;&#38144;&#20302;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377; $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ &#30340;&#36951;&#25022;&#20540;&#21644; $\tilde{\mathcal{O}}(dHM^2)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013; $d$ &#26159;&#29305;&#24449;&#32500;&#25968;&#65292;$H$ &#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$M$ &#26159;&#26234;&#33021;&#20307;&#24635;&#25968;&#65292;$K$ &#26159;&#24635;&#24773;&#33410;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#38480;&#35777;&#26126;&#65292;&#34920;&#26126;&#36890;&#36807;&#21327;&#20316;&#33267;&#23569;&#38656;&#35201; $\Omega(dM)$ &#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#25165;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multi-agent reinforcement learning in the setting of episodic Markov decision processes, where multiple agents cooperate via communication through a central server. We propose a provably efficient algorithm based on value iteration that enable asynchronous communication while ensuring the advantage of cooperation with low communication overhead. With linear function approximation, we prove that our algorithm enjoys an $\tilde{\mathcal{O}}(d^{3/2}H^2\sqrt{K})$ regret with $\tilde{\mathcal{O}}(dHM^2)$ communication complexity, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the total number of agents, and $K$ is the total number of episodes. We also provide a lower bound showing that a minimal $\Omega(dM)$ communication complexity is required to improve the performance through collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04876</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24179;&#34892;RCNN&#19982;&#26032;&#39062;&#29305;&#24449;&#34920;&#31034;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Parallel RCNN with Novel Feature Representation for Time Series Forecasting. (arXiv:2305.04876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#24448;&#24448;&#20250;&#21463;&#21040;&#22806;&#37096;&#21327;&#21464;&#37327;&#65292;&#20363;&#22914;&#22825;&#27668;&#25110;&#20154;&#31867;&#24178;&#39044;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#24433;&#21709;&#21487;&#20197;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34987;&#21512;&#29702;&#22320;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#31216;&#20026;&#39044;&#27979;&#30340;&#26410;&#26469;&#21327;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#36845;&#20195;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#26368;&#32456;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#30340;&#35823;&#24046;&#32047;&#31215;&#12290;&#20854;&#20182;&#32771;&#34385;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20998;&#21035;&#22788;&#29702;&#21382;&#21490;&#21644;&#26410;&#26469;&#25968;&#25454;&#30340;&#31574;&#30053;&#20250;&#23545;&#33258;&#24049;&#20135;&#29983;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#34920;&#31034;&#31574;&#30053;&#8212;&#8212;&#31227;&#20301;&#27861;&#65292;&#23558;&#36807;&#21435;&#30340;&#25968;&#25454;&#21644;&#26410;&#26469;&#21327;&#21464;&#37327;&#34701;&#21512;&#36215;&#26469;&#20197;&#32771;&#34385;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24182;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30001;RNN&#21644;CNN&#32452;&#25104;&#65292;&#20108;&#32773;&#37117;&#34987;&#23618;&#32423;&#22320;&#20351;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;&#36339;&#36291;&#36830;&#25509;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#24213;&#23618;&#36807;&#31243;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate time series forecasting is a fundamental challenge in data science. It is often affected by external covariates such as weather or human intervention, which in many applications, may be predicted with reasonable accuracy. We refer to them as predicted future covariates. However, existing methods that attempt to predict time series in an iterative manner with autoregressive models end up with exponential error accumulations. Other strategies hat consider the past and future in the encoder and decoder respectively limit themselves by dealing with the historical and future data separately. To address these limitations, a novel feature representation strategy -- shifting -- is proposed to fuse the past data and future covariates such that their interactions can be considered. To extract complex dynamics in time series, we develop a parallel deep learning framework composed of RNN and CNN, both of which are used hierarchically. We also utilize the skip connection technique to impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;</title><link>http://arxiv.org/abs/2305.02279</link><description>&lt;p&gt;
Learngene: &#20174;&#31062;&#20808;&#27169;&#22411;&#20013;&#32487;&#25215;&#21387;&#32553;&#30693;&#35782;&#21040;&#21518;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learngene: Inheriting Condensed Knowledge from the Ancestry Model to Descendant Models. (arXiv:2305.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335; Learngene&#65292;&#23558;&#31215;&#32047;&#30340;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#24182;&#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#29983;&#29289;&#30340;&#36830;&#32493;&#36827;&#21270;&#36807;&#31243;&#20013;&#65292;&#23427;&#30340;&#22522;&#22240;&#31215;&#32047;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#21644;&#30693;&#35782;&#65292;&#20351;&#26032;&#29983;&#21518;&#20195;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20854;&#29305;&#23450;&#29615;&#22659;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#21363; Learngene&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#34701;&#21512;&#22522;&#22240;&#30340;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290; (i) &#31215;&#32047;&#65306;&#30693;&#35782;&#22312;&#31062;&#20808;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#31215;&#32047;&#12290; (ii) &#21387;&#32553;&#65306;&#23558;&#31215;&#32047;&#30340;&#35814;&#23613;&#30693;&#35782;&#21387;&#32553;&#25104;&#26356;&#20026;&#32039;&#20945;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#21363; Learngene&#12290; (iii) &#32487;&#25215;&#65306;&#23558;&#21387;&#32553;&#30340; Learngene &#32487;&#25215;&#32473;&#21518;&#20195;&#27169;&#22411;&#65292;&#20197;&#20415;&#20110;&#36866;&#24212;&#26032;&#30340;&#29615;&#22659;&#12290;&#30001;&#20110;&#31215;&#32047;&#24050;&#22312;&#19968;&#20123;&#25104;&#29087;&#30340;&#33539;&#24335;&#20013;&#24471;&#21040;&#30740;&#31350;&#65292;&#22914;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#32456;&#36523;&#23398;&#20064;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#21387;&#32553;&#21644;&#32487;&#25215;&#65292;&#36825;&#24341;&#21457;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the continuous evolution of one organism's ancestry, its genes accumulate extensive experiences and knowledge, enabling newborn descendants to rapidly adapt to their specific environments. Motivated by this observation, we propose a novel machine learning paradigm \textit{Learngene} to enable learning models to incorporate three key characteristics of genes. (i) Accumulating: the knowledge is accumulated during the continuous learning of an \textbf{ancestry model}. (ii) Condensing: the exhaustive accumulated knowledge is condensed into a much more compact information piece, \ie \textbf{learngene}. (iii): Inheriting: the condensed \textbf{learngene} is inherited to make it easier for \textbf{descendant models} to adapt to new environments. Since accumulating has been studied in some well-developed paradigms like large-scale pre-training and lifelong learning, we focus on condensing and inheriting, which induces three key issues and we provide the preliminary solutions to these is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14621</link><description>&lt;p&gt;
MUDiff: &#32479;&#19968;&#25193;&#25955;&#29983;&#25104;&#23436;&#25972;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14621
&lt;/p&gt;
&lt;p&gt;
MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20998;&#23376;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#20998;&#23376;&#30340;&#20840;&#38754;&#34920;&#31034;&#65292;&#21253;&#25324;&#21407;&#23376;&#29305;&#24449;&#12289;&#20108;&#32500;&#31163;&#25955;&#20998;&#23376;&#32467;&#26500;&#21644;&#19977;&#32500;&#36830;&#32493;&#20998;&#23376;&#22352;&#26631;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#36716;&#25442;&#22120;&#23545;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#26159;&#31561;&#21464;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#19981;&#21464;&#30340;&#21407;&#23376;&#21644;&#36793;&#30028;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#23376;&#22352;&#26631;&#30340;&#31561;&#21464;&#24615;&#12290;&#36825;&#31181;&#36716;&#25442;&#22120;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#23545;&#20960;&#20309;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21644;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#29983;&#25104;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.11839</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#65288;SSA&#65289;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#12290; SSA&#33021;&#22815;&#27604;&#20856;&#22411;&#30340;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#26356;&#24555;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#26059;&#65288;&#27010;&#29575;&#27604;&#29305;&#65289;&#30340;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#26469;&#30830;&#23450;&#36229;&#21442;&#25968;&#12290;&#33258;&#26059;&#26159;SSA&#30340;&#22522;&#26412;&#35745;&#31639;&#20803;&#32032;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#19982;&#20854;&#20182;&#33258;&#26059;&#36827;&#34892;&#22270;&#24418;&#36830;&#25509;&#12290;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#21487;&#20197;&#22522;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#36827;&#34892;&#20272;&#35745;&#12290;&#22522;&#20110;CLT&#30340;&#27491;&#24577;&#20998;&#24067;&#29992;&#20110;&#30830;&#23450;&#36229;&#21442;&#25968;&#65292;&#20854;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20256;&#32479;&#26041;&#27861;&#30340;O(n^3)&#38477;&#20302;&#21040;O(1)&#12290;&#20351;&#29992;&#30830;&#23450;&#30340;&#36229;&#21442;&#25968;&#35780;&#20272;&#20102;SSA&#22312;Gset&#21644;K2000&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#26368;&#22823;&#21106;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#24179;&#22343;&#21106;&#20540;&#30340;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#32959;&#30244;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#20986;&#26469;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.10039</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#22810;&#20998;&#31867;&#21644;&#20998;&#21106;&#22312;MRO&#22270;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Brain tumor multi classification and segmentation in MRO images using deep learning. (arXiv:2304.10039v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#32959;&#30244;&#20174;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#20986;&#26469;&#65292;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25195;&#25551;&#20013;&#23545;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;EfficientNetB1&#26550;&#26500;&#65292;&#32463;&#36807;&#35757;&#32451;&#65292;&#23558;&#22270;&#20687;&#20998;&#20026;&#22235;&#31867;&#65306;&#33041;&#33180;&#30244;&#65292;&#33014;&#36136;&#30244;&#65292;&#22402;&#20307;&#33146;&#30244;&#21644;&#26080;&#32959;&#30244;&#12290;&#20998;&#21106;&#27169;&#22411;&#22522;&#20110;U-Net&#26550;&#26500;&#65292;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;MRI&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#32959;&#30244;&#12290;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#20998;&#21106;&#25351;&#26631;&#65292;&#34920;&#26126;&#20854;&#22312;&#33041;&#32959;&#30244;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20020;&#24202;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a deep learning model for the classification and segmentation of brain tumors from magnetic resonance imaging (MRI) scans. The classification model is based on the EfficientNetB1 architecture and is trained to classify images into four classes: meningioma, glioma, pituitary adenoma, and no tumor. The segmentation model is based on the U-Net architecture and is trained to accurately segment the tumor from the MRI images. The models are evaluated on a publicly available dataset and achieve high accuracy and segmentation metrics, indicating their potential for clinical use in the diagnosis and treatment of brain tumors.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#33391;&#22909;&#30340;&#20445;&#23432;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2304.06262</link><description>&lt;p&gt;
&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31639;&#23376;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Canonical and Noncanonical Hamiltonian Operator Inference. (arXiv:2304.06262v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#20855;&#26377;&#25910;&#25947;&#24615;&#21644;&#33391;&#22909;&#30340;&#20445;&#23432;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35268;&#33539;&#21644;&#38750;&#35268;&#33539;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#38750;&#20405;&#20837;&#24335;&#21644;&#32467;&#26500;&#20445;&#30041;&#30340;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#31639;&#23376;&#25512;&#29702;&#30340;&#24605;&#24819;&#65292;&#36825;&#31181;&#25216;&#26415;&#26159;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#24555;&#29031;&#25968;&#25454;&#21644;&#31995;&#32479;&#21704;&#23494;&#39039;&#30340;&#28784;&#30418;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32447;&#24615;&#27714;&#35299;&#38382;&#39064;&#12290;&#36890;&#36807;&#20960;&#20010;&#28041;&#21450;&#22810;&#20010;&#21452;&#26354;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20363;&#23376;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#31616;&#21270;&#27169;&#22411;&#38500;&#20102;&#20934;&#30830;&#19988;&#31283;&#23450;&#22320;&#22788;&#29702;&#22522;&#30784;&#27169;&#24335;&#30340;&#28155;&#21152;&#22806;&#65292;&#36824;&#21487;&#20197;&#24456;&#22909;&#22320;&#20445;&#30041;&#20854;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20043;&#22806;&#30340;&#23432;&#24658;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method for the nonintrusive and structure-preserving model reduction of canonical and noncanonical Hamiltonian systems is presented. Based on the idea of operator inference, this technique is provably convergent and reduces to a straightforward linear solve given snapshot data and gray-box knowledge of the system Hamiltonian. Examples involving several hyperbolic partial differential equations show that the proposed method yields reduced models which, in addition to being accurate and stable with respect to the addition of basis modes, preserve conserved quantities well outside the range of their training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;DP-FedSAM&#65292;&#23427;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#29983;&#25104;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.11242</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#20013;&#38477;&#20302;&#26799;&#24230;&#24179;&#38754;&#21270;
&lt;/p&gt;
&lt;p&gt;
Make Landscape Flatter in Differentially Private Federated Learning. (arXiv:2303.11242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;DP-FedSAM&#65292;&#23427;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#29983;&#25104;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#24481;&#25512;&#29702;&#25915;&#20987;&#24182;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#27844;&#28431;&#65292;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DPFL&#65289;&#36890;&#36807;&#35009;&#21098;&#26412;&#22320;&#26356;&#26032;&#21644;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DPFL&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#26356;&#23574;&#38160;&#30340;&#25439;&#22833;&#24179;&#38754;&#65292;&#24182;&#20855;&#26377;&#36739;&#24046;&#30340;&#26435;&#37325;&#25200;&#21160;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DPFL&#31639;&#27861;DP-FedSAM&#65292;&#23427;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#26469;&#20943;&#36731;DP&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DP-FedSAM&#23558;Sharpness Aware Minimization&#65288;SAM&#65289;&#20248;&#21270;&#22120;&#38598;&#25104;&#21040;&#20854;&#20013;&#65292;&#29983;&#25104;&#26356;&#31283;&#23450;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#37325;&#25200;&#21160;&#40065;&#26834;&#24615;&#30340;&#26412;&#22320;&#24179;&#22374;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#26412;&#22320;&#26356;&#26032;&#30340;&#33539;&#25968;&#23567;&#19988;&#23545;DP&#22122;&#22768;&#26377;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;DP-FedSAM&#22914;&#20309;&#20943;&#36731;DP&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;R&#233;nyi&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#32473;&#20986;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20998;&#26512;&#65292;&#20197;&#20445;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#25932;&#23545;&#35774;&#32622;&#19979;&#65292;DP-FedSAM&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;DPFL&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11138</link><description>&lt;p&gt;
&#22522;&#20110;&#21344;&#25454;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fault Detection via Occupation Kernel Principal Component Analysis. (arXiv:2303.11138v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31995;&#32479;&#30340;&#21487;&#38752;&#25805;&#20316;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26816;&#27979;&#22522;&#30784;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#65292;&#20294;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#22240;&#20854;&#26131;&#20110;&#37096;&#32626;&#21644;&#23545;&#19987;&#23478;&#30693;&#35782;&#38656;&#27714;&#26368;&#23567;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21344;&#25454;&#26680;&#36827;&#34892;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#21344;&#25454;&#26680;&#20135;&#29983;&#30340;&#29305;&#24449;&#26144;&#23556;&#36866;&#29992;&#20110;&#27979;&#37327;&#25968;&#25454;&#65292;&#30001;&#20110;&#20351;&#29992;&#31215;&#20998;&#20855;&#26377;&#20869;&#22312;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;&#38271;&#24230;&#21487;&#21464;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#31995;&#32479;&#36712;&#36857;&#36827;&#34892;PCA&#12290;&#21344;&#25454;&#26680;PCA&#26041;&#27861;&#34987;&#29992;&#20110;&#24320;&#21457;&#19968;&#31181;&#37325;&#26500;&#35823;&#24046;&#26041;&#27861;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliable operation of automatic systems is heavily dependent on the ability to detect faults in the underlying dynamical system. While traditional model-based methods have been widely used for fault detection, data-driven approaches have garnered increasing attention due to their ease of deployment and minimal need for expert knowledge. In this paper, we present a novel principal component analysis (PCA) method that uses occupation kernels. Occupation kernels result in feature maps that are tailored to the measured data, have inherent noise-robustness due to the use of integration, and can utilize irregularly sampled system trajectories of variable lengths for PCA. The occupation kernel PCA method is used to develop a reconstruction error approach to fault detection and its efficacy is validated using numerical simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#21333;&#20010;ReLU&#31070;&#32463;&#20803;&#30340;&#26377;&#38480;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24863;&#30693;&#22120;&#31639;&#27861;GLM-tron&#30340;&#39118;&#38505;&#19978;&#19979;&#30028;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#27530;&#24773;&#20917;&#65292;&#20026;&#39640;&#32500;ReLU&#22238;&#24402;&#38382;&#39064;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#21051;&#30011;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#31216;&#20271;&#21162;&#21033;&#25968;&#25454;&#30340;ReLU&#22238;&#24402;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#36807;&#22810;&#39118;&#38505;&#19981;&#22914;GLM-tron&#12290;</title><link>http://arxiv.org/abs/2303.02255</link><description>&lt;p&gt;
&#39640;&#32500;&#21333;&#20010;ReLU&#31070;&#32463;&#20803;&#30340;&#26377;&#38480;&#26679;&#26412;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Analysis of Learning High-Dimensional Single ReLU Neuron. (arXiv:2303.02255v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#21333;&#20010;ReLU&#31070;&#32463;&#20803;&#30340;&#26377;&#38480;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24863;&#30693;&#22120;&#31639;&#27861;GLM-tron&#30340;&#39118;&#38505;&#19978;&#19979;&#30028;&#65292;&#20854;&#20013;&#21253;&#25324;&#29305;&#27530;&#24773;&#20917;&#65292;&#20026;&#39640;&#32500;ReLU&#22238;&#24402;&#38382;&#39064;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#21051;&#30011;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#31216;&#20271;&#21162;&#21033;&#25968;&#25454;&#30340;ReLU&#22238;&#24402;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#36807;&#22810;&#39118;&#38505;&#19981;&#22914;GLM-tron&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#36755;&#20837;&#32500;&#24230;&#21487;&#33021;&#36229;&#20986;&#26679;&#26412;&#25968;&#65289;&#65292;&#23398;&#20064;&#20855;&#26377;&#24179;&#26041;&#25439;&#22833;&#30340;&#21333;&#20010;ReLU&#31070;&#32463;&#20803;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#31216;&#20026;GLM-tron&#65288;Kakade&#31561;&#20154;&#65292;2011&#65289;&#30340;&#24863;&#30693;&#22120;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#32500;&#24230;&#26080;&#20851;&#30340;&#39118;&#38505;&#19978;&#30028;&#65292;&#29992;&#20110;&#39640;&#32500;ReLU&#22238;&#24402;&#30340;&#33391;&#22909;&#35268;&#23450;&#21644;&#35268;&#23450;&#38169;&#35823;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#39118;&#38505;&#19978;&#30028;&#24674;&#22797;&#20102;&#20960;&#20010;&#29616;&#26377;&#32467;&#26524;&#20316;&#20026;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#22312;&#33391;&#22909;&#35268;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;GLM-tron&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#20363;&#21305;&#37197;&#39118;&#38505;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#19978;&#19979;&#39118;&#38505;&#30028;&#25552;&#20379;&#20102;&#23545;&#21487;&#20197;&#36890;&#36807;GLM-tron&#23398;&#20064;&#30340;&#39640;&#32500;ReLU&#22238;&#24402;&#38382;&#39064;&#30340;&#28165;&#26224;&#21051;&#30011;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#38024;&#23545;&#23545;&#31216;&#20271;&#21162;&#21033;&#25968;&#25454;&#30340;ReLU&#22238;&#24402;&#25552;&#20379;&#20102;&#19968;&#20123;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#36127;&#38754;&#32467;&#26524;&#65306;&#22914;&#26524;&#27169;&#22411;&#35268;&#23450;&#33391;&#22909;&#65292;&#21017;SGD&#30340;&#36807;&#22810;&#39118;&#38505;&#21487;&#35777;&#26126;&#19981;&#27604;&#26080;&#35270;&#24120;&#25968;&#22240;&#32032;&#30340;GLM-tron&#30340;&#36807;&#22810;&#39118;&#38505;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of learning a single ReLU neuron with squared loss (a.k.a., ReLU regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called GLM-tron (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional ReLU regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for GLM-tron. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional ReLU regression problems that can be learned via GLM-tron. On the other hand, we provide some negative results for stochastic gradient descent (SGD) for ReLU regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of SGD is provably no better than that of GLM-tron ignoring constant fa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#30740;&#31350;&#23545;&#27604;&#20102;&#24403;&#21069;8&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#19968;&#31181;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#20248;&#21270;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#21482;&#26377;16.9%&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.14698</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#24456;&#38590;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#30340;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heuristic Modularity Maximization Algorithms for Community Detection Rarely Return an Optimal Partition or Anything Similar. (arXiv:2302.14698v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#30740;&#31350;&#23545;&#27604;&#20102;&#24403;&#21069;8&#31181;&#22522;&#20110;&#27169;&#22359;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#19968;&#31181;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#20248;&#21270;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;&#21482;&#26377;16.9%&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#25110;&#30456;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#35745;&#31639;&#31185;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#35774;&#35745;&#31639;&#27861;&#26469;&#22312;&#32593;&#32476;&#33410;&#28857;&#30340;&#19981;&#21516;&#21010;&#20998;&#20043;&#38388;&#26368;&#22823;&#21270;&#27169;&#22359;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#32972;&#26223;&#30340;80&#20010;&#30495;&#23454;&#21644;&#38543;&#26426;&#32593;&#32476;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#24403;&#21069;&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26368;&#22823;&#21270;&#31639;&#27861;&#22312;&#36820;&#22238;&#26368;&#22823;&#27169;&#22359;&#21270;&#65288;&#26368;&#20248;&#65289;&#21010;&#20998;&#26041;&#38754;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#65288;1&#65289;&#31639;&#27861;&#36755;&#20986;&#27169;&#22359;&#21270;&#19982;&#27599;&#20010;&#36755;&#20837;&#22270;&#30340;&#26368;&#22823;&#27169;&#22359;&#21270;&#20043;&#27604;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23427;&#20204;&#30340;&#36755;&#20986;&#21010;&#20998;&#19982;&#35813;&#22270;&#30340;&#20219;&#20309;&#26368;&#20248;&#21010;&#20998;&#20043;&#38388;&#30340;&#26368;&#22823;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#23558;&#20843;&#31181;&#29616;&#26377;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#19982;&#20840;&#23616;&#26368;&#22823;&#21270;&#27169;&#22359;&#21270;&#30340;&#31934;&#30830;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#24179;&#22343;&#22522;&#20110;&#27169;&#22359;&#21270;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21482;&#20026;&#32771;&#34385;&#30340;80&#20010;&#22270;&#30340;16.9%&#36820;&#22238;&#26368;&#20248;&#21010;&#20998;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#35843;&#25972;&#21518;&#30340;&#20114;&#20449;&#24687;&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a fundamental problem in computational sciences with extensive applications in various fields. The most commonly used methods are the algorithms designed to maximize modularity over different partitions of the network nodes. Using 80 real and random networks from a wide range of contexts, we investigate the extent to which current heuristic modularity maximization algorithms succeed in returning maximum-modularity (optimal) partitions. We evaluate (1) the ratio of the algorithms' output modularity to the maximum modularity for each input graph, and (2) the maximum similarity between their output partition and any optimal partition of that graph. We compare eight existing heuristic algorithms against an exact integer programming method that globally maximizes modularity. The average modularity-based heuristic algorithm returns optimal partitions for only 16.9% of the 80 graphs considered. Additionally, results on adjusted mutual information reveal substantial diss
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.13939</link><description>&lt;p&gt;
SpikeGPT&#65306;&#24102;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#28608;&#27963;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#30340;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#24050;&#32463;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;SNN&#30340;&#35757;&#32451;&#20063;&#34987;&#35777;&#26126;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#33853;&#21518;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#23578;&#26410;&#30475;&#21040;SNN&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;Receptance Weighted Key Value&#65288;RWKV&#65289;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#8220;SpikeGPT&#8221;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27169;&#22411;&#21464;&#20307;&#19978;&#35757;&#32451;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65306;45M&#21644;216M&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SpikeGPT&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNN&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#33033;&#20914;&#27169;&#22411;&#36890;&#24120;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
&lt;/p&gt;</description></item><item><title>normflows&#26159;&#19968;&#20010;&#29992;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;PyTorch&#21253;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#22522;&#26412;&#20998;&#24067;&#12289;&#27969;&#23618;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#20197;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;</title><link>http://arxiv.org/abs/2302.12014</link><description>&lt;p&gt;
normflows&#65306;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;PyTorch&#21253;
&lt;/p&gt;
&lt;p&gt;
normflows: A PyTorch Package for Normalizing Flows. (arXiv:2302.12014v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12014
&lt;/p&gt;
&lt;p&gt;
normflows&#26159;&#19968;&#20010;&#29992;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;PyTorch&#21253;&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#22522;&#26412;&#20998;&#24067;&#12289;&#27969;&#23618;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#20197;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#36890;&#36807;&#19968;&#20010;&#21487;&#34920;&#36798;&#30340;&#21487;&#35299;&#23494;&#24230;&#26469;&#24314;&#27169;&#27010;&#29575;&#20998;&#24067;&#12290;&#23427;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#21487;&#36870;&#20989;&#25968;&#65288;&#31216;&#20026;&#22270;&#23618;&#65289;&#23558;&#31616;&#21333;&#30340;&#22522;&#30784;&#20998;&#24067;&#65288;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#36716;&#25442;&#12290;&#36825;&#20123;&#22270;&#23618;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23454;&#29616;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27491;&#21017;&#21270;&#27969;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#24050;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#24314;&#27169;&#12289;&#21464;&#20998;&#25512;&#26029;&#12289;&#36924;&#36817;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#31561;&#20247;&#22810;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;normflows&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;Python&#21253;&#12290;&#23427;&#20801;&#35768;&#20174;&#19968;&#31995;&#21015;&#22522;&#26412;&#20998;&#24067;&#12289;&#27969;&#23618;&#21644;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#12290;&#35813;&#21253;&#37319;&#29992;&#20102;&#24191;&#21463;&#27426;&#36814;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;PyTorch&#23454;&#29616;&#65292;&#36825;&#20351;&#24471;&#23558;&#27969;&#38598;&#25104;&#21040;&#26356;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#27969;&#27700;&#32447;&#20013;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#23427;&#25903;&#25345;&#22823;&#37096;&#20998;&#24120;&#35265;&#30340;&#27491;&#21017;&#21270;&#27969;&#26550;&#26500;&#65292;&#22914;Real NVP&#12289;Glow&#12289;Masked Autoregressive Flows&#12289;Neural Spline Flows&#12289;Residual Flows&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows model probability distributions through an expressive tractable density. They transform a simple base distribution, such as a Gaussian, through a sequence of invertible functions, which are referred to as layers. These layers typically use neural networks to become very expressive. Flows are ubiquitous in machine learning and have been applied to image generation, text modeling, variational inference, approximating Boltzmann distributions, and many other problems. Here, we present normflows, a Python package for normalizing flows. It allows to build normalizing flow models from a suite of base distributions, flow layers, and neural networks. The package is implemented in the popular deep learning framework PyTorch, which simplifies the integration of flows in larger machine learning models or pipelines. It supports most of the common normalizing flow architectures, such as Real NVP, Glow, Masked Autoregressive Flows, Neural Spline Flows, Residual Flows, and many more.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>Stochastic GFlowNets&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#26469;&#25193;&#23637;&#20102;GFlowNets&#30340;&#36866;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#21407;&#27169;&#22411;&#21482;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.09465</link><description>&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Generative Flow Networks. (arXiv:2302.09465v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09465
&lt;/p&gt;
&lt;p&gt;
Stochastic GFlowNets&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#27169;&#22411;&#26469;&#25193;&#23637;&#20102;GFlowNets&#30340;&#36866;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#21407;&#27169;&#22411;&#21482;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;&#25110;&#31616;&#31216;GFlowNets&#65289;&#26159;&#19968;&#31867;&#36890;&#36807;&#8220;&#25512;&#29702;&#21363;&#25511;&#21046;&#8221;&#23398;&#20064;&#37319;&#26679;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#30340;&#27010;&#29575;&#20195;&#29702;&#12290;&#23427;&#20204;&#22312;&#20174;&#32473;&#23450;&#30340;&#33021;&#37327;&#26223;&#35266;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GFlowNets&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#29615;&#22659;&#65292;&#24182;&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#26356;&#19968;&#33324;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;GFlowNets&#65292;&#23427;&#23558;GFlowNets&#25193;&#23637;&#21040;&#38543;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#23558;&#29366;&#24577;&#36716;&#31227;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#38543;&#26426;GFlowNets&#20998;&#31163;&#20102;&#29615;&#22659;&#38543;&#26426;&#24615;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;&#21160;&#24577;&#27169;&#22411;&#26469;&#25429;&#25417;&#23427;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#26426;GFlowNets&#22312;&#20855;&#26377;&#38543;&#26426;&#21160;&#24577;&#30340;&#21508;&#31181;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;GFlowNets&#20197;&#21450;MCMC&#21644;&#22522;&#20110;RL&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of "inference as control". They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.09357</link><description>&lt;p&gt;
&#22312;&#32447;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;: &#36951;&#25022;&#20998;&#26512;&#21644;Bandit&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Online Instrumental Variable Regression: Regret Analysis and Bandit Feedback. (arXiv:2302.09357v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#20869;&#29983;&#24615;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;O2SLS&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35782;&#21035;&#29575;&#21644;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#29983;&#24615;&#26159;&#23454;&#38469;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#22240;&#20026;&#36951;&#28431;&#21464;&#37327;&#12289;&#25112;&#30053;&#34892;&#20026;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#21407;&#22240;&#23548;&#33268;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#26080;&#30028;&#22122;&#22768;&#21644;&#32447;&#24615;Bandit&#38543;&#26426;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#22806;&#29983;&#24615;&#65292;&#21363;&#22122;&#22768;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#22238;&#24402;&#22312;&#38543;&#26426;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#36229;&#35782;&#21035;&#21644;&#24688;&#22909;&#35782;&#21035;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;Two-Stage Least Squares&#26041;&#27861;&#30340;&#22312;&#32447;&#21464;&#20307;&#65288;&#21363;O2SLS&#65289;&#26469;&#22788;&#29702;&#20869;&#29983;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;O2SLS&#23454;&#29616;&#20102;$ \mathcal{O} \left(d_x d_z \log ^ 2 T \right)$&#30340;&#35782;&#21035;&#29575;&#21644;$ \tilde {\mathcal {O}} \left(\gamma \sqrt {d_x T} \right)$&#30340;&#39044;&#27979;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endogeneity, i.e. the dependence between noise and covariates, is a common phenomenon in real data due to omitted variables, strategic behaviours, measurement errors etc. In contrast, the existing analyses of stochastic online linear regression with unbounded noise and linear bandits depend heavily on exogeneity, i.e. the independence between noise and covariates. Motivated by this gap, we study the over-and just-identified Instrumental Variable (IV) regression for stochastic online learning. IV regression and the Two-Stage Least Squares approach to it are widely deployed in economics and causal inference to identify the underlying model from an endogenous dataset. Thus, we propose to use an online variant of Two-Stage Least Squares approach, namely O2SLS, to tackle endogeneity in stochastic online learning. Our analysis shows that O2SLS achieves $\mathcal{O}\left(d_x d_z \log ^2 T\right)$ identification and $\tilde{\mathcal{O}}\left(\gamma \sqrt{d_x T}\right)$ oracle regret after $T$ 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#65292;&#23545;&#29616;&#26377;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#23545;&#27604;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#19981;&#22914;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#26679;&#22909;&#65292;&#20316;&#32773;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06658</link><description>&lt;p&gt;
&#23547;&#25214;&#19968;&#20010;&#36866;&#29992;&#20110;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#36890;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In Search for a Generalizable Method for Source Free Domain Adaptation. (arXiv:2302.06658v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#65292;&#23545;&#29616;&#26377;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#23545;&#27604;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#19981;&#22914;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#26679;&#22909;&#65292;&#20316;&#32773;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#23558;&#29616;&#25104;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;SFDA&#25216;&#26415;&#24212;&#29992;&#20110;&#29983;&#29289;&#22768;&#23398;&#20013;&#19968;&#32452;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#20123;&#20559;&#31227;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#36890;&#24120;&#30740;&#31350;&#30340;&#20559;&#31227;&#38750;&#24120;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#30456;&#23545;&#25490;&#21517;&#19978;&#30340;&#34920;&#29616;&#19982;&#35270;&#35273;&#22522;&#20934;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#21516;&#65292;&#24182;&#19988;&#26377;&#26102;&#27604;&#23436;&#20840;&#27809;&#26377;&#33258;&#36866;&#24212;&#34920;&#29616;&#26356;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22312;&#25105;&#20204;&#30340;&#26032;&#20559;&#31227;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;SFDA&#26041;&#27861;&#24182;&#19981;&#20687;&#20043;&#21069;&#24819;&#35937;&#30340;&#37027;&#26679;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#21040;&#19981;&#21516;&#30340;&#27169;&#24577;&#21487;&#20197;&#25104;&#20026;&#35774;&#35745;&#26356;&#24378;&#22823;&#27169;&#22411;&#30340;&#26377;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free domain adaptation (SFDA) is compelling because it allows adapting an off-the-shelf model to a new domain using only unlabelled data. In this work, we apply existing SFDA techniques to a challenging set of naturally-occurring distribution shifts in bioacoustics, which are very different from the ones commonly studied in computer vision. We find existing methods perform differently relative to each other than observed in vision benchmarks, and sometimes perform worse than no adaptation at all. We propose a new simple method which outperforms the existing methods on our new shifts while exhibiting strong performance on a range of vision datasets. Our findings suggest that existing SFDA methods are not as generalizable as previously thought and that considering diverse modalities can be a useful avenue for designing more robust models.
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#32676;&#20307;&#38656;&#35201;&#35299;&#20915;&#30340;&#21313;&#20010;&#26680;&#24515;&#38382;&#39064;&#19982;&#35299;&#20915;&#26041;&#26696;&#34987;&#24635;&#32467;&#22312;&#26412;&#25991;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.02596</link><description>&lt;p&gt;
&#26032;&#8220;Sparseland&#8221;&#20013;&#25105;&#20204;&#23398;&#21040;&#30340;&#21313;&#20010;&#25945;&#35757;: &#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#32773;&#30340;&#31616;&#30701;&#25163;&#20876;
&lt;/p&gt;
&lt;p&gt;
Ten Lessons We Have Learned in the New "Sparseland": A Short Handbook for Sparse Neural Network Researchers. (arXiv:2302.02596v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02596
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#32676;&#20307;&#38656;&#35201;&#35299;&#20915;&#30340;&#21313;&#20010;&#26680;&#24515;&#38382;&#39064;&#19982;&#35299;&#20915;&#26041;&#26696;&#34987;&#24635;&#32467;&#22312;&#26412;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#19981;&#26029;&#21457;&#23637;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;(SNN)&#30740;&#31350;&#32676;&#20307;&#26381;&#21153;&#12290;&#25105;&#20204;&#23581;&#35797;&#24635;&#32467;&#20102;SNN&#20013;&#26368;&#24120;&#35265;&#30340;&#19968;&#20123;&#22256;&#24785;&#65292;&#24182;&#20174;&#35768;&#22810;&#20851;&#38190;&#26041;&#38754;&#24635;&#32467;&#20102;&#21313;&#20010;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#21253;&#25324;&#23494;&#38598;&#19982;&#31232;&#30095;&#65292;&#26080;&#32467;&#26500;&#31232;&#30095;&#19982;&#32467;&#26500;&#31232;&#30095;&#65292;&#20462;&#21098;&#19982;&#31232;&#30095;&#35757;&#32451;&#65292;&#31264;&#23494;&#21040;&#31232;&#30095;&#35757;&#32451;&#19982;&#31232;&#30095;&#21040;&#31232;&#30095;&#35757;&#32451;&#65292;&#38745;&#24577;&#31232;&#30095;&#21644;&#21160;&#24577;&#31232;&#30095;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article does not propose any novel algorithm or new hardware for sparsity. Instead, it aims to serve the "common good" for the increasingly prosperous Sparse Neural Network (SNN) research community. We attempt to summarize some most common confusions in SNNs, that one may come across in various scenarios such as paper review/rebuttal and talks - many drawn from the authors' own bittersweet experiences! We feel that doing so is meaningful and timely, since the focus of SNN research is notably shifting from traditional pruning to more diverse and profound forms of sparsity before, during, and after training. The intricate relationships between their scopes, assumptions, and approaches lead to misunderstandings, for non-experts or even experts in SNNs. In response, we summarize ten Q\&amp;As of SNNs from many key aspects, including dense vs. sparse, unstructured sparse vs. structured sparse, pruning vs. sparse training, dense-to-sparse training vs. sparse-to-sparse training, static spars
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#23548;&#21521;&#30340;&#21160;&#24577;&#32467;&#26500;&#30340;&#26041;&#26696;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#19968;&#20123;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#31283;&#23450;&#36712;&#36857;&#36861;&#36394;&#65292;&#21516;&#26102;&#35780;&#20272;&#20102;&#25152;&#24471;&#21040;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02529</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#23548;&#21521;&#30340;&#21160;&#24577;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Control-Oriented Dynamical Structure from Data. (arXiv:2302.02529v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#23548;&#21521;&#30340;&#21160;&#24577;&#32467;&#26500;&#30340;&#26041;&#26696;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#19968;&#20123;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#31283;&#23450;&#36712;&#36857;&#36861;&#36394;&#65292;&#21516;&#26102;&#35780;&#20272;&#20102;&#25152;&#24471;&#21040;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#23545;&#20110;&#24050;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#20063;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#21033;&#29992;&#21160;&#21147;&#23398;&#30340;&#29305;&#23450;&#32467;&#26500;&#20197;&#35825;&#23548;&#31283;&#23450;&#30340;&#38381;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#21253;&#25324;&#37027;&#20123;&#36866;&#29992;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#21487;&#33021;&#27809;&#26377;&#36275;&#22815;&#30340;&#24050;&#30693;&#32467;&#26500;&#26469;&#21487;&#38752;&#22320;&#32508;&#21512;&#19968;&#20010;&#31283;&#23450;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#29366;&#24577;&#20381;&#36182;&#30340;&#38750;&#32447;&#24615;&#36319;&#36394;&#25511;&#21046;&#22120;&#21046;&#23450;&#26041;&#26696;&#65292;&#22522;&#20110;&#19968;&#31181;&#29992;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#25511;&#21046;&#20223;&#23556;&#31995;&#32479;&#30340;&#29366;&#24577;&#20381;&#36182;&#22411;&#29790;&#21345;&#33922;&#26041;&#31243;&#12290;&#35813;&#20844;&#24335;&#20381;&#36182;&#20110;&#23450;&#20041;&#25511;&#21046;&#20223;&#23556;&#21160;&#21147;&#23398;&#30340;&#21521;&#37327;&#22330;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#22240;&#23376;&#20998;&#35299;&#65292;&#35813;&#20998;&#35299;&#24635;&#26159;&#22312;&#28201;&#21644;&#30340;&#20809;&#28369;&#24615;&#20551;&#35774;&#19979;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36825;&#31181;&#20998;&#35299;&#30340;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#26469;&#35777;&#26126;&#25152;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#29256;&#26412;&#22312;&#31283;&#23450;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#22312;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#26049;&#36793;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#24471;&#21040;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even for known nonlinear dynamical systems, feedback controller synthesis is a difficult problem that often requires leveraging the particular structure of the dynamics to induce a stable closed-loop system. For general nonlinear models, including those fit to data, there may not be enough known structure to reliably synthesize a stabilizing feedback controller. In this paper, we discuss a state-dependent nonlinear tracking controller formulation based on a state-dependent Riccati equation for general nonlinear control-affine systems. This formulation depends on a nonlinear factorization of the system of vector fields defining the control-affine dynamics, which always exists under mild smoothness assumptions. We propose a method for learning this factorization from a finite set of data. On a variety of simulated nonlinear dynamical systems, we empirically demonstrate the efficacy of learned versions of this controller in stable trajectory tracking. Alongside our learning method, we eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00491</link><description>&lt;p&gt;
&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Prototype Classifiers for Long-Tailed Recognition. (arXiv:2302.00491v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#29289;&#20307;&#30340;&#24130;&#24459;&#20998;&#24067;&#65292;&#38271;&#23614;&#35782;&#21035;(LTR)&#38382;&#39064;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;LTR&#20013;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#24037;&#20316;&#20351;&#29992;softmax&#20998;&#31867;&#22120;&#65292;&#20854;&#20855;&#26377;&#23558;&#20998;&#31867;&#22120;&#33539;&#25968;&#19982;&#32473;&#23450;&#31867;&#21035;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#30456;&#20851;&#32852;&#30340;&#20542;&#21521;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21407;&#22411;&#20998;&#31867;&#22120;&#19981;&#21463;&#36825;&#31181;&#32570;&#28857;&#30340;&#22256;&#25200;&#65292;&#24182;&#19988;&#21482;&#20351;&#29992;&#26368;&#36817;&#31867;&#24179;&#22343;&#20540;&#65288;NCM&#65289;&#21363;&#21487;&#20132;&#20184;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#21407;&#22411;&#26159;&#32463;&#39564;&#36136;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;LTR&#20013;&#65292;&#21407;&#22411;&#20998;&#31867;&#22120;&#20316;&#20026;softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#32852;&#21512;&#23398;&#20064;&#21407;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#22522;&#20110;&#27010;&#29575;&#20998;&#25968;&#19982;&#21407;&#22411;&#20043;&#38388;&#36317;&#31163;&#30340;&#24179;&#22343;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#22522;&#20110;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#30340;&#24615;&#36136;&#65292;&#36825;&#23548;&#33268;&#20102;&#31283;&#23450;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33258;&#36866;&#24212;&#24179;&#34913;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#25439;&#22833;&#30340;&#37325;&#35201;&#24615;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#21407;&#22411;&#20998;&#31867;&#22120;&#12290;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#22312;LTR&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#22312;&#20960;&#31181;&#26368;&#26032;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of long-tailed recognition (LTR) has received attention in recent years due to the fundamental power-law distribution of objects in the real-world. Most recent works in LTR use softmax classifiers that have a tendency to correlate classifier norm with the amount of training data for a given class. On the other hand, Prototype classifiers do not suffer from this shortcoming and can deliver promising results simply using Nearest-Class-Mean (NCM), a special case where prototypes are empirical centroids. However, the potential of Prototype classifiers as an alternative to softmax in LTR is relatively underexplored. In this work, we propose Prototype classifiers, which jointly learn prototypes that minimize average cross-entropy loss based on probability scores from distances to prototypes. We theoretically analyze the properties of Euclidean distance based prototype classifiers that leads to stable gradient-based optimization which is robust to outliers. We further enhance Prot
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26631;&#20934;FTRL&#31639;&#27861;&#20013;&#25554;&#20837;&#20351;&#29992;&#22810;&#20010;&#20989;&#25968;&#35780;&#20272;&#30340;&#20869;&#26680;&#20272;&#35745;&#22120;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#26102;&#21464;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;$\tilde{O}(t^{1/2})$&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2302.00358</link><description>&lt;p&gt;
&#36172;&#24466;&#20984;&#20248;&#21270;&#30340;&#20877;&#25506;&#35752;&#65306;FTRL&#23454;&#29616;&#20102;$\tilde{O}(t^{1/2})$&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation Revisited: FTRL Achieves $\tilde{O}(t^{1/2})$ Regret. (arXiv:2302.00358v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00358
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26631;&#20934;FTRL&#31639;&#27861;&#20013;&#25554;&#20837;&#20351;&#29992;&#22810;&#20010;&#20989;&#25968;&#35780;&#20272;&#30340;&#20869;&#26680;&#20272;&#35745;&#22120;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#26102;&#21464;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#23454;&#29616;$\tilde{O}(t^{1/2})$&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#22810;&#20010;&#20989;&#25968;&#35780;&#20272;&#30340;&#20869;&#26680;&#20272;&#35745;&#22120;&#21487;&#20197;&#36731;&#26494;&#36716;&#25442;&#20026;&#37319;&#26679;&#22522;&#20110;&#30340;&#36172;&#24466;&#20272;&#35745;&#22120;&#65292;&#20854;&#26399;&#26395;&#20540;&#31561;&#20110;&#21407;&#22987;&#20869;&#26680;&#20272;&#35745;&#20540;&#12290;&#23558;&#27492;&#31867;&#36172;&#24466;&#20272;&#35745;&#22120;&#25554;&#20837;&#26631;&#20934;FTRL&#31639;&#27861;&#20013;&#65292;&#21487;&#33719;&#24471;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#38024;&#23545;&#25932;&#23545;&#30340;&#26102;&#21464;&#20984;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;$\tilde{O}(t^{1/2})$&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that a kernel estimator using multiple function evaluations can be easily converted into a sampling-based bandit estimator with expectation equal to the original kernel estimate. Plugging such a bandit estimator into the standard FTRL algorithm yields a bandit convex optimisation algorithm that achieves $\tilde{O}(t^{1/2})$ regret against adversarial time-varying convex loss functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12896</link><description>&lt;p&gt;
&#37492;&#23450;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#21644;&#24378;&#38887;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Identifying Adversarially Attackable and Robust Samples. (arXiv:2301.12896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21738;&#20123;&#26679;&#26412;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20174;&#32780;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23558;&#24494;&#23567;&#30340;&#65292;&#38590;&#20197;&#24863;&#30693;&#30340;&#25200;&#21160;&#25554;&#20837;&#36755;&#20837;&#26679;&#26412;&#65292;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21457;&#29983;&#22823;&#37327;&#19981;&#26399;&#26395;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#29983;&#25104;&#21644;&#38450;&#24481;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20174;&#36755;&#20837;&#25968;&#25454;&#35282;&#24230;&#29702;&#35299;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26679;&#26412;&#25915;&#20987;&#24615;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#30830;&#23450;&#26368;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#25915;&#20987;&#24615;&#26679;&#26412;&#65289;&#65292;&#20174;&#32780;&#21453;&#36807;&#26469;&#30830;&#23450;&#26368;&#19981;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#30340;&#26679;&#26412;&#65288;&#24378;&#38887;&#26679;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#26410;&#35265;&#25968;&#25454;&#38598;&#20013;&#65292;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24378;&#38887;&#24615;&#26679;&#26412;&#12290;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22522;&#20110;&#31616;&#21333;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25514;&#26045;&#30456;&#27604;&#65292;&#28145;&#24230;&#25915;&#20987;&#24615;&#26816;&#27979;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks insert small, imperceptible perturbations to input samples that cause large, undesired changes to the output of deep learning models. Despite extensive research on generating adversarial attacks and building defense systems, there has been limited research on understanding adversarial attacks from an input-data perspective. This work introduces the notion of sample attackability, where we aim to identify samples that are most susceptible to adversarial attacks (attackable samples) and conversely also identify the least susceptible samples (robust samples). We propose a deep-learning-based method to detect the adversarially attackable and robust samples in an unseen dataset for an unseen target model. Experiments on standard image classification datasets enables us to assess the portability of the deep attackability detector across a range of architectures. We find that the deep attackability detector performs better than simple model uncertainty-based measures for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25512;&#24191;&#22240;&#26524;&#22270;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#24322;&#36136;&#24615;&#22240;&#26524;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#19981;&#21516;&#35843;&#33410;&#22240;&#32032;&#23545;&#27835;&#30103;&#25928;&#26524;&#21644;&#28508;&#22312;&#20013;&#20171;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#29983;&#27963;&#20013;&#39640;&#32500;&#24230;&#22330;&#26223;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2301.12383</link><description>&lt;p&gt;
&#20851;&#20110;&#24322;&#36136;&#24615;&#22240;&#26524;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
On Heterogeneous Treatment Effects in Heterogeneous Causal Graphs. (arXiv:2301.12383v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25512;&#24191;&#22240;&#26524;&#22270;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#24322;&#36136;&#24615;&#22240;&#26524;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#19981;&#21516;&#35843;&#33410;&#22240;&#32032;&#23545;&#27835;&#30103;&#25928;&#26524;&#21644;&#28508;&#22312;&#20013;&#20171;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#29983;&#27963;&#20013;&#39640;&#32500;&#24230;&#22330;&#26223;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#21644;&#20849;&#30149;&#26159;&#35768;&#22810;&#21307;&#30103;&#22256;&#22659;&#30340;&#20004;&#22823;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#38024;&#23545;&#26377;&#25928;&#27835;&#30103;&#30340;&#30740;&#31350;&#20197;&#21450;&#23545;&#28508;&#22312;&#31070;&#32463;&#29983;&#29289;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#25512;&#24191;&#22240;&#26524;&#22270;&#27169;&#22411;&#19982;&#28151;&#26434;&#21464;&#37327;&#20132;&#20114;&#21644;&#22810;&#20010;&#20013;&#20171;&#21464;&#37327;&#30340;&#27010;&#24565;&#65292;&#25226;&#24322;&#36136;&#24615;&#22240;&#26524;&#22270;&#65288;HCGs&#65289;&#36827;&#34892;&#20102;&#27010;&#25324;&#25551;&#36848;&#65292;&#36825;&#20123;&#19982;&#27835;&#30103;&#20132;&#20114;&#20316;&#29992;&#30340;&#28151;&#26434;&#21464;&#37327;&#34987;&#31216;&#20026;&#35843;&#33410;&#22240;&#32032;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#20135;&#29983;HCGs&#65292;&#32473;&#20986;&#19981;&#21516;&#30340;&#35843;&#33410;&#22240;&#32032;&#65292;&#26126;&#30830;&#25551;&#36848;&#27835;&#30103;&#25110;&#28508;&#22312;&#20013;&#20171;&#21464;&#37327;&#23545;&#32467;&#26524;&#30340;HCEs&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;HCE&#30340;&#29702;&#35770;&#24418;&#24335;&#65292;&#24182;&#22312;&#20010;&#20307;&#23618;&#38754;&#19978;&#25512;&#23548;&#20102;&#20854;&#29305;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#39640;&#32500;&#24230;&#22330;&#26223;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#32467;&#26500;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#32435;&#20837;&#20272;&#35745;&#35843;&#33410;&#22240;&#32032;&#20013;&#65292;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23545;HIV&#38451;&#24615;&#24739;&#32773;&#25106;&#28895;&#24178;&#39044;&#30340;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#31034;&#20363;&#26469;&#28436;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#21457;&#29616;&#20986;&#20808;&#21069;&#26410;&#35760;&#24405;&#30340;&#26032;&#30340;HCE&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#20010;&#21307;&#30103;&#39046;&#22495;&#20013;&#20026;&#29702;&#35299;&#24322;&#36136;&#24615;&#27835;&#30103;&#25928;&#24212;&#21644;&#25913;&#21892;&#20010;&#24615;&#21270;&#21307;&#23398;&#25552;&#20379;&#20102;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity and comorbidity are two interwoven challenges associated with various healthcare problems that greatly hampered research on developing effective treatment and understanding of the underlying neurobiological mechanism. Very few studies have been conducted to investigate heterogeneous causal effects (HCEs) in graphical contexts due to the lack of statistical methods. To characterize this heterogeneity, we first conceptualize heterogeneous causal graphs (HCGs) by generalizing the causal graphical model with confounder-based interactions and multiple mediators. Such confounders with an interaction with the treatment are known as moderators. This allows us to flexibly produce HCGs given different moderators and explicitly characterize HCEs from the treatment or potential mediators on the outcome. We establish the theoretical forms of HCEs and derive their properties at the individual level in both linear and nonlinear models. An interactive structural learning is developed to 
&lt;/p&gt;</description></item><item><title>&#38754;&#37096;&#27880;&#37322;&#26631;&#27880;&#32773;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20010;&#20154;&#29305;&#24449;&#20250;&#24433;&#21709;&#20854;&#25968;&#25454;&#26631;&#27880;&#30340;&#20844;&#27491;&#24615;&#65292;&#24378;&#35843;&#38656;&#35201;&#23545;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#20197;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2301.09902</link><description>&lt;p&gt;
&#25506;&#31350;&#38754;&#37096;&#27880;&#37322;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#27880;&#32773;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Labeler Bias in Face Annotation for Machine Learning. (arXiv:2301.09902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09902
&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#27880;&#37322;&#26631;&#27880;&#32773;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20010;&#20154;&#29305;&#24449;&#20250;&#24433;&#21709;&#20854;&#25968;&#25454;&#26631;&#27880;&#30340;&#20844;&#27491;&#24615;&#65292;&#24378;&#35843;&#38656;&#35201;&#23545;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#20197;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#36234;&#26469;&#36234;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#30340;&#19990;&#30028;&#20013;&#65292;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#30340;&#20262;&#29702;&#24433;&#21709;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#19968;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26631;&#27880;&#32773;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#20250;&#20026;&#35757;&#32451;&#21019;&#24314;&#26412;&#36136;&#19978;&#24102;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#38543;&#21518;&#23548;&#33268;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#23601;&#19994;&#12289;&#25945;&#32946;&#21644;&#25191;&#27861;&#31561;&#39046;&#22495;&#20013;&#20986;&#29616;&#19981;&#20934;&#30830;&#25110;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20154;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#20219;&#21153;&#65292;&#20197;&#35843;&#26597;&#21644;&#34913;&#37327;&#26631;&#27880;&#32773;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21442;&#19982;&#32773;&#25317;&#26377;&#24433;&#21709;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#26631;&#27880;&#32773;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#23545;&#25152;&#20998;&#37197;&#30340;&#27880;&#37322;&#26631;&#31614;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26631;&#27880;&#32773;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#24433;&#21709;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22312;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20013;&#24517;&#39035;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#65292;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world increasingly reliant on artificial intelligence, it is more important than ever to consider the ethical implications of artificial intelligence on humanity. One key under-explored challenge is labeler bias, which can create inherently biased datasets for training and subsequently lead to inaccurate or unfair decisions in healthcare, employment, education, and law enforcement. Hence, we conducted a study to investigate and measure the existence of labeler bias using images of people from different ethnicities and sexes in a labeling task. Our results show that participants possess stereotypes that influence their decision-making process and that labeler demographics impact assigned labels. We also discuss how labeler bias influences datasets and, subsequently, the models trained on them. Overall, a high degree of transparency must be maintained throughout the entire artificial intelligence training process to identify and correct biases in the data as early as possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#21516;&#26102;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#22810;&#31181;&#29616;&#23454;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2301.09732</link><description>&lt;p&gt;
&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attacks in Peer-to-Peer Federated Learning. (arXiv:2301.09732v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#21516;&#26102;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#22810;&#31181;&#29616;&#23454;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#24320;&#25918;&#20102;&#26333;&#20809;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#32531;&#35299;&#20102;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#65292;&#20294;&#23427;&#20173;&#20381;&#36182;&#20110;&#21487;&#20449;&#30340;&#32858;&#21512;&#26381;&#21153;&#22120;&#26469;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#22312;&#38544;&#31169;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#37117;&#25552;&#20379;&#20102;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;P2PFL&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#26465;&#20214;&#19979;&#35780;&#20272;&#25105;&#20204;&#30340;&#25915;&#20987;&#65292;&#21253;&#25324;&#22810;&#20010;&#22270;&#24418;&#25299;&#25169;&#12289;&#32593;&#32476;&#20013;&#26377;&#38480;&#30340;&#25932;&#23545;&#33021;&#35265;&#24230;&#20197;&#21450;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;FL&#20013;&#36866;&#24212;&#30340;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning applications rely on centralized learning processes, opening up the risk of exposure of their training datasets. While federated learning (FL) mitigates to some extent these privacy risks, it relies on a trusted aggregation server for training a shared global model. Recently, new distributed learning architectures based on Peer-to-Peer Federated Learning (P2PFL) offer advantages in terms of both privacy and reliability. Still, their resilience to poisoning attacks during training has not been investigated. In this paper, we propose new backdoor attacks for P2PFL that leverage structural graph properties to select the malicious nodes, and achieve high attack success, while remaining stealthy. We evaluate our attacks under various realistic conditions, including multiple graph topologies, limited adversarial visibility of the network, and clients with non-IID data. Finally, we show the limitations of existing defenses adapted from FL and design a new defense that su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#27169;&#22411;&#26412;&#36523;&#65292;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#21518;&#22788;&#29702;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05761</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#35775;&#38382;&#30340;&#26412;&#22320;&#27169;&#22411;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Local Model Explanations Without Model Access. (arXiv:2301.05761v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#27169;&#22411;&#26412;&#36523;&#65292;&#36890;&#36807;&#33258;&#20030;&#26041;&#27861;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#29983;&#25104;&#21518;&#22788;&#29702;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21518;&#22788;&#29702;&#35299;&#37322;&#21644;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#65292;&#24403;&#20165;&#20855;&#26377;&#27169;&#22411;&#30340;&#38745;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#26679;&#26412;&#26102;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35775;&#38382;&#27169;&#22411;&#26412;&#36523;&#12290;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#20250;&#22312;&#27169;&#22411;&#35780;&#20272;&#26114;&#36149;&#12289;&#24378;&#21046;&#23454;&#34892;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#24102;&#23485;&#38480;&#21046;&#12289;&#25110;&#38656;&#35201;&#23454;&#26102;&#65292;&#35774;&#22791;&#19978;&#30340;&#23454;&#29616;&#26102;&#20986;&#29616;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#33258;&#20030;&#26041;&#27861;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#22312;&#21033;&#29992;&#26377;&#38480;&#27169;&#22411;&#35810;&#38382;&#30340;&#26679;&#26412;&#29983;&#25104;&#35299;&#37322;&#26102;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#19982;&#32463;&#20856;&#22238;&#24402;&#20998;&#26512;&#30340;&#31616;&#21333;&#32622;&#20449;&#21306;&#38388;&#20197;&#21450;&#24403;&#21069;&#36125;&#21494;&#26031;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26377;&#21033;&#30340;&#21306;&#38388;&#23485;&#24230;&#21644;&#35206;&#30422;&#27010;&#29575;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#26412;&#26041;&#27861;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a model-agnostic algorithm for generating post-hoc explanations and uncertainty intervals for a machine learning model when only a static sample of inputs and outputs from the model is available, rather than direct access to the model itself. This situation may arise when model evaluations are expensive; when privacy, security and bandwidth constraints are imposed; or when there is a need for real-time, on-device explanations. Our algorithm uses a bootstrapping approach to quantify the uncertainty that inevitably arises when generating explanations from a finite sample of model queries. Through a simulation study, we show that the uncertainty intervals generated by our algorithm exhibit a favorable trade-off between interval width and coverage probability compared to the naive confidence intervals from classical regression analysis as well as current Bayesian approaches for quantifying explanation uncertainty. We further demonstrate the capabilities of our method by applying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#23545;&#35821;&#38899;&#36136;&#37327;&#21644;&#21487;&#25026;&#24230;&#30340;&#35780;&#20272;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04388</link><description>&lt;p&gt;
&#24863;&#30693;&#21644;&#39044;&#27979;&#65306;&#22522;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Perceive and predict: self-supervised speech representation based loss functions for speech enhancement. (arXiv:2301.04388v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#23545;&#35821;&#38899;&#36136;&#37327;&#21644;&#21487;&#25026;&#24230;&#30340;&#35780;&#20272;&#26377;&#26174;&#33879;&#20316;&#29992;&#65292;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#22312;&#35821;&#38899;&#22686;&#24378;&#26041;&#38754;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#20851;&#35821;&#38899;&#22686;&#24378;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#26469;&#36741;&#21161;&#31070;&#32463;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#30456;&#20851;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#30340;&#26368;&#28145;&#25110;&#26368;&#32456;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#36739;&#26089;&#30340;&#29305;&#24449;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#24335;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#32463;&#24120;&#32570;&#20047;&#20805;&#20998;&#30340;&#21160;&#26426;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#24178;&#20928;&#21644;&#24102;&#22122;&#22768;&#35821;&#38899;&#30340;&#29305;&#24449;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#19982;&#24515;&#29702;&#22768;&#23398;&#34913;&#37327;&#26631;&#20934;&#20197;&#21450;&#20154;&#31867;&#24179;&#22343;&#24847;&#35265;&#24471;&#20998;&#26174;&#33879;&#30456;&#20851;&#12290;&#23454;&#39564;&#20351;&#29992;&#27492;&#36317;&#31163;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;(PESQ)&#21644;&#30701;&#26102;&#35821;&#38899;&#23458;&#35266;&#36136;&#37327;&#35780;&#20272;(STOI)&#31561;&#23458;&#35266;&#24230;&#37327;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#22522;&#20110;STFT&#39057;&#35889;&#36317;&#31163;&#21644;&#20854;&#20182;&#24120;&#35265;&#35821;&#38899;&#22686;&#24378;&#25991;&#29486;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and shor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#24212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#21644;&#33410;&#28857;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01835</link><description>&lt;p&gt;
&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Statistical Solver for Distribution System State Estimation. (arXiv:2301.01835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#24212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#21644;&#33410;&#28857;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20934;&#30830;&#30340;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#65288;DSSE&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#35266;&#27979;&#24615;&#19981;&#36275;&#21644;&#37197;&#30005;&#31995;&#32479;&#23494;&#24230;&#39640;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#26041;&#26696;&#21487;&#33021;&#26159;&#19968;&#31181;&#36873;&#25321;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#65292;&#23427;&#20204;&#22312;DSSE&#20013;&#21463;&#21040;&#24433;&#21709;&#12290;&#23454;&#38469;&#19978;&#65292;&#37197;&#30005;&#31995;&#32479;&#20013;&#30340;&#27979;&#37327;&#24448;&#24448;&#26159;&#22024;&#26434;&#12289;&#25439;&#22351;&#21644;&#19981;&#21487;&#29992;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#29992;&#20110;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#30340;&#28145;&#24230;&#32479;&#35745;&#27714;&#35299;&#22120;&#65288;DSS$^2$&#65289;&#65292;&#23427;&#32771;&#34385;&#21040;&#20102;&#37197;&#30005;&#31995;&#32479;&#30340;&#32593;&#32476;&#32467;&#26500;&#21644;&#29289;&#29702;&#25511;&#21046;&#21151;&#29575;&#27969;&#26041;&#31243;&#12290;DSS$^2$&#21033;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#37197;&#30005;&#31995;&#32479;&#30340;&#24322;&#26500;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#20197;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26356;&#26032;&#20854;&#28508;&#22312;&#34920;&#31034;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20197;&#23398;&#20064;&#20248;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;DSS$^2$&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSS$^2$&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20272;&#35745;&#31934;&#24230;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementing accurate Distribution System State Estimation (DSSE) faces several challenges, among which the lack of observability and the high density of the distribution system. While data-driven alternatives based on Machine Learning models could be a choice, they suffer in DSSE because of the lack of labeled data. In fact, measurements in the distribution system are often noisy, corrupted, and unavailable. To address these issues, we propose the Deep Statistical Solver for Distribution System State Estimation (DSS$^2$), a deep learning model based on graph neural networks (GNNs) that accounts for the network structure of the distribution system and for the physical governing power flow equations. DSS$^2$ leverages hypergraphs to represent the heterogeneous components of the distribution systems and updates their latent representations via a node-centric message-passing scheme. A weakly supervised learning approach is put forth to train the DSS$^2$ in a learning-to-optimize fashion w
&lt;/p&gt;</description></item><item><title>DMOps&#26088;&#22312;&#25351;&#23548;&#24037;&#19994;&#30028;&#20248;&#21270;&#26500;&#24314;NLP&#20135;&#21697;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#22522;&#32447;&#26469;&#31616;&#21270;&#25968;&#25454;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2301.01228</link><description>&lt;p&gt;
DMOps&#65306;&#25968;&#25454;&#31649;&#29702;&#25805;&#20316;&#21644;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
DMOps: Data Management Operation and Recipes. (arXiv:2301.01228v3 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01228
&lt;/p&gt;
&lt;p&gt;
DMOps&#26088;&#22312;&#25351;&#23548;&#24037;&#19994;&#30028;&#20248;&#21270;&#26500;&#24314;NLP&#20135;&#21697;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#22522;&#32447;&#26469;&#31616;&#21270;&#25968;&#25454;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;AI&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27969;&#31243;&#20013;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290; &#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#21644;&#25919;&#24220;&#37096;&#38376;&#24050;&#25552;&#20986;&#21508;&#31181;NLP&#25968;&#25454;&#30740;&#31350;&#35745;&#21010;&#12290; &#34429;&#28982;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#24037;&#19994;&#30028;&#65292;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#20026;&#20851;&#38190;&#12290;&#37492;&#20110;&#36825;&#19968;&#36235;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25968;&#25454;&#31649;&#29702;&#25805;&#20316;&#21644;&#37197;&#26041;&#8221;&#65292;&#26088;&#22312;&#25351;&#23548;&#24037;&#19994;&#30028;&#20248;&#21270;NLP&#20135;&#21697;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DMOps&#30340;&#27010;&#24565;&#65292;&#23427;&#26469;&#33258;&#20110;&#22788;&#29702;&#23454;&#38469;NLP&#25968;&#25454;&#31649;&#29702;&#30340;&#32463;&#39564;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#22522;&#32447;&#26469;&#31616;&#21270;&#25968;&#25454;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric AI has shed light on the significance of data within the machine learning (ML) pipeline. Recognizing its significance, academia, industry, and government departments have suggested various NLP data research initiatives. While the ability to utilize existing data is essential, the ability to build a dataset has become more critical than ever, especially in the industry. In consideration of this trend, we propose a "Data Management Operations and Recipes" to guide the industry in optimizing the building of datasets for NLP products. This paper presents the concept of DMOps which is derived from real-world experiences with NLP data management and aims to streamline data operations by offering a baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; QUBO &#30340;&#26032;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19982;&#33410;&#28857;&#25968;&#37327;&#30456;&#21516;&#30340;&#37327;&#23376;&#27604;&#29305;&#65292;&#24182;&#20197;&#19982;&#36755;&#20837;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#19968;&#26679;&#31232;&#30095;&#30340; QUBO &#30697;&#38453;&#34920;&#31034;&#65292;&#36890;&#36807;&#20998;&#31163;&#33410;&#28857;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102; QUBO &#30697;&#38453;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35782;&#21035;&#20998;&#31163;&#33410;&#28857;&#21363;&#21487;&#39640;&#25928;&#35782;&#21035;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2212.14717</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31163;&#33410;&#28857;&#35782;&#21035;&#30340; NISQ-ready &#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NISQ-ready community detection based on separation-node identification. (arXiv:2212.14717v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; QUBO &#30340;&#26032;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19982;&#33410;&#28857;&#25968;&#37327;&#30456;&#21516;&#30340;&#37327;&#23376;&#27604;&#29305;&#65292;&#24182;&#20197;&#19982;&#36755;&#20837;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#19968;&#26679;&#31232;&#30095;&#30340; QUBO &#30697;&#38453;&#34920;&#31034;&#65292;&#36890;&#36807;&#20998;&#31163;&#33410;&#28857;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102; QUBO &#30697;&#38453;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35782;&#21035;&#20998;&#31163;&#33410;&#28857;&#21363;&#21487;&#39640;&#25928;&#35782;&#21035;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#32467;&#26500;&#20998;&#26512;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20174;&#29983;&#29289;&#23398;&#21040;&#31038;&#20250;&#23398;&#12290;&#23558;&#36825;&#20123;&#32593;&#32476;&#32858;&#31867;&#25104;&#20998;&#21306;&#30340;&#35745;&#31639;&#20219;&#21153;&#65292;&#21363;&#35299;&#20915;&#31038;&#21306;&#26816;&#27979;&#38382;&#39064;&#65292;&#36890;&#24120;&#26159; NP-hard &#30340;&#65292;&#22240;&#27492;&#21551;&#21457;&#24335;&#35299;&#20915;&#26041;&#26696;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22312;&#26032;&#20852;&#30340;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#20013;&#65292;&#23545;&#26377;&#25928;&#21551;&#21457;&#24335;&#30340;&#25506;&#32034;&#24050;&#32463;&#23548;&#33268;&#20102;&#29305;&#21035;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#25152;&#26377;&#24050;&#24314;&#31435;&#30340;&#37327;&#23376;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#37117;&#23545;&#30828;&#20214;&#23384;&#22312;&#37325;&#22823;&#35201;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110; QUBO &#30340;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#19982;&#33410;&#28857;&#25968;&#37327;&#30456;&#21516;&#30340;&#37327;&#23376;&#27604;&#29305;&#65292;&#24182;&#20197;&#19982;&#36755;&#20837;&#22270;&#30340;&#37051;&#25509;&#30697;&#38453;&#19968;&#26679;&#31232;&#30095;&#30340; QUBO &#30697;&#38453;&#34920;&#31034;&#12290;&#36890;&#36807;&#20998;&#31163;&#33410;&#28857;&#30340;&#26032;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102; QUBO &#30697;&#38453;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#36825;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#36890;&#24120;&#38750;&#24120;&#23494;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#26159;&#30452;&#25509;&#23558;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#21040;&#19968;&#20010;&#31038;&#21306;&#20013;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20998;&#31163;&#33410;&#28857;&#30340;&#35782;&#21035;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#22320;&#35782;&#21035;&#31038;&#21306;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#22270;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#29366;&#24577;-of-the-art &#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of network structure is essential to many scientific areas, ranging from biology to sociology. As the computational task of clustering these networks into partitions, i.e., solving the community detection problem, is generally NP-hard, heuristic solutions are indispensable. The exploration of expedient heuristics has led to the development of particularly promising approaches in the emerging technology of quantum computing. Motivated by the substantial hardware demands for all established quantum community detection approaches, we introduce a novel QUBO based approach that only needs number-of-nodes many qubits and is represented by a QUBO-matrix as sparse as the input graph's adjacency matrix. The substantial improvement on the sparsity of the QUBO-matrix, which is typically very dense in related work, is achieved through the novel concept of separation-nodes. Instead of assigning every node to a community directly, this approach relies on the identification of a separati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;</title><link>http://arxiv.org/abs/2212.08057</link><description>&lt;p&gt;
&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#31070;&#32463;&#20809;&#22330;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Real-Time Neural Light Field on Mobile Devices. (arXiv:2212.08057v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#31070;&#32463;&#28210;&#26579;&#22330;&#65288;NeRF&#65289;&#22312;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;3D&#22330;&#26223;&#65292;&#24182;&#23454;&#29616;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#30001;&#20110;&#20307;&#31215;&#28210;&#26579;&#30340;&#36807;&#31243;&#65292;NeRF&#30340;&#25512;&#29702;&#36895;&#24230;&#38750;&#24120;&#32531;&#24930;&#65292;&#38480;&#21046;&#20102;&#22312;&#31227;&#21160;&#35774;&#22791;&#31561;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#21033;&#29992;NeRF&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#26377;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#20943;&#23569;&#36816;&#34892;NeRF&#27169;&#22411;&#30340;&#24310;&#36831;&#12290;&#20294;&#26159;&#65292;&#20182;&#20204;&#22823;&#22810;&#20173;&#38656;&#35201;&#39640;&#31471;GPU&#36827;&#34892;&#21152;&#36895;&#25110;&#39069;&#22806;&#30340;&#23384;&#20648;&#20869;&#23384;&#65292;&#36825;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#37117;&#19981;&#21487;&#29992;&#12290;&#21478;&#19968;&#20010;&#26032;&#20986;&#29616;&#30340;&#26041;&#21521;&#21017;&#21033;&#29992;&#31070;&#32463;&#20809;&#22330;&#65288;NeLF&#65289;&#26469;&#36827;&#34892;&#21152;&#36895;&#65292;&#22240;&#20026;&#19968;&#26465;&#23556;&#32447;&#19978;&#21482;&#38656;&#36827;&#34892;&#19968;&#27425;&#21521;&#21069;&#20256;&#36882;&#21363;&#21487;&#39044;&#27979;&#20687;&#32032;&#39068;&#33394;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#19982;NeRF&#31867;&#20284;&#30340;&#28210;&#26579;&#36136;&#37327;&#65292;NeLF&#20013;&#30340;&#32593;&#32476;&#35774;&#35745;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#65292;&#36825;&#23545;&#31227;&#21160;&#35774;&#22791;&#24182;&#19981;&#21451;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#39640;&#25928;&#32593;&#32476;&#65292;&#29992;&#20110;&#31070;&#32463;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts in Neural Rendering Fields (NeRF) have shown impressive results on novel view synthesis by utilizing implicit neural representation to represent 3D scenes. Due to the process of volumetric rendering, the inference speed for NeRF is extremely slow, limiting the application scenarios of utilizing NeRF on resource-constrained hardware, such as mobile devices. Many works have been conducted to reduce the latency of running NeRF models. However, most of them still require high-end GPU for acceleration or extra storage memory, which is all unavailable on mobile devices. Another emerging direction utilizes the neural light field (NeLF) for speedup, as only one forward pass is performed on a ray to predict the pixel color. Nevertheless, to reach a similar rendering quality as NeRF, the network in NeLF is designed with intensive computation, which is not mobile-friendly. In this work, we propose an efficient network that runs in real-time on mobile devices for neural rendering. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#20174;12&#23548;&#32852;ECG&#25968;&#25454;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;LBBB&#24515;&#24459;&#19981;&#40784;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.8%&#65292;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.04936</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;-&#20851;&#27880;&#27169;&#22411;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#24038;&#26463;&#25903;&#20256;&#23548;&#38459;&#28382;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Deep conv-attention model for diagnosing left bundle branch block from 12-lead electrocardiograms. (arXiv:2212.04936v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#20174;12&#23548;&#32852;ECG&#25968;&#25454;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;LBBB&#24515;&#24459;&#19981;&#40784;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.8%&#65292;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#37325;&#26032;&#21516;&#27493;&#27835;&#30103;&#65288;CRT&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#34917;&#20607;&#24515;&#36339;&#19981;&#35268;&#21017;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#27835;&#30103;&#22312;&#24739;&#26377;&#24038;&#26463;&#25903;&#20256;&#23548;&#38459;&#28382;&#65288;LBBB&#65289;&#24515;&#24459;&#19981;&#40784;&#30340;&#24515;&#33039;&#24739;&#32773;&#20013;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#35813;&#24515;&#24459;&#19981;&#40784;&#26159;&#30830;&#23450;&#26159;&#21542;&#20351;&#29992;CRT&#30340;&#37325;&#35201;&#21021;&#22987;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26816;&#27979;LBBB&#30340;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#20934;&#30830;&#30340;&#26041;&#27861;&#20174;ECG&#25968;&#25454;&#20013;&#35786;&#26029;&#36825;&#31181;&#24515;&#24459;&#19981;&#40784;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;12&#23548;&#32852;ECG&#25968;&#25454;&#26816;&#27979;LBBB&#24515;&#24459;&#19981;&#40784;&#12290;&#35813;&#27169;&#22411;&#30001;1D&#31232;&#30095;&#21367;&#31215;&#23618;&#26500;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#37325;&#35201;&#36755;&#20837;&#25968;&#25454;&#12290;&#35813;&#27169;&#22411;&#22312;PTB&#35786;&#26029;ECG&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;99.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac resynchronization therapy (CRT) is a treatment that is used to compensate for irregularities in the heartbeat. Studies have shown that this treatment is more effective in heart patients with left bundle branch block (LBBB) arrhythmia. Therefore, identifying this arrhythmia is an important initial step in determining whether or not to use CRT. On the other hand, traditional methods for detecting LBBB on electrocardiograms (ECG) are often associated with errors. Thus, there is a need for an accurate method to diagnose this arrhythmia from ECG data. Machine learning, as a new field of study, has helped to increase human systems' performance. Deep learning, as a newer subfield of machine learning, has more power to analyze data and increase systems accuracy. This study presents a deep learning model for the detection of LBBB arrhythmia from 12-lead ECG data. This model consists of 1D dilated convolutional layers. Attention mechanism has also been used to identify important input da
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.04407</link><description>&lt;p&gt;
&#21487;&#21464;&#21270;&#20915;&#31574;&#39057;&#29575;&#30340;&#36873;&#39033;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CTCO&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#65292;&#30456;&#27604;&#20256;&#32479;RL&#21644;&#26102;&#38388;&#25277;&#35937;RL&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20195;&#29702;&#22312;&#31163;&#25955;&#21644;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#20915;&#31574;&#20043;&#38388;&#30340;&#25345;&#32493;&#26102;&#38388;&#21464;&#25104;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#35774;&#32622;&#24471;&#22826;&#30701;&#21487;&#33021;&#20250;&#22686;&#21152;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#38656;&#35201;&#20195;&#29702;&#36827;&#34892;&#22810;&#27425;&#20915;&#31574;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#32780;&#35774;&#32622;&#24471;&#22826;&#38271;&#20250;&#23548;&#33268;&#20195;&#29702;&#22833;&#21435;&#23545;&#31995;&#32479;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#31995;&#32479;&#19981;&#19968;&#23450;&#38656;&#35201;&#24658;&#23450;&#30340;&#25511;&#21046;&#39057;&#29575;&#65292;&#23545;&#20110;&#23398;&#20064;&#20195;&#29702;&#26469;&#35828;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#24403;&#38656;&#35201;&#26102;&#20197;&#39640;&#39057;&#29575;&#36816;&#34892;&#65292;&#32780;&#22312;&#21487;&#33021;&#26102;&#20197;&#20302;&#39057;&#29575;&#36816;&#34892;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#36873;&#39033; (CTCO) &#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#36873;&#39033;&#20316;&#20026;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#23376;&#31574;&#30053;&#12290;&#36825;&#20123;&#36873;&#39033;&#26159;&#26102;&#38388;&#36830;&#32493;&#30340;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#39057;&#29575;&#19982;&#31995;&#32479;&#20132;&#20114;&#65292;&#20174;&#32780;&#25552;&#20379;&#24179;&#28369;&#30340;&#21160;&#20316;&#21464;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24615;&#33021;&#19982;&#20256;&#32479; RL &#21644;&#26102;&#38388;&#25277;&#35937; RL &#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102; CTCO &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.03853</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03853
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;&#65288;CNNI&#65289;&#12290;CNNI&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#20223;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#27979;&#35797;&#26032;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19982;K&#22343;&#20540;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#31561;&#20854;&#20182;&#32858;&#31867;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;CNNI&#21487;&#20197;&#27491;&#30830;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65307;CNNI&#37197;&#22791;&#20102;MMJ-SC&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#65288;&#38750;&#24179;&#38754;&#20960;&#20309;&#65289;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#65288;&#24402;&#32435;&#24335;&#65289;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#22810;&#37325;&#26816;&#39564;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2211.16059</link><description>&lt;p&gt;
&#20851;&#20110;&#32593;&#32476;&#35268;&#27169;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#37325;&#26816;&#39564;&#65306;&#19968;&#31181;&#28176;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Large-Scale Multiple Testing Over Networks: An Asymptotic Approach. (arXiv:2211.16059v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#22810;&#37325;&#26816;&#39564;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#39640;&#30340;&#22810;&#37325;&#26816;&#39564;&#32593;&#32476;&#26041;&#27861;&#65292;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#28176;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#26041;&#27861;&#65306;&#27604;&#20363;&#21305;&#37197;&#21644;&#36138;&#23146;&#32858;&#21512;&#12290;&#27604;&#20363;&#21305;&#37197;&#26041;&#27861;&#23454;&#29616;&#20102;&#20840;&#23616; BH&#65288;Benjamini&#8211;Hochberg&#65289;&#24615;&#33021;&#65292;&#20165;&#38656;&#35201;&#36890;&#20449;&#19968;&#27425;&#65288;&#20272;&#35745;&#65289;&#30495;&#38646;&#20551;&#35774;&#27604;&#20363;&#20197;&#21450;&#27599;&#20010;&#33410;&#28857;&#30340; p &#20540;&#25968;&#37327;&#12290;&#36890;&#36807;&#20851;&#27880;&#28176;&#36827;&#26368;&#20248;&#21151;&#29575;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102; BH &#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26368;&#20248;&#35299;&#30340;&#26174;&#24335;&#29305;&#24449;&#12290;&#36825;&#23548;&#33268;&#20102;&#36138;&#24515;&#32858;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;&#20102;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20248;&#25298;&#32477;&#21306;&#22495;&#65292;&#32780;&#35745;&#31639;&#25928;&#29575;&#33258;&#28982;&#26469;&#33258;&#36138;&#24515;&#31867;&#22411;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; FDR &#21644;&#21151;&#29575;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#24191;&#27867;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work concerns developing communication- and computation-efficient methods for large-scale multiple testing over networks, which is of interest to many practical applications. We take an asymptotic approach and propose two methods, proportion-matching and greedy aggregation, tailored to distributed settings. The proportion-matching method achieves the global BH performance yet only requires a one-shot communication of the (estimated) proportion of true null hypotheses as well as the number of p-values at each node. By focusing on the asymptotic optimal power, we go beyond the BH procedure by providing an explicit characterization of the asymptotic optimal solution. This leads to the greedy aggregation method that effectively approximates the optimal rejection regions at each node, while computation efficiency comes from the greedy-type approach naturally. Moreover, for both methods, we provide the rate of convergence for both the FDR and power. Extensive numerical results over a va
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14115</link><description>&lt;p&gt;
&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14115
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#20854;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#27169;&#22411;&#31034;&#20363;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#29992;&#25143;&#25968;&#37327;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#27010;&#24565;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#32447;&#24615;&#21069;&#21521;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#31034;&#20363;&#65292;&#20854;&#36870;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21442;&#19982;&#32473;&#23450;&#36845;&#20195;&#30340;&#22823;&#37327;&#29992;&#25143;&#26469;&#22686;&#21152;&#21487;&#35299;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#27010;&#24565;&#30340;&#21487;&#33021;&#25193;&#23637;&#65292;&#21253;&#25324;&#38750;&#32447;&#24615;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#22320;&#32447;&#21644;&#27700;&#24179;&#38754;&#25237;&#24433;&#30340;&#21452;&#26354;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#22522;&#30784;&#20998;&#23618;&#32467;&#26500;&#30340;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2211.10066</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#22320;&#32447;&#21644;&#27700;&#24179;&#38754;&#25237;&#24433;&#30340;&#21452;&#26354;&#20999;&#29255;Wasserstein
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Sliced-Wasserstein via Geodesic and Horospherical Projections. (arXiv:2211.10066v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#22320;&#32447;&#21644;&#27700;&#24179;&#38754;&#25237;&#24433;&#30340;&#21452;&#26354;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#22522;&#30784;&#20998;&#23618;&#32467;&#26500;&#30340;&#25968;&#25454;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#35768;&#22810;&#20855;&#26377;&#22522;&#30784;&#20998;&#23618;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#23558;&#20854;&#23884;&#20837;&#21452;&#26354;&#31354;&#38388;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#34987;&#25193;&#23637;&#21040;&#36825;&#20123;&#31354;&#38388;&#65292;&#20294;&#21482;&#26377;&#24456;&#23569;&#23545;&#20110;&#36825;&#20123;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#27604;&#36739;&#30340;&#19981;&#19968;&#33268;&#24615;&#23384;&#22312;&#12290;&#22312;&#21452;&#26354;&#31354;&#38388;&#19978;&#65292;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#22312;&#36825;&#26679;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#65292;&#20999;&#29255;Wasserstein&#36317;&#31163;&#26159;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#19968;&#32500;Wasserstein&#36317;&#31163;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20294;&#22312;&#21452;&#26354;&#31354;&#38388;&#19978;&#19981;&#26131;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#21452;&#26354;&#20999;&#29255;Wasserstein&#19981;&#19968;&#33268;&#24615;&#26500;&#36896;&#12290;&#36825;&#20123;&#26500;&#36896;&#20351;&#29992;&#22522;&#26412;&#27979;&#22320;&#32447;&#19978;&#30340;&#27700;&#24179;&#38754;&#25110;&#27979;&#22320;&#32447;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30740;&#31350;&#21644;&#27604;&#36739;&#23427;&#20204;&#65292;&#20854;&#20013;&#21452;&#26354;&#34920;&#31034;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown beneficial for many types of data which present an underlying hierarchical structure to be embedded in hyperbolic spaces. Consequently, many tools of machine learning were extended to such spaces, but only few discrepancies to compare probability distributions defined over those spaces exist. Among the possible candidates, optimal transport distances are well defined on such Riemannian manifolds and enjoy strong theoretical properties, but suffer from high computational cost. On Euclidean spaces, sliced-Wasserstein distances, which leverage a closed-form of the Wasserstein distance in one dimension, are more computationally efficient, but are not readily available on hyperbolic spaces. In this work, we propose to derive novel hyperbolic sliced-Wasserstein discrepancies. These constructions use projections on the underlying geodesics either along horospheres or geodesics. We study and compare them on different tasks where hyperbolic representations are relevant, such a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08262</link><description>&lt;p&gt;
&#19968;&#31181;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
A mixed-categorical correlation kernel for Gaussian process. (arXiv:2211.08262v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31867;&#21035;&#30456;&#20851;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#28151;&#21512;&#31867;&#21035;&#20803;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26680;&#65288;&#20363;&#22914;&#65292;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65289;&#25110;&#36890;&#36807;&#30452;&#25509;&#20272;&#35745;&#30456;&#20851;&#30697;&#38453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#23558;&#36830;&#32493;&#25351;&#25968;&#26680;&#25193;&#23637;&#20026;&#22788;&#29702;&#28151;&#21512;&#31867;&#21035;&#21464;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26680;&#24341;&#23548;&#21040;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#20195;&#29702;&#65292;&#23427;&#27010;&#25324;&#20102;&#36830;&#32493;&#26494;&#24347;&#21644;Gower&#36317;&#31163;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20998;&#26512;&#21644;&#24037;&#31243;&#38382;&#39064;&#19978;&#35777;&#26126;&#20102;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#27604;&#20854;&#20182;&#22522;&#20110;&#26680;&#30340;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#21644;&#26356;&#23567;&#30340;&#27531;&#24046;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;SMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest for mixed-categorical meta-models based on Gaussian process (GP) surrogates. In this setting, several existing approaches use different strategies either by using continuous kernels (e.g., continuous relaxation and Gower distance based GP) or by using a direct estimation of the correlation matrix. In this paper, we present a kernel-based approach that extends continuous exponential kernels to handle mixed-categorical variables. The proposed kernel leads to a new GP surrogate that generalizes both the continuous relaxation and the Gower distance based GP models. We demonstrate, on both analytical and engineering problems, that our proposed GP model gives a higher likelihood and a smaller residual error than the other kernel-based state-of-the-art models. Our method is available in the open-source software SMT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#21644;&#20943;&#23569;&#22312;&#32447;&#24191;&#21578;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#35745;&#31639;&#26041;&#27861;&#21644;&#19968;&#31181;&#20869;&#32852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00071</link><description>&lt;p&gt;
CarbonTag: &#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#36817;&#20284;&#35745;&#31639;&#22312;&#32447;&#24191;&#21578;&#33021;&#32791;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CarbonTag: A Browser-Based Method for Approximating Energy Consumption of Online Ads. (arXiv:2211.00071v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#21644;&#20943;&#23569;&#22312;&#32447;&#24191;&#21578;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#35745;&#31639;&#26041;&#27861;&#21644;&#19968;&#31181;&#20869;&#32852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#26159;&#24403;&#21069;&#26368;&#20005;&#37325;&#30340;&#29615;&#22659;&#25361;&#25112;&#12290;&#30899;&#25490;&#25918;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#36129;&#29486;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#33021;&#28304;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#27979;&#37327;&#21644;&#20943;&#23569;&#26381;&#21153;&#30340;&#33021;&#28304;&#28040;&#32791;&#26159;&#20943;&#23569;&#30899;&#25490;&#25918;&#23545;&#29615;&#22659;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#32447;&#24191;&#21578;&#22312;&#28210;&#26579;&#36807;&#31243;&#20013;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#33021;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#35745;&#31639;&#21333;&#20010;&#24191;&#21578;&#28210;&#26579;&#36807;&#31243;&#20013;&#33021;&#28304;&#28040;&#32791;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110; JavaScript &#30340;&#20869;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39029;&#38754;&#19978;&#38468;&#21152; CarbonTag&#65292;&#20197;&#35745;&#31639;&#21644;&#20943;&#23569;&#22312;&#32447;&#24191;&#21578;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy is today the most critical environmental challenge. The amount of carbon emissions contributing to climate change is significantly influenced by both the production and consumption of energy. Measuring and reducing the energy consumption of services is a crucial step toward reducing adverse environmental effects caused by carbon emissions. Millions of websites rely on online advertisements to generate revenue, with most websites earning most or all of their revenues from ads. As a result, hundreds of billions of online ads are delivered daily to internet users to be rendered in their browsers. Both the delivery and rendering of each ad consume energy. This study investigates how much energy online ads use in the rendering process and offers a way for predicting it as part of rendering the ad. To the best of the authors' knowledge, this is the first study to calculate the energy usage of single advertisements in the rendering process. Our research further introduces different lev
&lt;/p&gt;</description></item><item><title>GFlowOut&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#22797;&#26434;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.12928</link><description>&lt;p&gt;
GFlowOut&#65306;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout
&lt;/p&gt;
&lt;p&gt;
GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12928
&lt;/p&gt;
&lt;p&gt;
GFlowOut&#26159;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;Dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#22797;&#26434;&#30340;&#21518;&#39564;&#20998;&#24067;&#21644;&#26679;&#26412;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#33391;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#20026;&#35299;&#20915;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#35768;&#22810;&#20851;&#38190;&#38382;&#39064;&#65288;&#20363;&#22914;&#19981;&#33391;&#26657;&#20934;&#21644;&#27867;&#21270;&#65292;&#20197;&#21450;&#25968;&#25454;&#25928;&#29575;&#65289;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#25193;&#23637;&#21040;&#22823;&#22411;&#26550;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#19988;&#38656;&#35201;&#20005;&#26684;&#30340;&#36817;&#20284;&#12290;Monte Carlo Dropout&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#36817;&#20284;&#25512;&#29702;&#30340;&#30456;&#23545;&#20415;&#23452;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;dropout&#25513;&#30721;&#26159;&#20174;&#22266;&#23450;&#20998;&#24067;&#20013;&#29420;&#31435;&#37319;&#26679;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;dropout&#25513;&#30721;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#25512;&#26029;&#12290;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65306;&#65288;a&#65289;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#33021;&#39640;&#24230;&#22810;&#27169;&#24577;&#65292;&#24456;&#38590;&#29992;&#26631;&#20934;&#21464;&#20998;&#25512;&#29702;&#36827;&#34892;&#36817;&#20284;&#65307;&#65288;b&#65289;&#20805;&#20998;&#21033;&#29992;dropout&#25513;&#30721;&#20043;&#38388;&#30340;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#21644;&#30456;&#20851;&#24615;&#20197;&#25913;&#21892;&#21518;&#39564;&#20272;&#35745;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GFlowOut&#65292;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#25311;dropout&#25513;&#30721;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#22797;&#26434;&#30340;&#25513;&#30721;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#25513;&#30721;&#20043;&#38388;&#30340;&#26679;&#26412;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GFlowOut&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;DiffAN&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#25628;&#32034;&#31354;&#38388;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.06201</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25490;&#24207;&#30340;&#22240;&#26524;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Causal Discovery via Topological Ordering. (arXiv:2210.06201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;DiffAN&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#25628;&#32034;&#31354;&#38388;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#23558;&#21151;&#33021;&#20851;&#31995;&#32422;&#26463;&#20026;&#38750;&#32447;&#24615;&#24102;&#26377;&#21152;&#24615;&#22122;&#22768;&#65288;ANM&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#25104;&#20026;&#21487;&#33021;&#12290;&#21363;&#20351;&#24102;&#26377;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#22240;&#26524;&#21457;&#29616;&#20063;&#28041;&#21450;&#21040;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#31354;&#38388;&#20013;&#36827;&#34892;&#26114;&#36149;&#30340;&#25628;&#32034;&#38382;&#39064;&#12290;&#25299;&#25169;&#25490;&#24207;&#26041;&#27861;&#36890;&#36807;&#22312;&#25490;&#21015;&#31354;&#38388;&#20013;&#25628;&#32034;&#32780;&#19981;&#26159;&#22270;&#24418;&#31354;&#38388;&#20013;&#25628;&#32034;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#22240;&#26524;&#21457;&#29616;&#20248;&#21270;&#31354;&#38388;&#12290;&#23545;&#20110;ANMs&#65292;&#21487;&#20197;&#20351;&#29992;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;Hessian&#26469;&#25214;&#21040;&#22240;&#26524;&#22270;&#20013;&#30340;&#21494;&#33410;&#28857;&#65292;&#20174;&#32780;&#20801;&#35768;&#23427;&#30340;&#25299;&#25169;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#33719;&#21462;Hessian&#30340;&#35745;&#31639;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#25193;&#23637;&#20026;&#21464;&#37327;&#25968;&#37327;&#21644;&#26679;&#26412;&#25968;&#37327;&#22686;&#21152;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#26368;&#36817;&#21019;&#26032;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffAN&#30340;&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space. For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples increase. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}\footnote{Implementation is available at \url{https://github.com/vios-s/DiffAN} .}, a topological ordering algorithm t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05247</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#20559;&#32622;&#24615;&#65292;&#23548;&#33268;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32479;&#35745;&#35777;&#25454;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22312;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#21151;&#33021;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#25506;&#32034;&#20855;&#26377;&#24378;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#23376;&#32593;&#32476;&#23384;&#22312;&#38480;&#21046;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#23545;&#32467;&#26500;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#30340;&#65288;&#20266;&#65289;&#26080;&#20559;&#26679;&#26412;&#21644;&#36873;&#25321;&#24615;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21435;&#20559;&#32622;&#23545;&#27604;&#21098;&#26525;&#65288;DCWP&#65289;&#31639;&#27861;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102; DCWP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#25915;&#20987;&#31867;&#22411;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#23454;&#29616;&#23545;&#21508;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#26356;&#39640;&#24615;&#33021;&#65292;&#26159;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20013;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2210.03150</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Out-of-Distribution Adversarial Robustness. (arXiv:2210.03150v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03150
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;OOD&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#25915;&#20987;&#31867;&#22411;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#23454;&#29616;&#23545;&#21508;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#23454;&#29616;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#26356;&#39640;&#24615;&#33021;&#65292;&#26159;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#20013;&#30340;&#21019;&#26032;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#23545;&#19968;&#31181;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24448;&#24448;&#19981;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#29992;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#29992;&#25915;&#20987;&#20013;&#25913;&#21892;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#31181;&#25915;&#20987;&#35270;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#24182;&#24212;&#29992;&#39118;&#38505;&#22806;&#25512;&#26041;&#27861;&#65288;REx&#65289;&#65292;&#20419;&#36827;&#23545;&#25152;&#26377;&#35757;&#32451;&#25915;&#20987;&#30340;&#30456;&#20284;&#40065;&#26834;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#25915;&#20987;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#39640;&#32423;&#21035;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#23478;&#26063;&#25110;&#27979;&#35797;&#26102;&#21482;&#36935;&#21040;&#30340;&#25915;&#20987;&#30340;&#35843;&#25972;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#25915;&#20987;&#38598;&#21512;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;MNIST&#30340;&#26368;&#20339;&#29616;&#26377;&#22522;&#32447;&#30340;&#20934;&#30830;&#24615;&#20174;3.4%&#25552;&#39640;&#21040;25.9&#65285;&#65292;&#22312;CIFAR10&#19978;&#20174;16.9&#65285;&#25552;&#39640;&#21040;23.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness continues to be a major challenge for deep learning. A core issue is that robustness to one type of attack often fails to transfer to other attacks. While prior work establishes a theoretical trade-off in robustness against different $L_p$ norms, we show that there is potential for improvement against many commonly used attacks by adopting a domain generalisation approach. Concretely, we treat each type of attack as a domain, and apply the Risk Extrapolation method (REx), which promotes similar levels of robustness against all training attacks. Compared to existing methods, we obtain similar or superior worst-case adversarial robustness on attacks seen during training. Moreover, we achieve superior performance on families or tunings of attacks only encountered at test time. On ensembles of attacks, our approach improves the accuracy from 3.4% with the best existing baseline to 25.9% on MNIST, and from 16.9% to 23.5% on CIFAR10.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861; PathProx&#65292;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.03069</link><description>&lt;p&gt;
PathProx: &#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26435;&#20540;&#34928;&#20943;&#27491;&#21017;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861; PathProx&#65292;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#20540;&#34928;&#20943;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20043;&#19968;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#39537;&#21160;&#26435;&#20540;&#34928;&#20943;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#25439;&#22833;&#20043;&#21644;&#21152;&#19978;&#19982;&#26435;&#20540;&#24179;&#26041;&#21644;&#25104;&#27604;&#20363;&#30340;&#39033;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#33021;&#26159;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#31181;&#20302;&#25928;&#31639;&#27861;&#12290;&#23545;&#20110;&#24102;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26435;&#37325;&#34928;&#20943;&#30446;&#26631;&#30340;&#35299;&#19982;&#21478;&#19968;&#20010;&#30446;&#26631;&#30340;&#35299;&#26159;&#31561;&#20215;&#30340;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#25913;&#20026;&#19982;&#27599;&#20010;ReLU&#31070;&#32463;&#20803;&#20851;&#32852;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#26435;&#37325;&#30340;$\ell_2$&#65288;&#19981;&#26159;&#24179;&#26041;&#65289;&#33539;&#25968;&#20056;&#31215;&#20043;&#21644;&#12290;&#36825;&#31181;&#26367;&#20195;&#65288;&#24182;&#19988;&#26377;&#25928;&#31561;&#20215;&#65289;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#31034;&#23427;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#26631;&#20934;&#26435;&#20540;&#34928;&#20943;&#35757;&#32451;&#25152;&#20849;&#20139;&#30340;&#31232;&#30095;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017; RankMe&#65292;&#36890;&#36807;&#35780;&#20272;&#26377;&#25928;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2210.02885</link><description>&lt;p&gt;
&#36890;&#36807;&#25490;&#21517;&#35780;&#20272;&#39044;&#35757;&#32451;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;&#19979;&#28216;&#24615;&#33021;&#65306;RankMe
&lt;/p&gt;
&lt;p&gt;
RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. (arXiv:2210.02885v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017; RankMe&#65292;&#36890;&#36807;&#35780;&#20272;&#26377;&#25928;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;JE-SSL&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#24471;&#20986;&#29616;&#20102;&#35768;&#22810;&#26041;&#27861;&#21464;&#21270;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#21407;&#21017;&#24615;&#25351;&#23548;&#26041;&#38024;&#65292;&#33021;&#22815;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#25104;&#21151;&#22320;&#37096;&#32626;&#23427;&#20204;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#20934;&#21017;&#65292;&#21363;&#25928;&#26524;&#25490;&#21517;&#65292;&#21487;&#20197;&#25351;&#31034;&#23398;&#20064;JE-SSL&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#19988;&#35745;&#31639;&#21451;&#22909;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;JE-SSL&#34920;&#31034;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Self Supervised Learning (JE-SSL) has seen a rapid development, with the emergence of many method variations but only few principled guidelines that would help practitioners to successfully deploy them. The main reason for that pitfall comes from JE-SSL's core principle of not employing any input reconstruction therefore lacking visual cues of unsuccessful training. Adding non informative loss values to that, it becomes difficult to deploy SSL on a new dataset for which no labels can help to judge the quality of the learned representation. In this study, we develop a simple unsupervised criterion that is indicative of the quality of the learned JE-SSL representations: their effective rank. Albeit simple and computationally friendly, this method -- coined RankMe -- allows one to assess the performance of JE-SSL representations, even on different downstream datasets, without requiring any labels. A further benefit of RankMe is that it does not have any training or hyper-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#26550;&#26500;&#65292;&#38024;&#23545;&#31890;&#23376;&#25506;&#27979;&#22120;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#36890;&#36807;&#22806;&#31215;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#12289;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#31561;&#20248;&#21270;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#20122;&#24494;&#31186;&#32423;&#21035;&#30340;&#32593;&#32476;&#37096;&#32626;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.14065</link><description>&lt;p&gt;
LL-GNN: &#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#33021;&#29289;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. (arXiv:2209.14065v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#26550;&#26500;&#65292;&#38024;&#23545;&#31890;&#23376;&#25506;&#27979;&#22120;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#36890;&#36807;&#22806;&#31215;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#12289;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#31561;&#20248;&#21270;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#20122;&#24494;&#31186;&#32423;&#21035;&#30340;&#32593;&#32476;&#37096;&#32626;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#37325;&#26500;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#65292;&#20197;&#25903;&#25345;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#20302;&#24310;&#36831;&#24615;&#33021;&#30340;&#31890;&#23376;&#25506;&#27979;&#22120;&#12290;&#30001;&#20110;&#22312;&#27431;&#27954;&#26680;&#23376;&#30740;&#31350;&#20013;&#24515;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#19968;&#32423;&#35302;&#21457;&#22120;&#20013;&#38656;&#35201;&#20197;&#27599;&#31186;&#25968;&#30334;&#22826;&#23383;&#33410;&#30340;&#25968;&#25454;&#36895;&#29575;&#36827;&#34892;&#22312;&#32447;&#20107;&#20214;&#36873;&#25321;&#65292;&#22240;&#27492;&#23558;&#22522;&#20110;FPGA&#30340;GNNs&#24212;&#29992;&#20110;&#31890;&#23376;&#25506;&#27979;&#22120;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#20122;&#24494;&#31186;&#32423;&#21035;&#20869;&#37096;&#32626;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#31215;&#30340;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34701;&#21512;&#27493;&#39588;&#65292;&#36890;&#36807;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#36793;&#30028;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#31471;&#21040;&#31471;&#35774;&#35745;&#30340;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24310;&#36831;&#38480;&#21046;&#19979;&#23547;&#25214;&#26356;&#22909;&#24310;&#36831;&#21644;&#26356;&#39640;&#31934;&#24230;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.13492</link><description>&lt;p&gt;
&#26242;&#20572;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#65306;&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taking a Respite from Representation Learning for Molecular Property Prediction. (arXiv:2209.13492v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19968;&#31995;&#21015;&#20998;&#23376;&#34920;&#24449;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#22522;&#20110;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#23601;&#26159;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12290;&#34429;&#28982;&#20998;&#23376;&#34920;&#24449;&#23398;&#20064;&#30340;&#25216;&#26415;&#22914;&#27492;&#21457;&#36798;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#22522;&#30784;&#38382;&#39064;&#21364;&#26410;&#34987;&#35748;&#30495;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#20998;&#23376;&#34920;&#24449;&#23545;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;MoleculeNet&#22522;&#20934;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;ChEMBL&#25968;&#25454;&#24211;&#21644;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#19968;&#22871;&#19982;&#38463;&#29255;&#31867;&#29289;&#36136;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#20004;&#20010;&#39069;&#22806;&#30340;&#27963;&#24615;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#32452;&#35013;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#25551;&#36848;&#31526;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20849;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;62,820&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;50,220&#20010;&#20351;&#29992;&#22266;&#23450;&#34920;&#24449;&#30340;&#27169;&#22411;&#12289;4,200&#20010;&#20351;&#29992;SMILES&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;8,400&#20010;&#20351;&#29992;&#20998;&#23376;&#22270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#38463;&#29255;&#31867;&#29289;&#36136;&#20013;&#30340;&#27963;&#24615;&#26029;&#23830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely applied in drug discovery with a major task as molecular property prediction. Despite booming techniques in molecular representation learning, fundamentals underlying molecular property prediction haven't been carefully examined yet. In this study, we conducted a systematic evaluation on a collection of representative models using various molecular representations. In addition to the commonly used MoleculeNet benchmark datasets, we also assembled a suite of opioids-related datasets from ChEMBL and two additional activity datasets from literature. To interrogate the basic predictive power, we also assembled a series of descriptors datasets with varying sizes to evaluate the models' performance. In total, we trained 62,820 models, including 50,220 models on fixed representations, 4,200 models on SMILES sequences and 8,400 models on molecular graphs. We first conducted dataset profiling and highlighted the activity-cliffs issue in the opioids-r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#20855;&#26377;&#33258;&#27965;&#24615;&#30340;&#30446;&#26631;&#65292;&#23427;&#20849;&#21516;&#20248;&#21270;&#20102;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#22238;&#25253;&#65292;&#20174;&#32780;&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.08466</link><description>&lt;p&gt;
&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#36890;&#36807;&#19968;&#20010;&#30446;&#26631;&#23398;&#20064;&#34920;&#31034;&#12289;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective. (arXiv:2209.08466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08466
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#12289;&#20855;&#26377;&#33258;&#27965;&#24615;&#30340;&#30446;&#26631;&#65292;&#23427;&#20849;&#21516;&#20248;&#21270;&#20102;&#38544;&#31354;&#38388;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#39640;&#22238;&#25253;&#65292;&#20174;&#32780;&#31616;&#21270;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23398;&#20064;&#29615;&#22659;&#20869;&#37096;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#21487;&#33021;&#27604;&#20854;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#25163;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23398;&#20064;&#20174;&#39640;&#32500;&#20256;&#24863;&#22120;&#20013;&#27169;&#25311;&#21407;&#22987;&#35266;&#23519;&#32467;&#26524;&#30340;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-fre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2209.05732</link><description>&lt;p&gt;
R\'{e}nyi&#25955;&#24230;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
R\'{e}nyi Divergence Deep Mutual Learning. (arXiv:2209.05732v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#20013;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#24341;&#20837;&#22823;&#37327;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#8212;&#8212;&#28145;&#24230;&#20114;&#30456;&#23398;&#20064;&#65288;DML&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;R\'{e}nyi&#25955;&#24230;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#36825;&#31181;&#20570;&#27861;&#26356;&#21152;&#28789;&#27963;&#12289;&#21487;&#35843;&#65292;&#20197;&#25913;&#21892;vanilla DML&#12290;&#36825;&#31181;&#20462;&#25913;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#38468;&#21152;&#22797;&#26434;&#24615;&#19979;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#33539;&#20363;&#30340;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#19988;&#34920;&#26126;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#20248;&#21270;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#25910;&#25947;&#30340;&#20559;&#24046;&#20026;$\mathcal{O}(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits Deep Mutual Learning (DML), a simple yet effective computing paradigm. We propose using R\'{e}nyi divergence instead of the KL divergence, which is more flexible and tunable, to improve vanilla DML. This modification is able to consistently improve performance over vanilla DML with limited additional complexity. The convergence properties of the proposed paradigm are analyzed theoretically, and Stochastic Gradient Descent with a constant learning rate is shown to converge with $\mathcal{O}(1)$-bias in the worst case scenario for nonconvex optimization tasks. That is, learning will reach nearby local optima but continue searching within a bounded scope, which may help mitigate overfitting. Finally, our extensive empirical results demonstrate the advantage of combining DML and R\'{e}nyi divergence, which further improves generalized models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#37327;&#21270;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#26465;&#20214;&#29109;&#21644;&#20114;&#20449;&#24687;&#20316;&#20026;&#24230;&#37327;&#26041;&#27861;&#30340;&#25209;&#35780;&#21644;&#36136;&#30097;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#24230;&#37327;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23545;&#23558;&#24635;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#20854;Aleatoric&#21644;Epistemic&#25104;&#20998;&#30340;&#21152;&#27861;&#20998;&#35299;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24403;&#21069;&#26377;&#20851;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23454;&#36341;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.03302</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#37327;&#21270;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#65306;&#26465;&#20214;&#29109;&#21644;&#20114;&#20449;&#24687;&#26159;&#21542;&#36866;&#24403;&#30340;&#24230;&#37327;&#65311;
&lt;/p&gt;
&lt;p&gt;
Quantifying Aleatoric and Epistemic Uncertainty in Machine Learning: Are Conditional Entropy and Mutual Information Appropriate Measures?. (arXiv:2209.03302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03302
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#37327;&#21270;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#26465;&#20214;&#29109;&#21644;&#20114;&#20449;&#24687;&#20316;&#20026;&#24230;&#37327;&#26041;&#27861;&#30340;&#25209;&#35780;&#21644;&#36136;&#30097;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#24230;&#37327;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23545;&#23558;&#24635;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#20854;Aleatoric&#21644;Epistemic&#25104;&#20998;&#30340;&#21152;&#27861;&#20998;&#35299;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24403;&#21069;&#26377;&#20851;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#23454;&#36341;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20197;&#26465;&#20214;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#24418;&#24335;&#37327;&#21270;Aleatoric&#21644;Epistemic&#36825;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#36825;&#20123;&#24230;&#37327;&#30340;&#20449;&#24687;&#29702;&#35770;&#26681;&#22522;&#30475;&#36215;&#26469;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20102;&#21508;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#24230;&#37327;&#65292;&#25105;&#20204;&#36824;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;&#23558;&#24635;&#19981;&#30830;&#23450;&#24615;&#20998;&#35299;&#20026;&#20854;Aleatoric&#21644;Epistemic&#25104;&#20998;&#30340;&#21152;&#27861;&#20998;&#35299;&#30340;&#24819;&#27861;&#12290;&#19981;&#21516;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#39564;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24403;&#21069;&#23454;&#36341;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantification of aleatoric and epistemic uncertainty in terms of conditional entropy and mutual information, respectively, has recently become quite common in machine learning. While the properties of these measures, which are rooted in information theory, seem appealing at first glance, we identify various incoherencies that call their appropriateness into question. In addition to the measures themselves, we critically discuss the idea of an additive decomposition of total uncertainty into its aleatoric and epistemic constituents. Experiments across different computer vision tasks support our theoretical findings and raise concerns about current practice in uncertainty quantification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRBO&#30340;&#26032;&#22411;&#20132;&#26131;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#8220;&#36172;&#33218;&#26426;&#20013;&#30340;&#36172;&#33218;&#26426;&#8221;&#26694;&#26550;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#20132;&#26131;&#31574;&#30053;&#22312;&#27169;&#25311;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#36866;&#24212;&#24615;&#35843;&#25972;&#65292;&#19982;&#21442;&#32771;&#20132;&#26131;&#31574;&#30053;PRSH&#30456;&#27604;&#65292;PRBO&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2208.02901</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#36830;&#32493;&#36172;&#33218;&#31574;&#30053;&#22312;&#27169;&#25311;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#33258;&#21160;&#21270;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Nonstationary Continuum-Armed Bandit Strategies for Automated Trading in a Simulated Financial Market. (arXiv:2208.02901v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRBO&#30340;&#26032;&#22411;&#20132;&#26131;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#8220;&#36172;&#33218;&#26426;&#20013;&#30340;&#36172;&#33218;&#26426;&#8221;&#26694;&#26550;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#20132;&#26131;&#31574;&#30053;&#22312;&#27169;&#25311;&#37329;&#34701;&#24066;&#22330;&#20013;&#30340;&#36866;&#24212;&#24615;&#35843;&#25972;&#65292;&#19982;&#21442;&#32771;&#20132;&#26131;&#31574;&#30053;PRSH&#30456;&#27604;&#65292;PRBO&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35774;&#35745;&#19968;&#31181;&#33258;&#21160;&#21270;&#20132;&#26131;&#31574;&#30053;&#26469;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30528;&#30340;&#24066;&#22330;&#29366;&#20917;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#35270;&#20026;&#19968;&#20010;&#38750;&#31283;&#24577;&#36830;&#32493;&#36172;&#33218;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRBO&#30340;&#26032;&#22411;&#20132;&#26131;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#8220;&#36172;&#33218;&#26426;&#20013;&#30340;&#36172;&#33218;&#26426;&#8221;&#26694;&#26550;&#26469;&#26681;&#25454;&#24066;&#22330;&#29366;&#20917;&#21160;&#24577;&#35843;&#25972;&#31574;&#30053;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;Bristol&#32929;&#31080;&#20132;&#26131;&#25152;&#65288;BSE&#65289;&#26469;&#27169;&#25311;&#21253;&#21547;&#24322;&#36136;&#33258;&#21160;&#20132;&#26131;&#20195;&#29702;&#20154;&#32676;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#23558;PRBO&#19982;&#36890;&#36807;&#38543;&#26426;&#29228;&#23665;&#27861;&#35843;&#25972;&#31574;&#30053;&#21442;&#25968;&#30340;&#21442;&#32771;&#20132;&#26131;&#31574;&#30053;PRSH&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#35201;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#26356;&#23569;&#65292;&#20294;PRBO&#20135;&#29983;&#30340;&#21033;&#28070;&#26174;&#33879;&#39640;&#20110;PRSH&#12290; PRBO&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#25968;&#25454;&#24050;&#22312;GitHub&#19978;&#24320;&#28304;&#25552;&#20379;&#65288;https://github.com/HarmoniaLeo/PRZI-Bayesian-Optimisation&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We approach the problem of designing an automated trading strategy that can consistently profit by adapting to changing market conditions. This challenge can be framed as a Nonstationary Continuum-Armed Bandit (NCAB) problem. To solve the NCAB problem, we propose PRBO, a novel trading algorithm that uses Bayesian optimization and a ``bandit-over-bandit'' framework to dynamically adjust strategy parameters in response to market conditions. We use Bristol Stock Exchange (BSE) to simulate financial markets containing heterogeneous populations of automated trading agents and compare PRBO with PRSH, a reference trading strategy that adapts strategy parameters through stochastic hill-climbing. Results show that PRBO generates significantly more profit than PRSH, despite having fewer hyperparameters to tune. The code for PRBO and performing experiments is available online open-source (https://github.com/HarmoniaLeo/PRZI-Bayesian-Optimisation).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#31639;&#27861;&#20855;&#26377;&#31283;&#23450;&#24615;&#19982;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00290</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#28176;&#36827;&#24179;&#28369;&#20989;&#25968;&#31639;&#27861;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25130;&#26029;&#26607;&#35199;&#38543;&#26426;&#25200;&#21160;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#31639;&#27861;&#20855;&#26377;&#31283;&#23450;&#24615;&#19982;&#24555;&#36895;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#19968;&#20010;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26159;&#22122;&#22768;&#25104;&#26412;&#26679;&#26412;&#30340;&#26399;&#26395;&#65292;&#32780;&#21482;&#26377;&#21518;&#32773;&#23545;&#20219;&#20309;&#32473;&#23450;&#30340;&#21442;&#25968;&#36827;&#34892;&#35266;&#27979;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37319;&#29992;&#24102;&#26377;&#38543;&#26426;&#25200;&#21160;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#26696;&#65292;&#36825;&#20123;&#25200;&#21160;&#20351;&#29992;&#20174;delta&#29699;&#20013;&#24471;&#21040;&#30340;&#25130;&#26029;&#26607;&#35199;&#20998;&#24067;&#24418;&#25104;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#32780;&#21442;&#25968;&#32500;&#25968;&#24456;&#39640;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;&#20174;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#30830;&#23450;&#22320;&#25910;&#25947;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#38598;&#21512;&#65292;&#24182;&#33719;&#24471;&#20102;&#25910;&#25947;&#30340;&#28176;&#36817;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36991;&#20813;&#20102;&#19981;&#31283;&#23450;&#30340;&#24179;&#34913;&#28857;&#65292;&#24847;&#21619;&#30528;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#20445;&#35777;&#25910;&#25947;&#29575;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#26159;&#23454;&#25968;&#20540;&#30340;&#21644;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDML&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.12141</link><description>&lt;p&gt;
&#27963;&#22312;&#24403;&#19979;&#65306;&#36866;&#24212;&#24615;&#36827;&#21270;&#31574;&#30053;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDML&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#27604;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#23398;&#20064;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#29983;&#25104;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#23398;&#20064;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#36866;&#24212;&#25152;&#26377;&#21382;&#21490;&#31574;&#30053;&#19979;&#30340;&#32463;&#39564;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#65292;&#21363;&#26679;&#26412;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#8220;&#25152;&#26377;&#21382;&#21490;&#31574;&#30053;&#8221;&#19979;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#36866;&#24212;&#20998;&#24067;&#19981;&#19968;&#23450;&#26377;&#30410;&#20110;&#27169;&#22411;&#39044;&#27979;&#8220;&#24403;&#21069;&#31574;&#30053;&#8221;&#65292;&#22240;&#20026;&#27491;&#22312;&#20351;&#29992;&#30340;&#31574;&#30053;&#22312;&#26102;&#38388;&#19978;&#19981;&#26029;&#28436;&#21464;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#20250;&#23548;&#33268;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36825;&#31181;&#21382;&#21490;&#31574;&#30053;&#20998;&#24067;&#30340;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#21644;&#27169;&#22411;&#22238;&#28378;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#31574;&#30053;&#36866;&#24212;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;(PDML)&#12290;PDML&#21160;&#24577;&#35843;&#25972;&#29992;&#20110;&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#30340;&#21382;&#21490;&#31574;&#30053;&#28151;&#21512;&#20998;&#24067;&#65292;&#20197;&#36866;&#24212;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36827;&#21270;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PDML&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \emph{all historical policies} does not necessarily benefit model prediction for the \emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#24352;&#37327;&#21015;&#36710;&#20132;&#21449;&#36924;&#36817;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#31934;&#30830;&#21644;&#26377;&#22122;&#22768;&#27979;&#37327;&#30340;&#25972;&#20010;&#24352;&#37327;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2207.04327</link><description>&lt;p&gt;
&#24352;&#37327;&#21015;&#36710;&#20132;&#21449;&#36924;&#36817;&#30340;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error Analysis of Tensor-Train Cross Approximation. (arXiv:2207.04327v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#24352;&#37327;&#21015;&#36710;&#20132;&#21449;&#36924;&#36817;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#31934;&#30830;&#21644;&#26377;&#22122;&#22768;&#27979;&#37327;&#30340;&#25972;&#20010;&#24352;&#37327;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#30001;&#20110;&#20854;&#23545;&#39640;&#32500;&#24352;&#37327;&#30340;&#31616;&#27905;&#34920;&#31034;&#20811;&#26381;&#20102;&#32500;&#24230;&#28798;&#38590;&#65292;&#22240;&#27492;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#29289;&#29702;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20132;&#21449;&#36924;&#36817;&#26368;&#21021;&#26159;&#38024;&#23545;&#20174;&#19968;&#32452;&#36873;&#23450;&#30340;&#34892;&#21644;&#21015;&#20013;&#34920;&#31034;&#30697;&#38453;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#19988;&#26159;&#20174;&#20854;&#23569;&#37327;&#20803;&#32032;&#20013;&#26500;&#24314;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#24352;&#37327;&#21015;&#36710;&#20132;&#21449;&#36924;&#36817;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#29702;&#35770;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#36924;&#36817;&#35823;&#24046;&#30340;&#20998;&#26512;&#65292;&#36804;&#20170;&#20173;&#28982;&#32570;&#20047;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#32467;&#26524;&#20165;&#25552;&#20379;&#36880;&#20803;&#32032;&#36924;&#36817;&#20934;&#30830;&#24615;&#20445;&#35777;&#65292;&#24403;&#24310;&#20280;&#21040;&#25972;&#20010;&#24352;&#37327;&#26102;&#20250;&#23548;&#33268;&#38750;&#24120;&#26494;&#25955;&#30340;&#30028;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#31934;&#30830;&#21644;&#26377;&#22122;&#22768;&#27979;&#37327;&#30340;&#25972;&#20010;&#24352;&#37327;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#38416;&#26126;&#20102;&#25152;&#36873;&#23376;&#24352;&#37327;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#20132;&#21449;&#36924;&#36817;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#24179;&#34913;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#20934;&#30830;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor train decomposition is widely used in machine learning and quantum physics due to its concise representation of high-dimensional tensors, overcoming the curse of dimensionality. Cross approximation-originally developed for representing a matrix from a set of selected rows and columns-is an efficient method for constructing a tensor train decomposition of a tensor from few of its entries. While tensor train cross approximation has achieved remarkable performance in practical applications, its theoretical analysis, in particular regarding the error of the approximation, is so far lacking. To our knowledge, existing results only provide element-wise approximation accuracy guarantees, which lead to a very loose bound when extended to the entire tensor. In this paper, we bridge this gap by providing accuracy guarantees in terms of the entire tensor for both exact and noisy measurements. Our results illustrate how the choice of selected subtensors affects the quality of the cross appr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#35889;&#27714;&#35299;&#22120;&#22312;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#25351;&#23548;&#19979;&#21487;&#20197;&#27604;&#20256;&#32479;&#27714;&#35299;&#22120;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.00556</link><description>&lt;p&gt;
&#23398;&#20064;&#32416;&#27491;&#29992;&#20110;&#27169;&#25311;&#28237;&#27969;&#30340;&#35889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to correct spectral methods for simulating turbulent flows. (arXiv:2207.00556v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00556
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#35889;&#27714;&#35299;&#22120;&#22312;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#30340;&#25351;&#23548;&#19979;&#21487;&#20197;&#27604;&#20256;&#32479;&#27714;&#35299;&#22120;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#20013;&#21482;&#26377;&#23569;&#25968;&#20855;&#26377;&#35299;&#26512;&#25110;&#38381;&#21512;&#24418;&#24335;&#30340;&#35299;&#12290;&#36825;&#28608;&#21457;&#20102;&#22823;&#37327;&#32463;&#20856;&#30340; PDE &#25968;&#20540;&#27169;&#25311;&#24037;&#20316;&#20197;&#21450;&#26356;&#36817;&#26399;&#30340;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#20856;&#25968;&#20540;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#21487;&#20197;&#27604;&#21333;&#29420;&#20351;&#29992;&#20219;&#19968;&#26041;&#27861;&#24102;&#26469;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21152;&#20837;&#22522;&#20110;&#29289;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#26102;&#65292;&#25968;&#20540;&#26041;&#26696;&#30340;&#36873;&#25321;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#22522;&#20110; Fourier &#35889;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#36825;&#31181;&#35889;&#26041;&#27861;&#24050;&#30693;&#27604;&#20854;&#20182;&#25968;&#20540;&#26041;&#26696;&#26356;&#36866;&#21512;&#27169;&#25311;&#20855;&#26377;&#24179;&#28369;&#21644;&#21608;&#26399;&#24615;&#35299;&#30340; PDE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#19977;&#20010;&#27969;&#20307;&#21147;&#23398;&#24120;&#35265; PDE &#30340; ML&#22686;&#24378;&#30340;&#35889;&#27714;&#35299;&#22120;&#12290;&#22312;&#30456;&#21516;&#30340;&#35299;&#26512;&#24230;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#26631;&#20934;&#30340;&#35889;&#27714;&#35299;&#22120;&#26356;&#20934;&#30830;&#65288;2-4&#20493;&#65289;&#65292;&#20294;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#26356;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques and machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are known to be more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three common PDEs of fluid dynamics. Our models are more accurate (2-4x) than standard spectral solvers at the same resolution but have longer overall ru
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.12617</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30693;&#35782;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Language Models as Knowledge Embeddings. (arXiv:2206.12617v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12617
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#30340;&#26041;&#27861;LMKE&#65292;&#23427;&#26088;&#22312;&#25552;&#39640;&#23545;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#33021;&#21147;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23884;&#20837;&#26159;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#21040;&#36830;&#32493;&#21521;&#37327;&#31354;&#38388;&#20013;&#26469;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#32467;&#26500;&#25110;&#22522;&#20110;&#25551;&#36848;&#12290;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#23398;&#20064;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#22270;&#35889;&#20013;&#26377;&#38480;&#32467;&#26500;&#20449;&#24687;&#19979;&#20016;&#23500;&#30340;&#38271;&#23614;&#23454;&#20307;&#12290;&#22522;&#20110;&#25551;&#36848;&#30340;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#20808;&#21069;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36229;&#36234;&#22522;&#20110;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#23384;&#22312;&#26114;&#36149;&#30340;&#36127;&#37319;&#26679;&#21644;&#38480;&#21046;&#24615;&#25551;&#36848;&#38656;&#27714;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMKE&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#23548;&#30693;&#35782;&#23884;&#20837;&#65292;&#26088;&#22312;&#20016;&#23500;&#38271;&#23614;&#23454;&#20307;&#30340;&#34920;&#31034;&#24182;&#35299;&#20915;&#22522;&#20110;&#25551;&#36848;&#30340;&#20808;&#21069;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#34920;&#36848;&#22522;&#20110;&#25551;&#36848;&#30340;&#30693;&#35782;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#35780;&#20215;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMKE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#22522;&#20110;&#32467;&#26500;&#21644;&#22522;&#20110;&#20808;&#21069;&#25551;&#36848;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge embeddings (KE) represent a knowledge graph (KG) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of KGs. They cannot well represent abundant long-tail entities in real-world KGs with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose LMKE, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based KE learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.09821</link><description>&lt;p&gt;
&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#29992;&#20110;&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24402;&#30340;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#26469;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#27874;&#39640;&#39044;&#27979;&#26159;&#28023;&#27915;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#23545;&#20110;&#20272;&#35745;&#27874;&#33021;&#20135;&#29983;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#21450;&#26102;&#39044;&#27979;&#22823;&#28010;&#30340;&#21040;&#26469;&#23545;&#20110;&#30830;&#20445;&#33322;&#28023;&#20316;&#19994;&#30340;&#23433;&#20840;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#26174;&#33879;&#27874;&#39640;&#30340;&#26497;&#31471;&#20540;&#20316;&#20026;&#36229;&#26631;&#27010;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#20272;&#35745;&#26174;&#33879;&#27874;&#39640;&#23558;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#36890;&#24120;&#20351;&#29992;&#27010;&#29575;&#20108;&#20998;&#31867;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#35266;&#27979;&#30340;&#39044;&#27979;&#26469;&#26681;&#25454;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#20272;&#35745;&#36229;&#26631;&#27010;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21152;&#25343;&#22823;&#21704;&#21033;&#27861;&#20811;&#26031;&#28023;&#23736;&#30340;&#28014;&#26631;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#20102;&#35299;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20132;&#38598;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21462;&#24471;&#31934;&#30830;&#20851;&#31995;&#12290;&#22312;&#39640;&#27010;&#29575;&#19979;,&#36890;&#36807;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20854;&#20182;&#26377;&#24847;&#20041;&#30340;&#32479;&#35745;&#37327;&#21487;&#20197;&#35745;&#31639;&#20986;&#20132;&#38598;&#20844;&#24179;&#24615;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.05828</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#38469;&#20844;&#24179;&#24615;&#26469;&#30028;&#23450;&#21644;&#36924;&#36817;&#20132;&#38598;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bounding and Approximating Intersectional Fairness through Marginal Fairness. (arXiv:2206.05828v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#20102;&#35299;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20132;&#38598;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21462;&#24471;&#31934;&#30830;&#20851;&#31995;&#12290;&#22312;&#39640;&#27010;&#29575;&#19979;,&#36890;&#36807;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20854;&#20182;&#26377;&#24847;&#20041;&#30340;&#32479;&#35745;&#37327;&#21487;&#20197;&#35745;&#31639;&#20986;&#20132;&#38598;&#20844;&#24179;&#24615;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27495;&#35270;&#36890;&#24120;&#28041;&#21450;&#22810;&#20010;&#32500;&#24230;&#65288;&#21363;&#20445;&#25252;&#23646;&#24615;&#65289;&#65307;&#22240;&#27492;&#65292;&#30830;&#20445;&#8220;&#20132;&#38598;&#20844;&#24179;&#24615;&#8221;&#65292;&#21363;&#19981;&#27495;&#35270;&#20219;&#20309;&#23376;&#32452;&#65292;&#26159;&#29702;&#24819;&#30340;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20165;&#29420;&#31435;&#22320;&#20445;&#35777;&#27599;&#20010;&#32500;&#24230;&#30340;&#8220;&#36793;&#38469;&#20844;&#24179;&#24615;&#8221;&#36890;&#24120;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23376;&#32452;&#30340;&#25351;&#25968;&#25968;&#37327;&#65292;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#24230;&#37327;&#20132;&#38598;&#20844;&#24179;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#35814;&#32454;&#20102;&#35299;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20132;&#38598;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#32452;&#36275;&#22815;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#33719;&#24471;&#31934;&#30830;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#22312;&#39640;&#27010;&#29575;&#19979;&#36890;&#36807;&#36793;&#38469;&#20844;&#24179;&#24615;&#21644;&#20854;&#20182;&#26377;&#24847;&#20041;&#30340;&#32479;&#35745;&#37327;&#21487;&#20197;&#35745;&#31639;&#20986;&#20132;&#38598;&#20844;&#24179;&#24615;&#30340;&#30028;&#38480;&#12290;&#38500;&#20102;&#23427;&#20204;&#30340;&#25551;&#36848;&#20215;&#20540;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#29702;&#35770;&#30028;&#38480;&#21487;&#20197;&#21033;&#29992;&#21040;&#19968;&#31181;&#21551;&#21457;&#24335;&#30340;&#25552;&#39640;&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrimination in machine learning often arises along multiple dimensions (a.k.a. protected attributes); it is then desirable to ensure \emph{intersectional fairness} -- i.e., that no subgroup is discriminated against. It is known that ensuring \emph{marginal fairness} for every dimension independently is not sufficient in general. Due to the exponential number of subgroups, however, directly measuring intersectional fairness from data is impossible. In this paper, our primary goal is to understand in detail the relationship between marginal and intersectional fairness through statistical analysis. We first identify a set of sufficient conditions under which an exact relationship can be obtained. Then, we prove bounds (easily computable through marginal fairness and other meaningful statistical quantities) in high-probability on intersectional fairness in the general case. Beyond their descriptive value, we show that these theoretical bounds can be leveraged to derive a heuristic impro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#21457;&#29616;&#21363;&#20351;&#23384;&#22312;&#23725;&#20989;&#25968;&#25311;&#21512;&#25968;&#25454;&#65292;&#25554;&#20540;&#20989;&#25968;&#20063;&#26159;&#20869;&#22312;&#30340;&#22810;&#21464;&#37327;&#20989;&#25968;&#65307;&#24182;&#19988;&#65292;$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#23545;&#20110;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#32479;&#35745;&#26368;&#20248;&#27867;&#21270;&#19981;&#36275;&#22815;&#12290;</title><link>http://arxiv.org/abs/2206.05317</link><description>&lt;p&gt;
$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#30340;&#20869;&#22312;&#32500;&#24230;&#21644;&#27867;&#21270;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intrinsic dimensionality and generalization properties of the $\mathcal{R}$-norm inductive bias. (arXiv:2206.05317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#21457;&#29616;&#21363;&#20351;&#23384;&#22312;&#23725;&#20989;&#25968;&#25311;&#21512;&#25968;&#25454;&#65292;&#25554;&#20540;&#20989;&#25968;&#20063;&#26159;&#20869;&#22312;&#30340;&#22810;&#21464;&#37327;&#20989;&#25968;&#65307;&#24182;&#19988;&#65292;$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#23545;&#20110;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#32479;&#35745;&#26368;&#20248;&#27867;&#21270;&#19981;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{R}$-&#33539;&#26368;&#23567;&#21270;&#25554;&#20540;&#20989;&#25968;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#23427;&#20204;&#23545;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#35760;&#12290;$\mathcal{R}$-&#33539;&#26159;&#19968;&#20010;&#24402;&#32435;&#20559;&#24046;&#65292;&#29992;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#23427;&#26469;&#25429;&#25417;&#25511;&#21046;&#32593;&#32476;&#26435;&#37325;&#22823;&#23567;&#30340;&#21151;&#33021;&#25928;&#24212;&#65292;&#19981;&#21463;&#32593;&#32476;&#23485;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#25311;&#21512;&#25968;&#25454;&#30340;&#23725;&#20989;&#25968;&#65292;&#36825;&#20123;&#25554;&#20540;&#20989;&#25968;&#20063;&#26159;&#20869;&#22312;&#30340;&#22810;&#21464;&#37327;&#20989;&#25968;&#65307;&#21516;&#26102;&#65292;$\mathcal{R}$-&#33539;&#24402;&#32435;&#20559;&#24046;&#23545;&#20110;&#26576;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#32479;&#35745;&#26368;&#20248;&#27867;&#21270;&#19981;&#36275;&#22815;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#19982;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30456;&#20851;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the structural and statistical properties of $\mathcal{R}$-norm minimizing interpolants of datasets labeled by specific target functions. The $\mathcal{R}$-norm is the basis of an inductive bias for two-layer neural networks, recently introduced to capture the functional effect of controlling the size of network weights, independently of the network width. We find that these interpolants are intrinsically multivariate functions, even when there are ridge functions that fit the data, and also that the $\mathcal{R}$-norm inductive bias is not sufficient for achieving statistically optimal generalization for certain learning problems. Altogether, these results shed new light on an inductive bias that is connected to practical neural network training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20998;&#31867;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;&#24230;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#19968;&#32452;&#30456;&#20284;&#27169;&#22411;&#19978;&#39118;&#38505;&#35780;&#20272;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.01131</link><description>&lt;p&gt;
&#27010;&#29575;&#20998;&#31867;&#20013;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predictive Multiplicity in Probabilistic Classification. (arXiv:2206.01131v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20998;&#31867;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;&#24230;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#19968;&#32452;&#30456;&#20284;&#27169;&#22411;&#19978;&#39118;&#38505;&#35780;&#20272;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#39118;&#38505;&#35780;&#20272;&#20219;&#21153;&#65292;&#27604;&#22914;&#39044;&#27979;&#28040;&#36153;&#32773;&#36829;&#32422;&#39118;&#38505;&#12289;&#39044;&#27979;&#19968;&#20010;&#20154;&#26159;&#21542;&#24739;&#26377;&#20005;&#37325;&#30142;&#30149;&#25110;&#39044;&#27979;&#19968;&#20010;&#20154;&#22312;&#27861;&#24237;&#19978;&#20986;&#29616;&#30340;&#39118;&#38505;&#12290;&#20294;&#22914;&#26524;&#26377;&#22810;&#20010;&#27169;&#22411;&#22312;&#19968;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20960;&#20046;&#21516;&#26679;&#22909;&#65292;&#37027;&#20040;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20250;&#26377;&#22810;&#22823;&#30340;&#21464;&#21270;&#65311;&#22914;&#26524;&#30456;&#20284;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30456;&#23545;&#19968;&#33268;&#65292;&#21017;&#21487;&#20197;&#36873;&#25321;&#20248;&#21270;&#24809;&#32602;&#25439;&#22833;&#30340;&#27169;&#22411;&#12290;&#20294;&#22914;&#26524;&#30456;&#20284;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34987;&#31216;&#20026;&#39044;&#27979;&#22810;&#26679;&#24615;&#65292;&#21363;&#36890;&#36807;&#20248;&#21270;&#21028;&#23450;&#26631;&#20934;&#32780;&#33719;&#24471;&#30340;&#30456;&#20284;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#20998;&#31867;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;&#24230;&#37327;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#19968;&#20123;&#34913;&#37327;&#22312;&#19968;&#32452;&#30456;&#20284;&#27169;&#22411;&#19978;&#39118;&#38505;&#35780;&#20272;&#21464;&#21270;&#30340;&#37327;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often used to inform real world risk assessment tasks: predicting consumer default risk, predicting whether a person suffers from a serious illness, or predicting a person's risk to appear in court. Given multiple models that perform almost equally well for a prediction task, to what extent do predictions vary across these models? If predictions are relatively consistent for similar models, then the standard approach of choosing the model that optimizes a penalized loss suffices. But what if predictions vary significantly for similar models? In machine learning, this is referred to as predictive multiplicity i.e. the prevalence of conflicting predictions assigned by near-optimal competing models. In this paper, we present a framework for measuring predictive multiplicity in probabilistic classification (predicting the probability of a positive outcome). We introduce measures that capture the variation in risk estimates over the set of competing models, and d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.11677</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65306;&#36328;&#36234;&#20102;&#20449;&#24687;&#29702;&#35770;&#38376;&#27099;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#25968;&#25454;&#32858;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#26412;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#25968;&#21313;&#24180;&#26469;&#23545;&#35813;&#38382;&#39064;&#30340;&#24191;&#27867;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;Kesten-Stigum&#38376;&#27099;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#29305;&#21035;&#26377;&#36259;&#65292;&#20174;&#25968;&#23398;&#21644;&#24212;&#29992;&#35282;&#24230;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#34920;&#26126;&#65292;&#22914;&#26524;&#27169;&#22411;&#21442;&#25968;&#22312;&#26576;&#20010;&#38376;&#27099;&#20197;&#19979;&#65292;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#20219;&#20309;&#20272;&#35745;&#22120;&#22312;&#31232;&#30095;&#22270;&#19978;&#37117;&#19981;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#31245;&#24494;&#25193;&#23637;&#35270;&#37326;&#21040;&#26222;&#36941;&#23384;&#22312;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#36825;&#26679;&#30340;&#22522;&#26412;&#38480;&#21046;&#23558;&#23436;&#20840;&#28040;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25581;&#31034;&#20986;&#20219;&#24847;&#19968;&#37096;&#20998;&#26631;&#35760;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#21442;&#25968;&#22495;&#20869;&#23545;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#32452;&#21512;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#24102;&#26469;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#65292;&#26631;&#24535;&#30528;&#31232;&#30095;&#22270;&#32858;&#31867;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#24494;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36827;&#34892;&#39640;&#24230;&#21160;&#24577;&#21644;&#22797;&#26434;&#20219;&#21153;&#30340;&#36712;&#36857;&#20248;&#21270;&#65292;&#30828;&#20214;&#23454;&#39564;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21487;&#25104;&#21151;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2204.04558</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#26799;&#24230;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradient-Based Trajectory Optimization With Learned Dynamics. (arXiv:2204.04558v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04558
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#24494;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#36827;&#34892;&#39640;&#24230;&#21160;&#24577;&#21644;&#22797;&#26434;&#20219;&#21153;&#30340;&#36712;&#36857;&#20248;&#21270;&#65292;&#30828;&#20214;&#23454;&#39564;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21487;&#25104;&#21151;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#24050;&#32463;&#36798;&#21040;&#20102;&#38750;&#24120;&#39640;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#20294;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20934;&#30830;&#30340;&#20998;&#26512;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#26041;&#38754;&#20165;&#33021;&#20197;&#26377;&#38480;&#30340;&#26041;&#24335;&#34987;&#25429;&#25417;&#12290;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#24494;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#36712;&#36857;&#20248;&#21270;&#21644;&#27169;&#22411;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#32570;&#20047;&#31934;&#30830;&#20998;&#26512;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#39640;&#21160;&#24577;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20165;&#37319;&#38598;25&#20998;&#38047;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#65292;&#20934;&#30830;&#22320;&#27169;&#25311;&#22823;&#26102;&#38388;&#33539;&#22260;&#20869;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#34892;&#20026;&#65292;&#36825;&#20123;&#25968;&#25454;&#26469;&#33258;&#20004;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#65306;&#65288;i&#65289;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;&#26426;&#22120;&#29399;&#65292;&#65288;ii&#65289;&#36965;&#25511;&#27773;&#36710;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26469;&#25191;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#36712;&#36857;&#20248;&#21270;&#12290;&#22312;&#30828;&#20214;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#22312;&#29616;&#23454;&#26426;&#22120;&#20154;&#19978;&#33021;&#22815;&#25104;&#21151;&#22320;&#25191;&#34892;&#39640;&#24230;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#31070;&#32463;Q-&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#26925;&#22278;&#22411;PDE&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#32593;&#26684;&#19988;&#20855;&#26377;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21333;&#35843;PDE&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#26497;&#38480;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;PDE&#35299;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2203.17128</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;Q-&#23398;&#20064;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Q-learning for solving PDEs. (arXiv:2203.17128v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#31070;&#32463;Q-&#23398;&#20064;&#31639;&#27861;&#35299;&#20915;&#26925;&#22278;&#22411;PDE&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#26080;&#32593;&#26684;&#19988;&#20855;&#26377;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21333;&#35843;PDE&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#26497;&#38480;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#20110;PDE&#35299;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#31185;&#23398;&#35745;&#31639;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q-&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26925;&#22278;&#22411;PDE&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#8220;Q-PDE&#8221;&#31639;&#27861;&#26159;&#26080;&#32593;&#26684;&#30340;&#65292;&#22240;&#27492;&#20855;&#26377;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#26041;&#27861;&#35777;&#26126;&#65292;&#20351;&#29992;Q-PDE&#31639;&#27861;&#35757;&#32451;&#30340;PDE&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#22120;&#65292;&#38543;&#30528;&#38544;&#34255;&#23618;&#21333;&#20803;&#25968;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#20110;&#26080;&#31351;&#32500;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#36712;&#36857;&#12290;&#23545;&#20110;&#21333;&#35843;PDE&#65288;&#21363;&#30001;&#21333;&#35843;&#31639;&#31526;&#32473;&#20986;&#30340;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;PDE&#65289;&#65292;&#23613;&#31649;NTK&#20013;&#32570;&#20047;&#35889;&#38388;&#38553;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28385;&#36275;&#26080;&#31351;&#32500;ODE&#30340;&#26497;&#38480;&#31070;&#32463;&#32593;&#32476;&#20250;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#30340;&#22686;&#21152;&#25910;&#25947;&#20110;$L^2$&#20013;&#30340;PDE&#35299;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;wi&#30340;&#20219;&#20309;&#19981;&#21160;&#28857;&#37117;&#26159;&#35813;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving high-dimensional partial differential equations (PDEs) is a major challenge in scientific computing. We develop a new numerical method for solving elliptic-type PDEs by adapting the Q-learning algorithm in reinforcement learning. Our "Q-PDE" algorithm is mesh-free and therefore has the potential to overcome the curse of dimensionality. Using a neural tangent kernel (NTK) approach, we prove that the neural network approximator for the PDE solution, trained with the Q-PDE algorithm, converges to the trajectory of an infinite-dimensional ordinary differential equation (ODE) as the number of hidden units $\rightarrow \infty$. For monotone PDE (i.e. those given by monotone operators, which may be nonlinear), despite the lack of a spectral gap in the NTK, we then prove that the limit neural network, which satisfies the infinite-dimensional ODE, converges in $L^2$ to the PDE solution as the training time $\rightarrow \infty$. More generally, we can prove that any fixed point of the wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#24809;&#32602;&#39033;&#30340;&#31163;&#25955;&#36807;&#24230;&#21442;&#25968;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#25214;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340; $\hat f$&#12290;</title><link>http://arxiv.org/abs/2203.15994</link><description>&lt;p&gt;
&#26368;&#20248;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learning. (arXiv:2203.15994v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#24809;&#32602;&#39033;&#30340;&#31163;&#25955;&#36807;&#24230;&#21442;&#25968;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#25214;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340; $\hat f$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#20851;&#20110; $f$ &#30340;&#32473;&#23450;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#20989;&#25968; $f$ &#30340;&#38382;&#39064;&#12290;&#23398;&#20064;&#38382;&#39064;&#26159;&#32473;&#20986;&#19968;&#20010;&#36817;&#20284;&#20540; $ \hat f $&#65292;&#29992;&#20110;&#39044;&#27979;&#25968;&#25454;&#22806;&#30340; $f$ &#20540;&#12290;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#65306;&#65288;i&#65289;&#25105;&#20204;&#23545; $f$ &#26377;&#20160;&#20040;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#31216;&#20026;&#27169;&#22411;&#31867;&#20551;&#35774;&#65289;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#24230;&#37327; $\hat f$ &#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#65288;iii&#65289;&#25968;&#25454;&#21644;&#25968;&#25454;&#31449;&#28857;&#30340;&#24773;&#20917;&#65292;&#65288;iv&#65289;&#25968;&#25454;&#35266;&#23519;&#26159;&#21542;&#21463;&#21040;&#22122;&#22768;&#27745;&#26579;&#12290;&#22312;&#23384;&#22312;&#27169;&#22411;&#31867;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#30693;&#26368;&#20248;&#24674;&#22797;&#24615;&#33021;&#30340;&#25968;&#23398;&#25551;&#36848;&#12290;&#22312;&#26631;&#20934;&#27169;&#22411;&#31867;&#20551;&#35774;&#19979;&#65292;&#26412;&#25991;&#35777;&#26126;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#24809;&#32602;&#39033;&#30340;&#31163;&#25955;&#36807;&#24230;&#21442;&#25968;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#25214;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340; $\hat f$&#12290;&#26368;&#20248;&#25351;&#30340;&#26159;&#35823;&#24046;&#21463;&#21040;&#22266;&#23450;&#20493;&#25968;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of learning an unknown function $f$ from given data about $f$. The learning problem is to give an approximation $\hat f$ to $f$ that predicts the values of $f$ away from the data. There are numerous settings for this learning problem depending on (i) what additional information we have about $f$ (known as a model class assumption), (ii) how we measure the accuracy of how well $\hat f$ predicts $f$, (iii) what is known about the data and data sites, (iv) whether the data observations are polluted by noise. A mathematical description of the optimal performance possible (the smallest possible error of recovery) is known in the presence of a model class assumption. Under standard model class assumptions, it is shown in this paper that a near optimal $\hat f$ can be found by solving a certain discrete over-parameterized optimization problem with a penalty term. Here, near optimal means that the error is bounded by a fixed constant times the optimal error. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20984;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;&#20114;&#20013;&#24515;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#31181;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#28385;&#36275;PPL&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2203.06735</link><description>&lt;p&gt;
&#26080;&#38656;&#21487;&#20449;&#20219;&#26381;&#21153;&#22120;&#30340;&#31169;&#26377;&#38750;&#20984;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Private Non-Convex Federated Learning Without a Trusted Server. (arXiv:2203.06735v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.06735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20984;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#22411;&#20114;&#20013;&#24515;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#31181;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#28385;&#36275;PPL&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#38498;&#31561;&#22810;&#20010;&#25968;&#25454;&#20013;&#24515;&#38388;&#36827;&#34892;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#38656;&#35201;&#20445;&#35777;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#22312;&#27809;&#26377;&#21487;&#20449;&#36182;&#30340;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20114;&#20013;&#24515;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#24322;&#26500;&#25968;&#25454;&#21644;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28385;&#36275;PPL&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study federated learning (FL) -- especially cross-silo FL -- with non-convex loss functions and data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) must protect the privacy of each person's data (e.g. patient's medical record), even if the server or other silos act as adversarial eavesdroppers. To that end, we consider inter-silo record-level (ISRL) differential privacy (DP), which requires silo~$i$'s communications to satisfy record/item-level DP. We propose novel ISRL-DP algorithms for FL with heterogeneous (non-i.i.d.) silo data and two classes of Lipschitz continuous loss functions: First, we consider losses satisfying the Proximal Polyak-Lojasiewicz (PL) inequality, which is an extension of the classical PL condition to the constrained setting. In contrast to our result, prior works only considered unconstrained private optimization with Lipschitz PL loss, which rules out most interesting PL losses such as strongly convex prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#25628;&#32034;&#30340;&#21487;&#35299;&#37322;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#21512;&#21462;&#33539;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#36924;&#36817;&#20219;&#20309;&#21487;&#27979;&#20989;&#25968;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.02473</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#65306;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#25628;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Off-Policy Learning via Hyperbox Search. (arXiv:2203.02473v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#25628;&#32034;&#30340;&#21487;&#35299;&#37322;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#21512;&#21462;&#33539;&#24335;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#36924;&#36817;&#20219;&#20309;&#21487;&#27979;&#20989;&#25968;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#20915;&#31574;&#24050;&#25104;&#20026;&#29616;&#20195;&#21307;&#23398;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22240;&#27492;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#20010;&#20307;&#24739;&#32773;&#30340;&#29305;&#24449;&#36827;&#34892;&#27835;&#30103;&#20915;&#31574;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#26679;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#29305;&#23450;&#31574;&#30053;&#31867;&#21035;&#19979;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21487;&#35299;&#37322;&#24615;&#36890;&#24120;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#31435;&#26041;&#20307;&#25628;&#32034;&#30340;&#21487;&#35299;&#37322;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#29992;&#21512;&#21462;&#33539;&#24335;&#34920;&#31034;&#65288;&#21363;AND&#30340;OR&#65289;&#65292;&#22240;&#27492;&#26159;&#23481;&#26131;&#29702;&#35299;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#31867;&#21487;&#20197;&#28789;&#27963;&#22320;&#36924;&#36817;&#20219;&#20309;&#21487;&#27979;&#20989;&#25968;&#12290;&#20026;&#20102;&#20248;&#21270;&#65292;&#25105;&#20204;&#22312;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20869;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#21015;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized treatment decisions have become an integral part of modern medicine. Thereby, the aim is to make treatment decisions based on individual patient characteristics. Numerous methods have been developed for learning such policies from observational data that achieve the best outcome across a certain policy class. Yet these methods are rarely interpretable. However, interpretability is often a prerequisite for policy learning in clinical practice. In this paper, we propose an algorithm for interpretable off-policy learning via hyperbox search. In particular, our policies can be represented in disjunctive normal form (i.e., OR-of-ANDs) and are thus intelligible. We prove a universal approximation theorem that shows that our policy class is flexible enough to approximate any measurable function arbitrarily well. For optimization, we develop a tailored column generation procedure within a branch-and-bound framework. Using a simulation study, we demonstrate that our algorithm outpe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#36873;&#25321;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#21160;&#26426;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#22522;&#20110;&#20010;&#20154;&#30446;&#26631;&#21644;&#32972;&#26223;&#22240;&#32032;&#65292;&#35843;&#26597;&#36824;&#32473;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#30340;&#20845;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2203.01717</link><description>&lt;p&gt;
&#36873;&#25321;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#20174;&#19994;&#32773;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Practitioner Motives to Select Hyperparameter Optimization Methods. (arXiv:2203.01717v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01717
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#36873;&#25321;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#21160;&#26426;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#22522;&#20110;&#20010;&#20154;&#30446;&#26631;&#21644;&#32972;&#26223;&#22240;&#32032;&#65292;&#35843;&#26597;&#36824;&#32473;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#30340;&#20845;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#32534;&#31243;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#21487;&#38752;&#22320;&#25214;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#20540;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#32463;&#24120;&#24212;&#29992;&#26679;&#26412;&#25928;&#29575;&#36739;&#20302;&#30340;HPO&#26041;&#27861;&#65292;&#22914;&#32593;&#26684;&#25628;&#32034;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26410;&#32463;&#20248;&#21270;&#12290;&#25105;&#20204;&#24576;&#30097;&#65292;&#20174;&#19994;&#32773;&#36873;&#25321;HPO&#26041;&#27861;&#30340;&#21407;&#22240;&#22522;&#20110;&#20010;&#20154;&#21160;&#26426;&#65292;&#21253;&#25324;&#32972;&#26223;&#22240;&#32032;&#21644;&#20010;&#20154;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#30340;&#21160;&#26426;&#20173;&#28982;&#38656;&#35201;&#28548;&#28165;&#65292;&#36825;&#22952;&#30861;&#20102;&#35780;&#20272;HPO&#26041;&#27861;&#20197;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#21644;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#20026;&#20102;&#20102;&#35299;&#20174;&#19994;&#32773;&#20351;&#29992;&#29305;&#23450;HPO&#26041;&#27861;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;20&#20010;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#21644;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20849;&#26377;71&#21517;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#21442;&#19982;&#65292;&#20197;&#25910;&#38598;&#35775;&#35848;&#32467;&#26524;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;&#36890;&#36807;&#35774;&#32622;&#20845;&#20010;&#20027;&#35201;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#25913;&#36827;&#27169;&#22411;&#29702;&#35299;&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Advanced programmatic hyperparameter optimization (HPO) methods, such as Bayesian optimization, have high sample efficiency in reproducibly finding optimal hyperparameter values of machine learning (ML) models. Yet, ML practitioners often apply less sample-efficient HPO methods, such as grid search, which often results in under-optimized ML models. As a reason for this behavior, we suspect practitioners choose HPO methods based on individual motives, consisting of contextual factors and individual goals. However, practitioners' motives still need to be clarified, hindering the evaluation of HPO methods for achieving specific goals and the user-centered development of HPO tools. To understand practitioners' motives for using specific HPO methods, we used a mixed-methods approach involving 20 semi-structured interviews and a survey study with 71 ML experts to gather evidence of the external validity of the interview results. By presenting six main goals (e.g., improving model understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21015;&#32553;&#24433;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26679;&#26412;&#36991;&#20813;&#39640;&#32500;&#24230;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2202.11788</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#21015;&#32553;&#24433;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative modeling via tensor train sketching. (arXiv:2202.11788v6 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21015;&#32553;&#24433;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#23569;&#37327;&#26679;&#26412;&#36991;&#20813;&#39640;&#32500;&#24230;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#27010;&#29575;&#23494;&#24230;&#30340;&#24352;&#37327;&#21015;&#32553;&#24433;&#34920;&#31034;&#30340;&#33609;&#22270;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#36882;&#24402;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#21333;&#20010;&#24352;&#37327;&#21015;&#32553;&#24433;&#30340;&#23567;&#22411;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#23041;&#32961;&#21040;&#24674;&#22797;&#38382;&#39064;&#30340;&#31639;&#27861;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32500;&#24230;&#35781;&#21650;&#12290;&#38024;&#23545;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#24352;&#37327;&#26680;&#24515;&#21487;&#20197;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20197;&#23545;&#25968;&#24418;&#24335;&#38543;&#30528;&#32500;&#24230;&#32780;&#32553;&#25918;&#30340;&#33258;&#28982;&#26465;&#20214;&#19979;&#34987;&#24674;&#22797;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#23383;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a sketching algorithm for constructing a tensor train representation of a probability density from its samples. Our method deviates from the standard recursive SVD-based procedure for constructing a tensor train. Instead, we formulate and solve a sequence of small linear systems for the individual tensor train cores. This approach can avoid the curse of dimensionality that threatens both the algorithmic and sample complexities of the recovery problem. Specifically, for Markov models under natural conditions, we prove that the tensor cores can be recovered with a sample complexity that scales logarithmically in the dimensionality. Finally, we illustrate the performance of the method with several numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#39281;&#21644;&#22352;&#26631;&#21152;&#36895;&#35299;&#20915;&#38750;&#36127;&#21644;&#26377;&#30028;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.07258</link><description>&lt;p&gt;
&#20351;&#29992;&#23433;&#20840;&#31579;&#36873;&#21152;&#36895;&#38750;&#36127;&#21644;&#26377;&#30028;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerating Non-Negative and Bounded-Variable Linear Regression Algorithms with Safe Screening. (arXiv:2202.07258v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#39281;&#21644;&#22352;&#26631;&#21152;&#36895;&#35299;&#20915;&#38750;&#36127;&#21644;&#26377;&#30028;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#21644;&#26377;&#30028;&#21464;&#37327;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26377;&#25152;&#28041;&#21450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#35782;&#21035;&#39281;&#21644;&#22352;&#26631;&#65292;&#21152;&#36895;&#29616;&#26377;&#35299;&#20915;&#22120;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31867;&#20284;&#20110;&#20808;&#21069;&#38024;&#23545;&#31232;&#30095;&#27491;&#21017;&#21270;&#22238;&#24402;&#38382;&#39064;&#25552;&#20986;&#30340;&#23433;&#20840;&#31579;&#36873;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#26159;&#32463;&#36807;&#35777;&#26126;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#34920;&#26126;&#25152;&#35782;&#21035;&#30340;&#22352;&#26631;&#30830;&#23454;&#22312;&#26368;&#20248;&#35299;&#20013;&#26159;&#39281;&#21644;&#30340;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#36127;&#21644;&#26377;&#30028;&#21464;&#37327;&#38382;&#39064;&#37117;&#20855;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-negative and bounded-variable linear regression problems arise in a variety of applications in machine learning and signal processing. In this paper, we propose a technique to accelerate existing solvers for these problems by identifying saturated coordinates in the course of iterations. This is akin to safe screening techniques previously proposed for sparsity-regularized regression problems. The proposed strategy is provably safe as it provides theoretical guarantees that the identified coordinates are indeed saturated in the optimal solution. Experimental results on synthetic and real data show compelling accelerations for both non-negative and bounded-variable problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;ISNet&#65292;&#33021;&#22815;&#24573;&#30053;&#22270;&#20687;&#32972;&#26223;&#24182;&#22312;COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20013;&#27604;&#22810;&#20010;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20559;&#24046;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.00232</link><description>&lt;p&gt;
&#26080;&#38656;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65306;&#24573;&#30053;&#32972;&#26223;&#65292;&#25552;&#39640;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v6 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;DNN&#35270;&#35273;&#27880;&#24847;&#26426;&#21046;&#65292;&#21517;&#20026;ISNet&#65292;&#33021;&#22815;&#24573;&#30053;&#22270;&#20687;&#32972;&#26223;&#24182;&#22312;COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20013;&#27604;&#22810;&#20010;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20559;&#24046;&#24433;&#21709;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20307;&#31995;&#32467;&#26500;&#65292;&#31216;&#20026;ISNet&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;ISNet&#20351;&#29992;&#20998;&#21106;&#30446;&#26631;&#26469;&#23398;&#20064;&#22914;&#20309;&#25214;&#21040;&#22270;&#20687;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#24182;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#35813;&#21306;&#22495;&#19978;&#12290;&#35813;&#25552;&#35758;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;LRP&#35299;&#37322;&#28909;&#22270;&#20013;&#26368;&#23567;&#21270;&#32972;&#26223;&#30456;&#20851;&#24615;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#20219;&#20309;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#22312;&#36816;&#34892;&#26102;&#22686;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#30001;&#20110;&#33021;&#22815;&#24573;&#30053;&#32972;&#26223;&#65292;&#22240;&#27492;&#32467;&#26524;&#21333;&#20010;DNN&#21487;&#20197;&#20195;&#26367;&#24120;&#35265;&#30340;&#20998;&#21106;&#22120;&#21518;&#36319;&#20998;&#31867;&#22120;&#30340;&#27969;&#27700;&#32447;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#26356;&#36731;&#12290;&#22312;&#27880;&#20837;&#22270;&#20687;&#32972;&#26223;&#30340;&#21512;&#25104;&#20559;&#24046;&#20043;&#21518;&#65288;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65289;&#65292;&#25105;&#20204;&#23558;ISNet&#19982;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23450;&#37327;&#35777;&#26126;&#20854;&#22312;&#26368;&#23567;&#21270;&#20998;&#31867;&#22120;&#20915;&#31574;&#20013;&#20559;&#24046;&#24433;&#21709;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#12290; COVID-19&#21644;&#32467;&#26680;&#30149;&#26816;&#27979;&#20219;&#21153;&#20998;&#21035;&#20316;&#20026;&#20351;&#29992;&#26696;&#20363;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in ch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#32479;&#19968;&#22797;&#26434;&#31995;&#32479;&#20013;&#25104;&#23545;&#20132;&#20114;&#30340;&#32479;&#35745;&#24211;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#38469;&#26696;&#20363;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#26368;&#36866;&#21512;&#35299;&#20915;&#19968;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.11941</link><description>&lt;p&gt;
&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#25104;&#23545;&#20132;&#20114;&#36827;&#34892;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Unifying Pairwise Interactions in Complex Dynamics. (arXiv:2201.11941v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#32479;&#19968;&#22797;&#26434;&#31995;&#32479;&#20013;&#25104;&#23545;&#20132;&#20114;&#30340;&#32479;&#35745;&#24211;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#38469;&#26696;&#20363;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#26368;&#36866;&#21512;&#35299;&#20915;&#19968;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23478;&#20204;&#24050;&#24320;&#21457;&#20986;&#25968;&#30334;&#31181;&#25216;&#26415;&#26469;&#34913;&#37327;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#31243;&#23545;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20294;&#36825;&#20123;&#35745;&#31639;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#31995;&#25968;&#21040;&#22240;&#26524;&#25512;&#26029;&#65292;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#23450;&#37327;&#29702;&#35770;&#65292;&#36825;&#20123;&#29702;&#35770;&#20173;&#28982;&#22823;&#37096;&#20998;&#27809;&#26377;&#32852;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;237&#31181;&#25104;&#23545;&#20132;&#20114;&#32479;&#35745;&#37327;&#24211;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#26469;&#33258;&#24191;&#27867;&#23454;&#19990;&#30028;&#21644;&#27169;&#22411;&#29983;&#25104;&#31995;&#32479;&#30340;1053&#20010;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#19981;&#21516;&#25968;&#23398;&#20844;&#24335;&#20043;&#38388;&#30340;&#26032;&#20849;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#30340;&#32479;&#19968;&#22270;&#26223;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#31185;&#23398;&#30340;&#22810;&#31181;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#20986;&#26368;&#36866;&#21512;&#35299;&#20915;&#32473;&#23450;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#20851;&#31995;&#23545;&#30340;&#27010;&#24565;&#35268;&#23450;&#65292;&#25512;&#21160;&#25104;&#21151;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24320;&#25918;&#36719;&#20214;&#65292;&#20351;&#20849;&#21516;&#30740;&#31350;&#21464;&#24471;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientists have developed hundreds of techniques to measure the interactions between pairs of processes in complex systems. But these computational methods, from correlation coefficients to causal inference, rely on distinct quantitative theories that remain largely disconnected. Here we introduce a library of 237 statistics of pairwise interactions and assess their behavior on 1053 multivariate time series from a wide range of real-world and model-generated systems. Our analysis highlights new commonalities between different mathematical formulations, providing a unified picture of a rich interdisciplinary literature. Using three real-world case studies, we then show that simultaneously leveraging diverse methods from across science can uncover those most suitable for addressing a given problem, yielding interpretable understanding of the conceptual formulations of pairwise dependence that drive successful performance. Our framework is provided in extendable open software, enabling co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#23884;&#20837;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26032;&#27169;&#22411;DegreEmbed&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.09933</link><description>&lt;p&gt;
DegreEmbed: &#23558;&#23454;&#20307;&#23884;&#20837;&#34701;&#20837;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning. (arXiv:2112.09933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#23454;&#20307;&#23884;&#20837;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#30340;&#26032;&#27169;&#22411;DegreEmbed&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#34920;&#31034;&#65292;&#23427;&#20204;&#26159;&#26234;&#33021;&#25968;&#25454;&#24211;&#65292;&#23558;&#20154;&#31867;&#30693;&#35782;&#34701;&#20837;&#20854;&#20013;&#65292;&#24110;&#21161;&#26426;&#22120;&#27169;&#20223;&#20154;&#31867;&#30340;&#38382;&#39064;&#35299;&#20915;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#30340;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#65292;&#32780;&#19988;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#24212;&#29992;&#12290;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#36890;&#36807;&#22522;&#20110;&#29616;&#26377;&#30693;&#35782;&#25512;&#29702;&#26469;&#23436;&#25104;&#32570;&#22833;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#26377;&#20004;&#31181;&#30740;&#31350;&#27969;&#27966;&#65306;&#19968;&#31181;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20302;&#32500;&#24230;&#23884;&#20837;&#65292;&#21487;&#20197;&#25506;&#32034;&#28508;&#22312;&#27169;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#25366;&#25496;&#36923;&#36753;&#35268;&#21017;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28041;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#29616;&#20195;&#30693;&#35782;&#22270;&#35889;&#30340;&#24322;&#36136;&#24615;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DegreEmbed&#65292;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#23558;&#22522;&#20110;&#23884;&#20837;&#30340;&#23398;&#20064;&#21644;&#36923;&#36753;&#35268;&#21017;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;DegreEmbed&#23558;&#23454;&#20307;&#23884;&#20837;&#34701;&#20837;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#20013;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#23454;&#20307;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DegreEmbed&#22312;&#38142;&#25509;&#39044;&#27979;&#21644;&#35268;&#21017;&#25552;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability by mining logical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic rule mining for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#25361;&#25112;&#12289;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#22235;&#20010;&#26041;&#38754;&#12290;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#23558;&#31070;&#32463;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#31995;&#32479;&#30340;&#35748;&#30693;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2111.08164</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural-symbolic Learning Systems. (arXiv:2111.08164v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#25361;&#25112;&#12289;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#22235;&#20010;&#26041;&#38754;&#12290;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#23558;&#31070;&#32463;&#31995;&#32479;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#31995;&#32479;&#30340;&#35748;&#30693;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#31995;&#32479;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#26377;&#25928;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#20248;&#36234;&#30340;&#30693;&#35273;&#26234;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#31995;&#32479;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#35748;&#30693;&#26234;&#33021;&#65292;&#20294;&#19982;&#31070;&#32463;&#31995;&#32479;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;&#37492;&#20110;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#31070;&#32463;&#31995;&#32479;&#21644;&#31526;&#21495;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#22235;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#65288;&#25361;&#25112;&#12289;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#65289;&#35843;&#26597;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21521;&#21069;&#21457;&#23637;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20840;&#38754;&#21644;&#25972;&#20307;&#30340;&#27010;&#36848;&#12290;&#36825;&#20010;&#27010;&#36848;&#19981;&#20165;&#23558;&#31361;&#20986;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#36824;&#23558;&#30830;&#23450;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, neural systems have demonstrated highly effective learning ability and superior perception intelligence. However, they have been found to lack effective reasoning and cognitive ability. On the other hand, symbolic systems exhibit exceptional cognitive intelligence but suffer from poor learning capabilities when compared to neural systems. Recognizing the advantages and disadvantages of both methodologies, an ideal solution emerges: combining neural systems and symbolic systems to create neural-symbolic learning systems that possess powerful perception and cognition. The purpose of this paper is to survey the advancements in neural-symbolic learning systems from four distinct perspectives: challenges, methods, applications, and future directions. By doing so, this research aims to propel this emerging field forward, offering researchers a comprehensive and holistic overview. This overview will not only highlight the current state-of-the-art but also identify promising a
&lt;/p&gt;</description></item><item><title>OpenFWI&#26159;&#19968;&#20010;&#29992;&#20110;&#22320;&#38663;&#20840;&#27874;&#24418;&#21453;&#28436;&#30340;&#22823;&#35268;&#27169;&#22810;&#32467;&#26500;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#39046;&#22495;&#12289;&#19981;&#21516;&#22320;&#36136;&#22320;&#19979;&#32467;&#26500;&#21644;&#21508;&#31181;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#12290;&#20351;&#29992;OpenFWI&#36827;&#34892;&#20102;&#22235;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#29289;&#29702;&#39537;&#21160;&#30340;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2111.02926</link><description>&lt;p&gt;
OpenFWI&#65306;&#29992;&#20110;&#22320;&#38663;&#20840;&#27874;&#24418;&#21453;&#28436;&#30340;&#22823;&#35268;&#27169;&#22810;&#32467;&#26500;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenFWI: Large-Scale Multi-Structural Benchmark Datasets for Seismic Full Waveform Inversion. (arXiv:2111.02926v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02926
&lt;/p&gt;
&lt;p&gt;
OpenFWI&#26159;&#19968;&#20010;&#29992;&#20110;&#22320;&#38663;&#20840;&#27874;&#24418;&#21453;&#28436;&#30340;&#22823;&#35268;&#27169;&#22810;&#32467;&#26500;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#39046;&#22495;&#12289;&#19981;&#21516;&#22320;&#36136;&#22320;&#19979;&#32467;&#26500;&#21644;&#21508;&#31181;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#12290;&#20351;&#29992;OpenFWI&#36827;&#34892;&#20102;&#22235;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#29289;&#29702;&#39537;&#21160;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436; &#65288;FWI&#65289;&#26159;&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#29992;&#20110;&#20174;&#22320;&#38663;&#25968;&#25454;&#20013;&#37325;&#24314;&#39640;&#20998;&#36776;&#29575;&#36895;&#24230;&#22270;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;FWI&#26041;&#27861;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#23545;&#24320;&#25918;&#25968;&#25454;&#38598;&#22312;&#22320;&#29699;&#29289;&#29702;&#23398;&#30028;&#30340;&#38656;&#27714;&#24555;&#36895;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OpenFWI&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#32467;&#26500;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#65292;&#20197;&#20419;&#36827;FWI&#30340;&#22810;&#26679;&#21270;&#12289;&#20005;&#26684;&#21644;&#21487;&#37325;&#29616;&#30340;&#30740;&#31350;&#12290;OpenFWI&#30001;12&#20010;&#25968;&#25454;&#38598;&#65288;&#24635;&#20849;2.1TB&#65289;&#32452;&#25104;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;&#22810;&#20010;&#26469;&#28304;&#21512;&#25104;&#30340;&#65292;&#28085;&#30422;&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#19981;&#21516;&#30340;&#39046;&#22495;&#65288;&#30028;&#38754;&#12289;&#26029;&#23618;&#12289;CO2&#20648;&#23618;&#31561;&#65289;&#65292;&#35206;&#30422;&#19981;&#21516;&#30340;&#22320;&#36136;&#22320;&#19979;&#32467;&#26500;&#65288;&#24179;&#22374;&#30340;&#12289;&#26354;&#32447;&#30340;&#31561;&#65289;&#65292;&#21253;&#21547;&#21508;&#31181;&#25968;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#65288;2K-67K&#65289;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;3D FWI&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;OpenFWI&#23545;&#22235;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#38500;&#20102;&#22522;&#20934;&#27979;&#35797;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29289;&#29702;&#21270;&#39537;&#21160;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of 12 datasets (2.1TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO2 reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contains various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#27010;&#29575;&#19978;GNN&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#26377;&#20851;&#65292;&#24182;&#19988;&#32467;&#26524;&#21487;&#25193;&#23637;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2106.08992</link><description>&lt;p&gt;
GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;GNN&#22312;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#27010;&#29575;&#19978;GNN&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#26377;&#20851;&#65292;&#24182;&#19988;&#32467;&#26524;&#21487;&#25193;&#23637;&#29992;&#20110;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26159;&#19968;&#31181;&#24191;&#27867;&#30340;&#29992;&#20110;&#22270;&#22788;&#29702;&#30340;&#36830;&#25509;&#24335;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#36880;&#27493;&#36924;&#36817;&#20851;&#20110;&#22270;&#30340;&#20219;&#24847;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#30001;Weisfeiler--Lehman(WL)&#27979;&#35797;&#23450;&#20041;&#30340;&#22270;&#31561;&#20215;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#19968;&#26041;&#38754;&#22240;&#20026;&#23427;&#20204;&#26159;&#29992;Stone-Weierstrass&#23450;&#29702;&#26469;&#25512;&#23548;&#24471;&#20986;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#26412;&#36136;&#19978;&#21482;&#26159;&#19968;&#31181;&#23384;&#22312;&#24615;&#35777;&#26126;&#65292;&#21478;&#19968;&#26041;&#38754;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#24403;&#21069;&#30340;&#32467;&#26524;&#37117;&#26159;&#19987;&#38376;&#38024;&#23545;&#22270;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#30340;&#65292;&#32780;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#38382;&#39064;&#20063;&#38750;&#24120;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;GNN&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;/&#22238;&#24402;&#20219;&#21153;&#65292;&#26080;&#35770;&#30446;&#26631;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#36824;&#26159;&#31163;&#25955;&#30340;&#65292;GNN&#22312;&#27010;&#29575;&#19978;&#37117;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#20219;&#20309;&#26377;&#38480;&#22495;&#19978;&#30340;&#21487;&#27979;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21482;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#12289;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;&#20849;&#20139;&#25152;&#26377;&#33410;&#28857;&#30340;&#36793;&#26435;&#20540;&#30340;GNN&#36827;&#34892;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20063;&#25581;&#31034;&#20102;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#38543;&#20854;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#21464;&#21270;&#21450;&#20854;&#32467;&#26524;&#38024;&#23545;&#22823;&#22411;&#22270;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#36890;&#36807;&#21452;&#22240;&#32032;&#25200;&#21160;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2103.03102</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#22240;&#32032;&#25200;&#21160;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#36890;&#36807;&#21452;&#22240;&#32032;&#25200;&#21160;&#26469;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20316;&#32773;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22235;&#35937;&#38480;&#32479;&#35745;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#21253;&#25324;&#26368;&#23567;&#20934;&#30830;&#24615;&#12289;&#26368;&#22823;&#20934;&#30830;&#24615;&#12289;&#24179;&#22343;&#20934;&#30830;&#24615;&#21644;&#21464;&#24322;&#31995;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;69&#20010;&#22522;&#20934;&#27979;&#35797;&#22270;&#20687;&#38598;&#65292;&#21253;&#25324;&#19968;&#20010;&#24178;&#20928;&#30340;&#38598;&#21512;&#12289;&#21333;&#22240;&#32032;&#25200;&#21160;&#30340;&#38598;&#21512;&#21644;&#21452;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#30340;&#38598;&#21512;&#65292;&#20197;&#34913;&#37327;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#25910;&#38598;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25253;&#21578;&#20351;&#29992;&#21452;&#22240;&#32032;&#25200;&#21160;&#22270;&#20687;&#21487;&#20197;&#25552;&#39640;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#21452;&#22240;&#32032;&#25200;&#21160;&#21253;&#25324;&#65288;1&#65289;&#22312;&#20004;&#20010;&#24207;&#21015;&#20013;&#37117;&#24212;&#29992;&#30340;&#20004;&#20010;&#25968;&#23383;&#25200;&#21160;&#65288;&#26898;&#30416;&#22122;&#22768;&#21644;&#39640;&#26031;&#22122;&#22768;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#25968;&#23383;&#25200;&#21160;&#65288;&#26898;&#30416;&#22122;&#22768;&#65289;&#21644;&#19968;&#20010;&#20960;&#20309;&#25200;&#21160;&#65288;&#26059;&#36716;&#65289;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#23545;&#20004;&#22240;&#32032;&#25200;&#21160;&#26377;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#34892;&#20026;&#65292;&#26292;&#38706;&#20102;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#19981;&#22343;&#21248;&#24615;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20004;&#22240;&#32032;&#25200;&#21160;&#26465;&#20214;&#19979;DL&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#40065;&#26834;&#30340;DL&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper adds to the fundamental body of work on benchmarking the robustness of deep learning (DL) classifiers. We innovate a new benchmarking methodology to evaluate robustness of DL classifiers. Also, we introduce a new four-quadrant statistical visualization tool, including minimum accuracy, maximum accuracy, mean accuracy, and coefficient of variation, for benchmarking robustness of DL classifiers. To measure robust DL classifiers, we created a comprehensive 69 benchmarking image set, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. After collecting experimental results, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. The two-factor perturbation includes (1) two digital perturbations (salt &amp; pepper noise and Gaussian noise) applied in both sequences, and (2) one digital perturbation (salt &amp; pepper noise) and a geometric perturbation (rotation) applied in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#26041;&#27861;&#65292;&#26082;&#20351;&#36895;&#24230;-&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#26356;&#24179;&#28369;&#65292;&#21516;&#26102;&#22312;ImageNet&#21644;CIFAR-100&#25968;&#25454;&#38598;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#21487;&#20197;&#27604;&#26631;&#20934;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#25216;&#26415;&#26356;&#24555;30&#65285;&#12290;</title><link>http://arxiv.org/abs/2012.10769</link><description>&lt;p&gt;
&#32593;&#32476;&#20869;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmentation Inside the Network. (arXiv:2012.10769v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24341;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#23618;&#30340;&#26041;&#27861;&#65292;&#26082;&#20351;&#36895;&#24230;-&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#26356;&#24179;&#28369;&#65292;&#21516;&#26102;&#22312;ImageNet&#21644;CIFAR-100&#25968;&#25454;&#38598;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#21487;&#20197;&#27604;&#26631;&#20934;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#25216;&#26415;&#26356;&#24555;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#32593;&#32476;&#20869;&#25968;&#25454;&#22686;&#24378;&#8221;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20013;&#38388;&#23618;&#27169;&#25311;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#21464;&#25442;&#25968;&#25454;&#27969;&#65292;&#24182;&#22312;&#21487;&#33021;&#26102;&#20849;&#20139;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#33719;&#24471;&#26356;&#24179;&#28369;&#30340;&#36895;&#24230;-&#20934;&#30830;&#29575;&#25240;&#34935;&#35843;&#25972;&#65292;&#27604;&#20351;&#29992;&#26631;&#20934;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#25216;&#26415;&#26356;&#22909;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#30456;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;ImageNet-2012&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#32763;&#36716;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#26356;&#24555;30&#65285;&#65292;&#22312;CIFAR-100&#19978;&#36798;&#21040;&#20102;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present augmentation inside the network, a method that simulates data augmentation techniques for computer vision problems on intermediate features of a convolutional neural network. We perform these transformations, changing the data flow through the network, and sharing common computations when it is possible. Our method allows us to obtain smoother speed-accuracy trade-off adjustment and achieves better results than using standard test-time augmentation (TTA) techniques. Additionally, our approach can improve model performance even further when coupled with test-time augmentation. We validate our method on the ImageNet-2012 and CIFAR-100 datasets for image classification. We propose a modification that is 30% faster than the flip test-time augmentation and achieves the same results for CIFAR-100.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#36807;&#31243;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#39564;&#35777;&#36825;&#20123;&#26816;&#27979;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#31867;&#27861;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#29983;&#25104;&#36829;&#21453;&#31995;&#32479;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#25239;&#27450;&#39575;&#20449;&#21495;&#12290;&#22312;&#24050;&#21457;&#34920;&#30340;&#22235;&#20010;&#26816;&#27979;&#22120;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20854;&#40065;&#26834;&#24615;&#26469;&#33258;&#20110;&#23646;&#24615;&#30340;&#24341;&#20837;&#12290;&#36890;&#36807;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#26041;&#26696;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2012.03586</link><description>&lt;p&gt;
&#26080;&#38656;&#29289;&#29702;&#30693;&#35782;&#65306;&#22522;&#20110;&#36807;&#31243;&#30340;&#26080;&#27169;&#22411;&#24322;&#24120;&#26816;&#27979;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
No Need to Know Physics: Resilience of Process-based Model-free Anomaly Detection for Industrial Control Systems. (arXiv:2012.03586v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#36807;&#31243;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#39564;&#35777;&#36825;&#20123;&#26816;&#27979;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#31867;&#27861;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#29983;&#25104;&#36829;&#21453;&#31995;&#32479;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#25239;&#27450;&#39575;&#20449;&#21495;&#12290;&#22312;&#24050;&#21457;&#34920;&#30340;&#22235;&#20010;&#26816;&#27979;&#22120;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20854;&#40065;&#26834;&#24615;&#26469;&#33258;&#20110;&#23646;&#24615;&#30340;&#24341;&#20837;&#12290;&#36890;&#36807;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#26041;&#26696;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#36807;&#31243;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#23545;&#27492;&#31867;&#26041;&#26696;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#20171;&#32461;&#20102;&#39564;&#35777;&#36825;&#20123;&#26816;&#27979;&#31995;&#32479;&#30340;&#23646;&#24615;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#29983;&#25104;&#36829;&#21453;&#31995;&#32479;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#25239;&#27450;&#39575;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#26469;&#20998;&#26512;&#22312;&#39030;&#32423;&#23433;&#20840;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#22235;&#20010;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#19977;&#20010;&#26816;&#27979;&#22120;&#23481;&#26131;&#21463;&#21040;&#35768;&#22810;&#23545;&#25239;&#24615;&#25805;&#20316;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#39044;&#20808;&#35745;&#31639;&#30340;&#27169;&#24335;&#36827;&#34892;&#27450;&#39575;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21512;&#25104;&#20256;&#24863;&#22120;&#27450;&#39575;&#65292;&#21482;&#26377;&#19968;&#20010;&#26816;&#27979;&#22120;&#23545;&#25105;&#20204;&#30340;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#40065;&#26834;&#24615;&#30340;&#26681;&#28304;&#65292;&#24182;&#35777;&#26126;&#20102;&#26469;&#33258;&#25105;&#20204;&#24341;&#20837;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#38477;&#20302;&#20102;&#25915;&#20987;&#26041;&#26696;&#30340;&#21484;&#22238;&#29575;&#65288;&#30495;&#38451;&#24615;&#29575;&#65289;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#26816;&#27979;&#24322;&#24120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
In recent years, a number of process-based anomaly detection schemes for Industrial Control Systems were proposed. In this work, we provide the first systematic analysis of such schemes, and introduce a taxonomy of properties that are verified by those detection systems. We then present a novel general framework to generate adversarial spoofing signals that violate physical properties of the system, and use the framework to analyze four anomaly detectors published at top security conferences. We find that three of those detectors are susceptible to a number of adversarial manipulations (e.g., spoofing with precomputed patterns), which we call Synthetic Sensor Spoofing and one is resilient against our attacks. We investigate the root of its resilience and demonstrate that it comes from the properties that we introduced. Our attacks reduce the Recall (True Positive Rate) of the attacked schemes making them not able to correctly detect anomalies. Thus, the vulnerabilities we discovered in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2004.12908</link><description>&lt;p&gt;
&#20195;&#34920;&#24615;&#38598;&#25104;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#21327;&#21516;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24403;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#20043;&#21069;&#21644;&#23578;&#26410;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21017;&#20174;&#31354;&#30333;&#29366;&#24577;&#24320;&#22987;&#65292;&#20165;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;&#31216;&#20026;&#36951;&#24536;&#65289;&#12290;&#36817;&#26399;&#38024;&#23545;&#36830;&#32493;&#25110;&#32456;&#36523;&#23398;&#20064;&#30340;&#35768;&#22810;&#26041;&#27861;&#37117;&#35797;&#22270;&#22312;&#32473;&#23450;&#26032;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20165;&#21162;&#21147;&#36991;&#20813;&#24536;&#35760;&#23558;&#30446;&#26631;&#23450;&#24471;&#36807;&#20302;&#12290;&#32456;&#36523;&#23398;&#20064;&#30340;&#30446;&#26631;&#19981;&#20165;&#24212;&#35813;&#26159;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#65288;&#21069;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#24212;&#35813;&#26159;&#29992;&#20219;&#20309;&#26032;&#25968;&#25454;&#25552;&#39640;&#36807;&#21435;&#20219;&#21153;&#65288;&#21453;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21327;&#21516;&#38598;&#25104;&#20998;&#21035;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#38598;&#25104;&#65288;RELL&#65289;&#8221;&#65292;&#23427;&#38598;&#25104;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#34920;&#31034;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RELL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CRF&#27169;&#22411;&#65292;&#31216;&#20026;Speaker-change Aware CRF&#65292;&#20197;&#32771;&#34385;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#35828;&#35805;&#20154;&#21464;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2004.02913</link><description>&lt;p&gt;
&#22522;&#20110;&#35828;&#35805;&#20154;&#24863;&#30693;&#30340;CRF&#29992;&#20110;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Speaker-change Aware CRF for Dialogue Act Classification. (arXiv:2004.02913v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CRF&#27169;&#22411;&#65292;&#31216;&#20026;Speaker-change Aware CRF&#65292;&#20197;&#32771;&#34385;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#35828;&#35805;&#20154;&#21464;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23545;&#35805;&#34892;&#20026;&#65288;DA&#65289;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#23558;&#35813;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#26631;&#27880;&#38382;&#39064;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20197;&#21450;&#20316;&#20026;&#26368;&#21518;&#19968;&#23618;&#30340;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#12290;CRF&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#35805;&#35821;&#24207;&#21015;&#26469;&#24314;&#27169;&#30446;&#26631;DA&#26631;&#31614;&#24207;&#21015;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#20219;&#21153;&#36824;&#28041;&#21450;&#21040;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#21363;&#35828;&#35805;&#20154;&#24207;&#21015;&#65292;&#32780;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#36825;&#26041;&#38754;&#27809;&#26377;&#20570;&#20986;&#32771;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;CRF&#23618;&#30340;&#20462;&#25913;&#65292;&#35813;&#23618;&#32771;&#34385;&#21040;&#35828;&#35805;&#20154;&#21464;&#21270;&#12290;&#23545;SwDA&#35821;&#26009;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20462;&#25913;&#21518;&#30340;CRF&#23618;&#20248;&#20110;&#21407;&#22987;&#23618;&#65292;&#22312;&#26576;&#20123;DA&#26631;&#31614;&#19978;&#30340;&#24046;&#36317;&#38750;&#24120;&#22823;&#12290;&#27492;&#22806;&#65292;&#21487;&#35270;&#21270;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;CRF&#23618;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#22312;&#35828;&#35805;&#20154;&#20132;&#26367;&#26465;&#20214;&#19979;DA&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#12289;&#22797;&#26434;&#30340;&#36716;&#31227;&#27169;&#24335;&#12290;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in Dialogue Act (DA) classification approaches the task as a sequence labeling problem, using neural network models coupled with a Conditional Random Field (CRF) as the last layer. CRF models the conditional probability of the target DA label sequence given the input utterance sequence. However, the task involves another important input sequence, that of speakers, which is ignored by previous work. To address this limitation, this paper proposes a simple modification of the CRF layer that takes speaker-change into account. Experiments on the SwDA corpus show that our modified CRF layer outperforms the original one, with very wide margins for some DA labels. Further, visualizations demonstrate that our CRF layer can learn meaningful, sophisticated transition patterns between DA label pairs conditioned on speaker-change in an end-to-end way. Code is publicly available.
&lt;/p&gt;</description></item></channel></rss>