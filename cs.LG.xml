<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01632</link><description>&lt;p&gt;
&#36229;&#36234;&#23610;&#24230;&#65306;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#26080;&#36951;&#25022;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#38656;&#35201;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#32780;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#25351;&#23450;&#36229;&#21442;&#25968; - &#22823;&#37096;&#20998;&#29702;&#35770;&#25991;&#29486;&#20551;&#35774;&#36825;&#20123;&#36229;&#21442;&#25968;&#26159;&#24050;&#30693;&#30340;&#12290;&#20043;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#22312;&#31354;&#38388;&#20013;&#22343;&#21248;&#22635;&#20805;&#65292;&#32780;&#24120;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#21482;&#26377;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25165;&#26159;&#19968;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25968;&#25454;&#19981;&#19968;&#23450;&#28385;&#36275;&#36825;&#31181;&#22343;&#21248;&#22635;&#20805;&#30340;&#26465;&#20214;&#12290;&#30001;&#20110;&#26080;&#27861;&#20445;&#35777;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36229;&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#65292;&#22240;&#27492;&#23545;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#20165;&#33021;&#22788;&#29702;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26410;&#30693;&#38271;&#24230;&#23610;&#24230;&#12289;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#33539;&#25968;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#39057;&#29575;&#27966;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;HE-GP-UCB&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IM-3D&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#30452;&#25509;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.08682</link><description>&lt;p&gt;
IM-3D&#65306;&#29992;&#20110;&#39640;&#36136;&#37327;3D&#29983;&#25104;&#30340;&#36845;&#20195;&#22810;&#35270;&#35282;&#25193;&#25955;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IM-3D&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#30452;&#25509;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#22120;&#37117;&#26159;&#22522;&#20110;&#24050;&#35757;&#32451;&#36807;&#30340;&#25968;&#21313;&#20159;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26500;&#24314;&#30340;&#12290;&#23427;&#20204;&#20351;&#29992;Score Distillation Sampling (SDS)&#30340;&#21464;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#36739;&#24930;&#12289;&#19981;&#22826;&#31283;&#23450;&#19988;&#23481;&#26131;&#20135;&#29983;&#20266;&#24433;&#12290;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#26159;&#23558;2D&#29983;&#25104;&#22120;&#24494;&#35843;&#20026;&#22810;&#35270;&#35282;&#24863;&#30693;&#65292;&#21487;&#20197;&#24110;&#21161;&#28040;&#38500;&#20266;&#24433;&#65292;&#25110;&#32773;&#19982;&#37325;&#26500;&#32593;&#32476;&#32467;&#21512;&#65292;&#30452;&#25509;&#36755;&#20986;3D&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#32780;&#38750;&#22270;&#20687;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#22810;&#35270;&#35282;&#29983;&#25104;&#12290;&#32467;&#21512;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#40065;&#26834;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#30452;&#25509;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;IM-3D&#23558;2D&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#35745;&#31639;&#27425;&#25968;&#20943;&#23569;&#20102;10-100&#20493;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#12289;&#26356;&#23569;&#30340;&#20960;&#20309;&#19981;&#19968;&#33268;&#24615;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08679</link><description>&lt;p&gt;
COLD-Attack: &#29992;&#20110;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36234;&#29425;&#30340;&#27880;&#24847;&#21147;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#36234;&#29425;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#20197;&#21450;&#24773;&#24863;/&#39118;&#26684;&#21464;&#21270;&#65292;&#22240;&#27492;&#30740;&#31350;&#21487;&#25511;&#24615;&#36234;&#29425;&#26159;&#26377;&#30410;&#30340;&#65292;&#21363;&#22914;&#20309;&#23545;LLM&#25915;&#20987;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24418;&#24335;&#21270;&#20102;&#21487;&#25511;&#24615;&#25915;&#20987;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#19982;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#26032;&#22411;&#20851;&#32852;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#65288;COLD&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#19988;&#33258;&#21160;&#21270;&#22320;&#25628;&#32034;&#21508;&#31181;&#25511;&#21046;&#35201;&#27714;&#19979;&#30340;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#65292;&#20363;&#22914;&#27969;&#30021;&#24615;&#12289;&#38544;&#31192;&#24615;&#12289;&#24773;&#24863;&#21644;&#24038;&#21491;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#26694;&#26550;&#8212;&#8212;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;GNNs&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08678</link><description>&lt;p&gt;
&#22270;&#39532;&#24052;&#65306;&#38754;&#21521;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Mamba: Towards Learning on Graphs with State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#26694;&#26550;&#8212;&#8212;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;GNNs&#30340;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#22823;&#22810;&#25968;GNNs&#23450;&#20041;&#20102;&#19968;&#31181;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#36890;&#36807;&#22534;&#21472;&#22810;&#20010;&#23618;&#22312;&#22270;&#19978;&#20256;&#25773;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#30693;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#36807;&#24230;&#21387;&#32553;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#12290;&#26368;&#36817;&#65292;&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30340;&#19968;&#31181;&#24378;&#22823;&#26367;&#20195;&#26041;&#27861;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;GTs&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#65292;&#22312;&#22270;&#32467;&#26500;&#19978;&#32570;&#20047;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#19988;&#20381;&#36182;&#22797;&#26434;&#30340;&#20301;&#32622;/&#32467;&#26500;&#32534;&#30721;&#65288;SE/PE&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#65292;&#23613;&#31649;Transformer&#12289;&#22797;&#26434;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;SE/PE&#23545;&#20110;&#33391;&#22909;&#24615;&#33021;&#32780;&#35328;&#26159;&#36275;&#22815;&#30340;&#65292;&#20294;&#24182;&#38750;&#24517;&#38656;&#12290;&#21463;&#21040;&#26368;&#36817;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65288;&#20363;&#22914;Mamba&#65289;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#39532;&#24052;&#32593;&#32476;&#65288;GMNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#24615;SSMs&#30340;&#26032;&#31867;GNNs&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#35752;&#35770;&#24182;&#23545;&#26032;&#30340;c&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#21160;&#21147;&#23398;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08676</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#25910;&#25947;&#24615;&#20998;&#26512;&#21450;&#20854;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Convergence Analysis of Approximate Message Passing with Non-Separable Functions and Applications to Multi-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#21160;&#21147;&#23398;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26368;&#36817;&#23558;&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#24212;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#20984;&#20248;&#21270;&#20998;&#26512;&#30340;&#21551;&#21457;[Loureiro, et. al., 2021]&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#38750;&#21487;&#20998;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;AMP&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25152;&#28608;&#21457;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#23436;&#25972;&#65288;&#21644;&#29420;&#31435;&#30340;&#65289;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent application of approximate message passing (AMP) to the analysis of convex optimizations in multi-class classifications [Loureiro, et. al., 2021], we present a convergence analysis of AMP dynamics with non-separable multivariate nonlinearities. As an application, we present a complete (and independent) analysis of the motivated convex optimization problem.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08674</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20154;&#31867;&#35838;&#31243;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08674
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#35268;&#21017;&#32467;&#26500;&#21644;&#35757;&#32451;&#20013;&#25152;&#20351;&#29992;&#30340;&#31034;&#20363;&#35838;&#31243;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#30001;&#31616;&#27905;&#35268;&#21017;&#25511;&#21046;&#30340;&#20219;&#21153;&#20013;&#65292;&#24403;&#30456;&#20851;&#31034;&#20363;&#22312;&#22810;&#27425;&#35797;&#39564;&#20013;&#34987;&#20998;&#32452;&#26102;&#65292;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#65307;&#20294;&#22312;&#32570;&#20047;&#36825;&#26679;&#30340;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20132;&#38169;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#27809;&#26377;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#65288;ICL&#65289;&#22312;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#21457;&#20135;&#29983;&#20102;&#21516;&#26679;&#30340;&#26435;&#34913;&#12290;ICL&#26159;&#36890;&#36807;&#20869;&#23618;&#24490;&#29615;&#31639;&#27861;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#20013;&#23454;&#29616;&#30340;&#19968;&#31181;&#8220;&#19978;&#19979;&#25991;&#20869;&#23398;&#20064;&#8221;&#65288;in-context learning&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26435;&#37325;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20803;&#23398;&#20064;&#21464;&#21387;&#22120;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#22312;&#28041;&#21450;&#35268;&#21017;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20154;&#31867;&#25152;&#31034;&#30340;&#20998;&#32452;&#20248;&#21183;&#65292;&#32780;&#21516;&#26102;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#21017;&#22797;&#21046;&#20102;&#20154;&#31867;&#22312;&#32570;&#23569;&#36825;&#26679;&#32467;&#26500;&#30340;&#20219;&#21153;&#19978;&#25152;&#35266;&#23519;&#21040;&#30340;&#20132;&#38169;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08672</link><description>&lt;p&gt;
&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Assessment and Selection under Temporal Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#24403;&#21069;&#26102;&#26399;&#21644;&#21382;&#21490;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#21644;&#21487;&#33021;&#20219;&#24847;&#30340;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#32473;&#23450;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#36825;&#31181;&#31574;&#30053;&#36824;&#36890;&#36807;&#20272;&#35745;&#20004;&#20010;&#20505;&#36873;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#35823;&#24046;&#24046;&#24322;&#26469;&#26041;&#20415;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20004;&#20004;&#27604;&#36739;&#25972;&#21512;&#21040;&#21333;&#22330;&#28120;&#27760;&#36187;&#20013;&#65292;&#20174;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#25968;&#25454;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22238;&#24402;&#25439;&#22833;&#26469;&#20272;&#35745;&#30446;&#26631;&#20998;&#24067;&#30340;&#22122;&#22768;&#29256;&#26412;&#30340;&#20998;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#22122;&#22768;&#27700;&#24179;&#19979;&#33021;&#22815;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20998;&#25968;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08667</link><description>&lt;p&gt;
&#30446;&#26631;&#20998;&#25968;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Target Score Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22238;&#24402;&#25439;&#22833;&#26469;&#20272;&#35745;&#30446;&#26631;&#20998;&#24067;&#30340;&#22122;&#22768;&#29256;&#26412;&#30340;&#20998;&#25968;&#12290;&#19982;&#20256;&#32479;&#30340;&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#22122;&#22768;&#27700;&#24179;&#19979;&#33021;&#22815;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20998;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#36890;&#36807;&#26368;&#23567;&#21270;&#22238;&#24402;&#25439;&#22833;&#26469;&#20272;&#35745;&#30446;&#26631;&#20998;&#24067;&#30340;&#22122;&#22768;&#29256;&#26412;&#30340;&#20998;&#25968;&#65292;&#24182;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#27969;&#34892;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22122;&#22768;&#20998;&#25968;&#21305;&#37197;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#26159;&#22312;&#20302;&#22122;&#22768;&#27700;&#24179;&#19979;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#20998;&#25968;&#20272;&#35745;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#29289;&#29702;&#31185;&#23398;&#21644;&#23545;&#20110;&#24050;&#30693;&#24178;&#20928;&#30340;&#21407;&#22987;&#30446;&#26631;&#20998;&#25968;&#30340;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#20219;&#21153;&#20013;&#29305;&#21035;&#19981;&#21033;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20272;&#35745;&#30446;&#26631;&#31245;&#26377;&#22122;&#22768;&#29256;&#26412;&#30340;&#20998;&#25968;&#24212;&#35813;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21033;&#29992;&#30446;&#26631;&#20998;&#25968;&#30340;&#30693;&#35782;&#30830;&#23454;&#21487;&#20197;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#20998;&#25968;&#36523;&#20221;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#20998;&#25968;&#21305;&#37197;&#22238;&#24402;&#25439;&#22833;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20302;&#22122;&#22768;&#27700;&#24179;&#19979;&#33719;&#24471;&#20855;&#26377;&#26377;&#21033;&#23646;&#24615;&#30340;&#20998;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Score Matching estimates the score of a noised version of a target distribution by minimizing a regression loss and is widely used to train the popular class of Denoising Diffusion Models. A well known limitation of Denoising Score Matching, however, is that it yields poor estimates of the score at low noise levels. This issue is particularly unfavourable for problems in the physical sciences and for Monte Carlo sampling tasks for which the score of the clean original target is known. Intuitively, estimating the score of a slightly noised version of the target should be a simple task in such cases. In this paper, we address this shortcoming and show that it is indeed possible to leverage knowledge of the target score. We present a Target Score Identity and corresponding Target Score Matching regression loss which allows us to obtain score estimates admitting favourable properties at low noise levels.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#30456;&#20301;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25955;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#21160;&#29289;&#27493;&#24577;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.08662</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#25955;&#30340;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#26032;&#29983;&#27493;&#24577;&#65306;&#20851;&#20110;&#35266;&#23519;&#12289;&#22870;&#21169;&#21644;&#21453;&#39304;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#30456;&#20301;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25955;&#30456;&#20301;&#25391;&#33633;&#22120;&#23398;&#20064;&#21160;&#29289;&#27493;&#24577;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30456;&#20301;&#25391;&#33633;&#22120;&#27169;&#22411;&#26469;&#23398;&#20064;&#22235;&#36275;&#21160;&#29289;&#30340;&#36816;&#21160;&#12290;&#27599;&#20010;&#25391;&#33633;&#22120;&#20165;&#19982;&#33258;&#36523;&#21644;&#30456;&#24212;&#30340;&#33151;&#20043;&#38388;&#36890;&#36807;&#22320;&#38754;&#21453;&#20316;&#29992;&#21147;&#30340;&#23616;&#37096;&#21453;&#39304;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#35266;&#23519;&#32773;&#21453;&#39304;&#22686;&#30410;&#12290;&#25105;&#20204;&#23558;&#25391;&#33633;&#22120;&#26412;&#36523;&#35299;&#37322;&#20026;&#28508;&#22312;&#30340;&#25509;&#35302;&#29366;&#24577;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#28040;&#38500;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#30456;&#20301;&#35266;&#23519;&#12289;&#31616;&#21333;&#30340;&#22522;&#20110;&#30456;&#20301;&#30340;&#22870;&#21169;&#21644;&#23616;&#37096;&#21453;&#39304;&#21160;&#21147;&#23398;&#30340;&#32467;&#21512;&#24341;&#23548;&#20986;&#23637;&#29616;&#26032;&#29983;&#27493;&#24577;&#20559;&#22909;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#19968;&#32452;&#31616;&#21270;&#30340;&#22870;&#21169;&#65292;&#32780;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#29305;&#23450;&#30340;&#27493;&#24577;&#12290;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#35270;&#39057;&#25688;&#35201;&#21487;&#22312;https://youtu.be/1NKQ0rSV3jU&#22788;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;QuGAP-&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.08648</link><description>&lt;p&gt;
&#20026;&#37327;&#23376;&#20998;&#31867;&#22120;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Generating Universal Adversarial Perturbations for Quantum Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;QuGAP-&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#20316;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#33021;&#21147;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20687;&#32463;&#20856;&#30340;&#27169;&#22411;&#19968;&#26679;&#65292;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQC&#65289;&#30340;QML&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#22312;&#37327;&#23376;&#39046;&#22495;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#20102;&#23384;&#22312;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#65292;&#24182;&#19988;&#24050;&#22312;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuGAP&#65306;&#19968;&#20010;&#20026;&#37327;&#23376;&#20998;&#31867;&#22120;&#29983;&#25104;UAP&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;PQC&#30340;&#20998;&#31867;&#22120;&#30340;&#21152;&#24615;UAP&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#65288;QuGAP-A&#65289;&#26469;&#21046;&#36896;&#21152;&#24615;UAP&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#31867;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#21387;&#32553;&#26041;&#27861;&#29983;&#25104;&#24186;&#27491;UAP&#30340;&#26032;&#26041;&#27861;&#65288;QuGAP-U&#65289;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31397;&#25506;&#20102;&#27531;&#24046;&#23398;&#20064;&#30340;&#20869;&#24149;&#65292;&#21457;&#29616;&#20102;&#23548;&#33268;&#24179;&#38754;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#22833;&#36133;&#30340;&#8220;&#36880;&#28176;&#28040;&#22833;&#30340;&#36755;&#20837;&#8221;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20445;&#25345;&#26356;&#22909;&#30340;&#24184;&#23384;&#31070;&#32463;&#20803;&#19979;&#30028;&#30340;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.08645</link><description>&lt;p&gt;
&#31397;&#25506;&#27531;&#24046;&#23398;&#20064;&#30340;&#20869;&#24149;
&lt;/p&gt;
&lt;p&gt;
Peeking Behind the Curtains of Residual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31397;&#25506;&#20102;&#27531;&#24046;&#23398;&#20064;&#30340;&#20869;&#24149;&#65292;&#21457;&#29616;&#20102;&#23548;&#33268;&#24179;&#38754;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#22833;&#36133;&#30340;&#8220;&#36880;&#28176;&#28040;&#22833;&#30340;&#36755;&#20837;&#8221;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20445;&#25345;&#26356;&#22909;&#30340;&#24184;&#23384;&#31070;&#32463;&#20803;&#19979;&#30028;&#30340;&#27531;&#24046;&#36830;&#25509;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#23398;&#20064;&#22312;&#28145;&#24230;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#27531;&#24046;&#23398;&#20064;&#25104;&#21151;&#30340;&#22522;&#26412;&#21407;&#29702;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#36825;&#38459;&#30861;&#20102;&#28145;&#24230;&#21487;&#25193;&#23637;&#24179;&#38754;&#32593;&#32476;&#30340;&#26377;&#25928;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#25581;&#31034;&#23548;&#33268;&#24179;&#38754;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#22833;&#36133;&#30340;&#8220;&#36880;&#28176;&#28040;&#22833;&#30340;&#36755;&#20837;&#8221;&#29616;&#35937;&#65292;&#31397;&#25506;&#20102;&#27531;&#24046;&#23398;&#20064;&#30340;&#20869;&#24149;&#65306;&#30001;&#20110;&#38750;&#32447;&#24615;&#24615;&#36136;&#65292;&#22312;&#24179;&#38754;&#23618;&#20013;&#36755;&#20837;&#36880;&#28176;&#34987;&#25439;&#22351;&#65292;&#20174;&#32780;&#23548;&#33268;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24179;&#38754;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#36864;&#21270;&#20026;&#38543;&#26426;&#22122;&#22768;&#65292;&#24182;&#24378;&#35843;&#20102;&#36890;&#36807;&#20445;&#25345;&#26356;&#22909;&#30340;&#24184;&#23384;&#31070;&#32463;&#20803;&#19979;&#30028;&#30340;&#27531;&#24046;&#36830;&#25509;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24179;&#38754;&#31070;&#32463;&#32593;&#32476;&#20551;&#35774;&#8221;&#65288;PNNH&#65289;&#65292;&#35813;&#20551;&#35774;&#23558;&#38750;&#32447;&#24615;&#23618;&#20043;&#38388;&#30340;&#20869;&#37096;&#36335;&#24452;&#30830;&#23450;&#20026;&#27531;&#24046;&#23398;&#20064;&#20013;&#26368;&#20851;&#38190;&#30340;&#37096;&#20998;&#65292;&#24182;&#24314;&#31435;&#20102;
&lt;/p&gt;
&lt;p&gt;
The utilization of residual learning has become widespread in deep and scalable neural nets. However, the fundamental principles that contribute to the success of residual learning remain elusive, thus hindering effective training of plain nets with depth scalability. In this paper, we peek behind the curtains of residual learning by uncovering the "dissipating inputs" phenomenon that leads to convergence failure in plain neural nets: the input is gradually compromised through plain layers due to non-linearities, resulting in challenges of learning feature representations. We theoretically demonstrate how plain neural nets degenerate the input to random noise and emphasize the significance of a residual connection that maintains a better lower bound of surviving neurons as a solution. With our theoretical discoveries, we propose "The Plain Neural Net Hypothesis" (PNNH) that identifies the internal path across non-linear layers as the most critical part in residual learning, and establi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#19982;&#25991;&#26412;&#36136;&#37327;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26032;&#30340;&#25991;&#26412;logit&#25439;&#22833;&#26469;&#25913;&#21892;&#37325;&#26500;&#25991;&#26412;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#35813;&#25439;&#22833;&#20989;&#25968;&#19982;&#36866;&#24403;&#30340;&#21152;&#26435;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37325;&#26500;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08643</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#19982;&#25991;&#26412;&#36136;&#37327;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Learned Image Compression with Text Quality Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#19982;&#25991;&#26412;&#36136;&#37327;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26032;&#30340;&#25991;&#26412;logit&#25439;&#22833;&#26469;&#25913;&#21892;&#37325;&#26500;&#25991;&#26412;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#35813;&#25439;&#22833;&#20989;&#25968;&#19982;&#36866;&#24403;&#30340;&#21152;&#26435;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37325;&#26500;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#22312;&#23454;&#29616;&#36229;&#20302;&#27604;&#29305;&#29575;&#26041;&#38754;&#25928;&#29575;&#39640;&#65292;&#22240;&#27492;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#22823;&#37327;&#25991;&#26412;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#23631;&#24149;&#20869;&#23481;&#22270;&#20687;&#65288;SCI&#65289;&#65292;&#32463;&#36807;&#36825;&#26679;&#30340;&#21387;&#32553;&#21518;&#24448;&#24448;&#20250;&#20986;&#29616;&#25991;&#26412;&#22833;&#30495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;logit&#25439;&#22833;&#65292;&#29992;&#20110;&#37327;&#21270;&#21407;&#22987;&#22270;&#20687;&#21644;&#37325;&#26500;&#22270;&#20687;&#20043;&#38388;&#30340;&#25991;&#26412;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#26500;&#25991;&#26412;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#24182;&#37319;&#29992;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#35813;&#25439;&#22833;&#20989;&#25968;&#19982;&#36866;&#24403;&#30340;&#21152;&#26435;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#37325;&#26500;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;logit&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#23631;&#24149;&#25130;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#24179;&#22343;Bjontegaard delta (BD)&#20026;-32.64%&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#21644;-28.03%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23450;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#25991;&#26412;&#36136;&#37327;&#30340;&#25913;&#21892;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned image compression has gained widespread popularity for their efficiency in achieving ultra-low bit-rates. Yet, images containing substantial textual content, particularly screen-content images (SCI), often suffers from text distortion at such compressed levels. To address this, we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images, thereby improving the perceptual quality of the reconstructed text. Through rigorous experimentation across diverse datasets and employing state-of-the-art algorithms, our findings reveal significant enhancements in the quality of reconstructed text upon integration of the proposed loss function with appropriate weighting. Notably, we achieve a Bjontegaard delta (BD) rate of -32.64% for Character Error Rate (CER) and -28.03% for Word Error Rate (WER) on average by applying the text logit loss for two screenshot datasets. Additionally, we present quantitative metrics tai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08640</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#39044;&#27979;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Forecasting high-impact research topics via machine learning on evolving knowledge graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26500;&#25104;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#23427;&#36843;&#20351;&#30740;&#31350;&#32773;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#26356;&#29421;&#31364;&#30340;&#23376;&#39046;&#22495;&#19978;&#65292;&#20351;&#24471;&#21457;&#29616;&#20854;&#20182;&#39046;&#22495;&#30340;&#26032;&#39062;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#21512;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21150;&#27861;&#39044;&#27979;&#31185;&#23398;&#35770;&#25991;&#26410;&#26469;&#30340;&#24341;&#29992;&#27425;&#25968;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#21040;&#30740;&#31350;&#23436;&#25104;&#24182;&#19988;&#35770;&#25991;&#20889;&#25104;&#21518;&#25165;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26679;&#23601;&#38169;&#36807;&#20102;&#24819;&#27861;&#26500;&#24605;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#27979;&#20174;&#26410;&#34987;&#30740;&#31350;&#32773;&#21457;&#24067;&#30340;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#12290;&#23427;&#32467;&#21512;&#20102;&#20174;&#35770;&#25991;&#20869;&#23481;&#20013;&#21019;&#24314;&#30340;&#35821;&#20041;&#32593;&#32476;&#21644;&#20174;&#21382;&#21490;&#24341;&#29992;&#20013;&#21019;&#24314;&#30340;&#24433;&#21709;&#32593;&#32476;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#28436;&#21270;&#32593;&#32476;&#30340;&#21160;&#24577;&#24773;&#20917;&#65292;&#20174;&#32780;&#39044;&#27979;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#39044;&#26399;&#36825;&#31181;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#32773;&#21457;&#29616;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#30340;&#26080;&#24724;&#23398;&#20064;&#32773;&#21644;&#20248;&#21270;&#32773;&#20043;&#38388;&#30340;&#31574;&#30053;&#21338;&#24328;&#12290;&#23545;&#20110;&#19968;&#31867;&#24120;&#29992;&#30340;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26631;&#20934;&#30340;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#20248;&#21270;&#32773;&#26080;&#27861;&#33719;&#24471;&#36229;&#36807;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#65292;&#20294;&#22312;&#36125;&#21494;&#26031;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#20182;&#20204;&#21487;&#20197;&#33719;&#24471;&#36828;&#39640;&#20110;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#30340;&#25928;&#29992;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#23545;&#20110;&#19968;&#31867;&#26356;&#22797;&#26434;&#30340;&#31639;&#27861;&#65292;&#20248;&#21270;&#32773;&#30340;&#25928;&#29992;&#21487;&#20197;&#34987;&#38480;&#21046;&#22312;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.08637</link><description>&lt;p&gt;
&#38024;&#23545;&#26080;&#24724;&#23398;&#20064;&#32773;&#30340;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Strategizing against No-Regret Learners in First-Price Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08637
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#30340;&#26080;&#24724;&#23398;&#20064;&#32773;&#21644;&#20248;&#21270;&#32773;&#20043;&#38388;&#30340;&#31574;&#30053;&#21338;&#24328;&#12290;&#23545;&#20110;&#19968;&#31867;&#24120;&#29992;&#30340;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26631;&#20934;&#30340;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#20248;&#21270;&#32773;&#26080;&#27861;&#33719;&#24471;&#36229;&#36807;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#65292;&#20294;&#22312;&#36125;&#21494;&#26031;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#20182;&#20204;&#21487;&#20197;&#33719;&#24471;&#36828;&#39640;&#20110;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#30340;&#25928;&#29992;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#23545;&#20110;&#19968;&#31867;&#26356;&#22797;&#26434;&#30340;&#31639;&#27861;&#65292;&#20248;&#21270;&#32773;&#30340;&#25928;&#29992;&#21487;&#20197;&#34987;&#38480;&#21046;&#22312;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#21644;&#20004;&#20010;&#29609;&#23478;&#20043;&#38388;&#30340;&#36890;&#29992;&#37325;&#22797;&#36125;&#21494;&#26031;&#21338;&#24328;&#65292;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#65292;&#21363;&#23398;&#20064;&#32773;&#65292;&#37319;&#29992;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#32780;&#21478;&#19968;&#20010;&#29609;&#23478;&#65292;&#20248;&#21270;&#32773;&#65292;&#30693;&#36947;&#23398;&#20064;&#32773;&#30340;&#31639;&#27861;&#65292;&#24182;&#21046;&#23450;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#33258;&#24049;&#30340;&#25928;&#29992;&#12290;&#23545;&#20110;&#19968;&#31867;&#24120;&#29992;&#30340;&#31216;&#20026;&#22522;&#20110;&#22343;&#20540;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#34920;&#26126;&#65288;i&#65289;&#22312;&#26631;&#20934;&#65288;&#21363;&#23436;&#20840;&#20449;&#24687;&#65289;&#30340;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#20248;&#21270;&#32773;&#26080;&#27861;&#33719;&#24471;&#36229;&#36807;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#8212;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#22522;&#20934;&#65292;&#20294;&#26159;&#65288;ii&#65289;&#22312;&#36125;&#21494;&#26031;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#24773;&#20917;&#65292;&#20248;&#21270;&#32773;&#21487;&#20197;&#33719;&#24471;&#36828;&#39640;&#20110;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#30340;&#25928;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Mansour&#31561;&#20154;&#65288;2022&#65289;&#34920;&#26126;&#65292;&#19968;&#31867;&#26356;&#22797;&#26434;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26080;&#22810;&#38754;&#20307;&#20114;&#25442;&#21518;&#24724;&#31639;&#27861;&#65292;&#36275;&#20197;&#23558;&#20248;&#21270;&#32773;&#30340;&#25928;&#29992;&#38480;&#21046;&#22312;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#25928;&#29992;&#19978;&#65292;&#22312;&#20219;&#20309;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#21338;&#24328;&#65288;&#21253;&#25324;&#36125;&#21494;&#26031;&#31532;&#19968;&#20215;&#26684;&#25293;&#21334;&#65289;&#20013;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study repeated first-price auctions and general repeated Bayesian games between two players, where one player, the learner, employs a no-regret learning algorithm, and the other player, the optimizer, knowing the learner's algorithm, strategizes to maximize its own utility. For a commonly used class of no-regret learning algorithms called mean-based algorithms, we show that (i) in standard (i.e., full-information) first-price auctions, the optimizer cannot get more than the Stackelberg utility -- a standard benchmark in the literature, but (ii) in Bayesian first-price auctions, there are instances where the optimizer can achieve much higher than the Stackelberg utility.   On the other hand, Mansour et al. (2022) showed that a more sophisticated class of algorithms called no-polytope-swap-regret algorithms are sufficient to cap the optimizer's utility at the Stackelberg utility in any repeated Bayesian game (including Bayesian first-price auctions), and they pose the open question wh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#22312;&#38754;&#23545;&#19981;&#21516;&#31867;&#22411;&#23545;&#25163;&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08621</link><description>&lt;p&gt;
&#19968;&#31181;&#24191;&#20041;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generalized Approach to Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08621
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#22312;&#38754;&#23545;&#19981;&#21516;&#31867;&#22411;&#23545;&#25163;&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#29992;&#20110;&#20855;&#26377;&#23436;&#20840;&#33258;&#36866;&#24212;&#23545;&#25163;&#30340;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#30340;&#31639;&#27861;&#37117;&#26159;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20219;&#20309;&#38656;&#35201;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#31639;&#27861;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#30340;&#21322;&#21305;&#37197;&#21453;&#39304;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20351;&#29992;&#30830;&#23450;&#24615;&#21322;&#21305;&#37197;&#21453;&#39304;&#30340;&#20840;&#33258;&#36866;&#24212;&#23545;&#25163;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#38754;&#23545;&#26080;&#30693;&#23545;&#25163;&#26102;&#21487;&#20197;&#20351;&#29992;&#21482;&#26377;&#38543;&#26426;&#21322;&#21305;&#37197;&#21453;&#39304;&#30340;&#31639;&#27861;&#33719;&#24471;&#30456;&#20284;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#25551;&#36848;&#20102;&#23558;&#19968;&#38454;&#31639;&#27861;&#36716;&#21270;&#20026;&#38646;&#38454;&#31639;&#27861;&#30340;&#36890;&#29992;&#20803;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#20840;&#20449;&#24687;&#21453;&#39304;&#12289;&#21322;&#21305;&#37197;&#21453;&#39304;&#12289;&#38543;&#26426;&#36951;&#25022;&#12289;&#23545;&#25239;&#36951;&#25022;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#38750;&#24179;&#31283;&#36951;&#25022;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide
&lt;/p&gt;</description></item><item><title>gadjid&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26694;&#26550;&#26469;&#35745;&#31639;&#22240;&#26524;&#36317;&#31163;&#65292;&#36825;&#20123;&#36317;&#31163;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08616</link><description>&lt;p&gt;
Adjustment Identification Distance: &#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Adjustment Identification Distance: A gadjid for Causal Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08616
&lt;/p&gt;
&lt;p&gt;
gadjid&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26694;&#26550;&#26469;&#35745;&#31639;&#22240;&#26524;&#36317;&#31163;&#65292;&#36825;&#20123;&#36317;&#31163;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#30340;&#35780;&#20272;&#26159;&#22256;&#38590;&#30340;&#65306;&#20004;&#20010;&#22270;&#24418;&#20043;&#38388;&#19981;&#21516;&#30340;&#36793;&#30340;&#25968;&#37327;&#19981;&#33021;&#21453;&#26144;&#20986;&#23427;&#20204;&#22312;&#24314;&#35758;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#20844;&#24335;&#26041;&#38754;&#26377;&#20309;&#19981;&#21516;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#22270;&#24418;&#20043;&#38388;&#30340;&#22240;&#26524;&#36317;&#31163;&#65292;&#20854;&#20013;&#21253;&#25324;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#24320;&#21457;&#20102;&#25913;&#36827;&#30340;&#22522;&#20110;&#35843;&#25972;&#30340;&#36317;&#31163;&#65292;&#20197;&#21450;&#23545;&#23436;&#25104;&#30340;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#22240;&#26524;&#24207;&#21015;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#36798;&#24615;&#31639;&#27861;&#26469;&#39640;&#25928;&#35745;&#31639;&#36317;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;gadjid&#36719;&#20214;&#21253;&#20013;&#65288;&#22312;https://github.com/CausalDisco/gadjid&#19978;&#24320;&#28304;&#65289;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#36317;&#31163;&#23454;&#29616;&#65307;&#23427;&#20204;&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26080;&#27861;&#25193;&#23637;&#30340;&#22270;&#24418;&#23610;&#23544;&#25552;&#20379;&#20102;&#19968;&#20010;&#22240;&#26524;&#21457;&#29616;&#30340;&#25104;&#21151;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop polynomial-time reachability algorithms to compute the distances efficiently. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#21076;&#38500;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.08611</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#21076;&#38500;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#24471;&#30410;&#20110;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#20351;&#24471;&#22823;&#37327;&#25968;&#25454;&#30340;&#25910;&#38598;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#22833;&#20540;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#25104;&#26412;&#25935;&#24863;&#24615;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#24212;&#29992;&#20256;&#32479;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#23427;&#26159;&#20316;&#20026;&#19968;&#20010;&#31995;&#32479;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#65292;&#36824;&#25972;&#21512;&#20102;&#28151;&#21512;&#37325;&#37319;&#26679;&#22120;&#21644;&#22522;&#20110;&#22238;&#24402;&#30340;&#25554;&#34917;&#22120;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;Scania&#21345;&#36710;&#30340;APS&#25925;&#38556;&#25968;&#25454;&#38598;&#21644;SECOM&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21076;&#38500;&#23454;&#39564;&#26469;&#20998;&#26512;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid influx of data-driven models into the industrial sector has been facilitated by the proliferation of sensor technology, enabling the collection of vast quantities of data. However, leveraging these models for failure detection and prognosis poses significant challenges, including issues like missing values and class imbalances. Moreover, the cost sensitivity associated with industrial operations further complicates the application of conventional models in this context. This paper introduces a novel cost-sensitive transformer model developed as part of a systematic workflow, which also integrates a hybrid resampler and a regression-based imputer. After subjecting our approach to rigorous testing using the APS failure dataset from Scania trucks and the SECOM dataset, we observed a substantial enhancement in performance compared to state-of-the-art methods. Moreover, we conduct an ablation study to analyze the contributions of different components in our proposed method. Our fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.08606</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20219;&#24847;&#22810;&#39033;&#24335;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Polynomial Separations in Trainable Quantum Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35757;&#32451;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#26435;&#34913;&#65307;&#20316;&#20026;&#36825;&#20123;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#23454;&#38469;&#19978;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23454;&#29616;&#25351;&#25968;&#32423;&#30340;&#36229;&#36234;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31163;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;QNN&#35757;&#32451;&#26102;&#38388;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;QNNs&#26469;&#32469;&#24320;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#65292;&#22312;&#25191;&#34892;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;QNNs&#21487;&#20197;&#23637;&#31034;&#20986;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#65292;&#19988;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#27599;&#20010;&#21333;&#20803;&#26684;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#31163;&#36866;&#29992;&#20110;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#20869;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32463;&#20856;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#37327;&#23376;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers. We show that quantum contextuality is th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.08595</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#65306;&#20851;&#20110;&#22522;&#30784;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Counts for Graph Neural Networks: All About That Basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#23398;&#20064;&#22270;&#19978;&#19981;&#21464;&#20989;&#25968;&#30340;&#26550;&#26500;&#12290;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#19982;&#20854;&#34920;&#36798;&#33021;&#21147;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#23427;&#20204;&#26080;&#27861;&#35745;&#25968;&#22270;&#20013;&#30340;&#26576;&#20123;&#27169;&#24335;&#65288;&#20363;&#22914;&#24490;&#29615;&#65289;&#26159;&#36825;&#20123;&#38480;&#21046;&#30340;&#26680;&#24515;&#65292;&#22240;&#20026;&#35768;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#20381;&#36182;&#20110;&#35745;&#25968;&#36825;&#20123;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20004;&#31181;&#31361;&#20986;&#30340;&#33539;&#20363;&#26088;&#22312;&#36890;&#36807;&#20016;&#23500;&#22270;&#29305;&#24449;&#30340;&#23376;&#22270;&#25110;&#21516;&#24577;&#27169;&#24335;&#35745;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#37117;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#24335;&#30340;&#8220;&#22522;&#30784;&#8221;&#20013;&#30340;&#21516;&#24577;&#35745;&#25968;&#32435;&#20837;&#32771;&#34385;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#21152;&#34920;&#36798;&#21147;&#30340;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08593</link><description>&lt;p&gt;
&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#65306;&#23454;&#26102;&#20174;&#20132;&#26131;&#22270;&#20013;&#25552;&#21462;&#22522;&#20110;&#23376;&#22270;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#22270;&#20013;&#30340;&#20856;&#22411;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#34987;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#29992;&#20110;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#65292;&#22914;&#27927;&#38065;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#24211;&#21033;&#29992;&#22810;&#26680;&#24182;&#34892;&#24615;&#65292;&#32500;&#25252;&#19968;&#20010;&#21160;&#24577;&#30340;&#20869;&#23384;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25366;&#25496;&#20256;&#20837;&#20132;&#26131;&#27969;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#27969;&#30340;&#26041;&#24335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21512;&#25104;&#21453;&#27927;&#38065;&#65288;AML&#65289;&#21644;&#30495;&#23454;&#30340;&#20197;&#22826;&#22346;&#38035;&#40060;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#38750;&#27861;&#20132;&#26131;&#30340;&#27604;&#20363;&#38750;&#24120;&#23567;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#25105;&#20204;&#30340;&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#38754;&#37096;&#22270;&#20687;&#19978;&#30340;&#29781;&#30133;&#21644;&#30382;&#32932;&#30149;&#21464;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36866;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#30340;&#36895;&#24230;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08592</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38754;&#37096;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks Towards Facial Skin Lesions Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#38754;&#37096;&#22270;&#20687;&#19978;&#30340;&#29781;&#30133;&#21644;&#30382;&#32932;&#30149;&#21464;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36866;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#30340;&#36895;&#24230;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#21253;&#25324;&#32654;&#23481;&#25972;&#24418;&#12289;&#32654;&#23481;&#34892;&#19994;&#12289;&#25668;&#24433;&#21644;&#23089;&#20048;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#25805;&#32437;&#24739;&#32773;&#30340;&#22270;&#20687;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30340;&#22270;&#20687;&#22788;&#29702;&#36719;&#20214;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#23545;&#38754;&#37096;&#22270;&#20687;&#19978;&#30340;&#29781;&#30133;&#21644;&#30382;&#32932;&#30149;&#21464;&#30340;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#12289;&#24555;&#36895;&#21644;&#36866;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#65292;&#24182;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#21306;&#22495;&#36873;&#25321;&#12289;&#25152;&#36873;&#21306;&#22495;&#30340;&#25195;&#25551;&#12289;&#30149;&#21464;&#35786;&#26029;&#21644;&#26631;&#35760;&#24050;&#35782;&#21035;&#30340;&#30149;&#21464;&#12290;&#35813;&#30740;&#31350;&#30340;&#21407;&#22987;&#25968;&#25454;&#26159;&#20174;&#24503;&#40657;&#20848;&#19968;&#23478;&#19987;&#38376;&#20174;&#20107;&#30382;&#32932;&#25252;&#29702;&#21644;&#32654;&#23481;&#26381;&#21153;&#30340;&#30693;&#21517;&#35786;&#25152;&#25910;&#38598;&#30340;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#34892;&#25919;&#20449;&#24687;&#12289;&#20020;&#24202;&#25968;&#25454;&#21644;&#38754;&#37096;&#21644;&#20391;&#38754;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Facial analysis has emerged as a prominent area of research with diverse applications, including cosmetic surgery programs, the beauty industry, photography, and entertainment. Manipulating patient images often necessitates professional image processing software. This study contributes by providing a model that facilitates the detection of blemishes and skin lesions on facial images through a convolutional neural network and machine learning approach. The proposed method offers advantages such as simple architecture, speed and suitability for image processing while avoiding the complexities associated with traditional methods. The model comprises four main steps: area selection, scanning the chosen region, lesion diagnosis, and marking the identified lesion. Raw data for this research were collected from a reputable clinic in Tehran specializing in skincare and beauty services. The dataset includes administrative information, clinical data, and facial and profile images. A total of 230
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#26500;&#24314;&#26641;&#38598;&#25104;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#19968;&#32452;&#29305;&#24449;&#30340;&#27934;&#23519;&#21147;&#26469;&#21152;&#36895;&#26500;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08586</link><description>&lt;p&gt;
&#26641;&#38598;&#25104;&#20013;&#26356;&#24555;&#30340;&#37325;&#22797;&#36867;&#36991;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Faster Repeated Evasion Attacks in Tree Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08586
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#26500;&#24314;&#26641;&#38598;&#25104;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#19968;&#32452;&#29305;&#24449;&#30340;&#27934;&#23519;&#21147;&#26469;&#21152;&#36895;&#26500;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#25104;&#26159;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#20063;&#23601;&#26159;&#30053;&#24494;&#25200;&#21160;&#30340;&#26679;&#26412;&#20250;&#23548;&#33268;&#38169;&#35823;&#39044;&#27979;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#35774;&#35745;&#29992;&#20110;&#26641;&#38598;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#12290;&#20294;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27425;&#25968;&#30340;&#35299;&#20915;&#65288;&#20363;&#22914;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#35299;&#20915;&#65289;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35797;&#22270;&#20174;&#22836;&#24320;&#22987;&#23547;&#25214;&#36825;&#26679;&#30340;&#26679;&#26412;&#65292;&#36825;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#38382;&#39064;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#27491;&#22312;&#35299;&#20915;&#30340;&#22810;&#20010;&#30456;&#20284;&#38382;&#39064;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26641;&#38598;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#20542;&#21521;&#20110;&#25200;&#21160;&#19968;&#33268;&#20294;&#30456;&#23545;&#36739;&#23567;&#30340;&#19968;&#32452;&#29305;&#24449;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#36825;&#32452;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#21152;&#36895;&#26500;&#24314;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree ensembles are one of the most widely used model classes. However, these models are susceptible to adversarial examples, i.e., slightly perturbed examples that elicit a misprediction. There has been significant research on designing approaches to construct such examples for tree ensembles. But this is a computationally challenging problem that often must be solved a large number of times (e.g., for all examples in a training set). This is compounded by the fact that current approaches attempt to find such examples from scratch. In contrast, we exploit the fact that multiple similar problems are being solved. Specifically, our approach exploits the insight that adversarial examples for tree ensembles tend to perturb a consistent but relatively small set of features. We show that we can quickly identify this set of features and use this knowledge to speedup constructing adversarial examples.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;Link-MoE&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08583</link><description>&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture of Link Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;Link-MoE&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#39044;&#27979;&#22270;&#20013;&#26410;&#35265;&#36830;&#25509;&#12290;&#21551;&#21457;&#24335;&#26041;&#27861;&#21033;&#29992;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25104;&#23545;&#24230;&#37327;&#65292;&#22914;&#20849;&#21516;&#37051;&#23621;&#21644;&#26368;&#30701;&#36335;&#24452;&#65292;&#24120;&#24120;&#33021;&#22815;&#19982;&#32431;&#31929;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;GNNs&#22312;&#38142;&#25509;&#39044;&#27979;&#65288;GNN4LP&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25972;&#21512;&#19968;&#31181;&#25110;&#23569;&#25968;&#20960;&#31181;&#25104;&#23545;&#20449;&#24687;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#33410;&#28857;&#23545;&#38656;&#35201;&#19981;&#21516;&#30340;&#25104;&#23545;&#20449;&#24687;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#32780;&#21482;&#24212;&#29992;&#30456;&#21516;&#30340;&#25104;&#23545;&#20449;&#24687;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19987;&#23478;&#27169;&#22411;Link-MoE&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#12290;Link-MoE&#21033;&#29992;&#21508;&#31181;GNNs&#20316;&#20026;&#19987;&#23478;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Link-MoE&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets dem
&lt;/p&gt;</description></item><item><title>FedLPS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#21644;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08578</link><description>&lt;p&gt;
FedLPS: &#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08578
&lt;/p&gt;
&lt;p&gt;
FedLPS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#21644;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#20849;&#21516;&#20248;&#21270;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;FL&#36991;&#20813;&#20102;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#23454;&#38469;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;FL&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#12289;&#22810;&#20219;&#21153;&#37096;&#32626;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#20943;&#23569;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;FL&#35757;&#32451;&#25104;&#26412;&#65292;&#24573;&#30053;&#20102;&#22312;&#24322;&#26500;FL&#22330;&#26223;&#20013;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20855;&#26377;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65288;FedLPS&#65289;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;FedLPS&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#65292;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#23454;&#29616;&#22810;&#20219;&#21153;&#37096;&#32626;&#65292;&#36890;&#36807;&#23558;&#26412;&#22320;&#27169;&#22411;&#21010;&#20998;&#20026;&#21487;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65288;AnyDoor&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#20855;&#26377;&#20998;&#31163;&#35774;&#32622;&#21644;&#28608;&#27963;&#26377;&#23475;&#25928;&#26524;&#30340;&#26102;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08577</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Test-Time Backdoor Attacks on Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65288;AnyDoor&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#20855;&#26377;&#20998;&#31163;&#35774;&#32622;&#21644;&#28608;&#27963;&#26377;&#23475;&#25928;&#26524;&#30340;&#26102;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#36890;&#24120;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#26469;&#25191;&#34892;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#38454;&#27573;&#35302;&#21457;&#39044;&#23450;&#30340;&#26377;&#23475;&#25928;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnyDoor&#65292;&#19968;&#31181;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#27979;&#35797;&#26102;&#21453;&#21521;&#38376;&#25511;&#25915;&#20987;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#24615;&#27979;&#35797;&#22270;&#20687;&#23558;&#21453;&#21521;&#38376;&#25511;&#27880;&#20837;&#21040;&#25991;&#26412;&#27169;&#24577;&#20013;&#65288;&#20849;&#20139;&#30456;&#21516;&#30340;&#36890;&#29992;&#25200;&#21160;&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#25110;&#20462;&#25913;&#35757;&#32451;&#25968;&#25454;&#12290;AnyDoor&#37319;&#29992;&#31867;&#20284;&#20110;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#20294;&#20854;&#36890;&#36807;&#33021;&#22815;&#20998;&#31163;&#26377;&#23475;&#25928;&#26524;&#30340;&#35774;&#32622;&#21644;&#28608;&#27963;&#30340;&#26102;&#38388;&#26469;&#21306;&#21035;&#20110;&#20854;&#20182;&#25915;&#20987;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;AnyDoor&#23545;&#27969;&#34892;&#30340;MLLMs&#65288;&#22914;LLaVA-1.5&#12289;MiniGPT-4&#12289;InstructBLIP&#21644;BLIP-2&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#21453;&#21521;&#38376;&#25511;&#30001;&#36890;&#29992;&#25200;&#21160;&#27880;&#20837;&#65292;AnyDoor&#21487;&#20197;&#21160;&#24577;&#25913;&#21464;&#20854;&#21453;&#21521;&#38376;&#35302;&#21457;&#25552;&#31034;/&#26377;&#23475;&#25928;&#26524;&#65292;&#20174;&#32780;&#26292;&#38706;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks are commonly executed by contaminating training data, such that a trigger can activate predetermined harmful effects during the test phase. In this work, we present AnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), which involves injecting the backdoor into the textual modality using adversarial test images (sharing the same universal perturbation), without requiring access to or modification of the training data. AnyDoor employs similar techniques used in universal adversarial attacks, but distinguishes itself by its ability to decouple the timing of setup and activation of harmful effects. In our experiments, we validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, as well as provide comprehensive ablation studies. Notably, because the backdoor is injected by a universal perturbation, AnyDoor can dynamically change its backdoor trigger prompts/harmful effects, exposing 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08576</link><description>&lt;p&gt;
&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regret Minimization in Stackelberg Games with Side Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#22522;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;Stackelberg&#21338;&#24328;&#26159;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;Stackelberg&#21338;&#24328;&#31639;&#27861;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#30340;&#26368;&#22823;&#25104;&#21151;&#20043;&#19968;&#65292;&#22240;&#20026;Stackelberg&#21338;&#24328;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22330;&#23433;&#20840;&#12289;&#21453;&#30423;&#29454;&#21644;&#32593;&#32476;&#29359;&#32618;&#39044;&#38450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#26410;&#33021;&#32771;&#34385;&#21040;&#27599;&#20010;&#29609;&#23478;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#20132;&#36890;&#27169;&#24335;&#65292;&#22825;&#27668;&#26465;&#20214;&#65292;&#32593;&#32476;&#25317;&#22622;&#65289;&#65292;&#36825;&#26159;&#29616;&#23454;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#21040;&#20004;&#20010;&#29609;&#23478;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#20391;&#20449;&#24687;&#30340;Stackelberg&#21338;&#24328;&#65292;&#20854;&#20013;&#20004;&#20010;&#29609;&#23478;&#22312;&#36827;&#34892;&#28216;&#25103;&#20043;&#21069;&#37117;&#35266;&#23519;&#21040;&#19968;&#20010;&#22806;&#37096;&#29615;&#22659;&#12290;&#28982;&#21518;&#65292;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#21487;&#33021;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#23545;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#19978;&#19979;&#25991;&#37117;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
&lt;/p&gt;</description></item><item><title>&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08573</link><description>&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Two Tales of Single-Phase Contrastive Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08573
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#8220;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#8221;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#24050;&#32463;&#25910;&#25947;&#20110;&#23558;&#26799;&#24230;&#34920;&#31034;&#20026;&#27963;&#21160;&#24046;&#24322;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#31243;&#24230;&#30340;&#21516;&#27493;&#65288;&#23398;&#20064;&#26399;&#38388;&#30340;&#19981;&#21516;&#38454;&#27573;&#65289;&#24182;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20197;&#21450;&#20854;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#30340;&#28508;&#22312;&#25928;&#29992;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#36755;&#20986;&#21333;&#20803;&#26045;&#21152;&#26080;&#38480;&#23567;&#25200;&#21160;&#65288;nudges&#65289;&#65292;&#36825;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23558;&#20154;&#24037;&#31070;&#32463;&#20803;&#24314;&#27169;&#20026;&#20004;&#20010;&#30456;&#21453;&#25200;&#21160;&#30340;&#32452;&#20214;&#65292;&#21517;&#20026;&#8220;&#21452;&#21521;&#20256;&#25773;&#8221;&#30340;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#24357;&#21512;&#21040;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#21035;&#30340;&#23398;&#20064;&#38454;&#27573;&#25110;&#26080;&#38480;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#20381;&#36182;&#20110;&#23545;&#31216;&#25200;&#21160;&#65292;&#36825;&#21487;&#33021;&#22312;&#29983;&#29289;&#23398;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#37319;&#38598;&#38381;&#28304;&#27169;&#22411;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#24320;&#28304;&#32534;&#30721;&#22120;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.08570</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Online Foundation Model Selection in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#37319;&#38598;&#38381;&#28304;&#27169;&#22411;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#24320;&#28304;&#32534;&#30721;&#22120;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#21518;&#65292;&#22522;&#30784;&#27169;&#22411;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#23398;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#33719;&#24471;&#65306;&#24320;&#28304;&#25110;&#20184;&#36153;&#30340;&#38381;&#28304;&#36873;&#39033;&#12290;&#29992;&#25143;&#21516;&#26102;&#26377;&#20004;&#31181;&#36873;&#25321;&#26102;&#65292;&#20182;&#20204;&#20250;&#38754;&#20020;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#26377;&#25928;&#20294;&#26114;&#36149;&#30340;&#38381;&#28304;&#27169;&#22411;&#21644;&#20813;&#36153;&#20294;&#21151;&#33021;&#36739;&#24369;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#30001;&#20110;&#20174;&#38381;&#28304;&#27169;&#22411;&#20013;&#25910;&#38598;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#19978;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#21516;&#26102;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#23558;&#24320;&#28304;&#32534;&#30721;&#22120;&#19982;&#22788;&#29702;&#35813;&#19978;&#19979;&#25991;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#32534;&#30721;&#22120;&#23558;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#24067;&#25552;&#28860;&#25104;&#20302;&#32500;&#29305;&#24449;&#65292;&#21363;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, with
&lt;/p&gt;</description></item><item><title>Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.08567</link><description>&lt;p&gt;
Agent Smith:&#19968;&#24352;&#22270;&#20687;&#21487;&#20197;&#36805;&#36895;&#36234;&#29425;&#19968;&#30334;&#19975;&#20010;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08567
&lt;/p&gt;
&lt;p&gt;
Agent Smith&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#20256;&#26579;&#24615;&#36234;&#29425;&#65292;&#35813;&#38382;&#39064;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#36234;&#29425;&#19968;&#20010;&#20195;&#29702;&#26469;&#36805;&#36895;&#24863;&#26579;&#25152;&#26377;&#20195;&#29702;&#24182;&#23548;&#33268;&#26377;&#23475;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#20195;&#29702;&#21487;&#20197;&#25509;&#25910;&#25351;&#20196;&#65292;&#25429;&#25417;&#22270;&#20687;&#65292;&#20174;&#20869;&#23384;&#20013;&#26816;&#32034;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#20915;&#23450;&#20351;&#29992;&#21738;&#20123;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#32418;&#38431;&#35780;&#20272;&#21457;&#29616;&#24694;&#24847;&#22270;&#20687;/&#25552;&#31034;&#21487;&#20197;&#36234;&#29425;MLLM&#24182;&#23548;&#33268;&#19981;&#23545;&#40784;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#31216;&#20026;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;&#23427;&#28041;&#21450;&#21040;&#23545;&#21333;&#20010;&#20195;&#29702;&#36827;&#34892;&#31616;&#21333;&#30340;&#36234;&#29425;&#65292;&#26080;&#38656;&#26469;&#33258;&#23545;&#25163;&#30340;&#36827;&#19968;&#27493;&#24178;&#39044;&#65292;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#20195;&#29702;&#23558;&#20197;&#25351;&#25968;&#32423;&#21035;&#34987;&#24863;&#26579;&#24182;&#23637;&#31034;&#26377;&#23475;&#34892;&#20026;&#12290;&#20026;&#20102;&#39564;&#35777;&#20256;&#26579;&#24615;&#36234;&#29425;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#21253;&#21547;&#39640;&#36798;&#19968;&#30334;&#19975;&#20010;LLaVA-1.5&#20195;&#29702;&#30340;&#22810;&#20195;&#29702;&#29615;&#22659;&#65292;&#24182;&#23558;&#38543;&#26426;&#21305;&#37197;&#23545;&#32842;&#22825;&#20316;&#20026;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#65288;&#20256;&#26579;&#24615;&#65289;&#24694;&#24847;&#22270;&#20687;&#36755;&#20837;&#21040;&#20219;&#24847;&#36873;&#25321;&#30340;&#20195;&#29702;&#30340;&#20869;&#23384;&#20013;&#23601;&#36275;&#20197;&#23454;&#29616;&#20256;&#26579;&#24615;&#36234;&#29425;&#12290;
&lt;/p&gt;
&lt;p&gt;
A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#35299;&#20915;&#20102;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#65292;&#23545;&#20110;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#24674;&#22797;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.08563</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#35299;&#20915;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Restoration Tackles Forward and Inverse Problems for the Laplace Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#35299;&#20915;&#20102;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#65292;&#23545;&#20110;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#24674;&#22797;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#26377;&#21069;&#26223;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#22122;&#22768;&#36755;&#20837;&#26144;&#23556;&#20026;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#29983;&#25104;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#21453;&#21521;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20363;&#22914;&#27850;&#26494;&#26041;&#31243;&#65292;&#22240;&#20026;&#24133;&#24230;&#36739;&#22823;&#30340;&#29305;&#24449;&#20540;&#20250;&#25918;&#22823;&#27979;&#37327;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#65288;DDRM&#65289;&#26469;&#35299;&#20915;PDE&#30340;&#21453;&#21521;&#21644;&#27491;&#21521;&#38382;&#39064;&#12290;DDRM&#34987;&#29992;&#20110;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#31639;&#23376;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#24674;&#22797;&#21407;&#22987;&#24178;&#20928;&#20449;&#21495;&#12290;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#21033;&#29992;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#26469;&#24674;&#22797;&#27850;&#26494;&#26041;&#31243;&#30340;&#35299;&#21644;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#26174;&#33879;&#25913;&#21892;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a promising class of generative models that map noisy inputs to realistic images. More recently, they have been employed to generate solutions to partial differential equations (PDEs). However, they still struggle with inverse problems in the Laplacian operator, for instance, the Poisson equation, because the eigenvalues that are large in magnitude amplify the measurement noise. This paper presents a novel approach for the inverse and forward solution of PDEs through the use of denoising diffusion restoration models (DDRM). DDRMs were used in linear inverse problems to restore original clean signals by exploiting the singular value decomposition (SVD) of the linear operator. Equivalently, we present an approach to restore the solution and the parameters in the Poisson equation by exploiting the eigenvalues and the eigenfunctions of the Laplacian operator. Our results show that using denoising diffusion restoration significantly improves the estimation o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#20998;&#27495;&#65292;&#20197;&#21450;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#21644;&#27963;&#36291;&#31070;&#32463;&#20803;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08552</link><description>&lt;p&gt;
&#38754;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65306;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Confronting Reward Overoptimization for Diffusion Models: A Perspective of Inductive and Primacy Biases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#20316;&#32773;&#36890;&#36807;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#20998;&#27495;&#65292;&#20197;&#21450;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#21644;&#27963;&#36291;&#31070;&#32463;&#20803;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#34701;&#21512;&#26159;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#12290;&#34429;&#28982;&#36890;&#36807;&#20248;&#21270;&#19979;&#28216;&#22870;&#21169;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#26102;&#36807;&#24230;&#20248;&#21270;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#22320;&#38754;&#30495;&#23454;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#24402;&#32435;&#21644;&#20248;&#20808;&#20559;&#24046;&#30340;&#35282;&#24230;&#26469;&#23545;&#20184;&#25193;&#25955;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#26102;&#38388;&#24402;&#32435;&#20559;&#24046;&#23384;&#22312;&#20998;&#27495;&#65292;&#36825;&#21487;&#33021;&#26159;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#20010;&#28508;&#22312;&#26469;&#28304;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20196;&#20154;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;&#25105;&#20204;&#35780;&#35770;&#27169;&#22411;&#20013;&#30340;&#27785;&#30561;&#31070;&#32463;&#20803;&#20805;&#24403;&#19968;&#31181;&#23545;&#25239;&#36807;&#24230;&#20248;&#21270;&#30340;&#27491;&#21017;&#21270;&#25163;&#27573;&#65292;&#32780;&#27963;&#36291;&#31070;&#32463;&#20803;&#21017;&#21453;&#26144;&#20102;&#36825;&#20010;&#35774;&#32622;&#20013;&#30340;&#20248;&#20808;&#20559;&#24046;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#35780;&#35770;&#27169;&#22411;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#26102;&#38388;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the gap between diffusion models and human preferences is crucial for their integration into practical generative workflows. While optimizing downstream reward models has emerged as a promising alignment strategy, concerns arise regarding the risk of excessive optimization with learned reward models, which potentially compromises ground-truth performance. In this work, we confront the reward overoptimization problem in diffusion model alignment through the lenses of both inductive and primacy biases. We first identify the divergence of current methods from the temporal inductive bias inherent in the multi-step denoising process of diffusion models as a potential source of overoptimization. Then, we surprisingly discover that dormant neurons in our critic model act as a regularization against overoptimization, while active neurons reflect primacy bias in this setting. Motivated by these observations, we propose Temporal Diffusion Policy Optimization with critic active neuron Re
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#29983;&#25104;&#24335;&#21644;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#24037;&#31243;&#24418;&#29366;&#20248;&#21270;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#36890;&#36807;&#36866;&#24403;&#30340;&#24418;&#29366;&#32534;&#30721;&#21644;&#29289;&#29702;&#22686;&#24378;&#35774;&#35745;&#31354;&#38388;&#65292;&#21487;&#20197;&#20197;&#32463;&#27982;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#35774;&#35745;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.08540</link><description>&lt;p&gt;
&#24037;&#31243;&#24418;&#29366;&#20248;&#21270;&#20013;&#30340;&#29983;&#25104;&#24335;&#19982;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Generative VS non-Generative Models in Engineering Shape Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#29983;&#25104;&#24335;&#21644;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#24037;&#31243;&#24418;&#29366;&#20248;&#21270;&#20013;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#36890;&#36807;&#36866;&#24403;&#30340;&#24418;&#29366;&#32534;&#30721;&#21644;&#29289;&#29702;&#22686;&#24378;&#35774;&#35745;&#31354;&#38388;&#65292;&#21487;&#20197;&#20197;&#32463;&#27982;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#35774;&#35745;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#29983;&#25104;&#24335;&#27169;&#22411;&#21644;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#22312;&#26500;&#24314;&#35774;&#35745;&#31354;&#38388;&#12289;&#36827;&#34892;&#26032;&#39062;&#39640;&#25928;&#35774;&#35745;&#25506;&#32034;&#21644;&#24418;&#29366;&#20248;&#21270;&#26041;&#38754;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#32764;&#22411;/&#27700;&#32764;&#35774;&#35745;&#26696;&#20363;&#65292;&#24182;&#22312;&#29983;&#25104;&#30340;&#35774;&#35745;&#31354;&#38388;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#24615;&#33021;&#22686;&#24378;&#22810;&#26679;&#24615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(PaDGAN)&#19982;&#22522;&#20110;Karhunen-Lo&#232;ve&#23637;&#24320;&#21644;&#29289;&#29702;&#27169;&#22411;&#21270;&#24418;&#29366;&#31614;&#21517;&#21521;&#37327;(SSV-KLE)&#32806;&#21512;&#30340;&#32447;&#24615;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#24418;&#29366;&#32534;&#30721;&#21644;&#29289;&#29702;&#22686;&#24378;&#35774;&#35745;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#38750;&#29983;&#25104;&#24335;&#27169;&#22411;&#26377;&#28508;&#21147;&#20197;&#32463;&#27982;&#39640;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#35774;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#35774;&#35745;&#31354;&#38388;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#22823;&#22411;&#32764;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we perform a systematic comparison of the effectiveness and efficiency of generative and non-generative models in constructing design spaces for novel and efficient design exploration and shape optimization. We apply these models in the case of airfoil/hydrofoil design and conduct the comparison on the resulting design spaces. A conventional Generative Adversarial Network (GAN) and a state-of-the-art generative model, the Performance-Augmented Diverse Generative Adversarial Network (PaDGAN), are juxtaposed with a linear non-generative model based on the coupling of the Karhunen-Lo\`eve Expansion and a physics-informed Shape Signature Vector (SSV-KLE). The comparison demonstrates that, with an appropriate shape encoding and a physics-augmented design space, non-generative models have the potential to cost-effectively generate high-performing valid designs with enhanced coverage of the design space. In this work, both approaches are applied to two large foil profile dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25506;&#32034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#30142;&#30149;&#36827;&#23637;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;91%&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08539</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26234;&#33021;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Intelligent Diagnosis of Alzheimer's Disease Based on Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25506;&#32034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#30142;&#30149;&#36827;&#23637;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;91%&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#65292;&#26088;&#22312;&#25506;&#32034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#30142;&#30149;&#36827;&#23637;&#12290;&#25105;&#20204;&#37319;&#29992;&#21019;&#26032;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22635;&#34917;&#32570;&#22833;&#25968;&#25454;&#65292;&#22788;&#29702;&#24322;&#24120;&#20540;&#21644;&#26080;&#25928;&#25968;&#25454;&#65292;&#20174;&#32780;&#20805;&#20998;&#25366;&#25496;&#21644;&#21033;&#29992;&#36825;&#20123;&#26377;&#38480;&#30340;&#25968;&#25454;&#36164;&#28304;&#12290;&#36890;&#36807;Spearman&#30456;&#20851;&#31995;&#25968;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#19982;AD&#35786;&#26029;&#24378;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26500;&#24314;&#21644;&#27979;&#35797;&#20102;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;&#20854;&#20013;&#65292;XGBoost&#27169;&#22411;&#22312;&#35786;&#26029;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;91%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#20811;&#26381;&#20102;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#26089;&#26399;&#26816;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#30340;&#30740;&#31350;&#20215;&#20540;&#21644;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study is based on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and aims to explore early detection and disease progression in Alzheimer's disease (AD). We employ innovative data preprocessing strategies, including the use of the random forest algorithm to fill missing data and the handling of outliers and invalid data, thereby fully mining and utilizing these limited data resources. Through Spearman correlation coefficient analysis, we identify some features strongly correlated with AD diagnosis. We build and test three machine learning models using these features: random forest, XGBoost, and support vector machine (SVM). Among them, the XGBoost model performs the best in terms of diagnostic performance, achieving an accuracy of 91%. Overall, this study successfully overcomes the challenge of missing data and provides valuable insights into early detection of Alzheimer's disease, demonstrating its unique research value and practical significance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.08530</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21518;&#32493;&#34920;&#31034;&#30340;&#20998;&#24067;&#24335;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Distributional Analogue to the Successor Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#19982;&#21518;&#32493;&#34920;&#31034;&#65288;SR&#65289;&#25551;&#36848;&#25353;&#29031;&#32473;&#23450;&#31574;&#30053;&#34892;&#20026;&#30340;&#26399;&#26395;&#21518;&#26524;&#31867;&#20284;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#65288;SM&#65289;&#25551;&#36848;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20998;&#24067;&#24335;SM&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20998;&#24067;&#24335;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#24067;&#24335;SM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#23618;&#27425;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#19968;&#20123;&#29420;&#31435;&#26377;&#20215;&#20540;&#30340;&#23398;&#20064;&#29366;&#24577;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25216;&#26415;&#12290;&#20316;&#20026;&#20998;&#24067;&#24335;SM&#26377;&#29992;&#24615;&#30340;&#20363;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20351;&#24471;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#22312;&#20197;&#21069;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi
&lt;/p&gt;</description></item><item><title>&#23558;&#23545;&#31216;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;APEN&#65292;&#29992;&#20110;&#26500;&#24314;&#36817;&#20284;&#20998;&#27573;&#30340;E(3)&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#20855;&#26377;&#23616;&#37096;&#23545;&#31216;&#24615;&#30340;&#22810;&#37096;&#20998;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.08529</link><description>&lt;p&gt;
&#36817;&#20284;&#20998;&#27573;E(3)&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Approximately Piecewise E(3) Equivariant Point Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08529
&lt;/p&gt;
&lt;p&gt;
&#23558;&#23545;&#31216;&#24615;&#27010;&#24565;&#24212;&#29992;&#20110;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;APEN&#65292;&#29992;&#20110;&#26500;&#24314;&#36817;&#20284;&#20998;&#27573;&#30340;E(3)&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#20855;&#26377;&#23616;&#37096;&#23545;&#31216;&#24615;&#30340;&#22810;&#37096;&#20998;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23545;&#31216;&#24615;&#30340;&#27010;&#24565;&#38598;&#25104;&#21040;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#25552;&#39640;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23588;&#20854;&#24341;&#20154;&#20851;&#27880;&#30340;&#26159;E(3)&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#65292;&#20854;&#20013;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#27431;&#20960;&#37324;&#24503;&#21464;&#25442;&#22312;&#36755;&#20986;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25193;&#23637;E(3)&#31561;&#21464;&#32593;&#32476;&#65292;&#20197;&#36866;&#24212;&#30001;&#22810;&#20010;&#37096;&#20998;&#32452;&#25104;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#27599;&#20010;&#37096;&#20998;&#37117;&#34920;&#29616;&#20986;&#23616;&#37096;&#30340;E(3)&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#21333;&#29420;&#21464;&#25442;&#30340;&#21306;&#22495;&#26159;&#26410;&#30693;&#30340;&#12290;&#21010;&#20998;&#39044;&#27979;&#30340;&#38169;&#35823;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#26144;&#23556;&#20026;&#19981;&#31526;&#21512;&#30495;&#23454;&#36755;&#20837;&#23545;&#31216;&#24615;&#30340;&#38169;&#35823;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21010;&#20998;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#32500;&#25345;&#19982;&#23454;&#38469;&#21010;&#20998;&#31561;&#21464;&#24615;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#23384;&#22312;&#26080;&#27861;&#25511;&#21046;&#30340;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;APEN: &#19968;&#31181;&#26500;&#24314;&#36817;&#20284;&#20998;&#27573;E(3)&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#20989;&#25968;...
&lt;/p&gt;
&lt;p&gt;
Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are $E(3)$ equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent efforts aim to extend networks that are $E(3)$ equivariant, to accommodate inputs made of multiple parts, each of which exhibits local $E(3)$ symmetry. In practical settings, however, the partitioning into individually transforming regions is unknown a priori. Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry. Past works have proposed different ways to predict the partition, which may exhibit uncontrolled errors in their ability to maintain equivariance to the actual partition. To this end, we introduce APEN: a general framework for constructing approximate piecewise-$E(3)$ equivariant point networks. Our primary insight is that functions th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08522</link><description>&lt;p&gt;
&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fairness Auditing with Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#24037;&#20316;&#20551;&#35774;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#12290;&#26412;&#25991;&#32771;&#34385;&#22810;&#20010;&#20195;&#29702;&#20154;&#23545;&#21516;&#19968;&#24179;&#21488;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#30340;&#23457;&#35745;&#24773;&#20917;&#12290;&#20195;&#29702;&#20154;&#26377;&#20004;&#20010;&#26464;&#26438;&#65306;&#21327;&#20316;&#31574;&#30053;&#65288;&#26159;&#21542;&#36827;&#34892;&#21327;&#35843;&#65289;&#21644;&#25277;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#25110;&#21327;&#20316;&#26102;&#23545;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#30340;&#23457;&#35745;&#20934;&#30830;&#24615;&#19982;&#21327;&#20316;&#30340;&#26368;&#20248;&#25277;&#26679;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;PAC-Bayes&#24037;&#20855;&#31665;&#21644;Poincar&#233;&#19982;Log-Sobolev&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#26799;&#24230;&#39033;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#31361;&#20986;&#20102;&#24179;&#22374;&#26368;&#23567;&#20540;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08508</link><description>&lt;p&gt;
&#24191;&#20041;&#21644;&#24179;&#22343;&#23481;&#37327;&#20043;&#38388;&#30340;PAC-Bayes&#32852;&#32467;
&lt;/p&gt;
&lt;p&gt;
A PAC-Bayesian Link Between Generalisation and Flat Minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;PAC-Bayes&#24037;&#20855;&#31665;&#21644;Poincar&#233;&#19982;Log-Sobolev&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#26799;&#24230;&#39033;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#31361;&#20986;&#20102;&#24179;&#22374;&#26368;&#23567;&#20540;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20351;&#29992;&#36229;&#21442;&#25968;&#35774;&#32622;&#65288;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#22823;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#20013;&#30340;&#39044;&#27979;&#22120;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#19981;&#20165;&#20135;&#29983;&#33391;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#24615;&#33021;&#65292;&#32780;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#19968;&#29616;&#35937;&#25361;&#25112;&#20102;&#35768;&#22810;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#19988;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28041;&#21450;&#26799;&#24230;&#39033;&#30340;&#26032;&#22411;&#27867;&#21270;&#30028;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;PAC-Bayes&#24037;&#20855;&#31665;&#19982;Poincar&#233;&#21644;Log-Sobolev&#19981;&#31561;&#24335;&#30456;&#32467;&#21512;&#65292;&#36991;&#20813;&#20102;&#23545;&#39044;&#27979;&#22120;&#31354;&#38388;&#32500;&#25968;&#30340;&#26174;&#24335;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#8220;&#24179;&#22374;&#26368;&#23567;&#20540;&#8221;&#65288;&#20960;&#20046;&#33021;&#22815;&#26368;&#23567;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37051;&#36817;&#26368;&#23567;&#20540;&#65289;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#31215;&#26497;&#24433;&#21709;&#65292;&#30452;&#25509;&#28041;&#21450;&#21040;&#20248;&#21270;&#38454;&#27573;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning usually involves predictors in the overparametrised setting (number of trained parameters greater than dataset size), and their training yield not only good performances on training data, but also good generalisation capacity. This phenomenon challenges many theoretical results, and remains an open problem. To reach a better understanding, we provide novel generalisation bounds involving gradient terms. To do so, we combine the PAC-Bayes toolbox with Poincar\'e and Log-Sobolev inequalities, avoiding an explicit dependency on dimension of the predictor space. Our results highlight the positive influence of \emph{flat minima} (being minima with a neighbourhood nearly minimising the learning problem as well) on generalisation performances, involving directly the benefits of the optimisation phase.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#20013;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#26469;&#30830;&#23450;&#34892;&#20026;&#26159;&#21542;&#31526;&#21512;COLREGS&#35268;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.08502</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#35777;&#26126;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;
&lt;/p&gt;
&lt;p&gt;
Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#20013;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#26469;&#30830;&#23450;&#34892;&#20026;&#26159;&#21542;&#31526;&#21512;COLREGS&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#24517;&#39035;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#12290;&#36825;&#20123;&#35268;&#21017;&#36890;&#24120;&#20351;&#29992;&#26102;&#24577;&#36923;&#36753;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#23548;&#33268;&#20351;&#29992;&#22522;&#20110;&#20248;&#21270;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#35299;&#20915;&#36825;&#20123;&#32422;&#26463;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#31526;&#21512;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#32431;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#38543;&#26426;&#25506;&#32034;&#65292;&#36825;&#22312;&#26412;&#36136;&#19978;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#65292;&#22987;&#32456;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#24320;&#25918;&#28023;&#22495;&#19978;&#30340;&#33337;&#21482;&#65292;&#36825;&#20123;&#33337;&#21482;&#24517;&#39035;&#36981;&#23432;&#12298;&#28023;&#19978;&#36991;&#30896;&#35268;&#21017;&#20844;&#32422;&#12299;&#65288;COLREGS&#65289;&#30340;&#35268;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#34892;&#20026;&#19982;&#20351;&#29992;&#26102;&#24577;&#36923;&#36753;&#24418;&#24335;&#21270;&#30340;COLREGS&#30340;&#31526;&#21512;&#24615;&#12290;&#25105;&#20204;&#30340;&#34892;&#20026;&#39564;&#35777;&#34987;&#38598;&#25104;&#21040;RL&#36807;&#31243;&#20013;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#21482;&#36873;&#25321;&#32463;&#36807;&#39564;&#35777;&#30340;&#34892;&#20026;&#12290;&#19982;&#21482;&#38598;&#25104;&#20132;&#36890;&#35268;&#21017;&#30340;&#26234;&#33021;&#20307;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31616;&#27905;&#30340;&#31232;&#30095;&#20998;&#32452;k&#26368;&#22823;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#20998;&#32452;&#20869;&#21644;&#20998;&#32452;&#38388;&#30340;&#31232;&#30095;&#24615;&#65292;&#26356;&#25509;&#36817;l0&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08493</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#20998;&#32452;k&#26368;&#22823;&#35268;&#21017;&#21270;&#23454;&#29616;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sparsity via Sparse Group $k$-max Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31616;&#27905;&#30340;&#31232;&#30095;&#20998;&#32452;k&#26368;&#22823;&#35268;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#20998;&#32452;&#20869;&#21644;&#20998;&#32452;&#38388;&#30340;&#31232;&#30095;&#24615;&#65292;&#26356;&#25509;&#36817;l0&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#65292;l0&#27491;&#21017;&#21270;&#38382;&#39064;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#21033;&#29992;&#36138;&#23146;&#31639;&#27861;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#35201;&#20040;&#29992;&#20984;&#26144;&#23556;&#26469;&#36924;&#36817;l0&#27491;&#21017;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#31616;&#27905;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#20998;&#32452;k&#26368;&#22823;&#35268;&#21017;&#21270;&#65292;&#20854;&#19981;&#20165;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#20998;&#32452;&#20869;&#21644;&#20998;&#32452;&#38388;&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#19988;&#23545;&#27599;&#20010;&#20998;&#32452;&#20013;&#30340;&#21464;&#37327;&#30340;&#22823;&#23567;&#27809;&#26377;&#39069;&#22806;&#30340;&#38480;&#21046;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#23610;&#24230;&#30340;&#21464;&#37327;&#23588;&#20026;&#37325;&#35201;&#65292;&#20197;&#26356;&#25509;&#36817;l0&#33539;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#23616;&#37096;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#36845;&#20195;&#36719;&#38408;&#20540;&#31639;&#27861;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the linear inverse problem with sparsity constraints, the $l_0$ regularized problem is NP-hard, and existing approaches either utilize greedy algorithms to find almost-optimal solutions or to approximate the $l_0$ regularization with its convex counterparts. In this paper, we propose a novel and concise regularization, namely the sparse group $k$-max regularization, which can not only simultaneously enhance the group-wise and in-group sparsity, but also casts no additional restraints on the magnitude of variables in each group, which is especially important for variables at different scales, so that it approximate the $l_0$ norm more closely. We also establish an iterative soft thresholding algorithm with local optimality conditions and complexity analysis provided. Through numerical experiments on both synthetic and real-world datasets, we verify the effectiveness and flexibility of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08491</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#30340;&#24067;&#23572;&#27169;&#22411;&#21560;&#24341;&#23376;&#26223;&#35266;&#20013;&#30340;&#25511;&#21046;&#36941;&#21382;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#21487;&#29992;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#19981;&#21516;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#28287;&#23454;&#39564;&#21457;&#29616;&#37325;&#32534;&#31243;&#31574;&#30053;&#30340;&#25928;&#29575;&#21463;&#21040;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#20415;&#24110;&#21161;&#35782;&#21035;&#37325;&#32534;&#31243;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#26694;&#26550;&#30340;BNs&#21644;PBNs&#20197;&#21450;&#24322;&#27493;&#26356;&#26032;&#27169;&#24335;&#19979;&#21046;&#23450;&#20102;&#19968;&#20010;&#25511;&#21046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20266;&#21560;&#24341;&#23376;&#29366;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#20256;&#25773;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#24191;&#20041;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(GPNNs)&#21644;&#36830;&#32493;&#32479;&#19968;&#37324;&#22855;&#26354;&#29575;(CURC)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#21644;&#21152;&#26435;&#22270;&#19978;&#36827;&#34892;&#22270;&#20256;&#25773;&#20998;&#26512;&#21644;&#22270;&#32467;&#26500;&#30340;&#29305;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.08480</link><description>&lt;p&gt;
&#25581;&#31034;&#27867;&#21270;&#22270;&#20256;&#25773;&#20013;&#30340;&#26354;&#32447;&#27969;
&lt;/p&gt;
&lt;p&gt;
Revealing Decurve Flows for Generalized Graph Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#20256;&#25773;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#24191;&#20041;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(GPNNs)&#21644;&#36830;&#32493;&#32479;&#19968;&#37324;&#22855;&#26354;&#29575;(CURC)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#21644;&#21152;&#26435;&#22270;&#19978;&#36827;&#34892;&#22270;&#20256;&#25773;&#20998;&#26512;&#21644;&#22270;&#32467;&#26500;&#30340;&#29305;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#20855;&#26377;&#23450;&#21521;&#21644;&#21152;&#26435;&#22270;&#30340;{\em \textbf{&#24191;&#20041;&#20256;&#25773;}}&#26469;&#35299;&#20915;&#20256;&#32479;&#20449;&#24687;&#20256;&#36882;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#20998;&#26512;&#26041;&#27861;&#23545;&#22270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#37325;&#35201;&#24615;&#20307;&#29616;&#22312;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;{\em &#24191;&#20041;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;}(\textbf{GPNNs})&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#20102;&#22823;&#22810;&#25968;&#22522;&#20110;&#20256;&#25773;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#37051;&#25509;&#20989;&#25968;&#21644;&#36830;&#25509;&#20989;&#25968;&#30340;&#23450;&#21521;-&#21152;&#26435;&#20256;&#25773;&#22270;&#65292;GPNNs&#21487;&#20197;&#22686;&#24378;&#23545;&#21508;&#31181;&#22270;&#27169;&#22411;&#20013;&#27880;&#24847;&#26426;&#21046;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#25506;&#35752;&#20102;&#35774;&#35745;&#31354;&#38388;&#20013;&#30340;&#26435;&#34913;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24378;&#35843;&#20102;&#37051;&#25509;&#20989;&#25968;&#23545;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;{\em &#36830;&#32493;&#32479;&#19968;&#37324;&#22855;&#26354;&#29575;}(\textbf{CURC})&#65292;&#23427;&#26159;&#33879;&#21517;&#30340;{\em &#22885;&#21033;&#32500;&#23572;-&#37324;&#22855;&#26354;&#29575;}&#22312;&#23450;&#21521;&#21644;&#21152;&#26435;&#22270;&#19978;&#30340;&#25193;&#23637;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;CURC&#33021;&#22815;&#25581;&#31034;&#22270;&#32467;&#26500;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#21253;&#25324;&#22270;&#30340;&#36830;&#36890;&#24615;&#21644;&#27969;&#21160;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the limitations of the traditional analysis of message-passing, central to graph learning, by defining {\em \textbf{generalized propagation}} with directed and weighted graphs. The significance manifest in two ways. \textbf{Firstly}, we propose {\em Generalized Propagation Neural Networks} (\textbf{GPNNs}), a framework that unifies most propagation-based graph neural networks. By generating directed-weighted propagation graphs with adjacency function and connectivity function, GPNNs offer enhanced insights into attention mechanisms across various graph models. We delve into the trade-offs within the design space with empirical experiments and emphasize the crucial role of the adjacency function for model expressivity via theoretical analysis. \textbf{Secondly}, we propose the {\em Continuous Unified Ricci Curvature} (\textbf{CURC}), an extension of celebrated {\em Ollivier-Ricci Curvature} for directed and weighted graphs. Theoretically, we demonstrate that CURC po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#19978;&#26377;&#36229;&#36807;99%&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#21364;&#23436;&#20840;&#22833;&#36133;&#12290;&#36890;&#36807;&#32447;&#24615;&#36817;&#20284;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08473</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#31995;&#32479;&#24615;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#36259;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#19978;&#26377;&#36229;&#36807;99%&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#21364;&#23436;&#20840;&#22833;&#36133;&#12290;&#36890;&#36807;&#32447;&#24615;&#36817;&#20284;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;(&#38646;&#26679;&#26412;)&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#36825;&#20123;&#27169;&#22411;&#30446;&#21069;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#34429;&#28982;&#25506;&#27979;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#29305;&#23450;&#23646;&#24615;&#65292;&#20294;&#34920;&#31034;&#31354;&#38388;&#30340;&#32467;&#26500;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#24615;&#30340;&#21051;&#30011;&#65307;&#22240;&#27492;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#26679;&#30340;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21644;&#36807;&#24230;&#25512;&#24191;&#21040;&#36229;&#20986;&#25968;&#25454;&#38598;&#33539;&#22260;&#30340;&#26032;&#36755;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#25506;&#32034;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;Imagenette&#25968;&#25454;&#38598;&#34920;&#26126;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#36817;&#20284;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#20123;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#22411;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20197;&#25903;&#25345;&#36825;&#20010;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21451;&#22909;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;&#65292;&#24182;&#25552;&#20379;&#22312;&#32447;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.08470</link><description>&lt;p&gt;
&#38754;&#21521;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#30340;&#24182;&#34892;&#21451;&#22909;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21451;&#22909;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;&#65292;&#24182;&#25552;&#20379;&#22312;&#32447;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#36235;&#21183;&#20998;&#26512;&#26041;&#27861;(ST-GTrend)&#65292;&#29992;&#20110;&#36827;&#34892;&#20809;&#20239;&#30005;&#21147;&#32593;&#32476;&#30340;&#38598;&#32676;&#32423;&#24615;&#33021;&#34928;&#20943;&#20998;&#26512;&#12290;&#20809;&#20239;&#30005;&#31449;&#24050;&#25104;&#20026;&#20840;&#29699;&#21487;&#25345;&#32493;&#33021;&#28304;&#29983;&#20135;&#26684;&#23616;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20934;&#30830;&#20272;&#35745;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#23545;&#20110;&#20854;&#20316;&#20026;&#21457;&#30005;&#25216;&#26415;&#21644;&#37329;&#34701;&#36164;&#20135;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35780;&#20272;&#20809;&#20239;&#31995;&#32479;&#30340;&#27700;&#24179;&#21270;&#33021;&#28304;&#25104;&#26412;(LCOE)&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20102;&#35299;&#21644;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;(PLR)&#12290;ST-GTrend&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#23558;PLR&#20316;&#20026;&#38271;&#26399;&#30340;&#8220;&#32769;&#21270;&#8221;&#36235;&#21183;&#19982;&#20809;&#20239;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22810;&#20010;&#27874;&#21160;&#39033;&#20998;&#31163;&#24320;&#26469;&#12290;&#20026;&#20102;&#24212;&#23545;&#26102;&#24207;&#20013;&#22810;&#26679;&#30340;&#34928;&#20943;&#27169;&#24335;&#65292;ST-GTrend&#37319;&#29992;&#24182;&#34892;&#22270;&#33258;&#32534;&#30721;&#22120;&#25968;&#32452;&#21516;&#26102;&#25552;&#21462;&#32769;&#21270;&#21644;&#27874;&#21160;&#39033;&#12290;ST-GTrend&#21457;&#24067;&#20102;&#19968;&#20010;&#22312;&#32447;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term "aging" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend impo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ROS2&#30340;&#29289;&#29702;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#28183;&#36879;&#27979;&#35797;&#22312;&#19977;&#20010;&#23618;&#27425;&#30417;&#27979;&#31995;&#32479;&#29305;&#24449;&#65292;&#37325;&#29616;&#20102;&#27491;&#24120;&#21644;&#25915;&#20987;&#34892;&#20026;&#30340;&#20132;&#26367;&#21644;&#37325;&#21472;&#36807;&#31243;&#65292;&#21487;&#20197;&#27979;&#37327;&#26816;&#27979;&#25915;&#20987;&#32773;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24694;&#24847;&#27963;&#21160;&#30340;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.08468</link><description>&lt;p&gt;
ROSpace: ROS2&#22522;&#30784;&#30340;&#29289;&#29702;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ROSpace: Intrusion Detection Dataset for a ROS2-Based Cyber-Physical System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ROS2&#30340;&#29289;&#29702;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#28183;&#36879;&#27979;&#35797;&#22312;&#19977;&#20010;&#23618;&#27425;&#30417;&#27979;&#31995;&#32479;&#29305;&#24449;&#65292;&#37325;&#29616;&#20102;&#27491;&#24120;&#21644;&#25915;&#20987;&#34892;&#20026;&#30340;&#20132;&#26367;&#21644;&#37325;&#21472;&#36807;&#31243;&#65292;&#21487;&#20197;&#27979;&#37327;&#26816;&#27979;&#25915;&#20987;&#32773;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24694;&#24847;&#27963;&#21160;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#38024;&#23545;&#32593;&#32476;&#31995;&#32479;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#20174;&#19968;&#20010;&#23618;&#27425;&#19978;&#25910;&#38598;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25915;&#20987;&#24448;&#24448;&#26159;&#22312;&#19987;&#29992;&#30340;&#25915;&#20987;&#20250;&#35805;&#20013;&#29983;&#25104;&#30340;&#65292;&#32780;&#19981;&#26159;&#37325;&#29616;&#27491;&#24120;&#21644;&#25915;&#20987;&#34892;&#20026;&#30340;&#23454;&#38469;&#20132;&#26367;&#21644;&#37325;&#21472;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20837;&#20405;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;ROS2&#30340;&#23884;&#20837;&#24335;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#26469;&#33719;&#21462;&#12290;&#29305;&#24449;&#20174;&#19977;&#20010;&#23618;&#27425;&#30417;&#27979;&#65306;Linux&#25805;&#20316;&#31995;&#32479;&#12289;&#32593;&#32476;&#21644;ROS2&#26381;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#26102;&#38388;&#24207;&#21015;&#30340;&#24418;&#24335;&#36827;&#34892;&#32452;&#32455;&#65292;&#24182;&#25551;&#36848;&#20102;&#31995;&#32479;&#30340;&#39044;&#26399;&#34892;&#20026;&#20197;&#21450;&#23545;ROS2&#29305;&#23450;&#25915;&#20987;&#30340;&#21709;&#24212;&#65306;&#23427;&#21453;&#22797;&#20132;&#26367;&#26080;&#25915;&#20987;&#25805;&#20316;&#30340;&#26102;&#26399;&#21644;&#25191;&#34892;&#29305;&#23450;&#25915;&#20987;&#30340;&#26102;&#26399;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#21487;&#20197;&#27979;&#37327;&#26816;&#27979;&#25915;&#20987;&#32773;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#24694;&#24847;&#27963;&#21160;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the intrusion detection datasets to research machine learning-based intrusion detection systems (IDSs) are devoted to cyber-only systems, and they typically collect data from one architectural layer. Additionally, often the attacks are generated in dedicated attack sessions, without reproducing the realistic alternation and overlap of normal and attack actions. We present a dataset for intrusion detection by performing penetration testing on an embedded cyber-physical system built over Robot Operating System 2 (ROS2). Features are monitored from three architectural layers: the Linux operating system, the network, and the ROS2 services. The dataset is structured as a time series and describes the expected behavior of the system and its response to ROS2-specific attacks: it repeatedly alternates periods of attack-free operation with periods when a specific attack is being performed. Noteworthy, this allows measuring the time to detect an attacker and the number of malicious activ
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraphormer&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23376;&#22270;GNNs&#21644;&#22270;&#21464;&#25442;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#32508;&#21512;&#20102;&#23376;&#22270;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20197;&#21450;&#22270;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#12290;&#22522;&#20110;&#23376;&#22270;GNNs&#19982;&#22270;&#30340;&#20056;&#31215;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#20056;&#31215;&#22270;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.08450</link><description>&lt;p&gt;
Subgraphormer:&#36890;&#36807;&#22270;&#30340;&#20056;&#31215;&#23558;&#23376;&#22270;GNN&#21644;&#22270;&#21464;&#25442;&#22120;&#32479;&#19968;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
Subgraphormer: Unifying Subgraph GNNs and Graph Transformers via Graph Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08450
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraphormer&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#23376;&#22270;GNNs&#21644;&#22270;&#21464;&#25442;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#32508;&#21512;&#20102;&#23376;&#22270;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20197;&#21450;&#22270;&#21464;&#25442;&#22120;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#12290;&#22522;&#20110;&#23376;&#22270;GNNs&#19982;&#22270;&#30340;&#20056;&#31215;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#20056;&#31215;&#22270;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#39046;&#22495;&#20013;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#20004;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#23376;&#22270;GNN&#21644;&#22270;&#21464;&#25442;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;Subgraphormer&#65292;&#23427;&#23558;&#23376;&#22270;GNNs&#30340;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12289;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#21644;&#32858;&#21512;&#26041;&#26696;&#19982;&#22270;&#21464;&#25442;&#22120;&#20013;&#26368;&#37325;&#35201;&#30340;&#27880;&#24847;&#21147;&#21644;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25105;&#20204;&#25581;&#31034;&#30340;&#23376;&#22270;GNNs&#19982;&#20056;&#31215;&#22270;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#65292;&#36825;&#34920;&#26126;&#23376;&#22270;GNNs&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22312;&#22270;&#30340;&#20056;&#31215;&#19978;&#25805;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;(MPNNs)&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20844;&#24335;&#26469;&#35774;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#20056;&#31215;&#22270;&#30340;&#36830;&#25509;&#24615;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#23376;&#22270;GNNs&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#23558;&#20854;&#25512;&#23548;&#20026;&#20056;&#31215;&#22270;&#30340;&#20301;&#32622;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Graph Neural Networks (GNNs), two exciting research directions have recently emerged: Subgraph GNNs and Graph Transformers. In this paper, we propose an architecture that integrates both approaches, dubbed Subgraphormer, which combines the enhanced expressive power, message-passing mechanisms, and aggregation schemes from Subgraph GNNs with attention and positional encodings, arguably the most important components in Graph Transformers. Our method is based on an intriguing new connection we reveal between Subgraph GNNs and product graphs, suggesting that Subgraph GNNs can be formulated as Message Passing Neural Networks (MPNNs) operating on a product of the graph with itself. We use this formulation to design our architecture: first, we devise an attention mechanism based on the connectivity of the product graph. Following this, we propose a novel and efficient positional encoding scheme for Subgraph GNNs, which we derive as a positional encoding for the product graph. 
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#39057;&#29575;&#24863;&#30693;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65288;FaGSP&#65289;&#37319;&#29992;&#32423;&#32852;&#28388;&#27874;&#27169;&#22359;&#21644;&#24182;&#34892;&#28388;&#27874;&#27169;&#22359;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#24182;&#21033;&#29992;&#39640;&#38454;&#37051;&#22495;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#21327;&#21516;&#36807;&#28388;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08426</link><description>&lt;p&gt;
&#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#39057;&#29575;&#24863;&#30693;&#22270;&#20449;&#21495;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Frequency-aware Graph Signal Processing for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08426
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#39057;&#29575;&#24863;&#30693;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65288;FaGSP&#65289;&#37319;&#29992;&#32423;&#32852;&#28388;&#27874;&#27169;&#22359;&#21644;&#24182;&#34892;&#28388;&#27874;&#27169;&#22359;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#24182;&#21033;&#29992;&#39640;&#38454;&#37051;&#22495;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#21327;&#21516;&#36807;&#28388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22522;&#20110;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#25512;&#33616;&#31639;&#27861;&#22240;&#20854;&#39640;&#25928;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#32771;&#34385;&#21040;&#21453;&#26144;&#29992;&#25143;/&#29289;&#21697;&#29305;&#24449;&#30340;&#21508;&#31181;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26410;&#21033;&#29992;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#39640;&#38454;&#37051;&#22495;&#20449;&#24687;&#26469;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#65292;&#20174;&#32780;&#23548;&#33268;&#23376;&#20248;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21327;&#21516;&#36807;&#28388;&#30340;&#39057;&#29575;&#24863;&#30693;&#22270;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65288;FaGSP&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32423;&#32852;&#28388;&#27874;&#27169;&#22359;&#65292;&#30001;&#29702;&#24819;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#21644;&#29702;&#24819;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#32452;&#25104;&#65292;&#20197;&#36830;&#32493;&#30340;&#26041;&#24335;&#25429;&#33719;&#29420;&#29305;&#21644;&#20849;&#21516;&#30340;&#29992;&#25143;/&#29289;&#21697;&#29305;&#24449;&#65292;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#28388;&#27874;&#27169;&#22359;&#65292;&#30001;&#20004;&#20010;&#20302;&#36890;&#28388;&#27874;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#36731;&#26494;&#25429;&#33719;&#37051;&#22495;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#21033;&#29992;&#29992;&#25143;/&#29289;&#21697;&#30340;&#39640;&#38454;&#37051;&#22495;&#20449;&#24687;&#36827;&#34892;&#29992;&#25143;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Processing (GSP) based recommendation algorithms have recently attracted lots of attention due to its high efficiency. However, these methods failed to consider the importance of various interactions that reflect unique user/item characteristics and failed to utilize user and item high-order neighborhood information to model user preference, thus leading to sub-optimal performance. To address the above issues, we propose a frequency-aware graph signal processing method (FaGSP) for collaborative filtering. Firstly, we design a Cascaded Filter Module, consisting of an ideal high-pass filter and an ideal low-pass filter that work in a successive manner, to capture both unique and common user/item characteristics to more accurately model user preference. Then, we devise a Parallel Filter Module, consisting of two low-pass filters that can easily capture the hierarchy of neighborhood, to fully utilize high-order neighborhood information of users/items for more accurate user pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29109;&#20256;&#36755;&#26680;&#20174;&#25209;&#27425;&#30340;&#26410;&#37197;&#23545;&#28857;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#21644;$Y$&#30340;&#32852;&#21512;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.08425</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#20256;&#36755;&#26680;&#23558;&#26410;&#37197;&#23545;&#28857;&#30340;&#25209;&#27425;&#36716;&#31227;&#31639;&#23376;&#36827;&#34892;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Transfer Operators from Batches of Unpaired Points via Entropic Transport Kernels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29109;&#20256;&#36755;&#26680;&#20174;&#25209;&#27425;&#30340;&#26410;&#37197;&#23545;&#28857;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#21644;$Y$&#30340;&#32852;&#21512;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#36890;&#36807;$N$&#20010;&#29420;&#31435;&#35266;&#27979;&#22359;$(\boldsymbol{x}^i,\boldsymbol{y}^i)$&#65288;$i=1,\ldots,N$&#65289;&#26469;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;$X$&#21644;$Y$&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#27599;&#20010;&#35266;&#27979;&#22359;&#21253;&#21547;$M$&#20010;&#26679;&#26412;$(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)}) \bigr)_{j=1}^M$&#65292;&#20854;&#20013;$\sigma^i$&#34920;&#31034;&#19968;&#20010;&#26410;&#30693;&#30340;&#25490;&#21015;&#65292;&#29992;&#20110;&#23545;$i.i.d.$&#37319;&#26679;&#30340;&#23545;$(x^i_j,y_j^i)$&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;$j=1,\ldots,M$&#12290;&#36825;&#24847;&#21619;&#30528;&#35266;&#27979;&#22359;&#20869;&#37096;&#26679;&#26412;&#30340;&#39034;&#24207;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26368;&#22823;&#20284;&#28982;&#25512;&#26029;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35745;&#31639;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;$\Gamma$-&#25910;&#25947;&#32467;&#26524;&#65292;&#35828;&#26126;&#25105;&#20204;&#21487;&#20197;&#22312;&#22359;&#25968;$N$&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#20174;&#32463;&#39564;&#36817;&#20284;&#20013;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#23494;&#24230;&#12290;&#20351;&#29992;&#29109;&#26368;&#20248;&#20256;&#36755;&#26680;&#65292;&#25105;&#20204;&#23545;&#19968;&#31867;&#20551;&#35774;&#31354;&#38388;&#24314;&#27169;&#65292;&#35813;&#31354;&#38388;&#30340;&#23494;&#24230;&#20989;&#25968;&#21487;&#20197;&#26368;&#23567;&#21270;&#25512;&#26029;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we are concerned with estimating the joint probability of random variables $X$ and $Y$, given $N$ independent observation blocks $(\boldsymbol{x}^i,\boldsymbol{y}^i)$, $i=1,\ldots,N$, each of $M$ samples $(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma^i(j)}) \bigr)_{j=1}^M$, where $\sigma^i$ denotes an unknown permutation of i.i.d. sampled pairs $(x^i_j,y_j^i)$, $j=1,\ldots,M$. This means that the internal ordering of the $M$ samples within an observation block is not known. We derive a maximum-likelihood inference functional, propose a computationally tractable approximation and analyze their properties. In particular, we prove a $\Gamma$-convergence result showing that we can recover the true density from empirical approximations as the number $N$ of blocks goes to infinity. Using entropic optimal transport kernels, we model a class of hypothesis spaces of density functions over which the inference functional can be minimized. This hypothesis class is 
&lt;/p&gt;</description></item><item><title>&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08424</link><description>&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Neural Expert Processes for Learning from Demonstration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08424
&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#65288;LfD&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#25216;&#33021;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#30456;&#21516;&#25216;&#33021;&#30340;&#28436;&#31034;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#21516;&#26102;&#23581;&#35797;&#33719;&#21462;&#30456;&#21516;&#25216;&#33021;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#23558;&#36825;&#20123;&#21160;&#20316;&#32534;&#30721;&#20026;&#36816;&#21160;&#21407;&#35821;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;LfD&#26694;&#26550;&#65292;&#21363;&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#65292;&#23427;&#23398;&#20064;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#36215;&#26469;&#12290;CNEP&#19981;&#38656;&#35201;&#22312;&#21738;&#31181;&#27169;&#24335;&#19979;&#36712;&#36857;&#23646;&#20110;&#30340;&#30417;&#30563;&#12290;&#22312;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CNEP&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;CNEP&#19982;&#21478;&#19968;&#20010;LfD&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#31070;&#32463;&#36816;&#21160;&#21407;&#35821;&#65288;CNMP&#65289;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#26469;&#22312;$\ell_\infty$&#33539;&#25968;&#19979;&#20272;&#35745;&#31163;&#25955;&#27010;&#29575;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#36816;&#29992;&#20999;&#23572;&#35834;&#22827;&#22411;&#19981;&#31561;&#24335;&#21644;&#32463;&#39564;&#36125;&#23572;&#26031;&#22374;&#30028;&#31561;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#22522;&#26412;&#30340;&#36873;&#25321;&#24615;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08422</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#31351;&#33539;&#25968;&#30340;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Distribution Estimation under the Infinity Norm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#26469;&#22312;$\ell_\infty$&#33539;&#25968;&#19979;&#20272;&#35745;&#31163;&#25955;&#27010;&#29575;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#36816;&#29992;&#20999;&#23572;&#35834;&#22827;&#22411;&#19981;&#31561;&#24335;&#21644;&#32463;&#39564;&#36125;&#23572;&#26031;&#22374;&#30028;&#31561;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#22522;&#26412;&#30340;&#36873;&#25321;&#24615;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$\ell_\infty$&#33539;&#25968;&#19979;&#20272;&#35745;&#31163;&#25955;&#27010;&#29575;&#20998;&#24067;&#30340;&#26032;&#30028;&#38480;&#12290;&#36825;&#20123;&#30028;&#38480;&#22312;&#21508;&#31181;&#31934;&#30830;&#24847;&#20041;&#19978;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#65292;&#21253;&#25324;&#19968;&#31181;&#23454;&#20363;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#23545;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#30340;&#25968;&#25454;&#30456;&#20851;&#25910;&#25947;&#24615;&#20445;&#35777;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#24050;&#30693;&#32467;&#26524;&#12290;&#25105;&#20204;&#36816;&#29992;&#21644;&#21019;&#26032;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20999;&#23572;&#35834;&#22827;&#22411;&#19981;&#31561;&#24335;&#21644;&#32463;&#39564;&#36125;&#23572;&#26031;&#22374;&#30028;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#20010;&#22522;&#26412;&#30340;&#36873;&#25321;&#24615;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#20272;&#35745;&#26679;&#26412;&#20013;&#20986;&#29616;&#39057;&#29575;&#26368;&#39640;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present novel bounds for estimating discrete probability distributions under the $\ell_\infty$ norm. These are nearly optimal in various precise senses, including a kind of instance-optimality. Our data-dependent convergence guarantees for the maximum likelihood estimator significantly improve upon the currently known results. A variety of techniques are utilized and innovated upon, including Chernoff-type inequalities and empirical Bernstein bounds. We illustrate our results in synthetic and real-world experiments. Finally, we apply our proposed framework to a basic selective inference problem, where we estimate the most frequent probabilities in a sample.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#26469;&#35299;&#20915;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24102;&#26469;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08421</link><description>&lt;p&gt;
&#20445;&#23432;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#26469;&#35299;&#20915;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24102;&#26469;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#24179;&#21488;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#25511;&#21046;&#12289;&#20248;&#21270;&#21644;&#30417;&#25511;&#35832;&#22914;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20043;&#31867;&#30340;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#26377;&#24076;&#26395;&#25216;&#26415;&#12290;&#37319;&#29992;DT&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#31163;&#32447;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#32570;&#20047;&#23545;&#29289;&#29702;&#29615;&#22659;&#30340;&#30452;&#25509;&#35775;&#38382;&#12290;&#36825;&#19968;&#38480;&#21046;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23588;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22312;&#32447;&#20114;&#21160;&#12290;&#23558;&#22312;&#32447;MARL&#26041;&#26696;&#30452;&#25509;&#24212;&#29992;&#20110;&#31163;&#32447;&#29615;&#22659;&#36890;&#24120;&#20250;&#22240;&#26377;&#38480;&#25968;&#25454;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32780;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;DT&#30340;&#26080;&#32447;&#32593;&#32476;&#30340;&#31163;&#32447;MARL&#26041;&#26696;&#65292;&#23427;&#25972;&#21512;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;distributional RL&#65289;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#29615;&#22659;&#22266;&#26377;&#30340;&#26696;&#20363;&#24615;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#24314;&#27169;&#22810;&#26234;&#20307;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#25512;&#26029;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#24182;&#20351;&#29992;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#65288;ALS&#65289;&#31639;&#27861;&#21644;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#31639;&#23376;&#22238;&#24402;&#65288;ORALS&#65289;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#22312;&#20445;&#35777;&#21487;&#35782;&#21035;&#24615;&#21644;&#33391;&#23450;&#20041;&#24615;&#30340;&#26465;&#20214;&#19979;&#65292;ALS&#31639;&#27861;&#34920;&#29616;&#20986;&#32479;&#35745;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#32780;ORALS&#31639;&#27861;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20855;&#26377;&#27491;&#24577;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08412</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#19978;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;: &#32593;&#32476;&#21644;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#32852;&#21512;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interacting Particle Systems on Networks: joint inference of the network and the interaction kernel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32593;&#32476;&#19978;&#24314;&#27169;&#22810;&#26234;&#20307;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#25512;&#26029;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#30456;&#20114;&#20316;&#29992;&#26680;&#30340;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#24182;&#20351;&#29992;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#65288;ALS&#65289;&#31639;&#27861;&#21644;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#31639;&#23376;&#22238;&#24402;&#65288;ORALS&#65289;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#22312;&#20445;&#35777;&#21487;&#35782;&#21035;&#24615;&#21644;&#33391;&#23450;&#20041;&#24615;&#30340;&#26465;&#20214;&#19979;&#65292;ALS&#31639;&#27861;&#34920;&#29616;&#20986;&#32479;&#35745;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#32780;ORALS&#31639;&#27861;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20855;&#26377;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23398;&#31185;&#20013;&#65292;&#23545;&#32593;&#32476;&#19978;&#30340;&#22810;&#26234;&#20307;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#30001;&#22810;&#26465;&#36712;&#36857;&#32452;&#25104;&#30340;&#25968;&#25454;&#20013;&#32852;&#21512;&#25512;&#26029;&#32593;&#32476;&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#30456;&#20114;&#20316;&#29992;&#26680;&#65292;&#20998;&#21035;&#30830;&#23450;&#21738;&#20123;&#26234;&#20307;&#19982;&#21738;&#20123;&#20854;&#20182;&#26234;&#20307;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#33258;&#28982;&#22320;&#23548;&#33268;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#65288;ALS&#65289;&#31639;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#26368;&#23567;&#20108;&#20056;&#30340;&#31639;&#23376;&#22238;&#24402;&#65288;ORALS&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#21487;&#25193;&#23637;&#21040;&#22823;&#37327;&#25968;&#25454;&#36712;&#36857;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20445;&#35777;&#21487;&#35782;&#21035;&#24615;&#21644;&#33391;&#23450;&#20041;&#24615;&#30340;&#24378;&#21046;&#24615;&#26465;&#20214;&#12290;&#23613;&#31649;ALS&#31639;&#27861;&#22312;&#23567;&#25968;&#25454;&#24773;&#20917;&#19979;&#32570;&#20047;&#24615;&#33021;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20294;&#34920;&#29616;&#20986;&#32479;&#35745;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#24378;&#21046;&#24615;&#26465;&#20214;&#19979;&#65292;ORALS&#20272;&#35745;&#22120;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#20855;&#26377;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling multi-agent systems on networks is a fundamental challenge in a wide variety of disciplines. We jointly infer the weight matrix of the network and the interaction kernel, which determine respectively which agents interact with which others and the rules of such interactions from data consisting of multiple trajectories. The estimator we propose leads naturally to a non-convex optimization problem, and we investigate two approaches for its solution: one is based on the alternating least squares (ALS) algorithm; another is based on a new algorithm named operator regression with alternating least squares (ORALS). Both algorithms are scalable to large ensembles of data trajectories. We establish coercivity conditions guaranteeing identifiability and well-posedness. The ALS algorithm appears statistically efficient and robust even in the small data regime but lacks performance and convergence guarantees. The ORALS estimator is consistent and asymptotically normal under a coercivity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08406</link><description>&lt;p&gt;
&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition Constrained Bayesian Optimization via Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20851;&#27880;&#30340;&#26159;&#21487;&#20197;&#20219;&#24847;&#26597;&#35810;&#25628;&#32034;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#24182;&#19981;&#20855;&#22791;&#36825;&#31181;&#28789;&#27963;&#24615;&#65307;&#29305;&#21035;&#26159;&#65292;&#19979;&#19968;&#20010;&#26597;&#35810;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#21462;&#20915;&#20110;&#20808;&#21069;&#30340;&#26597;&#35810;&#12290;&#29289;&#29702;&#31185;&#23398;&#39046;&#22495;&#30340;&#20363;&#23376;&#20013;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#23616;&#37096;&#31227;&#21160;&#38480;&#21046;&#12289;&#29305;&#23450;&#21464;&#37327;&#30340;&#21333;&#35843;&#24615;&#35201;&#27714;&#20197;&#21450;&#36716;&#21464;&#24433;&#21709;&#27979;&#37327;&#31934;&#24230;&#12290;&#24635;&#20043;&#65292;&#36825;&#20123;&#36807;&#28193;&#32422;&#26463;&#38656;&#35201;&#19968;&#31181;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#25193;&#23637;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36845;&#20195;&#22320;&#35299;&#20915;&#25105;&#20204;&#30446;&#26631;&#30340;&#19968;&#20010;&#21487;&#34892;&#32447;&#24615;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#33021;&#22815;&#25552;&#21069;&#35268;&#21010;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20381;&#36182;&#21382;&#21490;&#30340;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#36890;&#29992;&#21270;&#33021;&#21147;&#30340;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#65292;&#21363;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23545;1NN&#20998;&#31867;&#22120;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#20219;&#24847;&#23494;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#25214;&#21040;&#20219;&#24847;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#24456;&#23567;&#30340;VC&#32500;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08405</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#36890;&#29992;&#21270;&#33021;&#21147;&#30340;1NN&#20998;&#31867;&#22120;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#21019;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
A Novel Approach to Regularising 1NN classifier for Improved Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#36890;&#29992;&#21270;&#33021;&#21147;&#30340;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#65292;&#21363;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23545;1NN&#20998;&#31867;&#22120;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#20219;&#24847;&#23494;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#25214;&#21040;&#20219;&#24847;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#24456;&#23567;&#30340;VC&#32500;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#23398;&#20064;&#20219;&#24847;&#36793;&#30028;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#24515;&#26041;&#27861;&#65292;&#23545;1NN&#20998;&#31867;&#22120;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#20998;&#31867;&#22120;&#31216;&#20026;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#12290;&#24050;&#30693;1NN&#20998;&#31867;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#20294;&#20855;&#26377;&#24456;&#22823;&#30340;VC&#32500;&#24230;&#65292;&#22240;&#27492;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#20219;&#24847;&#36275;&#22815;&#23494;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#25214;&#21040;&#20219;&#24847;&#36793;&#30028;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#23567;&#30340;VC&#32500;&#24230;&#65307;&#22240;&#27492;&#65292;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#20256;&#32479;&#30340;1NN&#20998;&#31867;&#22120;&#27491;&#21017;&#21270;&#26041;&#27861;&#26159;&#32771;&#34385;K&#20010;&#26368;&#36817;&#37051;&#12290;&#37051;&#22495;&#32452;&#20214;&#20998;&#26512;&#65288;NCA&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#65288;n-1&#65289;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#19968;&#33268;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;n&#34920;&#31034;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#19982;&#20998;&#27700;&#23725;&#20998;&#31867;&#22120;&#19968;&#33268;&#30340;&#34920;&#31034;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a class of non-parametric classifiers, that learn arbitrary boundaries and generalize well.   Our approach is based on a novel way to regularize 1NN classifiers using a greedy approach. We refer to this class of classifiers as Watershed Classifiers. 1NN classifiers are known to trivially over-fit but have very large VC dimension, hence do not generalize well. We show that watershed classifiers can find arbitrary boundaries on any dense enough dataset, and, at the same time, have very small VC dimension; hence a watershed classifier leads to good generalization.   Traditional approaches to regularize 1NN classifiers are to consider $K$ nearest neighbours. Neighbourhood component analysis (NCA) proposes a way to learn representations consistent with ($n-1$) nearest neighbour classifier, where $n$ denotes the size of the dataset. In this article, we propose a loss function which can learn representations consistent with watershed classifiers, and show that it out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOSS-GAT&#30340;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37319;&#29992;&#20102;&#26631;&#31614;&#20256;&#25773;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08401</link><description>&lt;p&gt;
LOSS-GAT: &#26631;&#31614;&#20256;&#25773;&#21644;&#19968;&#31867;&#21322;&#30417;&#30563;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOSS-GAT&#30340;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37319;&#29992;&#20102;&#26631;&#31614;&#20256;&#25773;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#31038;&#20132;&#32593;&#32476;&#30340;&#26102;&#20195;&#65292;&#34394;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#23041;&#32961;&#65292;&#22312;&#20154;&#20204;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35782;&#21035;&#20551;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#26631;&#35760;&#26032;&#38395;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#19968;&#20010;&#31867;&#23398;&#20064;&#65288;OCL&#65289;&#26041;&#27861;&#21487;&#20197;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#21512;&#36866;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#21487;&#20197;&#35775;&#38382;&#21040;&#19981;&#21516;&#20869;&#23481;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#19988;&#22270;&#19978;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#33410;&#28857;&#30340;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#31216;&#20026;LOSS-GAT&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#35748;&#35777;&#22270;&#20687;&#20687;&#32032;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.08400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hierarchical Certification for Segmentation using Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#30340;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#35748;&#35777;&#22270;&#20687;&#20687;&#32032;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#30340;&#35748;&#35777;&#26041;&#27861;&#26159;&#22312;&#39044;&#23450;&#20041;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#38598;&#19978;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#26356;&#26222;&#36941;&#19988;&#23454;&#29992;&#30340;&#35774;&#32622;&#65292;&#21363;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#35748;&#35777;&#21487;&#20197;&#22312;&#30001;&#32454;&#21040;&#31895;&#30340;&#22810;&#32423;&#23618;&#32423;&#26631;&#31614;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;&#19982;&#32463;&#20856;&#26041;&#27861;&#19981;&#21516;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#32452;&#20214;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#25918;&#26494;&#20102;&#35748;&#35777;&#21040;&#23618;&#32423;&#20013;&#30340;&#26356;&#31895;&#31890;&#24230;&#30340;&#32423;&#21035;&#12290;&#36825;&#31181;&#25918;&#26494;&#38477;&#20302;&#20102;&#25918;&#24323;&#29575;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#32463;&#36807;&#35748;&#35777;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25968;&#23398;&#22320;&#24418;&#24335;&#21270;&#20102;&#38382;&#39064;&#35774;&#32622;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#35748;&#35777;&#31639;&#27861;&#29992;&#20110;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#65292;&#23427;&#21487;&#20197;&#22312;&#23618;&#32423;&#20013;&#23545;&#22270;&#20687;&#20687;&#32032;&#36827;&#34892;&#35748;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20445;&#35777;&#30340;&#27491;&#30830;&#24615;&#12290;&#30001;&#20110;&#35748;&#35777;&#30340;&#20934;&#30830;&#24230;&#22312;&#36941;&#21382;&#26102;&#19981;&#32771;&#34385;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common certification methods operate on a flat pre-defined set of fine-grained classes. In this paper, however, we propose a novel, more general, and practical setting, namely adaptive hierarchical certification for image semantic segmentation. In this setting, the certification can be within a multi-level hierarchical label space composed of fine to coarse levels. Unlike classic methods where the certification would abstain for unstable components, our approach adaptively relaxes the certification to a coarser level within the hierarchy. This relaxation lowers the abstain rate whilst providing more certified semantically meaningful information. We mathematically formulate the problem setup and introduce, for the first time, an adaptive hierarchical certification algorithm for image semantic segmentation, that certifies image pixels within a hierarchy and prove the correctness of its guarantees. Since certified accuracy does not take the loss of information into account when traversing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08384</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#23398;&#20064;&#65306;&#23454;&#29616;&#21160;&#24577;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Selective Learning: Towards Robust Calibration with Dynamic Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35823;&#26657;&#20934;&#25351;&#30340;&#26159;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#26159;&#30001;&#36807;&#25311;&#21512;&#38382;&#39064;&#24341;&#36215;&#30340;&#65292;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#29305;&#28857;&#26159;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#25152;&#26377;&#20869;&#23481;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36827;&#34892;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#24182;&#32531;&#35299;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#29702;&#35299;&#20026;&#23547;&#25214;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#21487;&#20449;&#24230;&#26469;&#36866;&#24212;&#23454;&#38469;&#26631;&#31614;&#65292;&#21516;&#26102;&#36890;&#36807;&#38477;&#20302;&#21487;&#20449;&#24230;&#26469;&#26368;&#22823;&#21270;&#39044;&#27979;&#27010;&#29575;&#30340;&#29109;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#23548;&#33268;&#30446;&#26631;&#20914;&#31361;&#65288;&#22686;&#21152;&#20294;&#20063;&#38477;&#20302;&#21487;&#20449;&#24230;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#36991;&#20813;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LE-PDE-UQ&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08383</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#20840;&#23616;&#28436;&#21270;&#23454;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#36870;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LE-PDE-UQ&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#22312;&#36895;&#24230;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24448;&#24448;&#33021;&#22815;&#23454;&#29616;10&#21040;1000&#20493;&#30340;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21363;&#32570;&#20047;&#23545;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#20851;&#38190;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28508;&#22312;&#28436;&#21270;&#65288;LE-PDE-UQ&#65289;&#65292;&#20026;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#36171;&#20104;&#20102;&#28145;&#24230;&#23398;&#20064;&#26367;&#20195;&#27169;&#22411;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;LE-PDE-UQ&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#31639;&#27861;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#20808;&#21069;&#20449;&#24565;&#30456;&#20914;&#31361;&#65292;&#35299;&#37322;&#20250;&#23558;&#27880;&#24847;&#21147;&#23548;&#21521;&#20914;&#31361;&#29305;&#24449;&#24182;&#36828;&#31163;&#20854;&#20182;&#30456;&#20851;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22240;&#26524;&#36807;&#24230;&#24402;&#22240;&#24182;&#23545;&#20154;&#31867;&#30340;&#20449;&#24687;&#22788;&#29702;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08379</link><description>&lt;p&gt;
&#34920;&#31034;&#21644;&#35299;&#37322;&#30340;&#20108;&#37325;&#22863;&#21450;&#20854;&#21152;&#21095;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Duet of Representations and How Explanations Exacerbate It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#31639;&#27861;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#20808;&#21069;&#20449;&#24565;&#30456;&#20914;&#31361;&#65292;&#35299;&#37322;&#20250;&#23558;&#27880;&#24847;&#21147;&#23548;&#21521;&#20914;&#31361;&#29305;&#24449;&#24182;&#36828;&#31163;&#20854;&#20182;&#30456;&#20851;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22240;&#26524;&#36807;&#24230;&#24402;&#22240;&#24182;&#23545;&#20154;&#31867;&#30340;&#20449;&#24687;&#22788;&#29702;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#33021;&#22815;&#23545;&#20154;&#31867;&#30340;&#24863;&#30693;&#20013;&#29305;&#24449;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#22240;&#26524;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#20808;&#21069;&#20449;&#24565;&#30456;&#20914;&#31361;&#12290;&#35299;&#37322;&#21487;&#20197;&#23558;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#21040;&#20914;&#31361;&#30340;&#29305;&#24449;&#19978;&#65292;&#24182;&#20351;&#20854;&#36828;&#31163;&#20854;&#20182;&#30456;&#20851;&#29305;&#24449;&#12290;&#36825;&#23548;&#33268;&#22240;&#26524;&#36807;&#24230;&#24402;&#22240;&#65292;&#24182;&#21487;&#33021;&#23545;&#20154;&#31867;&#30340;&#20449;&#24687;&#22788;&#29702;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#19968;&#39033;&#23454;&#22320;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#32463;&#36807;XGBoost&#35757;&#32451;&#30340;&#27169;&#22411;&#20316;&#20026;&#20915;&#31574;&#36741;&#21161;&#24037;&#20855;&#24212;&#29992;&#20110;&#20844;&#20849;&#23601;&#19994;&#26381;&#21153;&#20013;&#24515;&#30340;&#39038;&#38382;&#65292;&#29992;&#20110;&#39044;&#27979;&#20505;&#36873;&#20154;&#38271;&#26399;&#22833;&#19994;&#30340;&#39118;&#38505;&#12290;&#27835;&#30103;&#32452;&#30340;&#39038;&#38382;&#36824;&#25552;&#20379;&#20102;SHAP&#20316;&#20026;&#35299;&#37322;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20914;&#31361;&#29305;&#24449;&#20316;&#20026;&#35299;&#37322;&#30340;&#19968;&#37096;&#20998;&#26174;&#31034;&#26102;&#65292;&#20154;&#31867;&#30340;&#20915;&#31574;&#36136;&#37327;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm effects a causal representation of relations between features and labels in the human's perception. Such a representation might conflict with the human's prior belief. Explanations can direct the human's attention to the conflicting feature and away from other relevant features. This leads to causal overattribution and may adversely affect the human's information processing. In a field experiment we implemented an XGBoost-trained model as a decision-making aid for counselors at a public employment service to predict candidates' risk of long-term unemployment. The treatment group of counselors was also provided with SHAP. The results show that the quality of the human's decision-making is worse when a feature on which the human holds a conflicting prior belief is displayed as part of the explanation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#29992;&#20110;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08373</link><description>&lt;p&gt;
&#21160;&#24577;&#31574;&#30053;&#19979;&#22810;&#27493;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#29992;&#20110;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22810;&#27493;&#39044;&#27979;&#65288;MSF&#65289;&#26159;&#20960;&#20046;&#25152;&#26377;&#26102;&#38388;&#39046;&#22495;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#31181;&#39044;&#27979;&#65292;&#24517;&#39035;&#20551;&#35774;&#26102;&#38388;&#21160;&#24577;&#30340;&#36882;&#24402;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#20551;&#35774;&#34987;&#31216;&#20026;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;&#26410;&#35265;&#25968;&#25454;&#20043;&#21069;&#65292;&#19981;&#28165;&#26970;&#21738;&#31181;&#39044;&#27979;&#31574;&#30053;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#65288;&#22266;&#23450;&#65289;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20248;&#39044;&#27979;&#31574;&#30053;&#30340;&#23454;&#20363;&#32423;&#21035;&#21464;&#21270;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;MSF&#30340;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#35268;&#27169;&#12289;&#39046;&#22495;&#21644;&#22810;&#27493;&#39044;&#27979;&#38271;&#24230;&#30340;10&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#20998;&#31867;&#22120;&#26102;&#65292;DyStrat&#22312;94%&#30340;&#26102;&#38388;&#20869;&#20248;&#20110;&#26368;&#20339;&#22266;&#23450;&#31574;&#30053;&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains. To make such forecasts, one must assume the recursive complexity of the temporal dynamics. Such assumptions are referred to as the forecasting strategy used to train a predictive model. Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data. Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons. When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%. Our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21327;&#21516;&#36807;&#28388;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#65292;&#21033;&#29992;&#22810;&#20010;&#20934;&#21017;&#20026;&#22823;&#23398;&#29983;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#26368;&#20339;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#31185;&#23572;&#22810;&#29926;&#22823;&#23398;&#30340;&#30495;&#23454;&#20449;&#24687;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.08371</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#22810;&#26631;&#20934;&#25512;&#33616;&#31995;&#32479;&#21644;&#36951;&#20256;&#20248;&#21270;&#24110;&#21161;&#22823;&#23398;&#29983;&#36873;&#25321;&#36873;&#20462;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21327;&#21516;&#36807;&#28388;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#65292;&#21033;&#29992;&#22810;&#20010;&#20934;&#21017;&#20026;&#22823;&#23398;&#29983;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#36873;&#20462;&#35838;&#31243;&#12290;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#26368;&#20339;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#31185;&#23572;&#22810;&#29926;&#22823;&#23398;&#30340;&#30495;&#23454;&#20449;&#24687;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#35838;&#31243;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#23398;&#26415;&#35745;&#21010;&#30340;&#28789;&#27963;&#24615;&#25581;&#31034;&#20102;&#25512;&#33616;&#31995;&#32479;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#31995;&#32479;&#26159;&#24110;&#21161;&#23398;&#29983;&#36873;&#25321;&#31526;&#21512;&#20010;&#20154;&#20852;&#36259;&#21644;&#23398;&#26415;&#34920;&#29616;&#30340;&#35838;&#31243;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25512;&#33616;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#21327;&#21516;&#36807;&#28388;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#65292;&#21033;&#29992;&#19982;&#23398;&#29983;&#21644;&#35838;&#31243;&#20449;&#24687;&#30456;&#20851;&#30340;&#22810;&#20010;&#20934;&#21017;&#26469;&#20026;&#23398;&#29983;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#35838;&#31243;&#12290;&#20026;&#20102;&#33258;&#21160;&#21457;&#29616;&#26368;&#20339;&#30340;&#25512;&#33616;&#31995;&#32479;&#37197;&#32622;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36951;&#20256;&#31639;&#27861;&#26469;&#30830;&#23450;&#21253;&#25324;&#26368;&#30456;&#20851;&#30340;&#20934;&#21017;&#21644;&#20854;&#20182;&#21442;&#25968;&#37197;&#32622;&#30340;&#26368;&#20248;&#37197;&#32622;&#12290;&#23454;&#39564;&#30740;&#31350;&#20351;&#29992;&#20102;&#31185;&#23572;&#22810;&#29926;&#22823;&#23398;&#65288;&#35199;&#29677;&#29273;&#65289;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#20301;&#30340;&#30495;&#23454;&#20449;&#24687;&#65292;&#21253;&#25324;&#19977;&#20010;&#23398;&#24180;&#26399;&#38388;&#20174;95&#21517;&#23398;&#29983;&#21644;63&#38376;&#35838;&#31243;&#30340;2500&#20010;&#26465;&#30446;&#20013;&#25910;&#38598;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of Recommendation Systems (RSs) in this area. These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance. This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students. A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters. The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses. Experimental r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;RBF-PINN&#26041;&#27861;&#65292;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;Fourier&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08367</link><description>&lt;p&gt;
RBF-PINN&#65306;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;RBF-PINN&#26041;&#27861;&#65292;&#22312;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#38750;Fourier&#20301;&#32622;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;Fourier&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#35768;&#22810;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#21464;&#20307;&#22312;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#20248;&#21183;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;Fourier&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#26465;&#20214;&#27491;&#23450;&#24615;&#36136;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#12290;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#22522;&#20110;&#22352;&#26631;&#30340;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#20026;PINNs&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many recent Physics-Informed Neural Networks (PINNs) variants have had considerable success in solving Partial Differential Equations, the empirical benefits of feature mapping drawn from the broader Neural Representations research have been largely overlooked. We highlight the limitations of widely used Fourier-based feature mapping in certain situations and suggest the use of the conditionally positive definite Radial Basis Function. The empirical findings demonstrate the effectiveness of our approach across a variety of forward and inverse problem cases. Our method can be seamlessly integrated into coordinate-based input neural networks and contribute to the wider field of PINNs research.
&lt;/p&gt;</description></item><item><title>NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08365</link><description>&lt;p&gt;
NeuRes: &#23398;&#20064;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
NeuRes: Learning Proofs of Propositional Satisfiability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08365
&lt;/p&gt;
&lt;p&gt;
NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;NeuRes&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;SAT&#35299;&#31639;&#27861;&#19981;&#21516;&#65292;NeuRes&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#23427;&#12290;NeuRes&#36890;&#36807;&#37319;&#29992;&#21629;&#39064;&#25512;&#29702;&#26469;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#22312;&#19981;&#21487;&#28385;&#36275;&#21644;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25214;&#21040;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#32593;&#32476;&#30340;&#20803;&#32032;&#65292;&#20174;&#21160;&#24577;&#22270;&#32467;&#26500;&#20013;&#33258;&#21160;&#36873;&#25321;&#33410;&#28857;&#23545;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#35299;&#26512;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;NeuroSAT&#30456;&#21516;&#30340;&#38543;&#26426;&#20844;&#24335;&#20998;&#24067;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#25945;&#24072;&#35777;&#26126;&#21644;&#30495;&#20540;&#20998;&#37197;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuRes&#22312;&#19981;&#21516;&#20998;&#24067;&#19978;&#27604;NeuroSAT&#35299;&#20915;&#26356;&#22810;&#30340;&#27979;&#35797;&#20844;&#24335;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;Noisy-SGD&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#24615;&#32780;&#38750;&#21098;&#35009;&#26799;&#24230;&#26159;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;&#30340;&#21407;&#22240;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#24046;&#20250;&#34987;&#21152;&#21095;&#65292;&#36825;&#23545;&#20110;&#20351;&#29992;&#24040;&#22823;&#25209;&#37327;&#25968;&#25454;&#30340;&#24378;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08344</link><description>&lt;p&gt;
&#22312;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;Noisy-SGD&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;&#21450;&#20854;&#22312;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias in Noisy-SGD: With Applications to Differentially Private Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08344
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;Noisy-SGD&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#24615;&#32780;&#38750;&#21098;&#35009;&#26799;&#24230;&#26159;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;&#30340;&#21407;&#22240;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#24046;&#20250;&#34987;&#21152;&#21095;&#65292;&#36825;&#23545;&#20110;&#20351;&#29992;&#24040;&#22823;&#25209;&#37327;&#25968;&#25454;&#30340;&#24378;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#20197;&#23567;&#25209;&#37327;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30456;&#36739;&#20110;&#22823;&#25209;&#37327;&#35757;&#32451;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;SGD&#29305;&#23450;&#30340;&#22122;&#22768;&#32467;&#26500;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#36825;&#31181;&#38544;&#24335;&#20559;&#24046;&#30340;&#21407;&#22240;&#12290;&#29992;&#20110;&#30830;&#20445;DNN&#35757;&#32451;&#20013;&#30340;&#24046;&#24322;&#38544;&#31169;(DP)&#30340;DP-SGD&#20250;&#32473;&#21098;&#35009;&#26799;&#24230;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#25209;&#37327;&#35757;&#32451;&#20173;&#28982;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#24378;DP&#20445;&#35777;&#38656;&#35201;&#20351;&#29992;&#22823;&#25209;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#29616;&#35937;&#22312;Noisy-SGD&#65288;&#27809;&#26377;&#21098;&#35009;&#30340;DP-SGD&#65289;&#20013;&#30340;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#38543;&#26426;&#24615;&#65288;&#32780;&#19981;&#26159;&#21098;&#35009;&#65289;&#26159;&#36825;&#31181;&#38544;&#24335;&#20559;&#24046;&#30340;&#21407;&#22240;&#65292;&#21363;&#20351;&#21152;&#20837;&#20102;&#39069;&#22806;&#30340;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#21644;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#35774;&#32622;&#19978;&#23545;&#20351;&#29992;&#36830;&#32493;&#29256;&#26412;&#30340;Noisy-SGD&#33719;&#24471;&#30340;&#35299;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#24335;&#20559;&#24046;&#30830;&#23454;&#34987;&#21152;&#21095;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training Deep Neural Networks (DNNs) with small batches using Stochastic Gradient Descent (SGD) yields superior test performance compared to larger batches. The specific noise structure inherent to SGD is known to be responsible for this implicit bias. DP-SGD, used to ensure differential privacy (DP) in DNNs' training, adds Gaussian noise to the clipped gradients. Surprisingly, large-batch training still results in a significant decrease in performance, which poses an important challenge because strong DP guarantees necessitate the use of massive batches. We first show that the phenomenon extends to Noisy-SGD (DP-SGD without clipping), suggesting that the stochasticity (and not the clipping) is the cause of this implicit bias, even with additional isotropic Gaussian noise. We theoretically analyse the solutions obtained with continuous versions of Noisy-SGD for the Linear Least Square and Diagonal Linear Network settings, and reveal that the implicit bias is indeed amplified by the add
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#36755;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08324</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36890;&#36807;&#31283;&#23450;&#20998;&#24067;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Stable Distribution Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08324
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#36755;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#22312;ReLU&#38750;&#32447;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#26159;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26368;&#20248;&#36817;&#20284;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#39640;&#26031;&#21644;&#26607;&#35199;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#20854;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#20256;&#25773;&#20998;&#24067;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#20256;&#25773;&#20998;&#24067;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35832;&#22914;&#30697;&#21305;&#37197;&#31561;&#20854;&#20182;&#26041;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#30417;&#27979;&#38382;&#39064;&#20013;&#25506;&#32034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08321</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#20248;&#21270;&#25506;&#32034;&#65306;&#22312;&#37096;&#20998;&#30417;&#27979;&#20013;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08321
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#30417;&#27979;&#38382;&#39064;&#20013;&#25506;&#32034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#30417;&#27979;&#26159;&#19968;&#31181;&#20855;&#26377;&#26377;&#38480;&#35266;&#27979;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#26377;&#38480;&#35266;&#27979;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#36866;&#24403;&#30340;&#25506;&#32034;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20248;&#21270;&#36827;&#34892;&#25506;&#32034;&#65288;ExO&#65289;&#65292;&#23427;&#21033;&#29992;&#36861;&#36394;&#27491;&#21017;&#21270;&#26368;&#20248;&#26041;&#27861;&#65292;&#22312;&#24191;&#27867;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#23545;&#25239;&#29615;&#22659;&#19979;&#30340;&#26368;&#20248;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#32431;&#31929;&#24212;&#29992;ExO&#20250;&#26174;&#33879;&#38477;&#20302;&#36951;&#25022;&#30028;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#37096;&#21487;&#35266;&#27979;&#28216;&#25103;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ExO&#19982;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#26694;&#26550;&#21644;&#20998;&#26512;&#12290;&#36825;&#20010;&#21457;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#26368;&#20339;&#21452;&#36194;&#31639;&#27861;&#65288;BOBW&#65289;&#30340;&#29616;&#26377;&#36951;&#25022;&#30028;&#38480;&#65292;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#37117;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#36951;&#25022;&#30028;&#38480;&#20026;$O(\sum_{a \neq a^*} k^2 m^2$
&lt;/p&gt;
&lt;p&gt;
Partial monitoring is a generic framework of online decision-making problems with limited observations. To make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration. Recently, a powerful approach for this purpose, exploration by optimization (ExO), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems. However, a naive application of ExO in stochastic environments significantly degrades regret bounds. To resolve this problem in locally observable games, we first establish a novel framework and analysis for ExO with a hybrid regularizer. This development allows us to significantly improve the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments. In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2 m^2 \
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36817;&#20284;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#38024;&#23545;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;&#34892;&#27874;&#35299;&#65292;&#24341;&#20837;&#20102;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#26469;&#25552;&#39640;&#35299;&#30340;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08313</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;Fisher&#26041;&#31243;&#30340;&#35299;&#30340;&#26063;&#32676;
&lt;/p&gt;
&lt;p&gt;
Approximating Families of Sharp Solutions to Fisher's Equation with Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36817;&#20284;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#38024;&#23545;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;&#34892;&#27874;&#35299;&#65292;&#24341;&#20837;&#20102;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#26469;&#25552;&#39640;&#35299;&#30340;&#31934;&#24230;&#65292;&#24182;&#36890;&#36807;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#27714;&#35299;Fisher&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#21453;&#24212;&#25193;&#25955;&#31995;&#32479;&#30340;&#22522;&#26412;&#34920;&#31034;&#12290;&#37325;&#28857;&#26159;&#30740;&#31350;&#22312;&#22823;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#26465;&#20214;&#19979;&#30340;Fisher&#26041;&#31243;&#65292;&#20854;&#20013;&#35299;&#21576;&#29616;&#20026;&#34892;&#27874;&#65292;&#30001;&#20110;&#27874;&#21069;&#30340;&#38497;&#23789;&#24615;&#65292;&#23545;&#25968;&#20540;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#20934;PINN&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27531;&#24046;&#21152;&#26435;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#26088;&#22312;&#36890;&#36807;&#32771;&#34385;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#20013;&#30340;&#21453;&#24212;&#39033;&#26469;&#22686;&#24378;&#34892;&#27874;&#21069;&#30340;&#36319;&#36394;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19987;&#38376;&#38024;&#23545;&#34892;&#27874;&#24418;&#24335;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23558;&#21453;&#24212;&#36895;&#29575;&#31995;&#25968;&#20316;&#20026;&#32593;&#32476;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#35780;&#20272;&#20102;PINN&#36817;&#20284;&#25972;&#20010;&#35299;&#26063;&#32676;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper employs physics-informed neural networks (PINNs) to solve Fisher's equation, a fundamental representation of a reaction-diffusion system with both simplicity and significance. The focus lies specifically in investigating Fisher's equation under conditions of large reaction rate coefficients, wherein solutions manifest as traveling waves, posing a challenge for numerical methods due to the occurring steepness of the wave front. To address optimization challenges associated with the standard PINN approach, a residual weighting scheme is introduced. This scheme is designed to enhance the tracking of propagating wave fronts by considering the reaction term in the reaction-diffusion equation. Furthermore, a specific network architecture is studied which is tailored for solutions in the form of traveling waves. Lastly, the capacity of PINNs to approximate an entire family of solutions is assessed by incorporating the reaction rate coefficient as an additional input to the network 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08309</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompted Contextual Vectors for Spear-Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#30005;&#23376;&#37038;&#20214;&#24182;&#26041;&#20415;&#30446;&#26631;&#20390;&#23519;&#26469;&#21319;&#32423;&#20102;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#38598;&#21512;&#26469;&#21019;&#24314;&#34920;&#31034;&#21521;&#37327;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#26469;&#25512;&#29702;&#21644;&#22238;&#31572;&#20154;&#24037;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37327;&#21270;&#30005;&#23376;&#37038;&#20214;&#20869;&#23481;&#20013;&#24120;&#35265;&#35828;&#26381;&#21407;&#21017;&#30340;&#23384;&#22312;&#65292;&#20026;&#19979;&#28216;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#25991;&#26723;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#26377;&#31995;&#32479;&#29983;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#30446;&#26631;&#20390;&#23519;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#21253;&#21547;&#20256;&#32479;&#38035;&#40060;&#21644;&#33391;&#24615;&#30005;&#23376;&#37038;&#20214;&#30340;&#35757;&#32451;&#38598;&#20013;&#23454;&#29616;&#20102;91%&#30340;F1&#24471;&#20998;&#65292;&#20854;&#20013;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23618;GNN&#39044;&#22788;&#29702;&#22120;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#35813;&#39044;&#22788;&#29702;&#22120;&#38598;&#25104;&#20102;GNN&#27169;&#22411;&#21644;&#22810;&#23618;&#22495;&#20998;&#35299;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22686;&#24378;Krylov&#26041;&#27861;&#30340;&#25928;&#29575;&#20197;&#19981;&#21516;&#31934;&#24230;&#27700;&#24179;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.08296</link><description>&lt;p&gt;
&#22810;&#23618;GNN&#39044;&#22788;&#29702;&#22120;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Level GNN Preconditioner for Solving Large Scale Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23618;GNN&#39044;&#22788;&#29702;&#22120;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#35813;&#39044;&#22788;&#29702;&#22120;&#38598;&#25104;&#20102;GNN&#27169;&#22411;&#21644;&#22810;&#23618;&#22495;&#20998;&#35299;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22686;&#24378;Krylov&#26041;&#27861;&#30340;&#25928;&#29575;&#20197;&#19981;&#21516;&#31934;&#24230;&#27700;&#24179;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#20540;&#27169;&#25311;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#33392;&#38590;&#30340;&#35745;&#31639;&#12290;&#39640;&#24615;&#33021;&#35745;&#31639;&#24050;&#32463;&#25913;&#36827;&#20102;&#36825;&#20010;&#36807;&#31243;&#65292;&#20294;&#26159;&#23558;&#20256;&#32479;&#20195;&#30721;&#36866;&#24212;&#24182;&#34892;GPU&#35745;&#31639;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;GPU&#35745;&#31639;&#65292;&#20294;&#24448;&#24448;&#22312;&#27867;&#21270;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#29305;&#21035;&#36866;&#29992;&#20110;&#23398;&#20064;&#20174;&#32593;&#26684;&#31561;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#65292;&#20294;&#24448;&#24448;&#21463;&#38480;&#20110;&#23567;&#35268;&#27169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25152;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#21516;&#26102;&#20174;&#20004;&#20010;&#39046;&#22495;&#21463;&#30410;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#39044;&#22788;&#29702;&#22120;&#65292;&#23558;GNN&#27169;&#22411;&#19982;&#22810;&#23618;&#22495;&#20998;&#35299;&#26694;&#26550;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;GNN&#30340;&#39044;&#22788;&#29702;&#22120;&#29992;&#20110;&#22686;&#24378;Krylov&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#28151;&#21512;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#30340;&#20934;&#30830;&#24615;&#25910;&#25947;&#12290;Krylov&#26041;&#27861;&#30340;&#25928;&#29575;&#20174;GNN&#39044;&#22788;&#29702;&#22120;&#20013;&#21463;&#30410;&#33391;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale numerical simulations often come at the expense of daunting computations. High-Performance Computing has enhanced the process, but adapting legacy codes to leverage parallel GPU computations remains challenging. Meanwhile, Machine Learning models can harness GPU computations effectively but often struggle with generalization and accuracy. Graph Neural Networks (GNNs), in particular, are great for learning from unstructured data like meshes but are often limited to small-scale problems. Moreover, the capabilities of the trained model usually restrict the accuracy of the data-driven solution. To benefit from both worlds, this paper introduces a novel preconditioner integrating a GNN model within a multi-level Domain Decomposition framework. The proposed GNN-based preconditioner is used to enhance the efficiency of a Krylov method, resulting in a hybrid solver that can converge with any desired level of accuracy. The efficiency of the Krylov method greatly benefits from the GN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08280</link><description>&lt;p&gt;
Pix2Code&#65306;&#23398;&#20064;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Pix2Code: Learning to Compose Neural Visual Concepts as Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08280
&lt;/p&gt;
&lt;p&gt;
Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20174;&#22270;&#20687;&#20013;&#25277;&#35937;&#27010;&#24565;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#23558;&#35270;&#35273;&#24863;&#30693;&#21644;&#36890;&#29992;&#20851;&#31995;&#25512;&#29702;&#36827;&#34892;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#20351;&#24471;&#20154;&#31867;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#33021;&#20462;&#27491;&#38169;&#35823;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Code&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#31243;&#24207;&#21512;&#25104;&#25193;&#23637;&#21040;&#35270;&#35273;&#20851;&#31995;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#26126;&#30830;&#30340;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#35780;&#20272;&#20102;Pix2Code&#30340;&#22810;&#26679;&#29305;&#24615;&#65292;&#20174;&#32780;&#27979;&#35797;&#20854;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#20960;&#20309;&#24341;&#23548;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.08269</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20960;&#20309;&#24341;&#23548;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Geometry-induced Implicit Regularization in Deep ReLU Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#20960;&#20309;&#24341;&#23548;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#27604;&#35757;&#32451;&#26679;&#26412;&#26356;&#22810;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#20250;&#36807;&#25311;&#21512;&#12290;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#65292;&#23545;&#8220;&#22909;&#8221;&#30340;&#32593;&#32476;&#26377;&#21033;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#32593;&#32476;&#65292;&#32780;&#21482;&#32771;&#34385;&#8220;&#22909;&#8221;&#30340;&#32593;&#32476;&#65292;&#21442;&#25968;&#25968;&#37327;&#23601;&#19981;&#26159;&#19968;&#20010;&#36275;&#22815;&#34913;&#37327;&#22797;&#26434;&#24615;&#30340;&#25351;&#26631;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21738;&#20123;&#32593;&#32476;&#21463;&#21040;&#38738;&#30544;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#25968;&#21464;&#21270;&#26102;&#36755;&#20986;&#38598;&#21512;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#22266;&#23450;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38598;&#21512;&#30340;&#32500;&#24230;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#23616;&#37096;&#32500;&#24230;&#65292;&#21363;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#65292;&#20960;&#20046;&#24635;&#26159;&#30001;&#38544;&#34255;&#23618;&#20013;&#30340;&#28608;&#27963;&#27169;&#24335;&#20915;&#23450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#23545;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#23545;&#31216;&#24615;&#65288;&#31070;&#32463;&#20803;&#25490;&#21015;&#21644;&#27491;&#21521;&#32553;&#25918;&#65289;&#26159;&#19981;&#21464;&#30340;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#23454;&#20102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#25209;&#27425;&#21151;&#33021;&#32500;&#24230;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#36807;&#31243;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that neural networks with many more parameters than training examples do not overfit. Implicit regularization phenomena, which are still not well understood, occur during optimization and 'good' networks are favored. Thus the number of parameters is not an adequate measure of complexity if we do not consider all possible networks but only the 'good' ones. To better understand which networks are favored during optimization, we study the geometry of the output set as parameters vary. When the inputs are fixed, we prove that the dimension of this set changes and that the local dimension, called batch functional dimension, is almost surely determined by the activation patterns in the hidden layers. We prove that the batch functional dimension is invariant to the symmetries of the network parameterization: neuron permutations and positive rescalings. Empirically, we establish that the batch functional dimension decreases during optimization. As a consequence, optimization l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08268</link><description>&lt;p&gt;
&#30334;&#19975;&#38271;&#24230;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Model on Million-Length Video And Language With RingAttention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#38590;&#20197;&#29992;&#25991;&#23383;&#25551;&#36848;&#30340;&#19990;&#30028;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#31687;&#20219;&#21153;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#35270;&#39057;&#24207;&#21015;&#25552;&#20379;&#20102;&#21482;&#26377;&#35821;&#35328;&#21644;&#38745;&#24577;&#22270;&#20687;&#25152;&#19981;&#20855;&#22791;&#30340;&#23453;&#36149;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#19982;&#35821;&#35328;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#26102;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#23545;&#20154;&#31867;&#30340;&#25991;&#26412;&#30693;&#35782;&#21644;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#29702;&#35299;&#65292;&#20026;&#36741;&#21161;&#20154;&#31867;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#38754;&#20020;&#30528;&#35760;&#24518;&#32422;&#26463;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#26679;&#21270;&#35270;&#39057;&#21644;&#20070;&#31821;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29615;&#24418;&#27880;&#24847;&#21147;&#25216;&#26415;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#65292;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#20174;4K&#21040;1M&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;
&lt;/p&gt;
&lt;p&gt;
Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#26497;&#38480;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#30340;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#29992;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08255</link><description>&lt;p&gt;
Distal Interference: &#25506;&#32034;&#22522;&#20110;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Distal Interference: Exploring the Limits of Model-Based Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#26497;&#38480;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#30340;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#29992;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25353;&#39034;&#24207;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#25345;&#32493;&#23398;&#20064;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#24178;&#25200;&#25110;&#36951;&#24536;&#30340;&#38459;&#30861;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#26799;&#24230;&#19979;&#38477;&#21644;&#36828;&#36317;&#31163;&#36755;&#20837;&#28857;&#20043;&#38388;&#37325;&#21472;&#34920;&#31034;&#22914;&#20309;&#23548;&#33268;&#36828;&#36317;&#31163;&#24178;&#25200;&#21644;&#28798;&#38590;&#24615;&#24178;&#25200;&#12290;&#36828;&#36317;&#31163;&#24178;&#25200;&#26159;&#25351;&#22312;&#23545;&#22495;&#30340;&#23376;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26102;&#65292;&#23545;&#22495;&#30340;&#20854;&#20182;&#23376;&#38598;&#36896;&#25104;&#38750;&#23616;&#37096;&#21464;&#21270;&#30340;&#29616;&#35937;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#22343;&#21248;&#21487;&#35757;&#32451;&#27169;&#22411;&#24517;&#39035;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#35268;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ABEL-Spline&#30340;&#26032;&#22411;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20855;&#26377;&#22343;&#21248;&#21487;&#35757;&#32451;&#24615;&#12289;&#22810;&#39033;&#24335;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#25910;&#38598;&#30340;&#28909;&#20687;&#20013;&#35782;&#21035;&#23567;&#22411;&#21644;&#24494;&#23567;&#29289;&#20307;&#12290;&#36890;&#36807;&#20351;&#29992;YOLOv5&#32467;&#26500;&#30340;&#39592;&#24178;&#32593;&#32476;&#12289;Transformer&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#31383;&#21475;&#65292;&#20197;&#21450;Sigmoid&#20989;&#25968;&#36827;&#34892;&#26816;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#20934;&#30830;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08251</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#28909;&#20687;&#20013;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#25910;&#38598;&#30340;&#28909;&#20687;&#20013;&#35782;&#21035;&#23567;&#22411;&#21644;&#24494;&#23567;&#29289;&#20307;&#12290;&#36890;&#36807;&#20351;&#29992;YOLOv5&#32467;&#26500;&#30340;&#39592;&#24178;&#32593;&#32476;&#12289;Transformer&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#31383;&#21475;&#65292;&#20197;&#21450;Sigmoid&#20989;&#25968;&#36827;&#34892;&#26816;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#20934;&#30830;&#24615;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#26080;&#20154;&#26426;&#25910;&#38598;&#30340;&#28909;&#20687;&#20013;&#30340;&#23567;&#22411;&#21644;&#24494;&#23567;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#39592;&#24178;&#32593;&#32476;&#65292;&#33046;&#23376;&#21644;&#39044;&#27979;&#22836;&#12290;&#39592;&#24178;&#32593;&#32476;&#22522;&#20110;YOLOv5&#30340;&#32467;&#26500;&#65292;&#24182;&#22312;&#26411;&#23614;&#20351;&#29992;&#20102;&#19968;&#20010;Transformer&#32534;&#30721;&#22120;&#12290;&#33046;&#23376;&#21253;&#25324;&#19968;&#20010;BI-FPN&#22359;&#65292;&#32467;&#21512;&#28369;&#21160;&#31383;&#21475;&#21644;Transformer&#65292;&#22686;&#21152;&#20102;&#36755;&#20837;&#21040;&#39044;&#27979;&#22836;&#30340;&#20449;&#24687;&#12290;&#39044;&#27979;&#22836;&#36890;&#36807;&#35780;&#20272;&#29305;&#24449;&#22270;&#20351;&#29992;Sigmoid&#20989;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;Transformer&#19982;&#27880;&#24847;&#21147;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#20351;&#29992;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#27169;&#22411;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#20445;&#25345;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;VEDAI&#21644;&#25105;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;ResNet&#12289;Faster RCNN&#12289;ComNet&#12289;ViT&#12289;YOLOv5&#12289;SMPNe&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNe
&lt;/p&gt;</description></item><item><title>APALU &#26159;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290; &#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;APALU&#30456;&#23545;&#20110;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08244</link><description>&lt;p&gt;
APALU: &#19968;&#31181;&#21487;&#35757;&#32451;&#12289;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08244
&lt;/p&gt;
&lt;p&gt;
APALU &#26159;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290; &#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;APALU&#30456;&#23545;&#20110;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26377;&#21161;&#20110;&#25552;&#21462;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#34429;&#28982;&#31867;&#20284;ReLU&#21450;&#20854;&#21464;&#31181;&#30340;&#32463;&#20856;&#28608;&#27963;&#20989;&#25968;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#38745;&#24577;&#29305;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#23613;&#31649;&#26377;&#21033;&#65292;&#20294;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#26377;&#26102;&#20063;&#38590;&#20197;&#36866;&#24212;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35757;&#32451;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#33258;&#36866;&#24212;&#20998;&#27573;&#36924;&#36817;&#28608;&#27963;&#32447;&#24615;&#21333;&#20803;&#65288;APALU&#65289;&#65292;&#20197;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#19968;&#22871;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#65292;&#24182;&#36866;&#24212;&#22797;&#26434;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;APALU&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;APALU&#25552;&#21319;&#20102;MobileNet&#21644;GoogleNet&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns. While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks. The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data. Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks. It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations. Experiments reveal significant improvements over widely used activation functions for different tasks. In image classification, APALU increases MobileNet and GoogleNet accur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#19982;&#20844;&#24179;&#33021;&#21147;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08242</link><description>&lt;p&gt;
&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#30740;&#21457;&#20013;&#30340;&#20844;&#24179;&#25935;&#25463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Equitable Agile Research and Development of AI and Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08242
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#19982;&#20844;&#24179;&#33021;&#21147;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#22797;&#21046;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#20559;&#35265;&#21644;&#25104;&#35265;&#65292;AI&#26426;&#22120;&#20154;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#20855;&#22791;&#38754;&#37096;&#35782;&#21035;&#21151;&#33021;&#30340;&#26426;&#22120;&#20154;&#26410;&#33021;&#23558;&#40657;&#20154;&#22899;&#24615;&#35782;&#21035;&#20026;&#20154;&#31867;&#65292;&#32780;&#20854;&#20182;&#26426;&#22120;&#20154;&#21017;&#20165;&#26681;&#25454;&#22806;&#34920;&#23558;&#20154;&#20204;&#65292;&#22914;&#40657;&#20154;&#30007;&#24615;&#65292;&#24402;&#31867;&#20026;&#32618;&#29359;&#12290;&#22312;&#8220;AI&#20379;&#24212;&#38142;&#8221;&#20013;&#65292;&#19968;&#31181;&#8220;&#27169;&#22359;&#21270;&#25991;&#21270;&#8221;&#24847;&#21619;&#30528;&#20260;&#23475;&#34987;&#35748;&#20026;&#26159;&#8220;&#36229;&#20986;&#33539;&#22260;&#8221;&#30340;&#65292;&#25110;&#32773;&#26159;&#21035;&#20154;&#30340;&#36131;&#20219;&#12290;&#20107;&#20214;&#30340;&#21457;&#29983;&#26159; routine enough&#65288;incidentdatabase.ai &#21015;&#20030;&#20102;2000&#22810;&#20010;&#20363;&#23376;&#65289;&#20197;&#34920;&#26126;&#24456;&#23569;&#26377;&#32452;&#32455;&#33021;&#22815;&#23436;&#20840;&#23562;&#37325;&#20154;&#20204;&#30340;&#26435;&#21033;&#65307;&#23454;&#29616;&#25152;&#22768;&#31216;&#30340;&#20844;&#24179;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21253;&#23481;&#24615;&#65288;EDI&#25110;DEI&#65289;&#30446;&#26631;&#65307;&#25110;&#32773;&#35782;&#21035;&#28982;&#21518;&#35299;&#20915;&#36825;&#20123;&#32452;&#32455;&#21644;&#24037;&#20214;&#20013;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#24191;&#27867;&#23454;&#36341;&#30340;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#65292;&#20197;&#24314;&#31435;&#32452;&#32455;&#30340;&#20844;&#24179;&#33021;&#21147;&#24182;&#26356;&#22909;&#22320;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&amp;D) project management methodologies to build organizational equity capabilities and better integr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32479;&#35745;&#22871;&#21033;&#20013;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#31471;&#21040;&#31471;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#23884;&#20837;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#21040;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#65292;&#30452;&#25509;&#36755;&#20986;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#39118;&#38505;&#35843;&#25972;&#22238;&#25253;&#30340;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.08233</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#35745;&#22871;&#21033;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#31471;&#21040;&#31471;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08233
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32479;&#35745;&#22871;&#21033;&#20013;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#36827;&#34892;&#31471;&#21040;&#31471;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#23884;&#20837;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#21040;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#65292;&#30452;&#25509;&#36755;&#20986;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#65292;&#24182;&#36890;&#36807;&#39118;&#38505;&#35843;&#25972;&#22238;&#25253;&#30340;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#22871;&#21033;&#20013;&#65292;&#32463;&#20856;&#30340;&#22343;&#20540;&#22238;&#24402;&#20132;&#26131;&#31574;&#30053;&#36890;&#24120;&#20381;&#36182;&#20110;&#36164;&#20135;&#23450;&#20215;&#25110;&#22522;&#20110;PCA&#30340;&#27169;&#22411;&#26469;&#35782;&#21035;&#21512;&#25104;&#36164;&#20135;&#30340;&#22343;&#20540;&#12290;&#19968;&#26086;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#65288;&#32447;&#24615;&#65289;&#27169;&#22411;&#65292;&#23601;&#21487;&#20197;&#21046;&#23450;&#19968;&#20010;&#29420;&#31435;&#30340;&#22343;&#20540;&#22238;&#24402;&#31574;&#30053;&#26469;&#29983;&#25104;&#20132;&#26131;&#20449;&#21495;&#12290;&#20026;&#20102;&#25512;&#24191;&#36825;&#31181;&#26041;&#27861;&#24182;&#20351;&#20854;&#30495;&#27491;&#25968;&#25454;&#39537;&#21160;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32479;&#35745;&#22871;&#21033;&#20013;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#25928;&#29992;&#12290;&#20316;&#20026;&#31532;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#22312;&#32654;&#22269;&#32929;&#31080;&#22238;&#25253;&#19978;&#30340;&#26631;&#20934;&#33258;&#32534;&#30721;&#22120;&#26469;&#22522;&#20110;Ornstein-Uhlenbeck&#65288;OU&#65289;&#36807;&#31243;&#25512;&#23548;&#20132;&#26131;&#31574;&#30053;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23558;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#23884;&#20837;&#19968;&#20010;&#25237;&#36164;&#32452;&#21512;&#20132;&#26131;&#31574;&#30053;&#31354;&#38388;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#12290;&#36825;&#31181;&#38598;&#25104;&#30452;&#25509;&#36755;&#20986;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#31574;&#30053;&#30340;&#39118;&#38505;&#35843;&#25972;&#22238;&#25253;&#30340;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In Statistical Arbitrage (StatArb), classical mean reversion trading strategies typically hinge on asset-pricing or PCA based models to identify the mean of a synthetic asset. Once such a (linear) model is identified, a separate mean reversion strategy is then devised to generate a trading signal. With a view of generalising such an approach and turning it truly data-driven, we study the utility of Autoencoder architectures in StatArb. As a first approach, we employ a standard Autoencoder trained on US stock returns to derive trading strategies based on the Ornstein-Uhlenbeck (OU) process. To further enhance this model, we take a policy-learning approach and embed the Autoencoder network into a neural network representation of a space of portfolio trading policies. This integration outputs portfolio allocations directly and is end-to-end trainable by backpropagation of the risk-adjusted returns of the neural policy. Our findings demonstrate that this innovative end-to-end policy learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#30446;&#26631;&#24178;&#39044;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#24178;&#39044;&#27169;&#22411;&#26469;&#23613;&#37327;&#20943;&#23569;&#24178;&#39044;&#27425;&#25968;&#12290;&#36890;&#36807;&#39564;&#35777;&#21644;&#25628;&#32034;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22810;&#23545;&#25968;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08229</link><description>&lt;p&gt;
&#38750;&#30446;&#26631;&#24178;&#39044;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery under Off-Target Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#30446;&#26631;&#24178;&#39044;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#24178;&#39044;&#27169;&#22411;&#26469;&#23613;&#37327;&#20943;&#23569;&#24178;&#39044;&#27425;&#25968;&#12290;&#36890;&#36807;&#39564;&#35777;&#21644;&#25628;&#32034;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22810;&#23545;&#25968;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#22270;&#21457;&#29616;&#26159;&#19968;&#20010;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#35266;&#23519;&#25968;&#25454;&#65292;&#21482;&#33021;&#24674;&#22797;&#21040;&#20854;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#28508;&#22312;&#22240;&#26524;&#22270;&#65292;&#24182;&#19988;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#20551;&#35774;&#25110;&#24178;&#39044;&#26469;&#32553;&#23567;&#30495;&#23454;&#22270;&#30340;&#33539;&#22260;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#38543;&#26426;&#24178;&#39044;&#35774;&#32622;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23613;&#37327;&#20943;&#23569;&#24178;&#39044;&#27425;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#38543;&#26426;&#24178;&#39044;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#33258;&#36866;&#24212;&#26080;&#22122;&#22768;&#24178;&#39044;&#65292;&#24182;&#33021;&#25429;&#25417;&#21040;&#33026;&#32938;&#25163;&#24178;&#39044;&#21644;CRISPR&#22522;&#22240;&#25970;&#38500;&#31561;&#24773;&#20917;&#65306;&#20219;&#20309;&#24178;&#39044;&#23581;&#35797;&#37117;&#20250;&#23548;&#33268;&#23545;&#19968;&#20010;&#38543;&#26426;&#39030;&#28857;&#23376;&#38598;&#30340;&#23454;&#38469;&#24178;&#39044;&#65292;&#36825;&#20010;&#23376;&#38598;&#30340;&#36873;&#25321;&#26159;&#20381;&#36182;&#20110;&#23581;&#35797;&#30340;&#21160;&#20316;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#39564;&#35777;&#21644;&#25628;&#32034;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#22810;&#23545;&#25968;&#22797;&#26434;&#24230;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal graph discovery is a significant problem with applications across various disciplines. However, with observational data alone, the underlying causal graph can only be recovered up to its Markov equivalence class, and further assumptions or interventions are necessary to narrow down the true graph. This work addresses the causal discovery problem under the setting of stochastic interventions with the natural goal of minimizing the number of interventions performed. We propose the following stochastic intervention model which subsumes existing adaptive noiseless interventions in the literature while capturing scenarios such as fat-hand interventions and CRISPR gene knockouts: any intervention attempt results in an actual intervention on a random subset of vertices, drawn from a distribution dependent on attempted action. Under this model, we study the two fundamental problems in causal discovery of verification and search and provide approximation algorithms with polylogarithmic c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08228</link><description>&lt;p&gt;
&#30740;&#31350;GNN&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65306;&#20174;&#26550;&#26500;&#35282;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20110;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#23545;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#30340;&#25506;&#32034;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#20004;&#20010;&#8220;&#27169;&#22411;&#26080;&#20851;&#8221;&#35282;&#24230;&#19978;&#65306;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#30340;GNN&#27169;&#22411;&#26550;&#26500;&#23545;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#20114;&#29420;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;OOD&#25512;&#24191;&#65292;&#24182;&#23545;&#29616;&#20195;GNN&#30340;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08225</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#37325;&#20889;&#25552;&#39640;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Black-box Robustness with In-Context Rewriting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#36755;&#20837;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#30340;&#25216;&#26415;&#22312;&#27169;&#22411;&#26159;&#40657;&#30418;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#65292;&#20363;&#22914;&#26435;&#37325;&#34987;&#20923;&#32467;&#65292;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#65292;&#25110;&#32773;&#36890;&#36807;API&#20351;&#29992;&#27169;&#22411;&#12290;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20107;&#21518;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#36755;&#20837;&#30340;&#22810;&#20010;&#22686;&#24378;&#36827;&#34892;&#39044;&#27979;&#32858;&#21512;&#26469;&#32469;&#36807;&#40657;&#30418;&#32422;&#26463;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#30001;&#20110;&#29983;&#25104;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22686;&#24378;&#30340;&#25361;&#25112;&#65292;TTA&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-TTA&#65292;&#23427;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;TTA&#30340;&#22686;&#24378;&#20989;&#25968;&#12290;LLM-TTA&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;BERT&#30340;OOD&#40065;&#26834;&#24615;&#25552;&#39640;&#20102;&#24179;&#22343;4.30&#20010;&#30334;&#20998;&#28857;&#32780;&#19981;&#20250;&#20943;&#36864;&#24179;&#22343;ID pe&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID pe
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#31639;&#27861;&#30340;&#35745;&#31639;&#33021;&#21147;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#30284;&#30151;&#27835;&#30103;&#20013;&#30340;&#20851;&#38190;&#20998;&#23376;&#38774;&#28857;KRAS&#30340;&#26032;&#30340;&#25233;&#21046;&#21058;&#12290;&#22312;&#30740;&#31350;&#20013;&#21512;&#25104;&#20102;15&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#20013;&#20004;&#31181;&#20998;&#23376;&#30340;&#26377;&#25928;&#32467;&#21512;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08210</link><description>&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#22686;&#24378;&#30340;&#31639;&#27861;&#25581;&#31034;&#20102;KRAS&#30340;&#26032;&#25233;&#21046;&#21058;
&lt;/p&gt;
&lt;p&gt;
Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08210
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#37327;&#23376;&#31639;&#27861;&#30340;&#35745;&#31639;&#33021;&#21147;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#30284;&#30151;&#27835;&#30103;&#20013;&#30340;&#20851;&#38190;&#20998;&#23376;&#38774;&#28857;KRAS&#30340;&#26032;&#30340;&#25233;&#21046;&#21058;&#12290;&#22312;&#30740;&#31350;&#20013;&#21512;&#25104;&#20102;15&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#20013;&#20004;&#31181;&#20998;&#23376;&#30340;&#26377;&#25928;&#32467;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#20855;&#26377;&#27835;&#30103;&#28508;&#21147;&#30340;&#23567;&#20998;&#23376;&#19968;&#30452;&#26159;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#26032;&#39062;&#30340;&#35745;&#31639;&#25216;&#26415;&#26469;&#31616;&#21270;&#33647;&#29289;&#24320;&#21457;&#27969;&#31243;&#65292;&#25552;&#39640;&#33647;&#29289;&#30340;&#21457;&#29616;&#29575;&#65292;&#24182;&#38477;&#20302;&#23558;&#33647;&#29289;&#25512;&#21521;&#24066;&#22330;&#30340;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23558;16&#27604;&#29305;IBM&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#31639;&#27861;&#30340;&#35745;&#31639;&#33021;&#21147;&#19982;&#35774;&#35745;&#23567;&#20998;&#23376;&#30340;&#20256;&#32479;&#21487;&#38752;&#30340;&#32463;&#20856;&#26041;&#27861;&#26080;&#32541;&#38598;&#25104;&#30340;&#37327;&#23376;-&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#29983;&#25104;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;KRAS&#25233;&#21046;&#21058;&#65292;&#36825;&#26159;&#30284;&#30151;&#27835;&#30103;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38774;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;15&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#23376;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#19982;&#38774;&#28857;&#30340;&#32467;&#21512;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20505;&#36873;&#20998;&#23376;&#20013;&#65292;&#20004;&#31181;&#20855;&#26377;&#29420;&#29305;&#39592;&#26550;&#30340;&#20998;&#23376;&#65292;ISM061-018-2&#21644;ISM061-22&#65292;&#36890;&#36807;&#23637;&#31034;&#26377;&#25928;&#30340;&#19982;&#38774;&#28857;&#30340;&#32467;&#21512;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology. Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market. To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules. Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy. We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target. Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engageme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#28165;&#27927;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08209</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#38408;&#20540;&#25968;&#25454;Shapley&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#28165;&#27927;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28165;&#27927;&#26088;&#22312;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21024;&#38500;&#19968;&#32452;&#26377;&#23475;&#23454;&#20363;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25968;&#25454;Shapley&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#23454;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65307;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#25152;&#26377;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#22914;&#26524;&#36827;&#34892;&#36275;&#22815;&#22810;&#30340;&#36845;&#20195;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#36873;&#25321;&#26377;&#23475;&#23454;&#20363;&#12290;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm. We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#36793;&#30028;&#21644;&#26679;&#26412;&#36817;&#37051;&#20851;&#31995;&#30340;&#26032;&#22411;&#20998;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#22240;&#31867;&#21035;&#19981;&#24179;&#34913;&#32780;&#23548;&#33268;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08202</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#20013;&#24212;&#23545;&#27495;&#35270;&#65306;&#22522;&#20110;&#26680;&#31354;&#38388;&#20013;&#36793;&#32536;&#23569;&#25968;&#32676;&#20307;&#30340;SMOTE&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Confronting Discrimination in Classification: Smote Based on Marginalized Minorities in the Kernel Space for Imbalanced Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#36793;&#30028;&#21644;&#26679;&#26412;&#36817;&#37051;&#20851;&#31995;&#30340;&#26032;&#22411;&#20998;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20013;&#22240;&#31867;&#21035;&#19981;&#24179;&#34913;&#32780;&#23548;&#33268;&#30340;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#38754;&#20020;&#30528;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#20856;&#22411;&#25361;&#25112;&#65292;&#21363;&#27450;&#35784;&#26696;&#20363;&#38750;&#24120;&#32597;&#35265;&#65292;&#20294;&#22914;&#26524;&#35823;&#26631;&#35782;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22312;&#20998;&#31867;&#20013;&#31934;&#30830;&#22320;&#23545;&#36825;&#20123;&#20851;&#38190;&#30340;&#23569;&#25968;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#22256;&#38590;&#26469;&#33258;&#20110;&#20027;&#27969;&#20998;&#31867;&#22120;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#23545;&#23569;&#25968;&#26679;&#26412;&#24120;&#24120;&#34920;&#29616;&#20986;&#8220;&#38544;&#24615;&#27495;&#35270;&#8221;&#65292;&#23548;&#33268;&#39057;&#32321;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22810;&#25968;&#26679;&#26412;&#21644;&#23569;&#25968;&#26679;&#26412;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#37325;&#21472;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36807;&#37319;&#26679;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#30446;&#21069;&#30340;&#32463;&#20856;&#36807;&#37319;&#26679;&#26041;&#27861;&#22312;&#26679;&#26412;&#36873;&#25321;&#19978;&#24120;&#24120;&#32570;&#20047;&#24517;&#35201;&#30340;&#35880;&#24910;&#65292;&#21152;&#21095;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#37325;&#21472;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#36793;&#30028;&#21644;&#26679;&#26412;&#36817;&#37051;&#20851;&#31995;&#30340;&#26032;&#22411;&#20998;&#31867;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial fraud detection poses a typical challenge characterized by class imbalance, where instances of fraud are extremely rare but can lead to unpredictable economic losses if misidentified. Precisely classifying these critical minority samples represents a challenging task within the classification. The primary difficulty arises from mainstream classifiers, which often exhibit "implicit discrimination" against minority samples in evaluation metrics, which results in frequent misclassifications, and the key to the problem lies in the overlap of feature spaces between majority and minority samples. To address these challenges, oversampling is a feasible solution, yet current classical oversampling methods often lack the necessary caution in sample selection, exacerbating feature space overlap. In response, we propose a novel classification oversampling approach based on the decision boundary and sample proximity relationships. This method carefully considers the distance between crit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24369;&#20998;&#24067;&#37325;&#21472;&#19979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#21452;&#37325;&#31283;&#20581;&#65288;TDR&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08201</link><description>&lt;p&gt;
&#24369;&#20998;&#24067;&#37325;&#21472;&#19979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24369;&#20998;&#24067;&#37325;&#21472;&#19979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#21452;&#37325;&#31283;&#20581;&#65288;TDR&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#65292;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#22312;&#24207;&#21015;&#21487;&#24573;&#30053;&#24615;&#19979;&#23545;&#31163;&#31574;&#30053;&#35780;&#20272;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65306;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#20102;&#38543;&#30528;&#26102;&#38271;T&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;$1/\sqrt{T}$&#65292;&#22312;&#22823;&#26679;&#26412;&#20013;&#20855;&#26377;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25191;&#34892;&#39044;&#20272;&#20219;&#21153;&#65292;&#20855;&#26377;&#27169;&#22359;&#21270;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20351;&#29992;&#20102;&#24378;&#20998;&#24067;&#37325;&#21472;&#20551;&#35774;&#65292;&#21363;&#30446;&#26631;&#25919;&#31574;&#21644;&#25968;&#25454;&#25910;&#38598;&#25919;&#31574;&#30340;&#31283;&#24577;&#20998;&#24067;&#30456;&#24046;&#22312;&#26377;&#38480;&#22240;&#23376;&#20869;&#65292;&#32780;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#21482;&#22312;MDP&#30340;&#29366;&#24577;&#31354;&#38388;&#26377;&#30028;&#26102;&#25165;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#24369;&#20998;&#24067;&#37325;&#21472;&#27010;&#24565;&#19979;&#30340;MDP&#31163;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#25130;&#26029;&#21452;&#37325;&#31283;&#20581;&#65288;TDR&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#24403;&#30446;&#26631;&#21644;&#25968;&#25454;&#25910;&#38598;&#30340;&#20998;&#24067;&#27604;&#29575;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-coll
&lt;/p&gt;</description></item><item><title>PSC-CPI&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.08198</link><description>&lt;p&gt;
PSC-CPI: &#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08198
&lt;/p&gt;
&lt;p&gt;
PSC-CPI&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;CPI&#65289;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#21644;&#24378;&#24230;&#65292;&#20197;&#29992;&#20110;&#29702;&#24615;&#33647;&#29289;&#21457;&#29616;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#25110;&#32467;&#26500;&#30340;&#21333;&#19968;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#20004;&#20010;&#27169;&#24577;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#20849;&#21516;&#24314;&#27169;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#65292;&#27169;&#24577;&#20002;&#22833;&#21644;&#39046;&#22495;&#36716;&#31227;&#65289;&#32780;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#20197;&#21333;&#19968;&#22266;&#23450;&#23610;&#24230;&#27169;&#25311;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#26356;&#31934;&#32454;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20363;&#22914;&#23884;&#20837;&#22312;&#20851;&#38190;&#34507;&#30333;&#36136;&#29255;&#27573;&#20013;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#29992;&#20110;CPI&#39044;&#27979;&#65288;PSC-CPI&#65289;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#21487;&#21464;&#38271;&#24230;&#30340;&#34507;&#30333;&#36136;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#20801;&#35768;&#23545;&#19981;&#21516;&#38271;&#24230;&#30340;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08191</link><description>&lt;p&gt;
THE COLOSSEUM&#65306;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#27867;&#21270;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08191
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#12289;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#35780;&#20272;&#26426;&#22120;&#20154;&#22312;&#19982;&#35757;&#32451;&#35774;&#32622;&#38750;&#24120;&#30456;&#20284;&#29978;&#33267;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;THE COLOSSEUM&#65292;&#20854;&#20013;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21487;&#20197;&#23545;&#27169;&#22411;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#36825;&#20123;&#24178;&#25200;&#21253;&#25324;&#29289;&#20307;&#12289;&#26700;&#38754;&#21644;&#32972;&#26223;&#30340;&#39068;&#33394;&#12289;&#32441;&#29702;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#65307;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#20809;&#29031;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#20351;&#29992;THE COLOSSEUM&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;4&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#22312;&#36825;&#20123;&#24178;&#25200;&#22240;&#32032;&#19979;&#19979;&#38477;&#20102;30-50%&#12290;&#24403;&#22810;&#20010;&#24178;&#25200;&#21516;&#26102;&#24212;&#29992;&#26102;&#65292;&#25104;&#21151;&#29575;&#19979;&#38477;&#33267;&#8805;75%&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;GraphDeepONet&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08187</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#20197;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#40065;&#26834;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning time-dependent PDE via graph neural networks and deep operator network for robust accuracy on irregular grids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;GraphDeepONet&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#31185;&#23398;&#35745;&#31639;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23398;&#20064;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#21442;&#25968;&#21040;&#30456;&#24212;&#35299;&#30340;&#31639;&#23376;&#30340;&#27169;&#22411;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;DeepONet&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#31561;&#27169;&#22411;&#34987;&#35774;&#35745;&#20026;&#36866;&#29992;&#20110;&#23558;&#20989;&#25968;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#22788;&#29702;&#30340;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#20316;&#20026;&#35299;&#31639;&#23376;&#30340;&#26367;&#20195;&#27169;&#22411;&#30340;&#23454;&#26102;&#39044;&#27979;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26367;&#20195;&#27169;&#22411;&#30740;&#31350;&#22312;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#30340;PDE&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;GraphDeepONet&#65292;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25104;&#21151;&#30340;&#31639;&#23376;&#23398;&#20064;&#27169;&#22411;DeepONet&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;PDE&#27714;&#35299;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;GraphDeepONet&#22312;&#39044;&#27979;&#35299;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#40065;&#26834;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#65292;&#23427;&#20445;&#25345;&#30528;&#19968;&#33268;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific computing using deep learning has seen significant advancements in recent years. There has been growing interest in models that learn the operator from the parameters of a partial differential equation (PDE) to the corresponding solutions. Deep Operator Network (DeepONet) and Fourier Neural operator, among other models, have been designed with structures suitable for handling functions as inputs and outputs, enabling real-time predictions as surrogate models for solution operators. There has also been significant progress in the research on surrogate models based on graph neural networks (GNNs), specifically targeting the dynamics in time-dependent PDEs. In this paper, we propose GraphDeepONet, an autoregressive model based on GNNs, to effectively adapt DeepONet, which is well-known for successful operator learning. GraphDeepONet exhibits robust accuracy in predicting solutions compared to existing GNN-based PDE solver models. It maintains consistent performance even on irre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;SMAC&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#26426;&#21160;&#25216;&#33021;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08184</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#26080;&#20851;&#34920;&#31034;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36716;&#31227;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;SMAC&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#26426;&#21160;&#25216;&#33021;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22312;&#21160;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#38656;&#35201;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#36825;&#31181;&#20219;&#21153;&#26159;&#22256;&#38590;&#19988;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#20132;&#20114;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#32780;&#35328;&#65292;&#30001;&#20110;&#26679;&#26412;&#22797;&#26434;&#24615;&#26497;&#39640;&#12290;&#22240;&#27492;&#65292;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#32463;&#39564;&#25110;&#20854;&#20182;&#26234;&#33021;&#20307;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26377;&#25928;&#21152;&#24555;&#23398;&#20064;&#36807;&#31243;&#24182;&#25552;&#21319;MARL&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;MARL&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;StarCraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#65288;SMAC&#65289;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36890;&#36807;&#26426;&#21160;&#25216;&#33021;&#23398;&#20064;&#24471;&#21040;&#30340;&#30693;&#35782;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#65292;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.08182</link><description>&lt;p&gt;
&#21464;&#20998;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variational Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#65292;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#39564;&#20559;&#31227;&#22312;&#21482;&#20351;&#29992;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#65288;CTTA&#65289;&#26041;&#27861;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;CTTA&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#22312;&#28304;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#30830;&#23450;&#24615;&#27169;&#22411;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#27880;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#30340;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#65292;&#23558;&#23398;&#29983;&#27169;&#22411;&#21644;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#27861;&#29992;&#20110;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#12290;&#35777;&#25454;&#19979;&#30028;&#34987;&#21046;&#23450;&#20026;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#65292;&#20197;&#21450;&#20808;&#39564;&#28151;&#21512;&#30340;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#22312;CTTA&#20013;&#30340;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods that only use unlabeled test data, as it can cause significant error propagation. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08180</link><description>&lt;p&gt;
&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#19982;Fenchel-Young&#25439;&#22833;&#21644;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#12290;&#23545;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#65292;van der Hoeven(2020)&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20248;&#32654;&#30340;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#30340;&#26694;&#26550;&#65292;&#33719;&#24471;&#20102;&#19982;&#26102;&#38388;&#33539;&#22260;&#26080;&#20851;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#21363;&#26377;&#38480;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#38480;&#20110;&#22810;&#31867;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#29305;&#23450;&#20110;&#20998;&#31867;&#30340;&#36807;&#31243;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#8220;Fenchel-Young&#25439;&#22833;&#8221;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#65292;&#36825;&#26159;&#19968;&#22823;&#31867;&#21253;&#25324;&#22810;&#31867;&#20998;&#31867;&#30340;&#36923;&#36753;&#25439;&#22833;&#22312;&#20869;&#30340;&#26367;&#20195;&#25439;&#22833;&#65292;&#33719;&#24471;&#20102;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#38543;&#26426;&#35299;&#30721;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#19968;&#33324;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#35299;&#30721;&#24212;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08174</link><description>&lt;p&gt;
&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#22270;&#24418;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#23545;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#34920;&#24615;&#33410;&#28857;&#65288;&#31216;&#20026;&#22320;&#26631;&#65289;&#26469;&#34920;&#31034;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#23569;&#37327;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#65292;&#23427;&#20204;&#20316;&#20026;&#33410;&#28857;&#20301;&#32622;&#30340;&#21442;&#32771;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#31574;&#30053;&#23545;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;&#28041;&#21450;&#22320;&#26631;&#30340;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#19978;&#30028;&#12290;&#22312;&#24130;&#24459;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22320;&#26631;&#20026;&#33410;&#28857;&#20043;&#38388;&#36317;&#31163;&#25552;&#20379;&#20102;&#28176;&#36817;&#23436;&#20840;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#65288;HPLC&#65289;&#26041;&#27861;&#12290;HPLC&#23558;&#22320;&#26631;&#36873;&#25321;&#21644;&#22270;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#22270;&#34987;&#20998;&#21106;&#20026;&#36830;&#36890;&#23494;&#38598;&#30340;&#32858;&#31867;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#12290;HPLC&#21033;&#29992;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#33410;&#28857;&#20301;&#32622;&#20449;&#24687;&#30340;&#23618;&#32423;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.08164</link><description>&lt;p&gt;
&#20851;&#20110;Transformer&#26550;&#26500;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On Limitations of the Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#20351;&#29992;&#36890;&#20449;&#22797;&#26434;&#24615;&#26469;&#35777;&#26126;&#65292;&#22914;&#26524;&#20989;&#25968;&#30340;&#23450;&#20041;&#22495;&#36275;&#22815;&#22823;&#65292;Transformer&#23618;&#26080;&#27861;&#32452;&#21512;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#22312;&#23478;&#35889;&#20013;&#26597;&#25214;&#19968;&#20010;&#20154;&#30340;&#31062;&#29238;&#65289;&#65307;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#26174;&#31034;&#65292;&#24403;&#23450;&#20041;&#22495;&#30456;&#24403;&#23567;&#30340;&#26102;&#20505;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#32570;&#20047;&#24050;&#32463;&#22312;&#32463;&#39564;&#19978;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#35768;&#22810;&#22312;&#25152;&#35859;&#30340;&#32452;&#21512;&#20219;&#21153;&#20013;&#30340;&#25968;&#23398;&#20219;&#21153;&#65292;&#35748;&#20026;&#23427;&#20204;&#23545;LLMs&#26469;&#35828;&#24456;&#38590;&#35299;&#20915;&#65292;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#23454;&#20363;&#26469;&#35828;&#65292;&#19988;&#20551;&#35774;&#35745;&#31639;&#22797;&#26434;&#24615;&#39046;&#22495;&#30340;&#26576;&#20123;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#29468;&#24819;&#26159;&#27491;&#30830;&#30340;&#65292;Transformers&#20063;&#19981;&#22826;&#21487;&#33021;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;</title><link>https://arxiv.org/abs/2402.08147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#39564;&#35777;&#30340;&#22810;&#27493;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22312;Dafny&#12289;Lean&#21644;Coq&#20013;&#39564;&#35777;&#30340;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;VMCTS&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#27493;&#39588;&#26816;&#26597;&#37096;&#20998;&#31243;&#24207;&#26469;&#21033;&#29992;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#39564;&#35777;&#22120;&#12290;&#32467;&#21512;LLM&#20808;&#39564;&#30693;&#35782;&#65292;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#25552;&#39640;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#21512;&#25104;&#33021;&#21147;&#12290;&#22312;&#19968;&#32452;&#20116;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22235;&#20010;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#37325;&#26032;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#19968;&#23567;&#26102;&#30340;&#37325;&#26032;&#37319;&#26679;&#65292;&#22522;&#26412;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;VMCTS&#21487;&#20197;&#22312;6&#20998;&#38047;&#20869;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#65292;&#22522;&#26412;&#27169;&#22411;&#21152;&#19978;VMCTS&#29978;&#33267;&#19982;&#20855;&#26377;&#25554;&#20214;&#21644;&#22810;&#27425;&#37325;&#35797;&#30340;ChatGPT4&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;https://github.com/namin/llm-verified-with-monte-carlo-tree-search&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#26469;&#26356;&#24555;&#12289;&#26356;&#21487;&#25193;&#23637;&#22320;&#35745;&#31639;&#23545;&#31216;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#20013;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#33609;&#22270;&#26469;&#35745;&#31639;&#21021;&#22987;&#20302;&#31209;&#36755;&#20837;&#30697;&#38453;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#26469;&#36817;&#20284;&#35299;&#20915;&#32422;&#26463;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32858;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08134</link><description>&lt;p&gt;
&#23545;&#31216;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomized Algorithms for Symmetric Nonnegative Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#26469;&#26356;&#24555;&#12289;&#26356;&#21487;&#25193;&#23637;&#22320;&#35745;&#31639;&#23545;&#31216;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65292;&#20854;&#20013;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#33609;&#22270;&#26469;&#35745;&#31639;&#21021;&#22987;&#20302;&#31209;&#36755;&#20837;&#30697;&#38453;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#26469;&#36817;&#20284;&#35299;&#20915;&#32422;&#26463;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32858;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;SymNMF&#65289;&#26159;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#31181;&#23558;&#23545;&#31216;&#30697;&#38453;&#36817;&#20284;&#34920;&#31034;&#20026;&#38750;&#36127;&#12289;&#20302;&#31209;&#30697;&#38453;&#21450;&#20854;&#36716;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#35774;&#35745;&#26356;&#24555;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;SymNMF&#31639;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#38543;&#26426;&#31639;&#27861;&#26469;&#36827;&#34892;&#35745;&#31639;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#20351;&#29992;&#38543;&#26426;&#30697;&#38453;&#33609;&#22270;&#35745;&#31639;&#21021;&#22987;&#30340;&#20302;&#31209;&#36755;&#20837;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#35813;&#36755;&#20837;&#36805;&#36895;&#35745;&#31639;SymNMF&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20351;&#29992;&#38543;&#26426;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#26469;&#36817;&#20284;&#35299;&#20915;&#32422;&#26463;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#35768;&#22810;&#25104;&#21151;&#30340;SymNMF&#26041;&#27861;&#20381;&#36182;&#20110;&#65288;&#36817;&#20284;&#65289;&#35299;&#20915;&#19968;&#31995;&#21015;&#32422;&#26463;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#26464;&#26438;&#24471;&#20998;&#37319;&#26679;&#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#36817;&#20284;&#35299;&#20915;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#36798;&#21040;&#25152;&#36873;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32858;&#31867;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Nonnegative Matrix Factorization (SymNMF) is a technique in data analysis and machine learning that approximates a symmetric matrix with a product of a nonnegative, low-rank matrix and its transpose. To design faster and more scalable algorithms for SymNMF we develop two randomized algorithms for its computation. The first algorithm uses randomized matrix sketching to compute an initial low-rank input matrix and proceeds to use this input to rapidly compute a SymNMF. The second algorithm uses randomized leverage score sampling to approximately solve constrained least squares problems. Many successful methods for SymNMF rely on (approximately) solving sequences of constrained least squares problems. We prove theoretically that leverage score sampling can approximately solve nonnegative least squares problems to a chosen accuracy with high probability. Finally we demonstrate that both methods work well in practice by applying them to graph clustering tasks on large real world d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#24635;&#32467;&#20102;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#65292;&#24490;&#29615;&#27169;&#22411;&#30340;&#22797;&#20852;&#21644;&#19982;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.08132</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;&#24490;&#29615;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#20013;&#30340;&#22797;&#20852;&#65306;&#22312;Transformer&#26102;&#20195;&#30340;&#35843;&#30740;&#21644;&#30740;&#31350;&#26426;&#20250;&#12299;
&lt;/p&gt;
&lt;p&gt;
On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#24635;&#32467;&#20102;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#65292;&#24490;&#29615;&#27169;&#22411;&#30340;&#22797;&#20852;&#21644;&#19982;Transformer&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#24320;&#21457;&#21487;&#20197;&#22788;&#29702;&#21644;&#23398;&#20064;&#38750;&#24120;&#38271;&#30340;&#25968;&#25454;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#20986;&#33394;&#32467;&#26524;&#25512;&#21160;&#20102;&#24182;&#34892;&#27880;&#24847;&#21147;&#30340;&#27010;&#24565;&#65292;&#23558;&#32463;&#20856;&#30340;&#39034;&#24207;&#22788;&#29702;&#30340;&#24490;&#29615;&#27169;&#22411;&#30340;&#20316;&#29992;&#25513;&#30422;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#34920;&#31034;&#20851;&#27880;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20860;&#39038;Transformer&#21644;&#24490;&#29615;&#32593;&#32476;&#20004;&#20010;&#19990;&#30028;&#20248;&#21183;&#30340;&#26032;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#28145;&#24230;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#20316;&#20026;&#26102;&#38388;&#20989;&#25968;&#36924;&#36817;&#30340;&#24378;&#22823;&#26041;&#27861;&#20986;&#29616;&#65292;&#20174;&#32780;&#20026;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#23545;&#27492;&#24863;&#20852;&#36259;&#24182;&#21033;&#29992;&#23427;&#26469;&#23454;&#29616;&#19968;&#31867;&#29305;&#27530;&#30340;&#65288;&#32447;&#24615;&#65289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#27010;&#36848;&#36825;&#20123;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#26080;&#20449;&#24687;&#35774;&#32622;&#30340;&#19978;&#19979;&#25991;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#38477;&#20302;&#25928;&#29575;&#21644;&#26368;&#20248;&#21270;&#31639;&#27861;&#65292;&#20851;&#38190;&#26159;&#23398;&#20064;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#30340;&#22270;&#24418;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.08127</link><description>&lt;p&gt;
&#26080;&#20449;&#24687;&#21453;&#39304;&#22270;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#36172;&#29575;
&lt;/p&gt;
&lt;p&gt;
Efficient Contextual Bandits with Uninformed Feedback Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#26080;&#20449;&#24687;&#35774;&#32622;&#30340;&#19978;&#19979;&#25991;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#38477;&#20302;&#25928;&#29575;&#21644;&#26368;&#20248;&#21270;&#31639;&#27861;&#65292;&#20851;&#38190;&#26159;&#23398;&#20064;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#30340;&#22270;&#24418;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#21518;&#24724;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#21453;&#39304;&#22270;&#30340;&#36172;&#29575;&#26159;&#24378;&#22823;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25554;&#20540;&#23436;&#25972;&#20449;&#24687;&#21644;&#32463;&#20856;&#36172;&#29575;&#38382;&#39064;&#65292;&#25429;&#25417;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#29256;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#32447;&#22238;&#24402;&#38477;&#20302;&#25928;&#29575;&#21644;&#26368;&#20248;&#21270;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#20915;&#31574;&#20043;&#21069;&#37117;&#20851;&#38190;&#20381;&#36182;&#20110;&#30475;&#21040;&#21453;&#39304;&#22270;&#65292;&#32780;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21453;&#39304;&#22270;&#26159;&#26410;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#23427;&#21482;&#22312;&#23398;&#20064;&#32773;&#20570;&#20986;&#20915;&#31574;&#21518;&#25165;&#34987;&#25581;&#31034;&#65292;&#29978;&#33267;&#20174;&#26410;&#23436;&#20840;&#25581;&#31034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39640;&#25928;&#22320;&#23558;&#22312;&#32447;&#22238;&#24402;&#24212;&#29992;&#20110;&#25439;&#22833;&#21644;&#22270;&#24418;&#65292;&#20026;&#36825;&#31181;&#26410;&#30693;&#35774;&#32622;&#24320;&#21457;&#20102;&#39318;&#20010;&#19978;&#19979;&#25991;&#31639;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#23398;&#20064;&#22270;&#24418;&#26159;&#20851;&#38190;&#30340;&#65292;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#25034;&#24724;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandits with feedback graphs are powerful online learning models that interpolate between the full information and classic bandit problems, capturing many real-life applications. A recent work by Zhang et al. (2023) studies the contextual version of this problem and proposes an efficient and optimal algorithm via a reduction to online regression. However, their algorithm crucially relies on seeing the feedback graph before making each decision, while in many applications, the feedback graph is uninformed, meaning that it is either only revealed after the learner makes her decision or even never fully revealed at all. This work develops the first contextual algorithm for such uninformed settings, via an efficient reduction to online regression over both the losses and the graphs. Importantly, we show that it is critical to learn the graphs using log loss instead of squared loss to obtain favorable regret guarantees. We also demonstrate the empirical effectiveness of our algorithm on a b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#26469;&#22788;&#29702;&#32447;&#24615;&#24773;&#20917;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08126</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Multinomial Logit Bandits with General Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#26469;&#22788;&#29702;&#32447;&#24615;&#24773;&#20917;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402; (MNL) &#36172;&#21338;&#26426;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#20013;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#27604;&#22914;&#22312;&#32447;&#38646;&#21806;/&#24191;&#21578;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#65288;&#24191;&#20041;&#30340;&#65289;&#32447;&#24615;&#20215;&#20540;&#20989;&#25968;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#20107;&#23454;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21253;&#21547;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#22659;MNL&#36172;&#21338;&#26426;&#65292;&#20511;&#37492;&#20102;&#26368;&#36817;&#23545;&#24773;&#22659;&#36172;&#21338;&#26426;&#30740;&#31350;&#30340;&#36235;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#31639;&#27861;&#65292;&#27599;&#20010;&#31639;&#27861;&#22312;&#35745;&#31639;&#21644;&#36951;&#25022;&#20043;&#38388;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#24403;&#24212;&#29992;&#20110;&#32447;&#24615;&#24773;&#20917;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#26159;&#31532;&#19968;&#20010;&#19981;&#20381;&#36182;&#20110;&#26576;&#20010;&#21487;&#33021;&#25351;&#25968;&#22686;&#38271;&#30340;&#38382;&#39064;&#30456;&#20851;&#24120;&#25968;&#30340;&#32467;&#26524;&#65292;&#36824;&#20855;&#26377;&#20854;&#20182;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#25928;&#29575;&#12289;&#26080;&#32500;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#20197;&#21450;&#22788;&#29702;&#23436;&#20840;&#23545;&#25239;&#24615;&#29615;&#22659;&#21644;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21387;&#32553;&#27169;&#22411;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20998;&#23376;&#24207;&#21015;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#26412;&#21387;&#32553;&#31639;&#27861;&#21644;&#26631;&#20934;&#21270;&#21387;&#32553;&#36317;&#31163;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08117</link><description>&lt;p&gt;
&#19968;&#31181;&#25913;&#36827;&#30340;&#20998;&#23376;&#24207;&#21015;&#20998;&#26512;&#30340;&#36890;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Universal Non-Parametric Approach For Improved Molecular Sequence Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08117
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21387;&#32553;&#27169;&#22411;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20998;&#23376;&#24207;&#21015;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#26412;&#21387;&#32553;&#31639;&#27861;&#21644;&#26631;&#20934;&#21270;&#21387;&#32553;&#36317;&#31163;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#29702;&#35299;&#20998;&#23376;&#24207;&#21015;&#30340;&#29305;&#24449;&#21644;&#21151;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20998;&#23376;&#24207;&#21015;&#30340;&#20998;&#31867;&#24050;&#32463;&#24191;&#27867;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#26356;&#22810;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;\cite{jiang2023low}&#30340;&#21551;&#21457;&#65292;&#23558;&#22522;&#26412;&#21387;&#32553;&#31639;&#27861;&#65288;&#22914;Gzip&#21644;Bz2&#65289;&#30340;&#31616;&#21333;&#24615;&#19982;&#26631;&#20934;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20247;&#25152;&#21608;&#30693;&#30340;&#21387;&#32553;&#31639;&#27861;&#65288;&#22914;Gzip&#21644;Bz2&#65289;&#23545;&#20998;&#23376;&#24207;&#21015;&#36827;&#34892;&#21387;&#32553;&#12290;&#36890;&#36807;&#21033;&#29992;&#21387;&#32553;&#25991;&#20214;&#20013;&#32534;&#30721;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#35745;&#31639;&#20986;&#27599;&#23545;&#20998;&#23376;&#24207;&#21015;&#20043;&#38388;&#30340;&#26631;&#20934;&#21270;&#21387;&#32553;&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#26159;&#20174;&#21387;&#32553;&#36317;&#31163;&#25512;&#23548;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of biological research, it is essential to comprehend the characteristics and functions of molecular sequences. The classification of molecular sequences has seen widespread use of neural network-based techniques. Despite their astounding accuracy, these models often require a substantial number of parameters and more data collection. In this work, we present a novel approach based on the compression-based Model, motivated from \cite{jiang2023low}, which combines the simplicity of basic compression algorithms like Gzip and Bz2, with Normalized Compression Distance (NCD) algorithm to achieve better performance on classification tasks without relying on handcrafted features or pre-trained models. Firstly, we compress the molecular sequence using well-known compression algorithms, such as Gzip and Bz2. By leveraging the latent structure encoded in compressed files, we compute the Normalized Compression Distance between each pair of molecular sequences, which is derived from t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08114</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Preference Learning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#24494;&#35843;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#26368;&#20851;&#38190;&#30340;&#32771;&#34385;&#26159;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;LLM&#26412;&#36523;&#34987;&#29992;&#20316;oracle&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#36164;&#28304;&#12290;&#20174;&#20154;&#31867;&#25110;AI&#20559;&#22909;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF / RLAIF&#65289;&#26159;&#36825;&#31181;&#25216;&#26415;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#24448;&#24448;&#22797;&#26434;&#19988;&#19981;&#31283;&#23450;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#21644;&#26356;&#31283;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;DPO&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#29109;&#21644;DPO&#20248;&#21270;&#30340;&#38544;&#24335;&#20559;&#22909;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#23454;&#29992;&#37319;&#38598;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
&lt;/p&gt;</description></item><item><title>&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08112</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;microRTS&#20013;&#33719;&#22870;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Competition Winning Deep Reinforcement Learning Agent in microRTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08112
&lt;/p&gt;
&lt;p&gt;
&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CIG&#21644;CoG&#20030;&#21150;&#30340;IEEE microRTS&#65288;$\mu$RTS&#65289;&#31454;&#36187;&#30340;&#20116;&#23626;&#20013;&#65292;&#33050;&#26412;&#20195;&#29702;&#20027;&#23548;&#20102;&#27604;&#36187;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#22312;&#23454;&#26102;&#31574;&#30053;&#65288;RTS&#65289;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#36164;&#28304;&#20197;&#21450;&#21019;&#24314;&#21644;&#35843;&#35797;&#27492;&#31867;&#20195;&#29702;&#25152;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20027;&#35201;&#26159;&#23398;&#26415;&#31454;&#36187;&#20013;&#30340;&#37319;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;RAISocketAI&#26159;&#31532;&#19968;&#20010;&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#33719;&#32988;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#19968;&#20010;&#27809;&#26377;&#24615;&#33021;&#38480;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RAISocketAI&#32463;&#24120;&#20987;&#36133;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#12290;&#36825;&#20010;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;DRL&#25552;&#20132;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;microRTS&#31454;&#36187;&#30340;&#22522;&#20934;&#65292;&#24182;&#25104;&#20026;&#26410;&#26469;DRL&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#23545;&#29305;&#23450;&#22320;&#22270;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#23545;RAISocketAI&#30340;&#33719;&#32988;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#32463;&#27982;&#35757;&#32451;&#26410;&#26469;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#25968;&#25454;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#25512;&#33616;&#24341;&#25806;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08109</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#21040;&#20915;&#31574;&#65306;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#20013;&#30340;&#36716;&#21464;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#25968;&#25454;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#25512;&#33616;&#24341;&#25806;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#23545;&#25512;&#33616;&#31995;&#32479;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#28436;&#21464;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#22609;&#36896;&#21644;&#25913;&#36827;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#26469;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#22686;&#24378;&#25512;&#33616;&#31639;&#27861;&#30340;&#36845;&#20195;&#24615;&#36136;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25512;&#33616;&#24341;&#25806;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39640;&#32423;&#31639;&#27861;&#21644;&#25968;&#25454;&#20998;&#26512;&#39537;&#21160;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#36825;&#20123;&#24341;&#25806;&#19981;&#20165;&#31616;&#21270;&#20102;&#20449;&#24687;&#21457;&#29616;&#21644;&#22686;&#24378;&#20102;&#21327;&#20316;&#65292;&#36824;&#21152;&#24555;&#20102;&#30693;&#35782;&#33719;&#21462;&#65292;&#23545;&#20225;&#19994;&#22312;&#25968;&#23383;&#21270;&#39046;&#22495;&#20013;&#30340;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#23545;&#38144;&#21806;&#12289;&#25910;&#20837;&#21644;&#20225;&#19994;&#31454;&#20105;&#20248;&#21183;&#30340;&#36129;&#29486;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of Recommendation Systems (RS), particularly in the context of their growing significance in commercial business environments. Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing recommendation algorithms. The deployment of Recommendation Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes. These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses. They contribute significantly to sales, revenue, and the competitive edge of enterpr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20985;&#20248;&#21270;&#26041;&#27861;&#23547;&#25214;&#21487;&#20197;&#21253;&#21547;&#26356;&#22810;&#36164;&#20135;&#30340;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#65292;&#20854;&#20013;&#20215;&#26684;&#24102;&#30340;&#20013;&#28857;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08108</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20985;&#20248;&#21270;&#23547;&#25214;&#31227;&#21160;&#24102;&#32479;&#35745;&#22871;&#21033;
&lt;/p&gt;
&lt;p&gt;
Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20985;&#20248;&#21270;&#26041;&#27861;&#23547;&#25214;&#21487;&#20197;&#21253;&#21547;&#26356;&#22810;&#36164;&#20135;&#30340;&#32479;&#35745;&#22871;&#21033;&#31574;&#30053;&#65292;&#20854;&#20013;&#20215;&#26684;&#24102;&#30340;&#20013;&#28857;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#21487;&#20197;&#21253;&#21547;&#38500;&#20256;&#32479;&#20132;&#26131;&#23545;&#20197;&#22806;&#26356;&#22810;&#36164;&#20135;&#30340;&#32479;&#35745;&#22871;&#21033;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24402;&#32467;&#20026;&#23547;&#25214;&#19968;&#20010;&#27874;&#21160;&#29575;&#26368;&#39640;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20351;&#20854;&#20215;&#26684;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#24182;&#28385;&#36275;&#26464;&#26438;&#38480;&#21046;&#12290;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#19981;&#26159;&#20984;&#30340;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#20984;&#20985;&#31243;&#24207;&#26469;&#36817;&#20284;&#27714;&#35299;&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#23450;&#30340;&#39034;&#24207;&#20984;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#25512;&#24191;&#21040;&#23547;&#25214;&#31227;&#21160;&#24102;&#32479;&#35745;&#22871;&#21033;&#65292;&#20854;&#20013;&#20215;&#26684;&#24102;&#30340;&#20013;&#28857;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#27979;&#24230;&#31354;&#38388;&#19978;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.08106</link><description>&lt;p&gt;
&#38024;&#23545;&#22343;&#22330;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mirror Descent-Ascent for mean-field min-max problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#27979;&#24230;&#31354;&#38388;&#19978;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#30340;&#20004;&#31181;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38236;&#20687;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#27979;&#24230;&#31354;&#38388;&#19978;&#35299;&#20915;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#21516;&#26102;&#21644;&#20381;&#27425;&#12290;&#25105;&#20204;&#22312;&#20984;&#24615;-&#20985;&#24615;&#21644;&#30456;&#23545;&#20809;&#28369;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#38024;&#23545;&#36866;&#24403;&#30340;Bregman&#25955;&#24230;&#22312;&#27979;&#24230;&#31354;&#38388;&#19978;&#36890;&#36807;&#24179;&#22374;&#23548;&#25968;&#36827;&#34892;&#20102;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25910;&#25947;&#36895;&#29575;&#21040;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#65292;&#29992;&#23612;&#20975;&#22810;-Isoda&#35823;&#24046;&#34920;&#31034;&#65292;&#23545;&#20110;&#21516;&#26102;&#21644;&#20381;&#27425;&#26041;&#26696;&#20998;&#21035;&#26159;$\mathcal{O}\left(N^{-1/2}\right)$&#21644;$\mathcal{O}\left(N^{-2/3}\right)$&#65292;&#36825;&#19982;&#30456;&#20851;&#26377;&#38480;&#32500;&#31639;&#27861;&#30340;&#26368;&#26032;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study two variants of the mirror descent-ascent algorithm for solving min-max problems on the space of measures: simultaneous and sequential. We work under assumptions of convexity-concavity and relative smoothness of the payoff function with respect to a suitable Bregman divergence, defined on the space of measures via flat derivatives. We show that the convergence rates to mixed Nash equilibria, measured in the Nikaid\`o-Isoda error, are of order $\mathcal{O}\left(N^{-1/2}\right)$ and $\mathcal{O}\left(N^{-2/3}\right)$ for the simultaneous and sequential schemes, respectively, which is in line with the state-of-the-art results for related finite-dimensional algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Laplacian&#32422;&#26463;&#19979;&#23398;&#20064;&#31515;&#21345;&#23572;&#20056;&#31215;&#22270;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#31515;&#21345;&#23572;&#20056;&#31215;Laplacian&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08105</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;Laplacian&#32422;&#26463;&#30340;&#31515;&#21345;&#23572;&#20056;&#31215;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Cartesian Product Graphs with Laplacian Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Laplacian&#32422;&#26463;&#19979;&#23398;&#20064;&#31515;&#21345;&#23572;&#20056;&#31215;&#22270;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#31515;&#21345;&#23572;&#20056;&#31215;Laplacian&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;Laplacian&#23398;&#20064;&#65292;&#20063;&#34987;&#31216;&#20026;&#32593;&#32476;&#25299;&#25169;&#25512;&#26029;&#65292;&#26159;&#19968;&#20010;&#21560;&#24341;&#22810;&#20010;&#39046;&#22495;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;&#22312;&#39640;&#26031;&#22270;&#27169;&#22411;&#65288;GM&#65289;&#20013;&#65292;&#22270;&#23398;&#20064;&#31561;&#20215;&#20110;&#21521;&#21327;&#26041;&#24046;&#36873;&#25321;&#28155;&#21152;Laplacian&#32467;&#26500;&#12290;&#22312;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#20013;&#65292;&#20174;&#36807;&#28388;&#31995;&#32479;&#30340;&#36755;&#20986;&#20013;&#25512;&#26029;&#26410;&#35266;&#23519;&#21040;&#30340;&#22270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Laplacian&#32422;&#26463;&#19979;&#23398;&#20064;&#31515;&#21345;&#23572;&#20056;&#31215;&#22270;&#30340;&#38382;&#39064;&#12290;&#31515;&#21345;&#23572;&#22270;&#20056;&#31215;&#26159;&#24314;&#27169;&#39640;&#38454;&#26465;&#20214;&#20381;&#36182;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#27861;&#65292;&#20063;&#26159;&#23558;GSP&#25512;&#24191;&#21040;&#22810;&#36335;&#24352;&#37327;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#31515;&#21345;&#23572;&#20056;&#31215;Laplacian&#30340;&#24809;&#32602;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#23384;&#22312;&#32467;&#26500;&#24615;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#32852;&#21512;&#22270;&#23398;&#20064;&#21644;&#25554;&#34917;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian learning, also known as network topology inference, is a problem of great interest to multiple communities. In Gaussian graphical models (GM), graph learning amounts to endowing covariance selection with the Laplacian structure. In graph signal processing (GSP), it is essential to infer the unobserved graph from the outputs of a filtering system. In this paper, we study the problem of learning Cartesian product graphs under Laplacian constraints. The Cartesian graph product is a natural way for modeling higher-order conditional dependencies and is also the key for generalizing GSP to multi-way tensors. We establish statistical consistency for the penalized maximum likelihood estimation (MLE) of a Cartesian product Laplacian, and propose an efficient algorithm to solve the problem. We also extend our method for efficient joint graph learning and imputation in the presence of structural missing values. Experiments on synthetic and real-world datasets demonstrate that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;GPT-3.5&#22312;&#38476;&#29983;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08100</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;GPT-3.5&#22312;&#38476;&#29983;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#25551;&#36848;&#20197;&#29983;&#25104;&#20195;&#30721;&#20284;&#20046;&#26159;&#38646;-shot&#22330;&#26223;&#19979;&#25351;&#20196;&#36981;&#24490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#39033;&#24050;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36825;&#31181;&#32763;&#35793;&#33021;&#21147;&#30340;&#22240;&#32032;&#26159;&#24050;&#32463;&#35265;&#36807;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#30456;&#20851;&#20195;&#30721;&#12290;&#36825;&#31181;&#24433;&#21709;&#34987;&#31216;&#20026;&#25968;&#25454;&#27745;&#26579;&#12290;  &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25968;&#25454;&#27745;&#26579;&#23545;GPT-3.5&#22312;&#25991;&#26412;&#21040;SQL&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;GPTs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#65292;&#24182;&#20351;&#29992;&#24050;&#30693;&#30340;Spider&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#38476;&#29983;&#25968;&#25454;&#38598;Termite&#26469;&#26816;&#26597;GPT-3.5&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#23545;&#25239;&#24615;&#34920;&#26029;&#24320;&#65288;ATD&#65289;&#26041;&#27861;&#20998;&#26512;&#20102;GPT-3.5&#22312;&#20855;&#26377;&#20462;&#25913;&#20449;&#24687;&#30340;&#25968;&#25454;&#24211;&#19978;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#24211;&#20013;&#21024;&#38500;&#32467;&#26500;&#20449;&#24687;&#26469;&#20351;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#22797;&#26434;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#22312;&#38476;&#29983;&#30340;Termite&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding textual description to generate code seems to be an achieved capability of instruction-following Large Language Models (LLMs) in zero-shot scenario. However, there is a severe possibility that this translation ability may be influenced by having seen target textual descriptions and the related code. This effect is known as Data Contamination.   In this study, we investigate the impact of Data Contamination on the performance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we introduce a novel method to detect Data Contamination in GPTs and examine GPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new unfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on databases with modified information via an adversarial table disconnection (ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of information from the database. Our results indicate a significant performance drop in GPT-3.5 on the unfamiliar T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#21644;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#19968;&#23450;&#31934;&#24230;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08097</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#27714;&#35299;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#20984;&#19979;&#23618;&#38382;&#39064;&#30340;&#31616;&#21333;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#21644;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#19968;&#23450;&#31934;&#24230;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#31616;&#21333;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#21478;&#19968;&#20010;&#20984;&#20809;&#28369;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#38598;&#19978;&#26368;&#23567;&#21270;&#19968;&#20010;&#20984;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20999;&#24179;&#38754;&#26041;&#27861;&#23616;&#37096;&#36924;&#36817;&#19979;&#23618;&#38382;&#39064;&#30340;&#35299;&#38598;&#65292;&#24182;&#37319;&#29992;&#21152;&#36895;&#26799;&#24230;&#26356;&#26032;&#26041;&#27861;&#38477;&#20302;&#36817;&#20284;&#35299;&#38598;&#19978;&#30340;&#19978;&#23618;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23376;&#26368;&#20248;&#35299;&#21644;&#19981;&#21487;&#34892;&#35823;&#24046;&#24230;&#37327;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#35823;&#24046;&#26631;&#20934;&#30340;&#38750;&#28176;&#36827;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#24403;&#21487;&#34892;&#38598;&#26159;&#32039;&#33268;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#22810;&#38656;&#35201;$\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$&#27425;&#36845;&#20195;&#25165;&#33021;&#25214;&#21040;&#19968;&#20010;$\epsilon_f$-&#23376;&#26368;&#20248;&#19988;$\epsilon_g$-&#19981;&#21487;&#34892;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#19979;&#23618;&#30446;&#26631;&#28385;&#36275;$r$&#38454;H\"olderian&#35823;&#24046;&#26102;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on simple bilevel optimization problems, where we minimize a convex smooth objective function over the optimal solution set of another convex smooth constrained optimization problem. We present a novel bilevel optimization method that locally approximates the solution set of the lower-level problem using a cutting plane approach and employs an accelerated gradient-based update to reduce the upper-level objective function over the approximated solution set. We measure the performance of our method in terms of suboptimality and infeasibility errors and provide non-asymptotic convergence guarantees for both error criteria. Specifically, when the feasible set is compact, we show that our method requires at most $\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ iterations to find a solution that is $\epsilon_f$-suboptimal and $\epsilon_g$-infeasible. Moreover, under the additional assumption that the lower-level objective satisfies the $r$-th H\"olderian err
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#33021;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#26080;&#32541;&#36816;&#29992;&#65292;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08096</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#37325;&#26032;&#32451;&#20064;&#21738;&#20123;&#39044;&#35757;&#32451;&#26679;&#26412;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#33021;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#26080;&#32541;&#36816;&#29992;&#65292;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#24050;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#24050;&#30693;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20250;&#36951;&#24536;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#26679;&#26412;&#26469;&#36827;&#34892;&#37325;&#26032;&#32451;&#20064;&#26159;&#32531;&#35299;&#36951;&#24536;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#28151;&#21512;&#19981;&#32463;&#24847;&#22320;&#21253;&#25324;&#20102;&#27169;&#22411;&#23578;&#26410;&#36951;&#24536;&#25110;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;"collateral damage"&#12290;&#30001;&#20110;&#30452;&#25509;&#35782;&#21035;"collateral damage"&#26679;&#26412;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36319;&#36394;&#24494;&#35843;&#26679;&#26412;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#20272;&#35745;&#36825;&#31867;&#26679;&#26412;&#20998;&#24067;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#27905;&#36731;&#37327;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#24615;&#33021;&#32780;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning. Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#26041;&#24335;&#30830;&#20999;&#23454;&#29616;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#37319;&#26679;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.08095</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#20998;&#26512;&#65306;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#30830;&#20999;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Discrete Diffusion Model: Exact Implementation through Uniformization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22343;&#21248;&#21270;&#30340;&#26041;&#24335;&#30830;&#20999;&#23454;&#29616;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#37319;&#26679;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#21162;&#21147;&#24050;&#32463;&#34987;&#20570;&#20986;&#26469;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#36866;&#24212;&#21040;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#65292;&#20026;&#24314;&#27169;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#25968;&#25454;&#65288;&#22914;&#35821;&#35328;&#21644;&#22270;&#24418;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#26041;&#27861;&#12290;&#36825;&#36890;&#36807;&#23558;&#21069;&#21521;&#22122;&#22768;&#36807;&#31243;&#21644;&#30456;&#24212;&#30340;&#36870;&#36807;&#31243;&#37117;&#26500;&#24314;&#20026;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;CTMC&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36830;&#32493;&#39532;&#23572;&#21487;&#22827;&#38142;&#22343;&#21248;&#21270;&#30340;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#26102;&#38388;&#28857;&#19978;&#23454;&#29616;&#36716;&#31227;&#12290;&#22312;&#20851;&#20110;&#31163;&#25955;&#24471;&#20998;&#20989;&#25968;&#23398;&#20064;&#30340;&#21512;&#29702;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20174;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#20219;&#20309;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#25152;&#38656;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#21644;KL&#25955;&#24230;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#22312;$\mathbb{R}^d$&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#23601;&#30456;&#19968;&#33268;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;d&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved huge empirical success in data generation tasks. Recently, some efforts have been made to adapt the framework of diffusion models to discrete state space, providing a more natural approach for modeling intrinsically discrete data, such as language and graphs. This is achieved by formulating both the forward noising process and the corresponding reversed process as Continuous Time Markov Chains (CTMCs). In this paper, we investigate the theoretical properties of the discrete diffusion model. Specifically, we introduce an algorithm leveraging the uniformization of continuous Markov chains, implementing transitions on random time points. Under reasonable assumptions on the learning of the discrete score function, we derive Total Variation distance and KL divergence guarantees for sampling from any distribution on a hypercube. Our results align with state-of-the-art achievements for diffusion models in $\mathbb{R}^d$ and further underscore the advantages of d
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08093</link><description>&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08093
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;10&#19975;&#23567;&#26102;&#25968;&#25454;&#30340;10&#20159;&#21442;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;BASE TTS&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#19978;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BASE TTS&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#65292;&#20854;&#20013;BASE&#20195;&#34920;&#22823;&#35268;&#27169;&#33258;&#36866;&#24212;&#21487;&#27969;&#24335;TTS&#21644;&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;&#12290;BASE TTS&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#20110;10&#19975;&#23567;&#26102;&#30340;&#20844;&#20849;&#39046;&#22495;&#35821;&#38899;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#33258;&#28982;&#24230;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;10&#20159;&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;Transformer&#65292;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#31163;&#25955;&#20195;&#30721;&#65288;"speechcodes"&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#21367;&#31215;&#30340;&#35299;&#30721;&#22120;&#23558;&#36825;&#20123;speechcodes&#20197;&#22686;&#37327;&#12289;&#21487;&#27969;&#24335;&#30340;&#26041;&#24335;&#36716;&#25442;&#20026;&#27874;&#24418;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;speechcodes&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#38899;&#26631;&#35760;&#21270;&#25216;&#26415;&#65292;&#20855;&#26377;&#35828;&#35805;&#32773;ID&#35299;&#32806;&#21644;&#23383;&#33410;&#23545;&#32534;&#30721;&#30340;&#21387;&#32553;&#29305;&#24615;&#12290;&#19982;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#25253;&#36947;&#30340;"&#26032;&#20986;&#29616;&#30340;&#33021;&#21147;"&#31867;&#20284;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;10K+&#23567;&#26102;&#21644;500M+&#21442;&#25968;&#26500;&#24314;&#30340;BASE TTS&#21464;&#20307;&#22312;&#25991;&#26412;&#22797;&#26434;&#21477;&#23376;&#19978;&#24320;&#22987;&#23637;&#29616;&#33258;&#28982;&#30340;&#38901;&#24459;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes ("speechcodes") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported "emergent abilities" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08090</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#25910;&#32553;&#21160;&#21147;&#23398;&#65306;&#25193;&#23637;&#32447;&#24615;&#21270;&#21644;&#20840;&#23616;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#30340;&#25193;&#23637;&#32447;&#24615;&#21270;&#23454;&#29616;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#35757;&#32451;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;ELCD&#33021;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20445;&#25345;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#20840;&#23616;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#23545;&#20110;&#30830;&#20445;&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#33391;&#22909;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#32447;&#24615;&#21270;&#25910;&#32553;&#21160;&#21147;&#23398;&#65288;ELCD&#65289;&#65292;&#35813;&#31995;&#32479;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#20219;&#24847;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#24615;&#20445;&#35777;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;ELCD&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#38750;&#32447;&#24615;&#21521;&#37327;&#22330;&#25193;&#23637;&#32447;&#24615;&#21270;&#30340;&#21442;&#25968;&#21270;&#12290;&#22312;&#20854;&#26368;&#22522;&#26412;&#24418;&#24335;&#19979;&#65292;ELCD&#20445;&#35777;&#20840;&#23616;&#25351;&#25968;&#31283;&#23450;&#12289;&#24179;&#34913;&#25910;&#32553;&#20197;&#21450;&#22312;&#26576;&#20123;&#24230;&#37327;&#19979;&#20840;&#23616;&#25910;&#32553;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#30456;&#23545;&#20110;&#26356;&#19968;&#33324;&#24230;&#37327;&#30340;&#25910;&#32553;&#65292;&#25105;&#20204;&#35757;&#32451;&#25968;&#25454;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#25910;&#32553;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#25968;&#25454;&#31354;&#38388;&#30340;&#20840;&#23616;&#25910;&#32553;&#24615;&#12290;&#25105;&#20204;&#22312;2D&#12289;4D&#21644;8D&#30340;LASA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;ELCD&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#21644;&#30417;&#27979;&#25968;&#25454;&#28418;&#31227;&#12290;&#35813;&#26694;&#26550;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;ML&#35774;&#22791;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#21644;&#24739;&#32773;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2402.08088</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#21644;&#25968;&#25454;&#28418;&#31227;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#21644;&#30417;&#27979;&#25968;&#25454;&#28418;&#31227;&#12290;&#35813;&#26694;&#26550;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;ML&#35774;&#22791;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#21644;&#24739;&#32773;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24448;&#24448;&#22312;&#25968;&#25454;&#20559;&#31163;&#20854;&#35757;&#32451;&#20998;&#24067;&#26102;&#22833;&#25928;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;ML&#35774;&#22791;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#28418;&#31227;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#21361;&#21450;&#24739;&#32773;&#23433;&#20840;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ML&#30340;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#65288;SPC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#21644;&#28418;&#31227;&#30417;&#27979;&#12290;SPC&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#30452;&#35266;&#22320;&#21644;&#32479;&#35745;&#19978;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#26399;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#25968;&#25454;&#28418;&#31227;&#30417;&#27979;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#19981;&#21516;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12289;&#28418;&#31227;&#37327;&#21270;&#21644;SPC&#21442;&#25968;&#36873;&#25321;&#31561;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;&#21306;&#20998;&#36724;&#21521;&#19982;&#38750;&#36724;&#21521;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#22270;&#20687;&#65307;2&#65289;&#23558;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#19982;&#20854;&#20182;&#27169;&#24577;&#36827;&#34892;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08086</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Text-centric Alignment for Multi-Modality Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21363;&#25512;&#29702;&#38454;&#27573;&#21487;&#29992;&#30340;&#27169;&#24577;&#19982;&#35757;&#32451;&#38454;&#27573;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;TAMML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#20511;&#21161;&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#31995;&#32479;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;TAMML&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;TAMML&#19981;&#20165;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#36824;&#33021;&#20445;&#25345;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20811;&#26381;&#20256;&#32479;&#30340;&#22266;&#23450;&#27169;&#24577;&#26694;&#26550;&#20013;&#30340;&#34920;&#31034;&#23884;&#20837;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availabili
&lt;/p&gt;</description></item><item><title>"&#20449;&#24687;&#32469;&#34892;"&#26159;&#19968;&#31181;&#29992;&#20110;&#23618;&#27425;&#24615;&#34920;&#24449;&#22270;&#20013;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.08085</link><description>&lt;p&gt;
&#20449;&#24687;&#32469;&#34892;&#65306;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#29992;&#20110;&#34920;&#36798;&#22270;&#23398;&#20064;&#30340;&#24490;&#29615;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08085
&lt;/p&gt;
&lt;p&gt;
"&#20449;&#24687;&#32469;&#34892;"&#26159;&#19968;&#31181;&#29992;&#20110;&#23618;&#27425;&#24615;&#34920;&#24449;&#22270;&#20013;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21270;&#23398;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#39640;&#38454;&#22270;&#24418;&#29305;&#24449;&#65292;&#22914;&#24490;&#29615;&#65292;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#12289;&#36793;&#39044;&#27979;&#21644;&#22270;&#20687;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#39640;&#38454;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#22312;&#35745;&#31639;&#19978;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#20449;&#24687;&#32469;&#34892;"&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#23618;&#27425;&#24615;&#22320;&#34920;&#24449;&#24490;&#29615;&#65292;&#21033;&#29992;&#27599;&#20010;&#22270;&#33410;&#28857;&#30456;&#20851;&#30340;&#19968;&#31995;&#21015;&#23616;&#37096;&#25299;&#25169;&#32467;&#26500;&#20013;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#32469;&#34892;&#26223;&#35266;&#20013;&#24471;&#21040;&#30340;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#20855;&#26377;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;&#38500;&#20102;&#19982;&#22270;&#26680;&#21644;&#20449;&#24687;&#30340;&#38598;&#25104;&#22806;
&lt;/p&gt;
&lt;p&gt;
Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node. The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with graph kernel and message
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.08082</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian probability distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08082
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#31361;&#30772;&#20102;&#32500;&#25968;&#28798;&#38590;&#65292;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#25968;&#23398;&#22522;&#30784;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SGMs&#22312;&#23398;&#20064;&#19968;&#20010;&#23376;&#39640;&#26031;&#27010;&#29575;&#20998;&#24067;&#26063;&#20013;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20851;&#20110;&#27010;&#29575;&#20998;&#24067;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#30456;&#23545;&#23494;&#24230;&#19982;&#26631;&#20934;&#39640;&#26031;&#27979;&#24230;&#30340;&#30456;&#23545;&#23494;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#23545;&#25968;&#30456;&#23545;&#23494;&#24230;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23616;&#37096;&#36924;&#36817;&#65292;&#24182;&#19988;&#32593;&#32476;&#21442;&#25968;&#21487;&#20197;&#36866;&#24403;&#22320;&#21463;&#38480;&#65292;&#37027;&#20040;&#36890;&#36807;&#32463;&#39564;&#20998;&#25968;&#21305;&#37197;&#29983;&#25104;&#30340;&#20998;&#24067;&#20197;&#32500;&#24230;&#26080;&#20851;&#30340;&#36895;&#29575;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#26576;&#20123;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#25512;&#23548;&#20986;&#19982;&#27491;&#21521;&#36807;&#31243;&#30456;&#20851;&#30340;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#30340;&#32500;&#24230;&#26080;&#20851;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While score-based generative models (SGMs) have achieved remarkable success in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of SGMs in learning a family of sub-Gaussian probability distributions. We introduce a notion of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is inte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#20851;&#38190;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.08078</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Agents in Two-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20004;&#20154;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#33021;&#22815;&#24471;&#21040;&#20851;&#38190;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#27491;&#24335;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#36890;&#24120;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65292;&#22312;&#25512;&#36827;LLM&#25216;&#26415;&#26041;&#38754;&#21487;&#20197;&#33719;&#24471;&#20851;&#38190;&#24615;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;LLM&#30340;&#35757;&#32451;&#26041;&#27861;&#19982;&#22312;&#21338;&#24328;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30740;&#31350;&#30340;&#20004;&#20154;&#28216;&#25103;&#20195;&#29702;&#24320;&#21457;&#25152;&#37319;&#29992;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#23398;&#20064;&#36807;&#31243;&#37325;&#26032;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#28216;&#25103;&#20013;&#30340;&#20195;&#29702;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26694;&#26550;&#25581;&#31034;&#20102;LLM&#24320;&#21457;&#20013;&#30340;&#25104;&#21151;&#19982;&#25361;&#25112;&#30340;&#21019;&#26032;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#23545;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#21644;&#20854;&#20182;&#25112;&#30053;&#32771;&#34385;&#30340;&#26032;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20004;&#20154;&#28216;&#25103;&#26041;&#27861;&#20026;&#35757;&#32451;LLMs&#25552;&#20379;&#20102;&#26032;&#30340;&#25968;&#25454;&#20934;&#22791;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#20989;&#25968;&#36827;&#34892;&#21516;&#32986;&#24230;&#37327;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#27010;&#29575;&#27979;&#24230;&#30340;&#20256;&#36755;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08077</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#21516;&#32986;&#24230;&#37327;&#21305;&#37197;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffeomorphic Measure Matching with Kernels for Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#20989;&#25968;&#36827;&#34892;&#21516;&#32986;&#24230;&#37327;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#23454;&#29616;&#20102;&#27010;&#29575;&#27979;&#24230;&#30340;&#20256;&#36755;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;(ODEs)&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHSs)&#23454;&#29616;&#27010;&#29575;&#27979;&#24230;&#30340;&#20256;&#36755;&#65292;&#20197;&#36798;&#21040;&#26368;&#23567;&#24046;&#24322;&#30340;&#29983;&#25104;&#24314;&#27169;&#21644;&#37319;&#26679;&#12290;&#35813;&#26694;&#26550;&#21463;&#21516;&#32986;&#21305;&#37197;&#21644;&#22270;&#20687;&#37197;&#20934;&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#25991;&#20013;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#20808;&#39564;&#35823;&#24046;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19982;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12289;&#35757;&#32451;&#38598;&#20013;&#26679;&#26412;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#35268;&#33539;&#24615;&#26377;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#29305;&#24615;&#12289;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#25193;&#23637;&#20102;&#20854;&#22312;&#26465;&#20214;&#27169;&#25311;&#21644;&#25512;&#26029;&#31561;&#20854;&#20182;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a general framework for the transport of probability measures towards minimum divergence generative modeling and sampling using ordinary differential equations (ODEs) and Reproducing Kernel Hilbert Spaces (RKHSs), inspired by ideas from diffeomorphic matching and image registration. A theoretical analysis of the proposed method is presented, giving a priori error bounds in terms of the complexity of the model, the number of samples in the training set, and model misspecification. An extensive suite of numerical experiments further highlights the properties, strengths, and weaknesses of the method and extends its applicability to other tasks, such as conditional simulation and inference.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lingo&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36866;&#24212;&#20102;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22810;&#26679;&#24615;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08075</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Scalable Fine-Tune of Language Models for Genome Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08075
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lingo&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36866;&#24212;&#20102;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22810;&#26679;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;DNA&#22522;&#22240;&#32452;&#27169;&#22411;&#22312;&#22522;&#22240;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22522;&#22240;&#32452;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#31181;&#38480;&#21046;&#19982;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#22312;&#26356;&#22823;&#35268;&#27169;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#22522;&#22240;&#29702;&#35299;&#28041;&#21450;&#35768;&#22810;&#20855;&#26377;&#22266;&#26377;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#19979;&#28216;&#22522;&#22240;&#32452;&#27880;&#37322;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#38024;&#23545;&#22522;&#22240;&#32452;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lingo&#65306;&#35821;&#35328;&#21069;&#32512;&#24494;&#35843;&#22522;&#22240;&#32452;&#27169;&#22411;&#12290;&#19982;DNA&#22522;&#22240;&#32452;&#27169;&#22411;&#19981;&#21516;&#65292;Lingo&#31574;&#30053;&#24615;&#22320;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#37325;&#26032;&#26657;&#20934;&#20854;&#22312;&#22522;&#22240;&#32452;&#24207;&#21015;&#26041;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#12290;Lingo&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36827;&#19968;&#27493;&#36866;&#24212;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#12289;&#24322;&#36136;&#30340;&#19979;&#28216;&#24494;&#35843;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21098;&#26525;&#24182;&#38543;&#26426;&#37325;&#26032;&#24341;&#20837;&#21098;&#26525;&#30340;&#22855;&#24322;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for \textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08073</link><description>&lt;p&gt;
&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#25903;&#25745;&#25968;&#25454;&#31185;&#23398;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Grounding Data Science Code Generation with Input-Output Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36755;&#20837;&#36755;&#20986;&#35268;&#33539;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#26102;&#19982;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;(NL)&#25552;&#31034;&#29983;&#25104;&#20195;&#30721;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;NL&#24448;&#24448;&#36807;&#20110;&#27169;&#31946;&#65292;&#26080;&#27861;&#25429;&#25417;&#32534;&#31243;&#38382;&#39064;&#32972;&#21518;&#30340;&#30495;&#23454;&#24847;&#22270;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#36755;&#20837;&#36755;&#20986;(I/O)&#35268;&#33539;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#21487;&#33021;&#38590;&#20197;&#23558;&#20854;&#36755;&#20986;&#19982;NL&#25552;&#31034;&#21644;I/O&#35268;&#33539;&#23545;&#40784;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#25968;&#25454;&#31185;&#23398;&#32534;&#31243;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#20219;&#21153;&#38656;&#35201;&#26126;&#30830;&#30340;I/O&#35268;&#33539;&#20197;&#20445;&#35777;&#28165;&#26224;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIFT4Code&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;I/O&#35268;&#33539;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;LLM&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#26412;&#36523;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#27966;&#29983;&#30340;&#21453;&#39304;&#20316;&#20026;&#20851;&#38190;&#30340;&#23398;&#20064;&#20449;&#21495;&#12290;&#23558;&#35813;&#21453;&#39304;&#20197;&#31243;&#24207;I/O&#35268;&#33539;&#30340;&#24418;&#24335;&#25552;&#20379;&#32473;LLM&#20197;&#20419;&#36827;&#25351;&#23548;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated a remarkable ability to generate code from natural language (NL) prompts. However, in the real world, NL is often too ambiguous to capture the true intent behind programming problems, requiring additional input-output (I/O) specifications. Unfortunately, LLMs can have difficulty aligning their outputs with both the NL prompt and the I/O specification. In this paper, we give a way to mitigate this issue in the context of data science programming, where tasks require explicit I/O specifications for clarity. Specifically, we propose GIFT4Code, a novel approach for the instruction fine-tuning of LLMs with respect to I/O specifications. Our method leverages synthetic data produced by the LLM itself and utilizes execution-derived feedback as a key learning signal. This feedback, in the form of program I/O specifications, is provided to the LLM to facilitate instruction fine-tuning. We evaluated our approach on two challenging data scien
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#20316;&#20026;&#32593;&#32476;&#27969;&#37327;&#25351;&#32441;&#25216;&#26415;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#29305;&#24449;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#23545;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24377;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08063</link><description>&lt;p&gt;
&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#25351;&#32441;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Hashing for Network Traffic Fingerprinting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#20316;&#20026;&#32593;&#32476;&#27969;&#37327;&#25351;&#32441;&#25216;&#26415;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#29305;&#24449;&#36873;&#25321;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20379;&#23545;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#20986;&#29616;&#32473;&#35745;&#31639;&#26426;&#32593;&#32476;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#21644;&#22256;&#38590;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#35774;&#22791;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#36825;&#20123;&#35774;&#22791;&#20197;&#36827;&#34892;&#32593;&#32476;&#31649;&#29702;&#21644;&#35782;&#21035;&#20219;&#20309;&#26377;&#23475;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#32593;&#32476;&#27969;&#37327;&#25351;&#32441;&#26159;&#35782;&#21035;&#35774;&#22791;&#21644;&#26816;&#27979;&#24322;&#24120;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#30446;&#21069;&#65292;&#36825;&#26041;&#38754;&#30340;&#20027;&#35201;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#36873;&#25321;&#29305;&#24449;&#12289;&#35843;&#25972;&#36229;&#21442;&#25968;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26469;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#20026;&#32593;&#32476;&#20013;&#26816;&#27979;&#21040;&#30340;&#27010;&#24565;&#28418;&#31227;&#25552;&#20379;&#24377;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#30340;&#32593;&#32476;&#27969;&#37327;&#25351;&#32441;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#30740;&#31350;Nilsimsa LSH&#20989;&#25968;&#30340;&#20960;&#31181;&#35774;&#35745;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of the Internet of Things (IoT) has brought forth additional intricacies and difficulties to computer networks. These gadgets are particularly susceptible to cyber-attacks because of their simplistic design. Therefore, it is crucial to recognise these devices inside a network for the purpose of network administration and to identify any harmful actions. Network traffic fingerprinting is a crucial technique for identifying devices and detecting anomalies. Currently, the predominant methods for this depend heavily on machine learning (ML). Nevertheless, machine learning (ML) methods need the selection of features, adjustment of hyperparameters, and retraining of models to attain optimal outcomes and provide resilience to concept drifts detected in a network. In this research, we suggest using locality-sensitive hashing (LSH) for network traffic fingerprinting as a solution to these difficulties. Our study focuses on examining several design options for the Nilsimsa LSH functio
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>MIML&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#21644;&#28789;&#27963;&#30340;Java&#36719;&#20214;&#24211;&#65292;&#25552;&#20379;&#20102;43&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#36890;&#36807;XML&#37197;&#32622;&#25991;&#20214;&#25191;&#34892;&#65292;&#36824;&#21253;&#25324;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#21306;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#24615;&#33021;&#35780;&#20272;&#31561;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08056</link><description>&lt;p&gt;
MIML&#24211;&#65306;&#29992;&#20110;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#21644;&#28789;&#27963;&#24211;
&lt;/p&gt;
&lt;p&gt;
MIML library: a Modular and Flexible Library for Multi-instance Multi-label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08056
&lt;/p&gt;
&lt;p&gt;
MIML&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#21644;&#28789;&#27963;&#30340;Java&#36719;&#20214;&#24211;&#65292;&#25552;&#20379;&#20102;43&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#36890;&#36807;XML&#37197;&#32622;&#25991;&#20214;&#25191;&#34892;&#65292;&#36824;&#21253;&#25324;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#21306;&#12289;&#20132;&#21449;&#39564;&#35777;&#21644;&#24615;&#33021;&#35780;&#20272;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MIML&#24211;&#26159;&#19968;&#20010;Java&#36719;&#20214;&#24037;&#20855;&#65292;&#29992;&#20110;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#27604;&#36739;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#12290;&#35813;&#24211;&#21253;&#25324;43&#31181;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#29305;&#23450;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#20998;&#21306;&#12289;&#20445;&#30041;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#24615;&#33021;&#35780;&#20272;&#30340;&#26631;&#20934;&#25351;&#26631;&#21644;&#25253;&#21578;&#29983;&#25104;&#30340;&#21151;&#33021;&#21644;&#26684;&#24335;&#12290;&#27492;&#22806;&#65292;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;XML&#37197;&#32622;&#25991;&#20214;&#25191;&#34892;&#65292;&#26080;&#38656;&#32534;&#31243;&#12290;&#23427;&#26159;&#24179;&#21488;&#26080;&#20851;&#12289;&#21487;&#25193;&#23637;&#12289;&#20813;&#36153;&#12289;&#24320;&#28304;&#65292;&#24182;&#22312;GNU General Public License&#19979;&#22312;GitHub&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
MIML library is a Java software tool to develop, test, and compare classification algorithms for multi-instance multi-label (MIML) learning. The library includes 43 algorithms and provides a specific format and facilities for data managing and partitioning, holdout and cross-validation methods, standard metrics for performance evaluation, and generation of reports. In addition, algorithms can be executed through $xml$ configuration files without needing to program. It is platform-independent, extensible, free, open-source, and available on GitHub under the GNU General Public License.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#35775;&#35848;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21161;&#25163;&#22312;&#36719;&#20214;&#24110;&#21161;&#23547;&#27714;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20248;&#21270;&#21518;&#30340;LLM&#21161;&#25163;&#30456;&#36739;&#20110;&#22522;&#20934;LLM&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;&#20110;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08030</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;LLM&#21161;&#25163;&#21487;&#33021;&#20986;&#38169;&#65306;&#25506;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#20132;&#20114;&#23545;&#36719;&#20214;&#23547;&#27714;&#24110;&#21161;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#35775;&#35848;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21161;&#25163;&#22312;&#36719;&#20214;&#24110;&#21161;&#23547;&#27714;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20248;&#21270;&#21518;&#30340;LLM&#21161;&#25163;&#30456;&#36739;&#20110;&#22522;&#20934;LLM&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;&#20110;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21161;&#25163;&#65292;&#22914;ChatGPT&#65292;&#24050;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#22312;&#22797;&#26434;&#30340;&#12289;&#21151;&#33021;&#20016;&#23500;&#30340;&#36719;&#20214;&#20013;&#23548;&#33322;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#27861;&#12290;LLM&#20351;&#29992;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25991;&#26412;&#12289;&#36719;&#20214;&#25163;&#20876;&#21644;&#20195;&#30721;&#24211;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#27169;&#25311;&#20154;&#31867;&#33324;&#30340;&#20132;&#20114;&#65292;&#25552;&#20379;&#37327;&#36523;&#23450;&#21046;&#30340;&#24110;&#21161;&#65292;&#21253;&#25324;&#36880;&#27493;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;16&#21517;&#21442;&#19982;&#32773;&#30340;&#34987;&#35797;&#23454;&#39564;&#21644;&#21518;&#32493;&#35775;&#35848;&#26469;&#35843;&#26597;LLM&#29983;&#25104;&#30340;&#36719;&#20214;&#25351;&#23548;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#32447;LLM&#21161;&#25163;&#21644;&#38024;&#23545;&#29305;&#23450;&#36719;&#20214;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#30340;LLM&#65292;&#21363;SoftAIBot&#65292;&#21518;&#32773;&#36824;&#25552;&#20379;&#20102;&#26500;&#24314;&#36866;&#24403;&#25552;&#31034;&#30340;&#25351;&#21335;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#24863;&#30693;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;SoftAIBot&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;LLM&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#22823;&#22810;&#25968;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to u
&lt;/p&gt;</description></item><item><title>UGMAE&#26159;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#12289;&#22270;&#20687;&#20449;&#24687;&#21033;&#29992;&#12289;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#37325;&#26500;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08023</link><description>&lt;p&gt;
UGMAE&#65306;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#29992;&#20110;&#22270;&#24418;Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
UGMAE: A Unified Framework for Graph Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08023
&lt;/p&gt;
&lt;p&gt;
UGMAE&#26159;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#12289;&#22270;&#20687;&#20449;&#24687;&#21033;&#29992;&#12289;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#37325;&#26500;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#65306;1&#65289;&#22312;&#25513;&#27169;&#20013;&#24573;&#35270;&#19981;&#22343;&#21248;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#65292;2&#65289;&#23545;&#25972;&#20307;&#22270;&#20687;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#65292;3&#65289;&#30001;&#20110;&#20165;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#26500;&#25439;&#22833;&#65292;&#24573;&#35270;&#20102;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#20197;&#21450;4&#65289;&#30001;&#20110;&#36974;&#34109;&#20869;&#23481;&#30340;&#22823;&#37327;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UGMAE&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#22270;&#24418;Masked Autoencoders&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20174;&#36866;&#24212;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35282;&#24230;&#26469;&#32771;&#34385;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#29305;&#24449;&#36974;&#32617;&#29983;&#25104;&#22120;&#65292;&#20197;&#32771;&#34385;&#33410;&#28857;&#30340;&#29420;&#29305;&#37325;&#35201;&#24615;&#24182;&#37319;&#26679;&#20449;&#24687;&#20016;&#23500;&#30340;&#36974;&#32617;&#65288;&#36866;&#24212;&#24615;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25490;&#21517;&#30340;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#22270;&#24418;&#30340;&#23436;&#25972;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65288;&#23436;&#25972;&#24615;&#21644;&#20114;&#34917;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#30340;&#20256;&#32479;Q-Learning&#31639;&#27861;&#38754;&#20020;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#30340;&#26032;&#39062;&#30340;&#38598;&#25104;Q-Learning&#31639;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#26080;&#32447;&#32593;&#32476;&#20013;&#21033;&#29992;&#25968;&#23383;&#22530;&#20804;&#26469;&#36924;&#36817;&#21487;&#35266;&#27979;&#26080;&#32447;&#32593;&#32476;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.08022</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26080;&#32447;&#32593;&#32476;&#20013;&#21033;&#29992;&#25968;&#23383;&#22530;&#20804;&#26469;&#36827;&#34892;&#38598;&#25104;Q-Learning
&lt;/p&gt;
&lt;p&gt;
Leveraging Digital Cousins for Ensemble Q-Learning in Large-Scale Wireless Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#30340;&#20256;&#32479;Q-Learning&#31639;&#27861;&#38754;&#20020;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#30340;&#26032;&#39062;&#30340;&#38598;&#25104;Q-Learning&#31639;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#26080;&#32447;&#32593;&#32476;&#20013;&#21033;&#29992;&#25968;&#23383;&#22530;&#20804;&#26469;&#36924;&#36817;&#21487;&#35266;&#27979;&#26080;&#32447;&#32593;&#32476;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#26080;&#32447;&#32593;&#32476;&#65292;&#21253;&#25324;&#26368;&#20248;&#36164;&#28304;&#31649;&#29702;&#12289;&#21151;&#29575;&#20998;&#37197;&#21644;&#21534;&#21520;&#37327;&#26368;&#22823;&#21270;&#65292;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38750;&#21487;&#35266;&#27979;&#30340;&#31995;&#32479;&#21160;&#24577;&#21644;&#24322;&#26500;&#19988;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;Q-Learning&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Q-Learning&#31639;&#27861;&#20248;&#21270;&#26080;&#32447;&#32593;&#32476;&#26102;&#38754;&#20020;&#30340;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#25361;&#25112;&#12290;&#36890;&#36807;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#38598;&#25104;&#23398;&#20064;&#65292;&#36890;&#36807;&#26032;&#27169;&#22411;&#36924;&#36817;&#22823;&#29366;&#24577;&#31354;&#38388;&#21487;&#35266;&#27979;&#26080;&#32447;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#25552;&#20986;&#20102;&#25968;&#23383;&#22530;&#20804;&#20316;&#20026;&#20256;&#32479;&#25968;&#23383;&#23402;&#29983;&#27010;&#24565;&#30340;&#24310;&#20280;&#65292;&#20854;&#20013;&#22312;&#22810;&#20010;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19978;&#36816;&#34892;&#22810;&#20010;Q-Learning&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#34701;&#21512;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;Q&#20989;&#25968;&#12290;&#25552;&#20379;&#20102;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;Q&#20989;&#25968;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#20197;&#21450;&#23545;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#19978;&#38480;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing large-scale wireless networks, including optimal resource management, power allocation, and throughput maximization, is inherently challenging due to their non-observable system dynamics and heterogeneous and complex nature. Herein, a novel ensemble Q-learning algorithm that addresses the performance and complexity challenges of the traditional Q-learning algorithm for optimizing wireless networks is presented. Ensemble learning with synthetic Markov Decision Processes is tailored to wireless networks via new models for approximating large state-space observable wireless networks. In particular, digital cousins are proposed as an extension of the traditional digital twin concept wherein multiple Q-learning algorithms on multiple synthetic Markovian environments are run in parallel and their outputs are fused into a single Q-function. Convergence analyses of key statistics and Q-functions and derivations of upper bounds on the estimation bias and variance are provided. Numeri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08018</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour Score Estimators for Diffusion Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#26159;&#35757;&#32451;&#21644;&#37319;&#26679;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#24120;&#29992;&#30340;&#20272;&#35745;&#22120;&#35201;&#20040;&#26159;&#26377;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65292;&#35201;&#20040;&#26159;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#30340;&#39640;&#26041;&#24046;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#20013;&#21033;&#29992;&#20102;&#20302;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26367;&#20195;&#23398;&#20064;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#27969;ODE&#31215;&#20998;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26377;&#21069;&#26223;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08017</link><description>&lt;p&gt;
Lumos : &#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#22686;&#24378;&#22810;&#27169;&#24335;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Lumos : Empowering Multimodal LLMs with Scene Text Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36816;&#29992;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#32452;&#20214;&#65292;&#33021;&#22815;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21152;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#20316;&#32773;&#20811;&#26381;&#20102;&#19982;&#25991;&#26412;&#35782;&#21035;&#36136;&#37327;&#12289;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#22810;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#32452;&#20214;&#35780;&#20272;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Lumos&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#22791;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#30340;&#31471;&#21040;&#31471;&#22810;&#27169;&#24335;&#38382;&#31572;&#31995;&#32479;&#12290;Lumos&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22330;&#26223;&#25991;&#26412;&#35782;&#21035;&#65288;STR&#65289;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#22686;&#24378;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#30340;&#36755;&#20837;&#12290;&#22312;&#26500;&#24314;Lumos&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#35768;&#22810;&#19982;STR&#36136;&#37327;&#12289;&#25972;&#20307;&#24310;&#36831;&#21644;&#27169;&#22411;&#25512;&#26029;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31995;&#32479;&#26550;&#26500;&#12289;&#35774;&#35745;&#36873;&#25321;&#21644;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#20010;&#32452;&#20214;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#29575;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.08012</link><description>&lt;p&gt;
&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Online Differentially Private Synthetic Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#23545;&#20110;&#22312;&#36229;&#31435;&#26041;&#20307;$[0,1]^d$&#20869;&#30340;&#25968;&#25454;&#27969;&#21644;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#27599;&#20010;&#26102;&#38388;$t$&#37117;&#29983;&#25104;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;1-Wasserstein&#36317;&#31163;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65306;&#24403;$d\geq 2$&#26102;&#20026;$O(t^{-1/d}\log(t)$&#65292;&#24403;$d=1$&#26102;&#20026;$O(t^{-1}\log^{4.5}(t)$&#12290;&#36825;&#20010;&#32467;&#26524;&#23558;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#25512;&#24191;&#21040;&#21253;&#25324;Lipschitz&#26597;&#35810;&#12290;&#19982;&#31163;&#32447;&#24773;&#20917;&#19981;&#21516;&#65292;&#31163;&#32447;&#24773;&#20917;&#19979;&#25972;&#20010;&#25968;&#25454;&#38598;&#19968;&#27425;&#24615;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#31934;&#24230;&#30028;&#38480;&#20013;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$ and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;</title><link>https://arxiv.org/abs/2402.08010</link><description>&lt;p&gt;
CNN&#38656;&#35201;&#21738;&#20123;&#39057;&#29575;&#65311;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#29942;&#39048;&#32467;&#26500;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#20351;&#29992;&#20854;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#20165;&#22312;&#20960;&#20010;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#21518;&#20960;&#23618;&#23558;&#20854;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;CBN&#31209;&#65292;&#25551;&#36848;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20869;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35777;&#26126;&#20102;&#34920;&#31034;&#20989;&#25968;$f$&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#25353;&#28145;&#24230;&#20056;&#20197;CBN&#31209;$f$&#30340;&#27604;&#20363;&#32553;&#25918;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#25968;&#33539;&#25968;&#22312;&#19979;&#19968;&#38454;&#20013;&#20381;&#36182;&#20110;$f$&#30340;&#27491;&#21017;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#22312;&#26435;&#37325;&#21644;&#65288;&#22312;&#32593;&#32476;&#23545;&#22823;&#23398;&#20064;&#29575;&#31283;&#23450;&#30340;&#20551;&#35774;&#19979;&#65289;&#28608;&#27963;&#20013;&#34920;&#29616;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#20419;&#20351;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#20570;&#27861;&#65307;&#24182;&#19988;&#25105;&#20204;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...
&lt;/p&gt;
&lt;p&gt;
We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#21644;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#29983;LLM&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;rDPO&#22312;&#22810;&#20010;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08005</link><description>&lt;p&gt;
&#24102;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#25913;&#36827;&#22411;&#30452;&#25509;&#20248;&#21270;&#27861;&#29992;&#20110;LLM&#30340;&#34892;&#20026;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#21644;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#29983;LLM&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;rDPO&#22312;&#22810;&#20010;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#22411;&#30452;&#25509;&#20248;&#21270;&#27861;&#65288;rDPO&#65289;&#65292;&#29992;&#20110;&#25913;&#21892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#35843;&#25972;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#25105;&#35780;&#35770;&#25552;&#31034;&#25945;&#24072;LLM&#26469;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#21033;&#29992;&#24191;&#20041;DPO&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#32431;&#32473;&#23398;&#29983;LLM&#12290;&#25439;&#22833;&#20989;&#25968;&#32467;&#21512;&#20102;&#39069;&#22806;&#30340;&#22806;&#37096;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20351;rDPO&#33021;&#22815;&#25269;&#25239;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#30340;&#28508;&#22312;&#22122;&#22768;&#12290;rDPO&#22312;&#22810;&#31181;&#34892;&#20026;&#35843;&#25972;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#25928;&#26524;&#65292;&#22914;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#25269;&#25239;&#35282;&#33394;&#25198;&#28436;&#65292;&#38477;&#20302;&#24052;&#32467;&#34892;&#20026;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/vicgalle/refined-dpo&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce \emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;ABCD&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25551;&#36848;&#20449;&#21495;&#21644;&#32972;&#26223;&#30340;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19981;&#21516;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#27979;&#37327;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08001</link><description>&lt;p&gt;
&#25913;&#36827;&#24182;&#25512;&#24191;&#20855;&#26377;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;ABCD&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improvement and generalization of ABCD method with Bayesian inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08001
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;ABCD&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25551;&#36848;&#20449;&#21495;&#21644;&#32972;&#26223;&#30340;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19981;&#21516;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#27979;&#37327;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LHC&#19978;&#23547;&#25214;&#26032;&#29289;&#29702;&#25110;&#31934;&#30830;&#25105;&#20204;&#23545;&#26631;&#20934;&#27169;&#22411;&#30340;&#35748;&#35782;&#26159;&#19968;&#20010;&#28041;&#21450;&#22810;&#20010;&#22240;&#32032;&#30340;&#20225;&#19994;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#29616;&#26377;&#20449;&#24687;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#24120;&#35268;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;ABCD&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#25913;&#36827;&#21644;&#25512;&#24191;&#23427;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#26469;&#25551;&#36848;&#30001;&#20449;&#21495;&#21644;&#22810;&#20010;&#32972;&#26223;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#25552;&#21462;&#20986;&#26679;&#26412;&#20013;&#30340;&#20449;&#21495;&#12289;&#32972;&#26223;&#21450;&#20854;&#30456;&#23545;&#20998;&#25968;&#12290;&#19982;ABCD&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23545;&#19981;&#21516;&#32972;&#26223;&#30340;&#26576;&#20123;&#23646;&#24615;&#30340;&#29702;&#35299;&#20197;&#21450;&#27599;&#20010;&#20107;&#20214;&#20013;&#38656;&#35201;&#27979;&#37327;&#30340;&#36229;&#36807;&#20004;&#20010;&#29420;&#31435;&#21487;&#35266;&#27979;&#37327;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#36125;&#21494;&#26031;&#26694;&#26550;&#20351;&#29992;&#36830;&#32493;&#20998;&#24067;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#30828;&#20999;&#21106;&#23450;&#20041;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
To find New Physics or to refine our knowledge of the Standard Model at the LHC is an enterprise that involves many factors. We focus on taking advantage of available information and pour our effort in re-thinking the usual data-driven ABCD method to improve it and to generalize it using Bayesian Machine Learning tools. We propose that a dataset consisting of a signal and many backgrounds is well described through a mixture model. Signal, backgrounds and their relative fractions in the sample can be well extracted by exploiting the prior knowledge and the dependence between the different observables at the event-by-event level with Bayesian tools. We show how, in contrast to the ABCD method, one can take advantage of understanding some properties of the different backgrounds and of having more than two independent observables to measure in each event. In addition, instead of regions defined through hard cuts, the Bayesian framework uses the information of continuous distribution to obt
&lt;/p&gt;</description></item><item><title>NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07999</link><description>&lt;p&gt;
NetInfoF &#26694;&#26550;&#65306;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
NetInfoF Framework: Measuring and Exploiting Network Usable Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07999
&lt;/p&gt;
&lt;p&gt;
NetInfoF&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#24230;&#37327;&#21644;&#21033;&#29992;&#33410;&#28857;&#23646;&#24615;&#22270;&#20013;&#30340;&#21487;&#21033;&#29992;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#33021;&#21516;&#26102;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#33410;&#28857;&#23646;&#24615;&#22270;&#21644;&#19968;&#20010;&#22270;&#20219;&#21153;&#65288;&#38142;&#36335;&#39044;&#27979;&#25110;&#33410;&#28857;&#20998;&#31867;&#65289;&#65292;&#25105;&#20204;&#33021;&#21542;&#21028;&#26029;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#21542;&#33021;&#24456;&#22909;&#22320;&#23436;&#25104;&#20219;&#21153;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#26159;&#21542;&#21253;&#21547;&#36275;&#22815;&#21487;&#21033;&#29992;&#30340;&#20449;&#24687;&#26469;&#23436;&#25104;&#20219;&#21153;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65288;1&#65289;&#24320;&#21457;&#19968;&#20010;&#24555;&#36895;&#24037;&#20855;&#26469;&#24230;&#37327;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26681;&#25454;&#20449;&#24687;&#37327;&#26469;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NetInfoF&#26694;&#26550;&#65292;&#21253;&#25324;NetInfoF_Probe&#21644;NetInfoF_Act&#20004;&#20010;&#37096;&#20998;&#65292;&#20998;&#21035;&#29992;&#20110;&#24230;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#21487;&#21033;&#29992;&#20449;&#24687;&#65288;NUI&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#22270;&#25968;&#25454;&#65292;NetInfoF_Probe&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24230;&#37327;NUI&#65292;&#32780;NetInfoF_Act&#21017;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#20004;&#20010;&#27169;&#22359;&#20849;&#20139;&#30456;&#21516;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#24635;&#20043;&#65292;NetInfoF&#20855;&#26377;&#20197;&#19979;&#26174;&#33879;&#20248;&#28857;&#65306;&#65288;a&#65289;&#36890;&#29992;&#24615;&#65292;&#33021;&#22788;&#29702;&#38142;&#36335;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#20004;&#31181;&#20219;&#21153;&#65307;&#65288;b&#65289;&#21407;&#21017;&#24615;&#65292;&#20855;&#22791;&#29702;&#35770;&#20445;&#35777;&#21644;&#38381;&#24335;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20302;&#32500;&#21270;&#23398;&#23884;&#20837;&#32467;&#21512;k-d&#26641;&#25968;&#25454;&#32467;&#26500;&#26159;&#21542;&#33021;&#22815;&#22312;&#20445;&#25345;&#23545;&#26631;&#20934;&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;&#22522;&#20934;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#26368;&#36817;&#37051;&#26597;&#35810;</title><link>https://arxiv.org/abs/2402.07970</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#32500;&#20998;&#23376;&#23884;&#20837;&#36827;&#34892;&#24555;&#36895;&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Utilizing Low-Dimensional Molecular Embeddings for Rapid Chemical Similarity Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20302;&#32500;&#21270;&#23398;&#23884;&#20837;&#32467;&#21512;k-d&#26641;&#25968;&#25454;&#32467;&#26500;&#26159;&#21542;&#33021;&#22815;&#22312;&#20445;&#25345;&#23545;&#26631;&#20934;&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;&#22522;&#20934;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#26368;&#36817;&#37051;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#30456;&#20284;&#24615;&#25628;&#32034;&#26159;&#21270;&#23398;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#31561;&#39046;&#22495;&#26377;&#30528;&#26174;&#33879;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#19968;&#20123;&#24120;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#34542;&#21147;&#25628;&#32034;&#30340;&#26041;&#24335;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#36896;&#25104;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#26102;&#38388;&#28040;&#32791;&#22823;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#29616;&#20195;&#21270;&#23398;&#25968;&#25454;&#24211;&#30340;&#35268;&#27169;&#20043;&#22823;&#12290;&#20197;&#24448;&#23545;&#36825;&#19968;&#20219;&#21153;&#30340;&#35745;&#31639;&#25216;&#26415;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;&#30828;&#20214;&#25913;&#36827;&#25110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#25216;&#24039;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#32780;&#21033;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#30340;&#26041;&#27861;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#31867;&#31639;&#27861;&#37117;&#26159;&#36817;&#20284;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#32773;&#22312;&#20856;&#22411;&#30340;&#39640;&#32500;&#21270;&#23398;&#23884;&#20837;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20302;&#32500;&#21270;&#23398;&#23884;&#20837;&#32467;&#21512;k-d&#26641;&#25968;&#25454;&#32467;&#26500;&#26159;&#21542;&#33021;&#22815;&#22312;&#20445;&#25345;&#23545;&#26631;&#20934;&#21270;&#23398;&#30456;&#20284;&#24615;&#25628;&#32034;&#22522;&#20934;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#26368;&#36817;&#37051;&#26597;&#35810;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#38477;&#32500;&#26041;&#27861;&#36827;
&lt;/p&gt;
&lt;p&gt;
Nearest neighbor-based similarity searching is a common task in chemistry, with notable use cases in drug discovery. Yet, some of the most commonly used approaches for this task still leverage a brute-force approach. In practice this can be computationally costly and overly time-consuming, due in part to the sheer size of modern chemical databases. Previous computational advancements for this task have generally relied on improvements to hardware or dataset-specific tricks that lack generalizability. Approaches that leverage lower-complexity searching algorithms remain relatively underexplored. However, many of these algorithms are approximate solutions and/or struggle with typical high-dimensional chemical embeddings. Here we evaluate whether a combination of low-dimensional chemical embeddings and a k-d tree data structure can achieve fast nearest neighbor queries while maintaining performance on standard chemical similarity search benchmarks. We examine different dimensionality redu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>ProtIR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#20266;&#20284;&#28982;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21151;&#33021;&#39044;&#27979;&#22120;&#21644;&#26816;&#32034;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#21151;&#33021;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#27809;&#26377;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#19982;&#25110;&#32988;&#36807;&#39044;&#27979;&#22120;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07955</link><description>&lt;p&gt;
ProtIR&#65306;&#29992;&#20110;&#34507;&#30333;&#36136;&#21151;&#33021;&#27880;&#37322;&#30340;&#26816;&#32034;&#22120;&#21644;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#36845;&#20195;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
ProtIR: Iterative Refinement between Retrievers and Predictors for Protein Function Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07955
&lt;/p&gt;
&lt;p&gt;
ProtIR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#20266;&#20284;&#28982;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#21151;&#33021;&#39044;&#27979;&#22120;&#21644;&#26816;&#32034;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#21151;&#33021;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#27809;&#26377;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#19982;&#25110;&#32988;&#36807;&#39044;&#27979;&#22120;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#21151;&#33021;&#27880;&#37322;&#26159;&#29983;&#29289;&#23398;&#20013;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#23637;&#26174;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#30340;&#21151;&#33021;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#34507;&#30333;&#36136;&#30456;&#20284;&#24615;&#24314;&#27169;&#65292;&#36825;&#26159;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#20351;&#29992;&#24207;&#21015;&#25110;&#32467;&#26500;&#26816;&#32034;&#24037;&#20855;&#24120;&#29992;&#30340;&#24605;&#24819;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#34507;&#30333;&#36136;&#21151;&#33021;&#27880;&#37322;&#20219;&#21153;&#19978;&#23545;&#27604;&#26816;&#32034;&#22120;&#21644;&#39044;&#27979;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#32034;&#22120;&#21487;&#20197;&#22312;&#27809;&#26377;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#19982;&#25110;&#32988;&#36807;&#39044;&#27979;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21464;&#20998;&#20266;&#20284;&#28982;&#26694;&#26550;ProtIR&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#30456;&#20284;&#24615;&#24314;&#27169;&#26469;&#25913;&#36827;&#21151;&#33021;&#39044;&#27979;&#22120;&#12290;&#35813;&#26694;&#26550;&#22312;&#21151;&#33021;&#39044;&#27979;&#22120;&#21644;&#26816;&#32034;&#22120;&#20043;&#38388;&#36845;&#20195;&#22320;&#20248;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein function annotation is an important yet challenging task in biology. Recent deep learning advancements show significant potential for accurate function prediction by learning from protein sequences and structures. Nevertheless, these predictor-based methods often overlook the modeling of protein similarity, an idea commonly employed in traditional approaches using sequence or structure retrieval tools. To fill this gap, we first study the effect of inter-protein similarity modeling by benchmarking retriever-based methods against predictors on protein function annotation tasks. Our results show that retrievers can match or outperform predictors without large-scale pre-training. Building on these insights, we introduce a novel variational pseudo-likelihood framework, ProtIR, designed to improve function predictors by incorporating inter-protein similarity modeling. This framework iteratively refines knowledge between a function predictor and retriever, thereby combining the stren
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#27835;&#30103;&#31574;&#30053;&#65292;&#20943;&#23569;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#20559;&#24046;&#65292;&#24182;&#19988;&#38477;&#20302;&#27880;&#23556;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07949</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#35774;&#35745;&#20197;&#25913;&#21892;&#31958;&#23615;&#30149;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07949
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#27835;&#30103;&#31574;&#30053;&#65292;&#20943;&#23569;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#20559;&#24046;&#65292;&#24182;&#19988;&#38477;&#20302;&#27880;&#23556;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#24930;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#32654;&#22269;&#22659;&#20869;&#26377;3800&#19975;&#20154;&#65292;&#23427;&#20250;&#24433;&#21709;&#36523;&#20307;&#23558;&#39135;&#29289;&#36716;&#21270;&#20026;&#33021;&#37327;&#65288;&#21363;&#34880;&#31958;&#65289;&#30340;&#33021;&#21147;&#12290;&#26631;&#20934;&#30340;&#27835;&#30103;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#33008;&#33146;&#65292;&#21363;&#25345;&#32493;&#33008;&#23707;&#32032;&#27893;&#65288;&#22522;&#30784;&#27880;&#23556;&#65289;&#65292;&#20197;&#21450;&#23450;&#26399;&#27880;&#23556;&#33008;&#23707;&#32032;&#65288;&#31361;&#21457;&#27880;&#23556;&#65289;&#26469;&#34917;&#20805;&#30899;&#27700;&#21270;&#21512;&#29289;&#25668;&#20837;&#37327;&#12290;&#27835;&#30103;&#30446;&#26631;&#26159;&#23558;&#34880;&#31958;&#20445;&#25345;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#30340;&#20013;&#24515;&#20301;&#32622;&#65292;&#36890;&#36807;&#25345;&#32493;&#34880;&#31958;&#27979;&#37327;&#26469;&#36827;&#34892;&#34913;&#37327;&#12290;&#27425;&#35201;&#30446;&#26631;&#26159;&#20943;&#23569;&#27880;&#23556;&#27425;&#25968;&#65292;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#26469;&#35828;&#27880;&#23556;&#26159;&#19981;&#24841;&#24555;&#19988;&#38590;&#20197;&#23454;&#26045;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#36827;&#21270;&#26469;&#21457;&#29616;&#27835;&#30103;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22522;&#20110;30&#22825;&#30340;&#27835;&#30103;&#21644;&#21333;&#20010;&#24739;&#32773;&#30340;&#27979;&#37327;&#25968;&#25454;&#38598;&#65292;&#39318;&#20808;&#35757;&#32451;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;&#28982;&#21518;&#36890;&#36807;&#36827;&#21270;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#25351;&#23450;&#30899;&#27700;&#21270;&#21512;&#29289;&#25668;&#20837;&#37327;&#12289;&#22522;&#30784;&#27880;&#23556;&#27700;&#24179;&#21644;&#31361;&#21457;&#27880;&#23556;&#12290;&#36827;&#21270;&#21457;&#29616;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20943;&#23569;&#20102;&#19982;&#30446;&#26631;&#20540;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes, a chronic condition that impairs how the body turns food into energy, i.e. blood glucose, affects 38 million people in the US alone. The standard treatment is to supplement carbohydrate intake with an artificial pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional insulin injections (bolus shots). The goal of the treatment is to keep blood glucose at the center of an acceptable range, as measured through a continuous glucose meter. A secondary goal is to minimize injections, which are unpleasant and difficult for some patients to implement. In this study, neuroevolution was used to discover an optimal strategy for the treatment. Based on a dataset of 30 days of treatment and measurements of a single patient, a random forest was first trained to predict future glucose levels. A neural network was then evolved to prescribe carbohydrates, basal pumping levels, and bolus injections. Evolution discovered a Pareto front that reduced deviation from the targe
&lt;/p&gt;</description></item><item><title>evolSOM&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;R&#21253;&#65292;&#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;( SOMs)&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#29983;&#29289;&#21464;&#37327;&#20445;&#23432;&#24615;&#65292;&#20197;&#20415;&#26356;&#23481;&#26131;&#22320;&#25972;&#21512;&#34920;&#22411;&#21644;&#22522;&#22240;&#22411;&#23646;&#24615;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#29289;&#31181;&#25110;&#26465;&#20214;&#20043;&#38388;&#30340;&#29983;&#29289;&#21464;&#37327;&#20301;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.07948</link><description>&lt;p&gt;
evolSOM:&#19968;&#31181;&#29992;&#20110;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#36827;&#21270;&#20445;&#23432;&#24615;&#20998;&#26512;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
evolSOM: an R Package for evolutionary conservation analysis with SOMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07948
&lt;/p&gt;
&lt;p&gt;
evolSOM&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;R&#21253;&#65292;&#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;( SOMs)&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#29983;&#29289;&#21464;&#37327;&#20445;&#23432;&#24615;&#65292;&#20197;&#20415;&#26356;&#23481;&#26131;&#22320;&#25972;&#21512;&#34920;&#22411;&#21644;&#22522;&#22240;&#22411;&#23646;&#24615;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#29289;&#31181;&#25110;&#26465;&#20214;&#20043;&#38388;&#30340;&#29983;&#29289;&#21464;&#37327;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;:&#35299;&#24320;&#22522;&#22240;&#21644;&#24615;&#29366;&#20043;&#38388;&#30340;&#32852;&#31995;&#23545;&#20110;&#35299;&#20915;&#35768;&#22810;&#29983;&#29289;&#38590;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#22240;&#25552;&#20379;&#20102;&#26500;&#24314;&#32454;&#32990;&#26426;&#22120;&#30340;&#25351;&#20196;&#65292;&#25351;&#23548;&#32500;&#25345;&#29983;&#21629;&#30340;&#36807;&#31243;&#12290;&#20174;&#36825;&#20123;&#36951;&#20256;&#25351;&#20196;&#20013;&#34893;&#29983;&#20986;&#30340;RNA&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#22609;&#36896;&#32454;&#32990;&#32467;&#26500;&#12289;&#24433;&#21709;&#21453;&#24212;&#21644;&#25351;&#23548;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20010;&#22522;&#26412;&#30340;&#29983;&#29289;&#21407;&#21017;&#23558;&#22522;&#22240;&#26500;&#25104;&#19982;&#21487;&#35266;&#23519;&#24615;&#29366;&#32852;&#31995;&#36215;&#26469;&#65292;&#20294;&#26159;&#20174;&#36825;&#31181;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#25972;&#21512;&#21644;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26524;:&#25105;&#20204;&#20171;&#32461;&#20102;evolSOM&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOMs)&#25506;&#32034;&#21644;&#21487;&#35270;&#21270;&#29983;&#29289;&#21464;&#37327;&#20445;&#23432;&#24615;&#30340;&#21019;&#26032;R&#21253;&#65292;&#31616;&#21270;&#20102;&#34920;&#22411;&#21644;&#22522;&#22240;&#22411;&#23646;&#24615;&#30340;&#25972;&#21512;&#12290;&#36890;&#36807;&#26500;&#24314;&#25429;&#25417;&#38750;&#20887;&#20313;&#27169;&#24335;&#30340;&#29289;&#31181;&#29305;&#23450;&#25110;&#26465;&#20214;&#29305;&#23450;&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#65292;evolSOM&#20801;&#35768;&#20998;&#26512;&#29289;&#31181;&#25110;&#26465;&#20214;&#20043;&#38388;&#29983;&#29289;&#21464;&#37327;&#30340;&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Unraveling the connection between genes and traits is crucial for solving many biological puzzles. Genes provide instructions for building cellular machinery, directing the processes that sustain life. RNA molecules and proteins, derived from these genetic instructions, play crucial roles in shaping cell structures, influencing reactions, and guiding behavior. This fundamental biological principle links genetic makeup to observable traits, but integrating and extracting meaningful relationships from this complex, multimodal data presents a significant challenge. Results: We introduce evolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore and visualize the conservation of biological variables, easing the integration of phenotypic and genotypic attributes. By constructing species-specific or condition-specific SOMs that capture non-redundant patterns, evolSOM allows the analysis of displacement of biological variables between species or conditions. Va
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29702;&#20256;&#24863;&#22120;&#30340;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#65292;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#24037;&#20316;&#65292;&#29992;&#20110;&#30417;&#27979;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#21644;&#39550;&#39542;&#34920;&#29616;&#12290;&#36890;&#36807;&#37197;&#32622;&#12289;&#36873;&#25321;&#12289;&#25509;&#25910;&#12289;&#22788;&#29702;&#12289;&#22270;&#24418;&#21270;&#34920;&#31034;&#21644;&#23384;&#20648;&#22810;&#31181;&#20256;&#24863;&#22120;&#30340;&#20449;&#21495;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20998;&#26512;&#39550;&#39542;&#21592;&#30340;&#24515;&#30005;&#22270;&#12289;&#32908;&#30005;&#22270;&#12289;&#30382;&#32932;&#30005;&#21453;&#24212;&#21644;&#21160;&#21147;&#23398;&#21442;&#25968;&#65292;&#24182;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#20998;&#26512;&#39550;&#39542;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07937</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29702;&#20256;&#24863;&#22120;&#30340;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#30340;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;&#39550;&#39542;&#21592;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Physiological Sensor-Based Android Application Synchronized with a Driving Simulator for Driver Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29702;&#20256;&#24863;&#22120;&#30340;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#65292;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#24037;&#20316;&#65292;&#29992;&#20110;&#30417;&#27979;&#39550;&#39542;&#21592;&#30340;&#29983;&#29702;&#29366;&#24577;&#21644;&#39550;&#39542;&#34920;&#29616;&#12290;&#36890;&#36807;&#37197;&#32622;&#12289;&#36873;&#25321;&#12289;&#25509;&#25910;&#12289;&#22788;&#29702;&#12289;&#22270;&#24418;&#21270;&#34920;&#31034;&#21644;&#23384;&#20648;&#22810;&#31181;&#20256;&#24863;&#22120;&#30340;&#20449;&#21495;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20998;&#26512;&#39550;&#39542;&#21592;&#30340;&#24515;&#30005;&#22270;&#12289;&#32908;&#30005;&#22270;&#12289;&#30382;&#32932;&#30005;&#21453;&#24212;&#21644;&#21160;&#21147;&#23398;&#21442;&#25968;&#65292;&#24182;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#20998;&#26512;&#39550;&#39542;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#25511;&#21046;&#21644;&#30417;&#27979;&#26469;&#33258;Shimmer&#24179;&#21488;&#30340;&#29983;&#29702;&#20256;&#24863;&#22120;&#65292;&#24182;&#19982;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#24037;&#20316;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#30417;&#27979;&#39550;&#39542;&#21592;&#65292;&#20854;&#21442;&#25968;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20182;&#20204;&#30340;&#29983;&#29702;&#29366;&#24577;&#19982;&#39550;&#39542;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#37197;&#32622;&#12289;&#36873;&#25321;&#12289;&#25509;&#25910;&#12289;&#22788;&#29702;&#12289;&#22270;&#24418;&#21270;&#34920;&#31034;&#21644;&#23384;&#20648;&#26469;&#33258;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#12289;&#32908;&#30005;&#22270;&#65288;EMG&#65289;&#12289;&#30382;&#32932;&#30005;&#21453;&#24212;&#65288;GSR&#65289;&#27169;&#22359;&#21644;&#21152;&#36895;&#24230;&#35745;&#12289;&#30913;&#21147;&#35745;&#21644;&#38464;&#34746;&#20202;&#30340;&#20449;&#21495;&#12290;&#35813;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#19982;&#25105;&#20204;&#20043;&#21069;&#20351;&#29992;Unity&#28216;&#25103;&#24341;&#25806;&#24320;&#21457;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#21516;&#27493;&#65292;&#20197;&#20998;&#26512;&#39550;&#39542;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#22312;&#19981;&#21516;&#37319;&#26679;&#29575;&#21644;&#19981;&#21516;&#23433;&#21331;&#35774;&#22791;&#19978;&#21516;&#26102;&#27979;&#35797;&#20102;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#24037;&#20316;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;25&#20154;&#19968;&#36215;&#27979;&#35797;&#39550;&#39542;&#27169;&#25311;&#22120;&#21644;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#30340;&#21516;&#27493;&#24037;&#20316;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an Android application to control and monitor the physiological sensors from the Shimmer platform and its synchronized working with a driving simulator. The Android app can monitor drivers and their parameters can be used to analyze the relation between their physiological states and driving performance. The app can configure, select, receive, process, represent graphically, and store the signals from electrocardiogram (ECG), electromyogram (EMG) and galvanic skin response (GSR) modules and accelerometers, a magnetometer and a gyroscope. The Android app is synchronized in two steps with a driving simulator that we previously developed using the Unity game engine to analyze driving security and efficiency. The Android app was tested with different sensors working simultaneously at various sampling rates and in different Android devices. We also tested the synchronized working of the driving simulator and the Android app with 25 people and analyzed the relation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.07933</link><description>&lt;p&gt;
&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#26080;&#20195;&#30721;AutoML&#65306;&#27010;&#24565;&#26694;&#26550;&#12289;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#20135;&#21697;&#30340;&#29305;&#28857;&#26159;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#23545;&#38750;&#19987;&#23478;&#30340;&#19981;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#12290;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#26080;&#32541;&#25191;&#34892;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#65292;&#36825;&#23545;&#20110;&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#34892;&#19994;&#21644;&#21019;&#26032;&#30456;&#20851;&#65292;&#23427;&#24433;&#21709;&#25112;&#30053;&#20915;&#31574;&#21644;&#25237;&#36164;&#39118;&#38505;&#30340;&#38477;&#20302;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24819;&#27861;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26080;&#20195;&#30721;AutoML&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24320;&#21457;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#20351;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#21463;&#30410;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#20013;&#20351;&#29992;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#65292;&#20351;&#38750;RL&#19987;&#23478;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.07928</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Abstracted Trajectory Visualization for Explainability in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07928
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#20013;&#20351;&#29992;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#65292;&#20351;&#38750;RL&#19987;&#23478;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#19994;&#32773;&#29702;&#35299;RL&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;RL&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#65288;&#38750;RL&#19987;&#23478;&#65289;&#65292;XAI&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#36825;&#23548;&#33268;&#38750;RL&#19987;&#23478;&#38590;&#20197;&#21442;&#19982;&#22914;&#20309;&#20026;&#20154;&#31867;&#21644;AI&#20849;&#23384;&#30340;&#31038;&#20250;&#35774;&#35745;RL&#27169;&#22411;&#30340;&#22522;&#26412;&#35752;&#35770;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23558;&#20351;RL&#19987;&#23478;&#33021;&#22815;&#19982;&#38750;RL&#19987;&#23478;&#36827;&#34892;&#27807;&#36890;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#36866;&#21512;&#25105;&#20204;&#31038;&#20250;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25551;&#36848;RL&#27169;&#22411;&#20027;&#35201;&#29366;&#24577;&#20043;&#38388;&#36716;&#25442;&#30340;&#27169;&#31946;&#36712;&#36857;&#23545;&#20110;&#38750;RL&#19987;&#23478;&#26500;&#24314;&#20195;&#29702;&#30340;&#24515;&#29702;&#27169;&#22411;&#23558;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#31946;&#36712;&#36857;&#30340;&#21487;&#35270;&#21270;&#65292;&#27809;&#26377;RL&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#25506;&#35752;&#20102;&#32769;&#24180;&#20154;&#19982;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#30340;&#20132;&#20114;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27880;&#20837;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#22312;&#28040;&#38500;&#24180;&#40836;&#30456;&#20851;&#30340;&#25968;&#23383;&#40511;&#27807;&#26041;&#38754;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#35266;&#21487;&#35270;&#21270;&#21644;&#31616;&#26126;&#35299;&#37322;&#31561;&#35774;&#35745;&#22240;&#32032;&#23545;&#20110;&#32769;&#24180;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;XAI&#22312;&#30005;&#23376;&#20581;&#24247;&#39046;&#22495;&#30340;&#38761;&#21629;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07915</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32769;&#24180;&#20154;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#20132;&#20114;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on Older Adults' Interaction with E-Health Interface Based on Explainable Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#25506;&#35752;&#20102;&#32769;&#24180;&#20154;&#19982;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#30340;&#20132;&#20114;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27880;&#20837;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#22312;&#28040;&#38500;&#24180;&#40836;&#30456;&#20851;&#30340;&#25968;&#23383;&#40511;&#27807;&#26041;&#38754;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#35266;&#21487;&#35270;&#21270;&#21644;&#31616;&#26126;&#35299;&#37322;&#31561;&#35774;&#35745;&#22240;&#32032;&#23545;&#20110;&#32769;&#24180;&#29992;&#25143;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;XAI&#22312;&#30005;&#23376;&#20581;&#24247;&#39046;&#22495;&#30340;&#38761;&#21629;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#21508;&#31181;&#26679;&#26412;&#30340;&#32769;&#24180;&#20154;&#30340;&#29992;&#25143;&#20307;&#39564;&#12289;&#21487;&#29992;&#24615;&#35780;&#20272;&#21644;&#28145;&#20837;&#35775;&#35848;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#35775;&#35848;&#25910;&#38598;&#32769;&#24180;&#20154;&#19982;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#30340;&#20132;&#20114;&#32463;&#39564;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#21033;&#29992;XAI&#26041;&#27861;&#35299;&#37322;&#36825;&#20123;&#25910;&#38598;&#21040;&#30340;&#35775;&#35848;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#27880;&#20837;XAI&#30340;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#33021;&#22815;&#22312;&#35843;&#26597;&#32769;&#24180;&#20154;&#19982;&#30005;&#23376;&#20581;&#24247;&#30028;&#38754;&#20132;&#20114;&#26102;&#65292;&#25198;&#28436;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20419;&#36827;&#24180;&#40836;&#30456;&#20851;&#30340;&#25968;&#23383;&#40511;&#27807;&#30340;&#28040;&#38500;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#30830;&#23450;&#20102;&#37325;&#35201;&#30340;&#35774;&#35745;&#22240;&#32032;&#65292;&#22914;&#30452;&#35266;&#30340;&#21487;&#35270;&#21270;&#21644;&#31616;&#26126;&#25212;&#35201;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#20110;&#21019;&#24314;&#32769;&#24180;&#29992;&#25143;&#20043;&#38388;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;XAI &#22312;&#30005;&#23376;&#20581;&#24247;&#39046;&#22495;&#30340;&#38761;&#21629;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposed a comprehensive mixed-methods framework with varied samples of older adults, including user experience, usability assessments, and in-depth interviews with the integration of Explainable Artificial Intelligence (XAI) methods. The experience of older adults' interaction with the Ehealth interface is collected through interviews and transformed into operatable databases whereas XAI methods are utilized to explain the collected interview results in this research work. The results show that XAI-infused e-health interfaces could play an important role in bridging the age-related digital divide by investigating elders' preferences when interacting with E-health interfaces. Furthermore, the study identifies important design factors, such as intuitive visualization and straightforward explanations, that are critical for creating efficient Human Computer Interaction (HCI) tools among older users. Furthermore, this study emphasizes the revolutionary potential of XAI in e-heal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#21098;&#26525;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#20934;&#30830;&#24230;&#24182;&#36991;&#20813;&#31934;&#35843;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.07839</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#20803;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Towards Meta-Pruning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#21098;&#26525;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#20934;&#30830;&#24230;&#24182;&#36991;&#20813;&#31934;&#35843;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21098;&#26525;&#36890;&#24120;&#20381;&#36182;&#20110;&#35782;&#21035;&#21644;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#31181;&#20570;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#38656;&#35201;&#38543;&#21518;&#30340;&#31934;&#35843;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#21098;&#26525;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#35774;&#35745;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#20803;&#37325;&#35201;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;Intra-Fusion&#37325;&#26032;&#23450;&#20041;&#20102;&#19978;&#23618;&#21098;&#26525;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21033;&#29992;&#32473;&#23450;&#30340;&#19981;&#21487;&#30693;&#37325;&#35201;&#24615;&#24230;&#37327;&#65292;&#24471;&#21040;&#20102;&#26356;&#26377;&#25928;&#30340;&#31232;&#30095;&#27169;&#22411;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#20351;&#20854;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#39640;&#25928;&#19988;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#38656;&#36890;&#36807;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#26041;&#27861;&#33021;&#22815;&#27867;&#21270;&#21040;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#26469;&#22788;&#29702;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07735</link><description>&lt;p&gt;
&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#65306;&#24341;&#20837;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#38656;&#36890;&#36807;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#26041;&#27861;&#33021;&#22815;&#27867;&#21270;&#21040;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#26469;&#22788;&#29702;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#35266;&#27979;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#22522;&#26412;&#20381;&#36182;&#32467;&#26500;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20165;&#38656;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#20043;&#38388;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#65292;&#29992;&#20110;&#26174;&#24335;&#22788;&#29702;&#20381;&#36182;&#20449;&#24687;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;</title><link>https://arxiv.org/abs/2402.07595</link><description>&lt;p&gt;
&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24120;&#24120;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#36801;&#31227;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21516;&#26102;&#33410;&#32422;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;DINOv2&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#24182;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;DINOv2&#22312;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#38656;&#39564;&#35777;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#20020;&#24202;&#27169;&#24577;&#30340;&#33041;&#37096;MRI&#25968;&#25454;&#25191;&#34892;&#20102;&#19968;&#20010;&#33014;&#36136;&#30244;&#20998;&#32423;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;ImageNet&#21644;DINOv2&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#20923;&#32467;&#26426;&#21046;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#20854;&#20182;&#19977;&#31181;&#31867;&#22411;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65306;&#33016;&#37096;&#25918;&#23556;&#23398;&#12289;&#30524;&#24213;&#25918;&#23556;&#23398;&#21644;&#30382;&#32932;&#38236;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image analysis frequently encounters data scarcity challenges. Transfer learning has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2's performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#35299;&#37322;&#24615;&#30340;&#25299;&#25169;&#20445;&#25252;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;&#36867;&#36991;&#25915;&#20987;&#26159;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#25915;&#20987;&#65292;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#23545;&#25239;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07480</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#30340;&#36867;&#36991;&#25915;&#20987;&#30340;&#25299;&#25169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#35299;&#37322;&#24615;&#30340;&#25299;&#25169;&#20445;&#25252;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;&#36867;&#36991;&#25915;&#20987;&#26159;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#25915;&#20987;&#65292;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#23545;&#25239;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#25552;&#20986;&#65292;&#20026;&#27599;&#20010;&#39046;&#22495;&#24102;&#26469;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#20851;&#20110;&#32593;&#32476;&#23433;&#20840;&#26041;&#38754;&#30340;&#26032;&#23041;&#32961;&#12290;&#36825;&#20123;&#23454;&#26045;&#30340;&#27169;&#22411;&#24102;&#26469;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#20851;&#30340;&#20960;&#31181;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20801;&#35768;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#33719;&#24471;&#31169;&#20154;&#20449;&#24687;&#65292;&#29978;&#33267;&#20462;&#25913;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#28431;&#27934;/&#25915;&#20987;&#36827;&#34892;&#30740;&#31350;&#24182;&#35774;&#35745;&#38450;&#24481;&#25514;&#26045;&#20197;&#36991;&#20813;&#25110;&#23545;&#25239;&#23427;&#20204;&#30340;&#20852;&#36259;&#22312;&#30740;&#31350;&#20013;&#26085;&#30410;&#31361;&#20986;&#12290;&#29305;&#21035;&#26159;&#65292;&#33879;&#21517;&#30340;&#36867;&#36991;&#25915;&#20987;&#27491;&#22312;&#34987;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#65292;&#22240;&#27492;&#22312;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#20960;&#31181;&#36991;&#20813;&#27492;&#23041;&#32961;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#33258;L-BFG&#31639;&#27861;&#25552;&#20986;&#20197;&#26469;&#65292;&#36825;&#31181;&#23041;&#32961;&#19968;&#30452;&#20851;&#27880;&#30740;&#31350;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27809;&#26377;&#36866;&#29992;&#20110;&#25152;&#26377;&#24050;&#30693;&#36867;&#36991;&#31639;&#27861;&#30340;&#23436;&#32654;&#38450;&#24481;&#65292;&#22240;&#27492;&#20173;&#22312;&#19981;&#26029;&#24320;&#21457;&#26032;&#30340;&#24039;&#22937;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a no
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07462</link><description>&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#30340;&#28608;&#32032;&#36866;&#24212;&#26041;&#27861;&#65306;&#39044;&#38450;&#22238;&#24418;&#38024;&#21551;&#31034;&#24405;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07462
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#26088;&#22312;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#30340;&#23433;&#20840;&#21644;&#26368;&#20248;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#65288;&#28608;&#32032;&#36866;&#24212;&#36890;&#36807;&#23545;&#25163;&#36807;&#31243;&#65289;&#36825;&#20010;&#27861;&#35268;&#27169;&#24335;&#65292;&#23427;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#34892;&#20026;&#28608;&#32032;&#36866;&#24212;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#20302;&#39057;&#29575;&#30340;&#34892;&#20026;&#20855;&#26377;&#30410;&#22788;&#65292;&#32780;&#39640;&#39057;&#29575;&#30340;&#34892;&#20026;&#21017;&#26377;&#23475;&#12290;&#36890;&#36807;&#23558;&#34892;&#20026;&#24314;&#27169;&#20026;&#21464;&#24577;&#23545;&#25163;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#34892;&#20026;&#39057;&#29575;&#21709;&#24212;&#20998;&#26512;&#65288;BFRA&#65289;&#25110;&#34892;&#20026;&#35745;&#25968;&#21709;&#24212;&#20998;&#26512;&#65288;BCRA&#65289;&#26469;&#37327;&#21270;&#21487;&#37325;&#22797;&#34892;&#20026;&#30340;&#28608;&#32032;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;HALO&#26469;&#35299;&#20915;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#65292;&#36825;&#26159;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#20854;&#20013;&#19968;&#20010;&#26410;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23558;&#25152;&#26377;&#29289;&#36136;&#36716;&#21270;&#20026;&#22238;&#24418;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#26469;&#21019;&#24314;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#65292;&#20197;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.07357</link><description>&lt;p&gt;
&#22238;&#24402;&#26641;&#29992;&#20110;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Regression Trees for Fast and Adaptive Prediction Intervals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#26469;&#21019;&#24314;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#65292;&#20197;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#20250;&#29359;&#38169;&#65292;&#22240;&#27492;&#38656;&#35201;&#37327;&#21270;&#19982;&#20854;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#31526;&#21512;&#24615;&#25512;&#26029;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#28857;&#39044;&#27979;&#21608;&#22260;&#21019;&#24314;&#32479;&#35745;&#19978;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#20294;&#26159;&#23427;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#20250;&#20135;&#29983;&#38750;&#33258;&#36866;&#24212;&#30340;&#21306;&#22495;&#12290;&#26032;&#30340;&#31526;&#21512;&#24615;&#24471;&#20998;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#22120;&#25110;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#21019;&#24314;&#39044;&#27979;&#24102;&#26041;&#38754;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24471;&#20998;&#19982;&#37327;&#21270;&#20219;&#24847;&#39044;&#27979;&#27169;&#22411;&#21608;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#22987;&#30446;&#26631;&#33073;&#33410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36861;&#27714;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#26469;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31526;&#21512;&#24615;&#24471;&#20998;&#36827;&#34892;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#35757;&#32451;&#26469;&#21019;&#24314;&#36825;&#20010;&#21010;&#20998;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#23558;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#24212;&#29992;&#20110;&#31526;&#21512;&#24615;&#25512;&#26029;&#30340;&#26032;&#39046;&#22495;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07355</link><description>&lt;p&gt;
&#20174;&#22343;&#22330;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from the Mean-Field Stationary Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#65292;&#21363;&#21253;&#21547;&#20132;&#20114;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#26368;&#23567;&#21270;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27934;&#23519;&#26159;&#23558;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#35299;&#32806;&#65306;(1) &#36890;&#36807;&#26377;&#38480;&#31890;&#23376;&#31995;&#32479;&#36924;&#36817;&#22343;&#22330;SDE&#65292;&#36890;&#36807;&#26102;&#38388;&#22343;&#21248;&#20256;&#25773;&#28151;&#27788;&#65292;&#21644;(2) &#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20985;&#25277;&#26679;&#22120;&#20174;&#26377;&#38480;&#31890;&#23376;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#20854;&#28789;&#27963;&#24615;&#20801;&#35768;&#32467;&#21512;&#29992;&#20110;&#31639;&#27861;&#21644;&#29702;&#35770;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20998;&#35010;&#31215;&#20998;&#22120;&#36827;&#34892;&#21407;&#21017;&#24615;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#19979;&#30340;FID&#20998;&#25968;&#20026;2.36&#12290;</title><link>https://arxiv.org/abs/2402.07211</link><description>&lt;p&gt;
&#38754;&#21521;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Fast Stochastic Sampling in Diffusion Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20998;&#35010;&#31215;&#20998;&#22120;&#36827;&#34892;&#21407;&#21017;&#24615;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#19979;&#30340;FID&#20998;&#25968;&#20026;2.36&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26679;&#26412;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#22312;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#37319;&#26679;&#25928;&#29575;&#65292;&#20294;&#20173;&#28982;&#26377;&#24453;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#35010;&#31215;&#20998;&#22120;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;&#12290;&#20998;&#35010;&#31215;&#20998;&#22120;&#36890;&#24120;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#22312;&#28041;&#21450;&#25968;&#25454;&#12289;&#36741;&#21161;&#25110;&#22122;&#22768;&#21464;&#37327;&#30340;&#25968;&#20540;&#26356;&#26032;&#20043;&#38388;&#20132;&#26367;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#24555;&#36895;&#37319;&#26679;&#65292;&#31616;&#21333;&#24212;&#29992;&#20998;&#35010;&#31215;&#20998;&#22120;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#21407;&#21017;&#19978;&#20462;&#25913;&#20102;&#31616;&#21333;&#20998;&#35010;&#37319;&#26679;&#22120;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#24471;&#21040;&#30340;&#37319;&#26679;&#22120;&#31216;&#20026;&#20943;&#23567;&#20998;&#35010;&#31215;&#20998;&#22120;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#31354;&#38388;&#26391;&#20043;&#19975;&#25193;&#25955; (PSLD) [Pandey \&amp; Mandt, 2023] &#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#37319;&#26679;&#22120;&#22312;&#20165;&#36827;&#34892;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#21518;&#65292;&#23454;&#29616;&#20102;2.36&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \&amp; Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06974</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#34701;&#21512;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#22810;&#20010;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#21644;&#23454;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#25193;&#23637;&#20854;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#20197;&#36866;&#24212;&#26410;&#30693;&#39046;&#22495;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65288;FDG&#65289;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#31216;&#20026;hFedF&#65288;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#31243;&#24230;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#22522;&#26412;&#19978;&#65292;&#36229;&#32593;&#32476;&#25903;&#25345;&#23545;&#23458;&#25143;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;FL&#20013;&#24378;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DG&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exce
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06938</link><description>&lt;p&gt;
&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06938
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#20449;&#24687;&#21644;&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20652;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#20449;&#24687;&#12290;&#20449;&#24687;&#29190;&#28856;&#25512;&#21160;&#35768;&#22810;&#20225;&#19994;&#25110;&#20010;&#20154;&#23547;&#27714;&#31199;&#29992;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26469;&#23558;&#20182;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#25918;&#32622;&#22312;&#20113;&#20013;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#30340;&#21327;&#35758;&#36890;&#24120;&#19981;&#39640;&#25928;&#12290;&#35768;&#22810;&#22240;&#32032;&#24433;&#21709;&#25928;&#29575;&#65292;&#22914;&#25552;&#20379;&#21830;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#38386;&#32622;&#21644;&#23545;&#23458;&#25143;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24341;&#20837;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#35848;&#21028;&#31867;&#30340;&#21338;&#24328;&#65292;&#24182;&#26681;&#25454;&#35848;&#21028;&#32467;&#26524;&#23433;&#25490;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#29992;&#20110;&#36164;&#28304;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23436;&#25104;&#19968;&#23545;&#19968;&#30340;&#33258;&#21160;&#35848;&#21028;&#36807;&#31243;&#65292;&#24182;&#20026;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#29983;&#25104;&#26368;&#20248;&#30340;&#25253;&#20215;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25104;&#21592;&#20989;&#25968;&#12289;&#27169;&#31946;&#35268;&#21017;&#38598;&#21644;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#36164;&#28304;&#35843;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06864</link><description>&lt;p&gt;
&#20855;&#26377;&#36776;&#21035;&#24615;&#23545;&#25239;&#23398;&#20064;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Discriminative Adversarial Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#24050;&#24314;&#31435;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#20419;&#36827;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21453;&#23398;&#20064;&#29305;&#23450;&#26679;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;$\mathbf{A}$&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#38450;&#24481;&#32773; $\mathbf{D}$&#22312;&#23545;&#25239;&#30446;&#26631;&#19979;&#30456;&#20114;&#23545;&#25239;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#30340;&#20449;&#24687;&#20197;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#65292;&#32780;&#38450;&#24481;&#32773;&#22312;&#21453;&#20987;&#20013;&#36827;&#34892;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36981;&#24490;&#24050;&#30693;&#30340;&#36845;&#20195;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#26469;&#26356;&#26032;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#12290;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36951;&#24536;&#38598;&#21644;&#39564;&#35777;&#38598;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#24046;&#24322;&#65292;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06282</link><description>&lt;p&gt;
&#33719;&#21462;&#12289;&#21512;&#24182;&#12289;&#39044;&#27979;&#65306;&#36890;&#36807;&#25968;&#25454;&#28246;&#22686;&#24378;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#26159;&#32473;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#26684;&#22686;&#24378;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26816;&#32034;&#21487;&#36830;&#25509;&#30340;&#34920;&#26684;&#12289;&#21512;&#24182;&#20449;&#24687;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26684;&#12290;&#20316;&#20026;&#25968;&#25454;&#28246;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;YADL&#65288;&#21478;&#19968;&#20010;&#25968;&#25454;&#28246;&#65289;-&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#27492;&#25968;&#25454;&#21457;&#29616;&#20219;&#21153;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;-&#21644;Open Data US&#65292;&#19968;&#20010;&#34987;&#24341;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#28246;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#25968;&#25454;&#28246;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#65292;&#26088;&#22312;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.05098</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#23494;&#24230;&#25110;&#33021;&#37327;&#20989;&#25968;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#25193;&#25955;&#32467;&#26500;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#25311;&#30340;&#21464;&#20998;&#26041;&#27861;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#65288;&#36830;&#32493;&#29983;&#25104;&#27969;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#21516;&#26102;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#36136;&#30097;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#25506;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#22312;https://github.com/GFNOrg/gfn-diffusion&#65292;&#20316;&#20026;&#26410;&#26469;&#22312;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#19978;&#24037;&#20316;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04691</link><description>&lt;p&gt;
&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#31639;&#23376;&#30340;&#24369;&#21644;&#24378;&#35268;&#21017;&#26465;&#20214;&#65292;&#20197;&#25551;&#36848;&#20854;&#20869;&#22312;&#32467;&#26500;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36827;&#34892;&#20102;&#26497;&#23567;&#20540;&#19979;&#30028;&#20998;&#26512;&#65292;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#21644;&#35268;&#21017;&#26465;&#20214;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#20351;&#29992;SGD&#31639;&#27861;&#35299;&#20915;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12290;&#20540;&#24471;&#24378;&#35843;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#20272;&#35745;&#22120;&#23558;&#25910;&#25947;&#20110;&#38750;&#32447;&#24615;&#30446;&#26631;&#31639;&#23376;&#30340;&#26368;&#20339;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#24212;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#20540;&#21644;&#23454;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#20135;&#29983;&#20102;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20174;&#32780;&#23436;&#21892;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing litera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21435;&#25481;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21527;&#65311;&#19968;&#20010;&#20108;&#38454;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03496
&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#22914;Adam(W)&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65288;&#22914;transformers&#65289;&#30340;&#40664;&#35748;&#35757;&#32451;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#23545;&#35282;&#20808;&#39564;&#22522;&#20110;&#26799;&#24230;&#22806;&#31215;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21152;&#20837;&#21040;&#21442;&#25968;&#26356;&#26032;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#36817;&#20284;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#20294;&#24179;&#26041;&#26681;&#34920;&#31034;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21435;&#25481;&#24179;&#26041;&#26681;&#21518;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#34892;&#20026;&#22914;&#20309;&#21464;&#21270;&#65292;&#21363;&#21152;&#24378;&#23427;&#20204;&#30340;&#20108;&#38454;&#21160;&#26426;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21435;&#25481;&#24179;&#26041;&#26681;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#32553;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;transformers&#19978;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20108;&#38454;&#35282;&#24230;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;&#23545;&#35282;&#20808;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#19982;&#20687;Shampoo&#36825;&#26679;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#23545;&#24212;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#30697;&#38453;&#24179;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02316</link><description>&lt;p&gt;
&#20320;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Certifiably Robust Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#20316;&#20026;&#40065;&#26834;&#20998;&#31867;&#30340;&#29983;&#25104;&#22120;&#20998;&#31867;&#22120;&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25193;&#25955;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#65292;&#36825;&#35753;&#25105;&#20204;&#24576;&#30097;&#23427;&#20204;&#26159;&#21542;&#20250;&#23481;&#26131;&#21463;&#21040;&#26410;&#26469;&#26356;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#21629;&#21517;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#36825;&#20123;&#20998;&#24067;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBOs&#65289;&#65292;&#21033;&#29992;ELBO&#36817;&#20284;&#20284;&#28982;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#35745;&#31639;&#20998;&#31867;&#27010;&#29575;&#65292;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25512;&#24191;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;NDCs&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#35748;&#35777;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36798;&#21040;80%&#30340;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02196</link><description>&lt;p&gt;
&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#26679;&#26412;&#39640;&#25928;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02196
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#32858;&#31867;&#21644;&#24449;&#26381;"&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25171;&#30772;&#26679;&#26412;&#25928;&#29575;&#30340;&#29942;&#39048;&#12290;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;O(p)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#36895;&#24230;&#65292;&#36825;&#26159;&#29702;&#35770;&#19978;&#21487;&#36798;&#21040;&#30340;&#26368;&#20339;&#20943;&#23569;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#22312;&#22266;&#23450;&#39044;&#31639;&#21644;&#22266;&#23450;&#31934;&#24230;&#30340;&#33539;&#24335;&#19979;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21508;&#31181;&#24120;&#35265;&#30340;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#26080;&#38656;&#39640;&#31934;&#30830;&#24230;&#30456;&#20851;&#20272;&#35745;&#21644;&#31934;&#30830;&#32858;&#31867;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25913;&#36827;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#22914;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26080;&#31579;&#36873;&#29256;&#26412;&#30340;&#26041;&#27861;&#24778;&#20154;&#22320;&#36229;&#36807;&#20102;&#23436;&#20840;&#39034;&#24207;&#21270;&#30340;&#22522;&#20934;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#34920;&#26126;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#26159;&#32469;&#36807;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#26465;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&amp;S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&amp;S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
&lt;/p&gt;</description></item><item><title>PresAIse&#26159;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38598;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#25968;&#25454;&#38480;&#21046;&#12289;&#24314;&#35758;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#19982;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#38556;&#30861;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02006</link><description>&lt;p&gt;
PresAIse&#65292;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
PresAIse, An Enterprises Prescriptive AI Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02006
&lt;/p&gt;
&lt;p&gt;
PresAIse&#26159;&#19968;&#31181;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38598;&#25104;&#65292;&#26088;&#22312;&#35299;&#20915;&#20225;&#19994;&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#25968;&#25454;&#38480;&#21046;&#12289;&#24314;&#35758;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#19982;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#38556;&#30861;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#20154;&#24037;&#26234;&#33021;&#20195;&#34920;&#20102;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#19968;&#27425;&#36716;&#22411;&#24615;&#21464;&#38761;&#65292;&#25552;&#20379;&#22240;&#26524;&#27934;&#23519;&#21644;&#21487;&#25805;&#20316;&#30340;&#24314;&#35758;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20225;&#19994;&#37319;&#29992;&#24120;&#24120;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35266;&#23519;&#25968;&#25454;&#30340;&#38480;&#21046;&#20351;&#24471;&#20934;&#30830;&#30340;&#22240;&#26524;&#25512;&#26029;&#25104;&#20026;&#20102;&#33391;&#22909;&#20915;&#31574;&#21046;&#23450;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#24314;&#35758;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#20225;&#19994;&#20915;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#31532;&#19977;&#20010;&#25361;&#25112;&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#19994;&#21153;&#29992;&#25143;&#20043;&#38388;&#30340;&#38548;&#31163;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#21512;&#20316;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;IBM&#30740;&#31350;&#30340;&#19968;&#39033;&#20513;&#35758;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#39044;&#27979;&#24615; AI &#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#30740;&#31350;&#35770;&#25991;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#26041;&#26696;&#22871;&#20214;&#21253;&#25324;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#12289;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#20197;&#21450;&#36890;&#36807;&#23545;&#35805;&#26694;&#26550;&#26469;&#24357;&#21512;&#27807;&#36890;&#38548;&#38402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive AI represents a transformative shift in decision-making, offering causal insights and actionable recommendations. Despite its huge potential, enterprise adoption often faces several challenges. The first challenge is caused by the limitations of observational data for accurate causal inference which is typically a prerequisite for good decision-making. The second pertains to the interpretability of recommendations, which is crucial for enterprise decision-making settings. The third challenge is the silos between data scientists and business users, hindering effective collaboration. This paper outlines an initiative from IBM Research, aiming to address some of these challenges by offering a suite of prescriptive AI solutions. Leveraging insights from various research papers, the solution suite includes scalable causal inference methods, interpretable decision-making approaches, and the integration of large language models (LLMs) to bridge communication gaps via a conversati
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;TD&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15719</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21450;&#20854;&#22312;TD&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rates of Convergence in the Central Limit Theorem for Markov Chains, with an Application to TD Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;TD&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;Stein&#26041;&#27861;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30340;&#12289;&#30690;&#37327;&#20540;&#38789;&#24046;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#21033;&#29992;&#27850;&#26494;&#26041;&#31243;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#20989;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#24212;&#29992;&#20110;&#24314;&#31435;&#22522;&#20110;&#24179;&#22343;&#30340;&#38750;&#28176;&#36817;&#30340;TD&#23398;&#20064;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a non-asymptotic central limit theorem for vector-valued martingale differences using Stein's method, and use Poisson's equation to extend the result to functions of Markov Chains. We then show that these results can be applied to establish a non-asymptotic central limit theorem for Temporal Difference (TD) learning with averaging.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.00691</link><description>&lt;p&gt;
&#28155;&#21152;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent for Additive Nonparametric Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#32452;&#20214;&#20989;&#25968;&#30340;&#25130;&#26029;&#22522;&#25193;&#23637;&#30340;&#31995;&#25968;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#28385;&#36275;&#19968;&#20010;&#22885;&#25289;&#20811;&#19981;&#31561;&#24335;&#65292;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19977;&#20010;&#19981;&#21516;&#38454;&#27573;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#39118;&#38505;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20381;&#36182;&#26041;&#38754;&#26159;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23558;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#25311;&#21512;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
&lt;/p&gt;</description></item><item><title>LEFL&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#20026;&#30446;&#26631;&#30340;&#32852;&#37030;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#22312;&#27599;&#19968;&#36718;&#27425;&#20013;&#30340;&#20998;&#23618;&#37319;&#26679;&#65292;&#20174;&#32780;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.17430</link><description>&lt;p&gt;
LEFL: &#39640;&#21364;&#29109;&#23458;&#25143;&#31471;&#37319;&#26679;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LEFL: Low Entropy Client Sampling in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17430
&lt;/p&gt;
&lt;p&gt;
LEFL&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#20026;&#30446;&#26631;&#30340;&#32852;&#37030;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#22312;&#27599;&#19968;&#36718;&#27425;&#20013;&#30340;&#20998;&#23618;&#37319;&#26679;&#65292;&#20174;&#32780;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#20316;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#20248;&#21270;&#21333;&#19968;&#20840;&#23616;&#27169;&#22411;&#12290;&#20840;&#23616;&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32500;&#25252;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#35757;&#32451;&#36718;&#27425;&#26469;&#36827;&#34892;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#32534;&#25490;&#12290;&#22312;&#27599;&#20010;&#36718;&#27425;&#65292;&#26381;&#21153;&#22120;&#20174;&#23458;&#25143;&#31471;&#27744;&#20013;&#36827;&#34892;&#23458;&#25143;&#31471;&#37319;&#26679;&#65292;&#22312;&#21457;&#36865;&#26368;&#26032;&#30340;&#20840;&#23616;&#27169;&#22411;&#21442;&#25968;&#32473;&#23458;&#25143;&#31471;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#20043;&#21069;&#12290;&#20256;&#32479;&#30340;&#37319;&#26679;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#23458;&#25143;&#31471;&#37319;&#26679;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#21407;&#22240;&#26410;&#33021;&#32771;&#34385;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEFL&#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#26681;&#25454;&#27169;&#22411;&#23398;&#21040;&#30340;&#39640;&#32423;&#29305;&#24449;&#36827;&#34892;&#19968;&#27425;&#24615;&#23458;&#25143;&#31471;&#32858;&#31867;&#65292;&#21516;&#26102;&#23562;&#37325;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#20351;&#24471;&#26381;&#21153;&#22120;&#33021;&#22815;&#22312;&#27599;&#19968;&#36718;&#27425;&#20013;&#23545;&#31751;&#36827;&#34892;&#20998;&#23618;&#23458;&#25143;&#31471;&#37319;&#26679;&#12290;&#25105;&#20204;&#23637;&#31034;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#36873;&#25321;&#30340;&#26679;&#26412;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#30456;&#23545;&#29109;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;FL&#35757;&#32451;&#21464;&#24471;...
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm where multiple clients collaborate to optimize a single global model using their private data. The global model is maintained by a central server that orchestrates the FL training process through a series of training rounds. In each round, the server samples clients from a client pool before sending them its latest global model parameters for further optimization. Naive sampling strategies implement random client sampling and fail to factor client data distributions for privacy reasons. Hence we propose LEFL, an alternative sampling strategy by performing a one-time clustering of clients based on their model's learned high-level features while respecting data privacy. This enables the server to perform stratified client sampling across clusters in every round. We show datasets of sampled clients selected with this approach yield a low relative entropy with respect to the global data distribution. Consequently, the FL training becom
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#24314;&#31435;&#20102;&#26377;&#25928;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#24182;&#26356;&#26131;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2312.17173</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Non-Vacuous Generalization Bounds for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17173
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#24314;&#31435;&#20102;&#26377;&#25928;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#24182;&#26356;&#26131;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#65292;&#25110;&#32773;&#21482;&#26159;&#37325;&#22797;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#27979;&#24179;&#28369;&#23548;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26080;&#30028;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#24182;&#19988;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#30028;&#38480;&#20197;&#22788;&#29702;&#23376;&#37319;&#26679;&#65292;&#21152;&#36895;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#30028;&#38480;&#35745;&#31639;&#12290;&#20026;&#20102;&#23454;&#29616;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#25152;&#38656;&#30340;&#26497;&#31471;&#21387;&#32553;&#31243;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;SubLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#20302;&#32500;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#26131;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. To achieve the extreme level of compression required for non-vacuous generalization bounds, we devise SubLoRA, a low-dimensional non-linear parameterization. Using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.15230</link><description>&lt;p&gt;
PERP: &#22312;LLMs&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#20462;&#21098;-&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#65292;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#20687;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65292;Han&#31561;&#65292;2015&#65289;&#36825;&#26679;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20197;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20869;&#23384;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#21482;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#36890;&#24120;&#36275;&#20197;&#24674;&#22797;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;GPT-&#32467;&#26500;&#30340;0.27%-0.35%&#30340;&#21442;&#25968;&#21363;&#21487;&#22312;&#19981;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#23454;&#29616;&#19982;&#19968;&#27425;&#24615;IMP&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#21098;&#21518;&#21442;&#25968;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#65288;PERP&#65289;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;$k$-means&#32858;&#31867;&#31639;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#21644;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#22312;&#23545;&#25239;&#22330;&#26223;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.09533</link><description>&lt;p&gt;
&#29992;$k$-means&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness on Image Classification with $k$-means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;$k$-means&#32858;&#31867;&#31639;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#21644;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#22312;&#23545;&#25239;&#22330;&#26223;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22686;&#24378;$k$-means&#32858;&#31867;&#31639;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25361;&#25112;&#19982;&#31574;&#30053;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#32858;&#31867;&#31639;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#30456;&#20851;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#25915;&#20987;&#24378;&#24230;&#23545;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#20102;&#30417;&#30563;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#27010;&#24565;&#65292;&#24182;&#24378;&#35843;&#20102;&#26080;&#30417;&#30563;&#27169;&#22411;&#23545;&#26679;&#26412;&#20998;&#24067;&#30340;&#25935;&#24863;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23545;&#25239;&#22330;&#26223;&#20013;&#25913;&#21892;&#20102;&#27979;&#35797;&#24615;&#33021;&#65292;&#24182;&#24378;&#35843;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#20013;&#21508;&#31181;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#22914;&#36830;&#32493;&#23398;&#20064;&#12289;&#20013;&#24515;&#21021;&#22987;&#21270;&#21644;&#23545;&#25239;&#27493;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we explore the challenges and strategies for enhancing the robustness of $k$-means clustering algorithms against adversarial manipulations. We evaluate the vulnerability of clustering algorithms to adversarial attacks, emphasising the associated security risks. Our study investigates the impact of incremental attack strength on training, introduces the concept of transferability between supervised and unsupervised models, and highlights the sensitivity of unsupervised models to sample distributions. We additionally introduce and evaluate an adversarial training method that improves testing performance in adversarial scenarios, and we highlight the importance of various parameters in the proposed training method, such as continuous learning, centroid initialisation, and adversarial step-count.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.06441</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#21644;&#35889;&#38382;&#39064;&#19979;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#65288;GFD&#65289;&#21487;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;GFD&#65292;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#26469;&#21051;&#30011;&#33410;&#28857;&#30340;&#24322;&#24120;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#27450;&#35784;&#22270;&#22312;&#26412;&#36136;&#19978;&#26159;&#24322;&#36136;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;GNN&#30001;&#20110;&#20551;&#35774;&#21516;&#36136;&#24615;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29616;&#26377;&#27169;&#22411;&#26410;&#20805;&#20998;&#21033;&#29992;&#23453;&#36149;&#30340;&#33410;&#28857;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#12290;&#35813;&#26816;&#27979;&#22120;&#21253;&#25324;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#65292;&#36825;&#20004;&#20010;&#27169;&#22359;&#20998;&#21035;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#20174;&#35889;&#22495;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#23558;&#22270;&#20998;&#21106;&#31216;&#19981;&#21516;&#30340;&#35889;&#25104;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03096</link><description>&lt;p&gt;
&#24341;&#36215;&#22810;&#20041;&#24615;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#20598;&#28982;&#22240;&#32032;&#30340;&#28151;&#21512;&#36873;&#25321;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20041;&#24615;&#31070;&#32463;&#20803;&#8212;&#8212;&#28608;&#27963;&#19968;&#32452;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#8212;&#8212;&#34987;&#35270;&#20026;&#35299;&#37322;&#20219;&#21153;&#20248;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#23545;AI&#23433;&#20840;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#22810;&#20041;&#24615;&#36215;&#28304;&#25925;&#20107;&#26159;&#25968;&#25454;&#21253;&#21547;&#30340;&#8220;&#29305;&#24449;&#8221;&#22810;&#20110;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#36843;&#20351;&#32593;&#32476;&#23558;&#22810;&#20010;&#19981;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#32473;&#21516;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#21361;&#21450;&#25105;&#20204;&#29702;&#35299;&#32593;&#32476;&#20869;&#37096;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#31532;&#20108;&#20010;&#19988;&#38750;&#20114;&#26021;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#20063;&#21487;&#33021;&#20135;&#29983;&#65292;&#36825;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20598;&#28982;&#22810;&#20041;&#24615;&#8221;&#30340;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#21487;&#20197;&#30001;&#22810;&#31181;&#21407;&#22240;&#24341;&#36215;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#22122;&#38899;&#65307;&#36825;&#31181;&#20598;&#28982;&#22810;&#20041;&#24615;&#21457;&#29983;&#26159;&#22240;&#20026;&#38543;&#26426;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21253;&#38598;&#25104;&#26041;&#27861;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.01046</link><description>&lt;p&gt;
Bagged Regularized $k$-Distances&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bagged Regularized $k$-Distances for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21253;&#38598;&#25104;&#26041;&#27861;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#33539;&#24335;&#65292;&#21363;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#23613;&#31649;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#23545;&#20110;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#26368;&#36817;&#37051;&#25968;&#37327;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#65292;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;BRDAD&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#26367;&#20195;&#39118;&#38505;&#65288;&#21363;&#32463;&#39564;&#39118;&#38505;&#30340;&#26377;&#38480;&#26679;&#26412;&#19978;&#30028;&#65289;&#26469;&#36873;&#25321;&#26435;&#37325;&#65292;&#20197;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#24102;&#26435;&#37325;&#30340;$k$-distances&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#21253;&#38598;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. Though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. In this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (BRDAD) converting the unsupervised anomaly detection problem into a convex optimization problem. Our BRDAD algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (BWDDE). This approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. Moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated baggi
&lt;/p&gt;</description></item><item><title>&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18083</link><description>&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#65306;&#20004;&#31181;&#35270;&#35282;&#20248;&#20110;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
Meta Co-Training: Two Views are Better than One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18083
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#24456;&#22810;&#65292;&#20294;&#26631;&#31614;&#21364;&#31232;&#32570;&#19988;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25552;&#21319;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#24050;&#32463;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#35201;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#26159;&#20849;&#35757;&#32451;&#12290;&#22312;&#20849;&#35757;&#32451;&#20013;&#65292;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21033;&#29992;&#25968;&#25454;&#30340;&#19981;&#21516;&#29420;&#31435;&#21644;&#36275;&#22815;&#30340;&#8220;&#35270;&#35282;&#8221;&#26469;&#20849;&#21516;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#22312;&#20849;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#19978;&#21019;&#24314;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#25913;&#36827;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#24403;&#29420;&#31435;&#35270;&#35282;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#24265;&#20215;&#22320;&#26500;&#24314;&#36825;&#20123;&#35270;&#35282;&#12290;&#22312;&#26500;&#24314;&#30340;&#35270;&#35282;&#19978;&#36827;&#34892;&#20849;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20248;&#20110;&#25105;&#20204;&#26500;&#24314;&#30340;&#20219;&#20309;&#21333;&#20010;&#35270;&#35282;&#65292;&#24182;&#19988;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#19981;&#21487;&#21462;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23884;&#20837;&#21521;&#37327;&#21387;&#32553;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#29305;&#24449;&#21644;&#26041;&#27861;&#23398;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21387;&#32553;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.15578</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21487;&#23398;&#20064;&#21521;&#37327;&#23384;&#20648;&#21387;&#32553;&#30340;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Analysis of Large-scale Learnable Vector Storage Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23884;&#20837;&#21521;&#37327;&#21387;&#32553;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#29305;&#24449;&#21644;&#26041;&#27861;&#23398;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#21521;&#37327;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#22312;&#21508;&#31181;&#19982;&#25968;&#25454;&#24211;&#30456;&#20851;&#30340;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#31232;&#30095;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#30340;&#24040;&#22823;&#23481;&#37327;&#23548;&#33268;&#23884;&#20837;&#34920;&#30340;&#20869;&#23384;&#28040;&#32791;&#24456;&#22823;&#65292;&#32473;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#21387;&#32553;&#23884;&#20837;&#21521;&#37327;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#27169;&#22411;&#36136;&#37327;&#31245;&#24494;&#19979;&#38477;&#25110;&#24341;&#20837;&#20854;&#20182;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#20854;&#30456;&#23545;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#29616;&#26377;&#30340;&#23454;&#39564;&#27604;&#36739;&#21482;&#28085;&#30422;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#20851;&#27880;&#26377;&#38480;&#30340;&#25351;&#26631;&#12290;&#26412;&#25991;&#23545;&#23884;&#20837;&#21521;&#37327;&#21387;&#32553;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#26681;&#25454;&#29305;&#24449;&#21644;&#26041;&#27861;&#23398;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnable embedding vector is one of the most important applications in machine learning, and is widely used in various database-related domains. However, the high dimensionality of sparse data in recommendation tasks and the huge volume of corpus in retrieval-related tasks lead to a large memory consumption of the embedding table, which poses a great challenge to the training and deployment of models. Recent research has proposed various methods to compress the embeddings at the cost of a slight decrease in model quality or the introduction of other overheads. Nevertheless, the relative performance of these methods remains unclear. Existing experimental comparisons only cover a subset of these methods and focus on limited metrics. In this paper, we perform a comprehensive comparative analysis and experimental evaluation of embedding compression. We introduce a new taxonomy that categorizes these techniques based on their characteristics and methodologies, and further develop a modular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#29983;&#25104;&#26684;&#32599;&#24067;&#32435;&#22522;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.12904</link><description>&lt;p&gt;
&#23398;&#20064;&#35745;&#31639;&#26684;&#32599;&#24067;&#32435;&#22522;
&lt;/p&gt;
&lt;p&gt;
Learning to Compute Gr\"obner Bases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#29983;&#25104;&#26684;&#32599;&#24067;&#32435;&#22522;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22810;&#39033;&#24335;&#31995;&#32479;&#65292;&#25110;&#35745;&#31639;&#30456;&#20851;&#30340;&#26684;&#32599;&#24067;&#32435;&#22522;&#65292;&#19968;&#30452;&#26159;&#35745;&#31639;&#20195;&#25968;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#26114;&#36149;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#26159;&#25351;&#25968;&#32423;&#21452;&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#26469;&#23454;&#29616;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#12290;&#35757;&#32451;&#38656;&#35201;&#35768;&#22810;&#22810;&#39033;&#24335;&#31995;&#32479;&#21644;&#30456;&#20851;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#37197;&#23545;&#65292;&#24341;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#20195;&#25968;&#38382;&#39064;&#65306;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#38543;&#26426;&#29983;&#25104;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#26684;&#32599;&#24067;&#32435;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#32500;&#26681;&#27491;&#29702;&#24819;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#29702;&#24819;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#26041;&#27861;&#27604;&#26420;&#32032;&#26041;&#27861;&#24555;&#19977;&#21040;&#20845;&#20010;&#25968;&#37327;&#32423;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#35745;&#31639;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving a polynomial system, or computing an associated Gr\"obner basis, has been a fundamental task in computational algebra. However, it is also known for its notoriously expensive computational cost - doubly exponential time complexity in the number of variables in the worst case. In this paper, we achieve for the first time Gr\"obner basis computation through the training of a Transformer. The training requires many pairs of a polynomial system and the associated Gr\"obner basis, raising two novel algebraic problems: random generation of Gr\"obner bases and the transformation of them into non-Gr\"obner polynomial systems, termed as backward Gr\"obner problem. We resolve these problems with zero-dimensional radical ideals, the ideals appearing in various applications. The experiments show that the proposed dataset generation method is three to six orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Gr\"obner bases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.03415</link><description>&lt;p&gt;
PowerFlowNet: &#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03415
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#21151;&#29575;&#27969;&#20998;&#26512;&#23545;&#20110;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#30340;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#20026;&#23567;&#22411;&#21644;&#22823;&#22411;&#30005;&#21147;&#32593;&#32476;&#25552;&#20379;&#20934;&#30830;&#21644;&#24555;&#36895;&#35299;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#30001;&#20110;&#30005;&#21147;&#32593;&#32476;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#22270;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#25913;&#21892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PowerFlowNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#21151;&#29575;&#27969;&#36817;&#20284;&#65292;&#22312;&#31616;&#21333;&#30340;IEEE 14&#24635;&#32447;&#31995;&#32479;&#20013;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;(6470rte)&#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;4&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#36817;&#20284;&#26041;&#27861;(&#22914;&#30452;&#27969;&#26494;&#24347;&#27861;)&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#23427;&#20204;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2310.09877</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32463;&#20856;&#25216;&#26415;&#30340;&#32479;&#35745;&#25512;&#26029;&#65306;&#22522;&#20110;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;
&lt;/p&gt;
&lt;p&gt;
Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#32467;&#26524;&#36827;&#34892;&#20840;&#23616;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#38754;&#20020;&#33267;&#23569;&#19977;&#20010;&#25361;&#25112;&#65306;&#30830;&#20445;ALE&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#22312;&#23567;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65307;&#30452;&#35266;&#22320;&#34920;&#24449;&#21464;&#37327;&#22312;ML&#20013;&#30340;&#25972;&#20307;&#25928;&#24212;&#65307;&#20197;&#21450;&#20174;ML&#25968;&#25454;&#20998;&#26512;&#20013;&#36827;&#34892;&#20581;&#22766;&#30340;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#24314;&#31435;&#20102;&#36866;&#24212;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#33258;&#21161;&#27861;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30452;&#35266;&#25351;&#31034;&#23545;&#32467;&#26524;&#21464;&#37327;&#21644;&#26631;&#20934;&#21270;&#23610;&#24230;&#19978;&#30340;&#25928;&#24212;&#30340;ALE&#25928;&#24212;&#22823;&#23567;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#32472;&#21046;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#21453;&#26144;&#20102;ALE&#29087;&#32451;&#31361;&#20986;&#30340;&#28789;&#27963;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;R&#20013;&#8220;ale&#8221;&#21253;&#20013;&#30340;&#23454;&#29616;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;&#20851;&#20110;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#65292;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#22686;&#24378;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2308.10630</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Homogenization Approach for Gradient-Dominated Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#65292;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#22686;&#24378;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#26159;&#19968;&#31181;&#27604;&#24378;&#20984;&#24615;&#26465;&#20214;&#26356;&#24369;&#20294;&#36275;&#20197;&#30830;&#20445;&#20840;&#23616;&#25910;&#25947;&#30340;&#26465;&#20214;&#65292;&#21363;&#20351;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20063;&#21487;&#20197;&#24212;&#29992;&#24191;&#27867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#22343;&#21248;&#21270;&#26041;&#27861;&#30340;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20108;&#38454;&#19979;&#38477;&#26041;&#27861;&#65288;SHSODM&#65289;&#65292;&#29992;&#20110;&#28385;&#36275;&#26799;&#24230;&#20027;&#23548;&#24615;&#36136;&#30340;&#38543;&#26426;&#20989;&#25968;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#22686;&#24378;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;SHSODM&#19982;&#20854;&#20182;&#26799;&#24230;&#20027;&#23548;&#38543;&#26426;&#20248;&#21270;&#30340;&#20108;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#36798;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32780;&#26080;&#38656;&#31435;&#26041;&#27491;&#21017;&#21270;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#30001;&#20110;&#22343;&#21248;&#21270;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#27599;&#27425;&#36845;&#20195;&#20013;&#35299;&#26497;&#20540;&#29305;&#24449;&#21521;&#37327;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#29275;&#39039;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#25152;&#20197;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient dominance property is a condition weaker than strong convexity, yet sufficiently ensures global convergence even in non-convex optimization. This property finds wide applications in machine learning, reinforcement learning (RL), and operations management. In this paper, we propose the stochastic homogeneous second-order descent method (SHSODM) for stochastic functions enjoying gradient dominance property based on a recently proposed homogenization approach. Theoretically, we provide its sample complexity analysis, and further present an enhanced result by incorporating variance reduction techniques. Our findings show that SHSODM matches the best-known sample complexity achieved by other second-order methods for gradient-dominated stochastic optimization but without cubic regularization. Empirically, since the homogenization approach only relies on solving extremal eigenvector problem at each iteration instead of Newton-type system, our methods gain the advantage of cheaper com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26799;&#24230;&#36319;&#36394;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#23450;&#35268;&#21010;&#20998;&#26512;&#26041;&#27861;&#26469;&#33719;&#24471;&#29702;&#24819;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#21253;&#25324;&#26368;&#20808;&#36827;&#31639;&#27861;&#21644;&#26032;&#21160;&#24577;&#29256;&#26412;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2306.06375</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#26799;&#24230;&#36319;&#36394;&#29992;&#20110;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimized Gradient Tracking for Decentralized Online Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26799;&#24230;&#36319;&#36394;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#23450;&#35268;&#21010;&#20998;&#26512;&#26041;&#27861;&#26469;&#33719;&#24471;&#29702;&#24819;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#21253;&#25324;&#26368;&#20808;&#36827;&#31639;&#27861;&#21644;&#26032;&#21160;&#24577;&#29256;&#26412;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#36319;&#36394;&#20998;&#24067;&#22312;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#26102;&#38388;&#21464;&#21270;&#20989;&#25968;&#30340;&#26368;&#20248;&#20540;&#20043;&#21644;&#12290;&#20989;&#25968;&#21644;&#26799;&#24230;&#30340;&#23616;&#37096;&#21487;&#29992;&#24615;&#38656;&#35201;&#33410;&#28857;&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#20849;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#26799;&#24230;&#36319;&#36394;&#65288;GGT&#65289;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#26032;&#39062;&#20998;&#26512;&#26041;&#27861;&#23545;&#25152;&#25552;&#20986;&#30340;GGT&#31639;&#27861;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#22312;&#38750;&#24120;&#19968;&#33324;&#30340;&#26465;&#20214;&#19979;&#19988;&#19981;&#38656;&#35201;&#26799;&#24230;&#26377;&#30028;&#24615;&#20551;&#35774;&#30340;&#29702;&#24819;&#36951;&#25022;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;GGT&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#21253;&#25324;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20197;&#21450;&#21508;&#31181;&#21476;&#20856;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#26032;&#21160;&#24577;&#29256;&#26412;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23567;&#36951;&#25022;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21482;&#26377;&#22235;&#20010;&#33258;&#30001;&#21442;&#25968;&#30340;GGT&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work considers the problem of decentralized online learning, where the goal is to track the optimum of the sum of time-varying functions, distributed across several nodes in a network. The local availability of the functions and their gradients necessitates coordination and consensus among the nodes. We put forth the Generalized Gradient Tracking (GGT) framework that unifies a number of existing approaches, including the state-of-the-art ones. The performance of the proposed GGT algorithm is theoretically analyzed using a novel semidefinite programming-based analysis that yields the desired regret bounds under very general conditions and without requiring the gradient boundedness assumption. The results are applicable to the special cases of GGT, which include various state-of-the-art algorithms as well as new dynamic versions of various classical decentralized algorithms. To further minimize the regret, we consider a condensed version of GGT with only four free parameters. A proc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#24555;&#36895;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.08353</link><description>&lt;p&gt;
&#24555;&#36895;&#39640;&#25928;&#30340;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#21305;&#37197;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and Efficient Matching Algorithm with Deadline Instances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#24555;&#36895;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#30001;&#20110;&#20854;&#20247;&#22810;&#24212;&#29992;&#32780;&#25104;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#20570;&#20102;&#24456;&#22810;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#31639;&#27861;&#35201;&#20040;&#36895;&#24230;&#22826;&#24930;&#65292;&#35201;&#20040;&#27809;&#26377;&#32771;&#34385;&#21040;&#25130;&#27490;&#26399;&#38480;&#65288;&#33410;&#28857;&#21487;&#20197;&#21305;&#37197;&#30340;&#26368;&#38271;&#26102;&#38388;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#27491;&#30830;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#22312;FastGreedy&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19968;&#20010;&#33410;&#28857;&#26159;&#20080;&#23478;&#36824;&#26159;&#21334;&#23478;&#12290;&#20294;&#22312;FastPostponedGreedy&#31639;&#27861;&#20013;&#65292;&#19968;&#24320;&#22987;&#25105;&#20204;&#19981;&#30693;&#36947;&#27599;&#20010;&#33410;&#28857;&#30340;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#19968;&#20010;&#33609;&#22270;&#30697;&#38453;&#65292;&#20197;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#21407;&#22987;&#31639;&#27861;&#21644;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#35774; &#949; &#8712;&#65288;0,0.1&#65289;&#34920;&#31034;&#27599;&#26465;&#36793;&#30340;&#30495;&#23454;&#26435;&#37325;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;&#21407;&#22987;&#30340;Greedy&#21644;Po&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#29575;&#26159;&#22810;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online weighted matching problem is a fundamental problem in machine learning due to its numerous applications. Despite many efforts in this area, existing algorithms are either too slow or don't take $\mathrm{deadline}$ (the longest time a node can be matched) into account. In this paper, we introduce a market model with $\mathrm{deadline}$ first. Next, we present our two optimized algorithms (\textsc{FastGreedy} and \textsc{FastPostponedGreedy}) and offer theoretical proof of the time complexity and correctness of our algorithms. In \textsc{FastGreedy} algorithm, we have already known if a node is a buyer or a seller. But in \textsc{FastPostponedGreedy} algorithm, the status of each node is unknown at first. Then, we generalize a sketching matrix to run the original and our algorithms on both real data sets and synthetic data sets. Let $\epsilon \in (0,0.1)$ denote the relative error of the real weight of each edge. The competitive ratio of original \textsc{Greedy} and \textsc{Po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#23458;&#25143;&#30340;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#39640;&#36136;&#37327;&#36127;&#33655;&#26354;&#32447;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2304.12076</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#21147;&#23458;&#25143;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.12076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#23458;&#25143;&#30340;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#39640;&#36136;&#37327;&#36127;&#33655;&#26354;&#32447;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#30340;&#36127;&#33655;&#26354;&#32447;&#26159;&#25903;&#25345;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#30340;&#20851;&#38190;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37319;&#38598;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#32570;&#20047;&#36275;&#22815;&#30340;&#21382;&#21490;&#36127;&#33655;&#26354;&#32447;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#20026;&#23458;&#25143;&#25552;&#20379;&#21512;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#24314;&#31435;&#39640;&#24615;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#36127;&#33655;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#20351;&#29992;&#22522;&#20110;&#21508;&#33258;&#23458;&#25143;&#25968;&#25454;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#36127;&#33655;&#26354;&#32447;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#23450;&#21046;&#21512;&#25104;&#36716;&#21270;&#20026;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#20026;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customers' load profiles are critical resources to support data analytics applications in modern power systems. However, there are usually insufficient historical load profiles for data analysis, due to the collection cost and data privacy issues. To address such data shortage problems, load profiles synthesis is an effective technique that provides synthetic training data for customers to build high-performance data-driven models. Nonetheless, it is still challenging to synthesize high-quality load profiles for each customer using generation models trained by the respective customer's data owing to the high heterogeneity of customer load. In this paper, we propose a novel customized load profiles synthesis method based on conditional diffusion models for heterogeneous customers. Specifically, we first convert the customized synthesis into a conditional data generation issue. We then extend traditional diffusion models to conditional diffusion models to realize conditional data generat
&lt;/p&gt;</description></item><item><title>MFAI&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#26469;&#20811;&#26381;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#24046;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#28789;&#27963;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#23545;&#36741;&#21161;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.02566</link><description>&lt;p&gt;
MFAI:&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#26469;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MFAI: A Scalable Bayesian Matrix Factorization Approach to Leveraging Auxiliary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02566
&lt;/p&gt;
&lt;p&gt;
MFAI&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#26469;&#20811;&#26381;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#24046;&#23548;&#33268;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#28789;&#27963;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#23545;&#36741;&#21161;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#25968;&#25454;&#36136;&#37327;&#24046;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20363;&#22914;&#25968;&#25454;&#31232;&#30095;&#24615;&#39640;&#21644;&#20449;&#22122;&#27604;&#20302;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#30340;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#36741;&#21161;&#20449;&#24687;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#22823;&#37327;&#21487;&#29992;&#30340;&#65292;&#20197;&#20811;&#26381;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#24046;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#23558;&#36741;&#21161;&#20449;&#24687;&#19982;&#20027;&#25968;&#25454;&#30697;&#38453;&#32467;&#21512;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26799;&#24230;&#22686;&#24378;&#26641;&#38598;&#25104;&#21040;&#27010;&#29575;&#30697;&#38453;&#20998;&#35299;&#26694;&#26550;&#20013;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;(MFAI)&#12290;&#22240;&#27492;&#65292;MFAI&#33258;&#28982;&#22320;&#32487;&#25215;&#20102;&#26799;&#24230;&#22686;&#24378;&#26641;&#30340;&#20960;&#20010;&#26174;&#33879;&#29305;&#28857;&#65292;&#22914;&#28789;&#27963;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#12289;&#23545;&#36741;&#21161;&#20449;&#24687;&#20013;&#30340;&#19981;&#30456;&#20851;&#29305;&#24449;&#21644;&#32570;&#22833;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;MFAI&#20013;&#30340;&#21442;&#25968;&#21487;&#20197;&#22312;&#32463;&#39564;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#33258;&#21160;&#30830;&#23450;&#65292;&#20351;&#20854;&#36866;&#24212;&#20110;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various practical situations, matrix factorization methods suffer from poor data quality, such as high data sparsity and low signal-to-noise ratio (SNR). Here, we consider a matrix factorization problem by utilizing auxiliary information, which is massively available in real-world applications, to overcome the challenges caused by poor data quality. Unlike existing methods that mainly rely on simple linear models to combine auxiliary information with the main data matrix, we propose to integrate gradient boosted trees in the probabilistic matrix factorization framework to effectively leverage auxiliary information (MFAI). Thus, MFAI naturally inherits several salient features of gradient boosted trees, such as the capability of flexibly modeling nonlinear relationships and robustness to irrelevant features and missing values in auxiliary information. The parameters in MFAI can be automatically determined under the empirical Bayes framework, making it adaptive to the utilization of a
&lt;/p&gt;</description></item><item><title>nSimplex Zen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#38477;&#32500;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25104;&#23545;&#36317;&#31163;&#26469;&#38477;&#20302;&#39640;&#32500;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#23427;&#22312;&#29289;&#29702;&#20869;&#23384;&#20351;&#29992;&#21644;&#36317;&#31163;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2302.11508</link><description>&lt;p&gt;
nSimplex Zen:&#19968;&#31181;&#26032;&#39062;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19982;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
nSimplex Zen: A Novel Dimensionality Reduction for Euclidean and Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.11508
&lt;/p&gt;
&lt;p&gt;
nSimplex Zen&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#38477;&#32500;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25104;&#23545;&#36317;&#31163;&#26469;&#38477;&#20302;&#39640;&#32500;&#31354;&#38388;&#30340;&#32500;&#24230;&#12290;&#23427;&#22312;&#29289;&#29702;&#20869;&#23384;&#20351;&#29992;&#21644;&#36317;&#31163;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#25968;&#32422;&#31616;&#25216;&#26415;&#23558;&#39640;&#32500;&#31354;&#38388;&#30340;&#20540;&#26144;&#23556;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#12290;&#32467;&#26524;&#26159;&#38656;&#35201;&#36739;&#23569;&#29289;&#29702;&#20869;&#23384;&#19988;&#20855;&#26377;&#26356;&#24555;&#36317;&#31163;&#35745;&#31639;&#30340;&#31354;&#38388;&#12290;&#36825;&#20123;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38656;&#35201;&#28385;&#36275;&#38477;&#32500;&#31354;&#38388;&#25152;&#20855;&#26377;&#30340;&#21487;&#25509;&#21463;&#31934;&#24230;&#19982;&#21407;&#22987;&#31354;&#38388;&#30456;&#27604;&#30340;&#22330;&#26223;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#36716;&#25442;&#26041;&#27861;&#24050;&#32463;&#34987;&#25551;&#36848;&#20986;&#26469;&#12290;&#23427;&#20204;&#34987;&#20998;&#20026;&#20004;&#22823;&#31867;&#65306;&#32447;&#24615;&#21644;&#25299;&#25169;&#12290;&#32447;&#24615;&#26041;&#27861;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;(PCA)&#21644;&#38543;&#26426;&#25237;&#24433;(RP)&#23558;&#36716;&#25442;&#25104;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#36739;&#20302;&#32500;&#24230;&#30340;&#22522;&#20110;&#30697;&#38453;&#30340;&#36716;&#25442;&#12290;&#25299;&#25169;&#26041;&#27861;&#22914;&#22810;&#32500;&#32553;&#25918;(MDS)&#23581;&#35797;&#20445;&#25345;&#26356;&#39640;&#23618;&#27425;&#30340;&#29305;&#24615;&#65292;&#22914;&#26368;&#36817;&#37051;&#20851;&#31995;&#65292;&#26377;&#20123;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25299;&#25169;&#38477;&#32500;&#26041;&#27861;nSimplex Zen&#12290;&#19982;MDS&#31867;&#20284;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#21407;&#22987;&#31354;&#38388;&#20013;&#30340;&#25104;&#23545;&#36317;&#31163;&#12290;&#20351;&#29992;d
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction techniques map values from a high dimensional space to one with a lower dimension. The result is a space which requires less physical memory and has a faster distance calculation. These techniques are widely used where required properties of the reduced-dimension space give an acceptable accuracy with respect to the original space. Many such transforms have been described. They have been classified in two main groups: linear and topological. Linear methods such as Principal Component Analysis (PCA) and Random Projection (RP) define matrix-based transforms into a lower dimension of Euclidean space. Topological methods such as Multidimensional Scaling (MDS) attempt to preserve higher-level aspects such as the nearest-neighbour relation, and some may be applied to non-Euclidean spaces. Here, we introduce nSimplex Zen, a novel topological method of reducing dimensionality. Like MDS, it relies only upon pairwise distances measured in the original space. The use of d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.00284</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Selective Uncertainty Propagation in Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#27573;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#65292;&#30446;&#26631;&#22312;&#20110;&#24212;&#23545;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20013;&#27599;&#19968;&#27493;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35780;&#20272;&#31163;&#24320;&#34892;&#20026;&#31574;&#30053;&#22312;&#31532;h&#27493;&#26102;&#30340;&#22788;&#29702;&#25928;&#26524;&#65292;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#36825;&#19968;&#27493;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#27599;&#19968;&#27493;&#31574;&#30053;&#37117;&#20250;&#24433;&#21709;&#19979;&#19968;&#29366;&#24577;&#30340;&#20998;&#24067;&#65292;&#30456;&#20851;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#19978;&#27604;&#38543;&#26426;&#24773;&#22659;&#25361;&#25112;&#19979;&#30340;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#26356;&#21152;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#38590;&#24230;&#20171;&#20110;&#36825;&#20004;&#31181;&#24773;&#22659;&#20043;&#38388;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#29992;&#20110;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#38590;&#24230;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#38236;&#38754;&#19979;&#38477;&#33258;&#28982;&#22320;&#36866;&#24212;&#36890;&#29992;&#21442;&#25968;&#21270;&#65292;&#24182;&#33719;&#24471;&#20102;&#24212;&#29992;&#20110;&#36890;&#29992;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#25919;&#31574;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2301.13139</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#36890;&#29992;&#21442;&#25968;&#21270;&#21644;&#32447;&#24615;&#25910;&#25947;&#30340;&#25919;&#31574;&#38236;&#38754;&#19979;&#38477;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Novel Framework for Policy Mirror Descent with General Parameterization and Linear Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13139
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#38236;&#38754;&#19979;&#38477;&#33258;&#28982;&#22320;&#36866;&#24212;&#36890;&#29992;&#21442;&#25968;&#21270;&#65292;&#24182;&#33719;&#24471;&#20102;&#24212;&#29992;&#20110;&#36890;&#29992;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#25919;&#31574;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#29616;&#20195;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30340;&#25104;&#21151;&#24402;&#21151;&#20110;&#21442;&#25968;&#21270;&#25919;&#31574;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#20026;&#36825;&#31867;&#31639;&#27861;&#22312;&#26631;&#31614;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20294;&#23545;&#20110;&#36890;&#29992;&#21442;&#25968;&#21270;&#26041;&#26696;&#30340;&#20351;&#29992;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38236;&#38754;&#19979;&#38477;&#30340;&#25919;&#31574;&#20248;&#21270;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;&#36890;&#29992;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#26041;&#26696;&#25152;&#20135;&#29983;&#30340;&#25919;&#31574;&#31867;&#21487;&#20197;&#24674;&#22797;&#24050;&#30693;&#30340;&#31867;&#65292;&#22914;softmax&#65292;&#24182;&#26681;&#25454;&#38236;&#38754;&#26144;&#23556;&#30340;&#36873;&#25321;&#29983;&#25104;&#26032;&#31867;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20851;&#20110;&#28041;&#21450;&#36890;&#29992;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#25919;&#31574;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#24212;&#36890;&#29992;&#21442;&#25968;&#21270;&#26041;&#26696;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern policy optimization methods in reinforcement learning, such as TRPO and PPO, owe their success to the use of parameterized policies. However, while theoretical guarantees have been established for this class of algorithms, especially in the tabular setting, the use of general parameterization schemes remains mostly unjustified. In this work, we introduce a novel framework for policy optimization based on mirror descent that naturally accommodates general parameterizations. The policy class induced by our scheme recovers known classes, e.g., softmax, and generates new ones depending on the choice of mirror map. Using our framework, we obtain the first result that guarantees linear convergence for a policy-gradient-based method involving general parameterization. To demonstrate the ability of our framework to accommodate general parameterization schemes, we provide its sample complexity when using shallow neural networks, show that it represents an improvement upon the previous be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#22240;&#26524;DAG&#30340;&#23376;&#38598;&#20851;&#31995;&#26102;&#65292;&#35782;&#21035;&#25152;&#38656;&#26368;&#23567;&#24178;&#39044;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#23376;&#38598;&#39564;&#35777;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#26368;&#23567;&#24178;&#39044;&#38598;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#22312;&#23376;&#38598;&#25628;&#32034;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2301.03180</link><description>&lt;p&gt;
&#22240;&#26524;DAG&#30340;&#23376;&#38598;&#39564;&#35777;&#21644;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Subset verification and search algorithms for causal DAGs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#22240;&#26524;DAG&#30340;&#23376;&#38598;&#20851;&#31995;&#26102;&#65292;&#35782;&#21035;&#25152;&#38656;&#26368;&#23567;&#24178;&#39044;&#38598;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#31639;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#23376;&#38598;&#39564;&#35777;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#26368;&#23567;&#24178;&#39044;&#38598;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#22312;&#23376;&#38598;&#25628;&#32034;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26159;&#34920;&#31034;&#22240;&#26524;&#20851;&#31995;&#30340;&#24120;&#35265;&#36873;&#25321;&#12290;&#30001;&#20110;&#25105;&#20204;&#21482;&#33021;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340; Markov &#31561;&#20215;&#31867;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#24178;&#39044;&#26469;&#36827;&#34892;&#24674;&#22797;&#20219;&#21153;&#12290;&#24178;&#39044;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#35774;&#35745;&#33021;&#22815;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#30340;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23398;&#20064;&#19968;&#32452;&#36793;&#32536;&#65288;&#30446;&#26631;&#36793;&#32536;&#65289;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35782;&#21035;&#25152;&#38656;&#26368;&#23567;&#24178;&#39044;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#12289;&#22240;&#26524;&#20805;&#20998;&#24615;&#21644;&#29702;&#24819;&#24178;&#39044;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#24403;&#24213;&#23618;&#30495;&#23454;&#22240;&#26524;&#22270;&#24050;&#30693;&#26102;&#65288;&#23376;&#38598;&#39564;&#35777;&#65289;&#65292;&#20197;&#21450;&#24403;&#20854;&#26410;&#30693;&#26102;&#65288;&#23376;&#38598;&#25628;&#32034;&#65289;&#12290;&#23545;&#20110;&#23376;&#38598;&#39564;&#35777;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#23567;&#24178;&#39044;&#38598;&#65307;&#25105;&#20204;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#23376;&#38598;&#25628;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal relationships between variables is a fundamental task in causal inference and directed acyclic graphs (DAGs) are a popular choice to represent the causal relationships. As one can recover a causal graph only up to its Markov equivalence class from observations, interventions are often used for the recovery task. Interventions are costly in general and it is important to design algorithms that minimize the number of interventions performed. In this work, we study the problem of identifying the smallest set of interventions required to learn the causal relationships between a subset of edges (target edges). Under the assumptions of faithfulness, causal sufficiency, and ideal interventions, we study this problem in two settings: when the underlying ground truth causal graph is known (subset verification) and when it is unknown (subset search). For the subset verification problem, we provide an efficient algorithm to compute a minimum sized interventional set; we further ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#12290;&#36890;&#36807;&#25490;&#38500;&#31526;&#21495;&#20449;&#24687;&#24182;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#65292;&#35813;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2209.10712</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#31526;&#21495;&#26816;&#32034;&#22312;&#22522;&#20110;DCT&#30340;&#22270;&#20687;&#32534;&#30721;&#20013;&#21387;&#32553;&#31526;&#21495;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Compressing Sign Information in DCT-based Image Coding via Deep Sign Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.10712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26469;&#39640;&#25928;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#12290;&#36890;&#36807;&#25490;&#38500;&#31526;&#21495;&#20449;&#24687;&#24182;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#65292;&#35813;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31526;&#21495;&#30340;&#31561;&#27010;&#29575;&#29305;&#24615;&#65292;&#21387;&#32553;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#26159;&#22270;&#20687;&#32534;&#30721;&#26041;&#26696;&#20013;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31526;&#21495;&#26816;&#32034;&#8221;&#30340;&#31526;&#21495;&#20449;&#24687;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#30456;&#20301;&#24674;&#22797;&#30340;&#21551;&#21457;&#65292;&#30456;&#20301;&#24674;&#22797;&#26159;&#20174;&#20854;&#24133;&#24230;&#20013;&#25214;&#21040;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#31995;&#25968;&#30340;&#30456;&#20301;&#20449;&#24687;&#30340;&#32463;&#20856;&#20449;&#21495;&#24674;&#22797;&#38382;&#39064;&#12290;&#22312;&#32534;&#30721;&#22120;&#20013;&#65292;&#25152;&#26377;DCT&#31995;&#25968;&#30340;&#31526;&#21495;&#20449;&#24687;&#34987;&#25490;&#38500;&#22312;&#27604;&#29305;&#27969;&#20043;&#22806;&#65292;&#24182;&#19988;&#36890;&#36807;&#25105;&#20204;&#30340;&#31526;&#21495;&#26816;&#32034;&#26041;&#27861;&#22312;&#35299;&#30721;&#22120;&#20013;&#34917;&#20805;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31526;&#21495;&#20301;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;Python&#35821;&#35328;&#23454;&#29616;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;https://github.com/ctsutake/dsr&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing the sign information of discrete cosine transform (DCT) coefficients is an intractable problem in image coding schemes due to the equiprobable characteristics of the signs. To overcome this difficulty, we propose an efficient compression method for the sign information called "sign retrieval." This method is inspired by phase retrieval, which is a classical signal restoration problem of finding the phase information of discrete Fourier transform coefficients from their magnitudes. The sign information of all DCT coefficients is excluded from a bitstream at the encoder and is complemented at the decoder through our sign retrieval method. We show through experiments that our method outperforms previous ones in terms of the bit amount for the signs and computation cost. Our method, implemented in Python language, is available from https://github.com/ctsutake/dsr.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27969;&#27880;&#35270;&#20272;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;MSGazeNet&#65292;&#36890;&#36807;&#21033;&#29992;&#30524;&#37096;&#35299;&#21078;&#20449;&#24687;&#23398;&#20064;&#27880;&#35270;&#34920;&#31034;&#12290;&#30740;&#31350;&#39318;&#20808;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#32593;&#32476;&#26469;&#38548;&#31163;&#30524;&#37096;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#23454;&#29616;&#23558;&#35813;&#32593;&#32476;&#36716;&#31227;&#21040;&#30495;&#23454;&#39046;&#22495;&#12290;&#36890;&#36807;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30524;&#37096;&#22270;&#20687;&#19978;&#29983;&#25104;&#25513;&#27169;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2206.09256</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21040;&#30495;&#23454;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#35299;&#21078;&#30524;&#21306;&#38548;&#31163;&#30340;&#22810;&#27969;&#27880;&#35270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multistream Gaze Estimation with Anatomical Eye Region Isolation by Synthetic to Real Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.09256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27969;&#27880;&#35270;&#20272;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;MSGazeNet&#65292;&#36890;&#36807;&#21033;&#29992;&#30524;&#37096;&#35299;&#21078;&#20449;&#24687;&#23398;&#20064;&#27880;&#35270;&#34920;&#31034;&#12290;&#30740;&#31350;&#39318;&#20808;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#32593;&#32476;&#26469;&#38548;&#31163;&#30524;&#37096;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#23454;&#29616;&#23558;&#35813;&#32593;&#32476;&#36716;&#31227;&#21040;&#30495;&#23454;&#39046;&#22495;&#12290;&#36890;&#36807;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30524;&#37096;&#22270;&#20687;&#19978;&#29983;&#25104;&#25513;&#27169;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;MSGazeNet&#65292;&#36890;&#36807;&#22810;&#27969;&#27880;&#35270;&#20272;&#35745;&#20174;&#30524;&#37096;&#35299;&#21078;&#20449;&#24687;&#20013;&#23398;&#20064;&#27880;&#35270;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#31532;&#19968;&#20010;&#26159;&#29992;&#20110;&#38548;&#31163;&#35299;&#21078;&#30524;&#21306;&#30340;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#26159;&#29992;&#20110;&#22810;&#27969;&#27880;&#35270;&#20272;&#35745;&#30340;&#32593;&#32476;&#12290;&#30524;&#37096;&#21306;&#22495;&#38548;&#31163;&#26159;&#20351;&#29992;U-Net&#39118;&#26684;&#30340;&#32593;&#32476;&#36827;&#34892;&#30340;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#21487;&#35265;&#30524;&#29699;&#21644;&#34425;&#33180;&#21306;&#22495;&#30524;&#37096;&#21306;&#22495;&#25513;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#38454;&#27573;&#20351;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;UnityEyes&#27169;&#25311;&#22120;&#33719;&#24471;&#30340;&#65292;&#21253;&#21547;&#20102;80,000&#20010;&#30524;&#37096;&#22270;&#20687;&#12290;&#22312;&#35757;&#32451;&#21518;&#65292;&#30524;&#37096;&#21306;&#22495;&#38548;&#31163;&#32593;&#32476;&#34987;&#36716;&#31227;&#21040;&#30495;&#23454;&#39046;&#22495;&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;&#30524;&#37096;&#22270;&#20687;&#30340;&#25513;&#27169;&#12290;&#20026;&#20102;&#25104;&#21151;&#36827;&#34892;&#36801;&#31227;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;&#20266;&#20687;&#30340;&#22686;&#24378;&#25216;&#26415;&#20351;&#21512;&#25104;&#22270;&#20687;&#21463;&#30410;&#20110;&#26356;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural pipeline, MSGazeNet, that learns gaze representations by taking advantage of the eye anatomy information through a multistream framework. Our proposed solution comprises two components, first a network for isolating anatomical eye regions, and a second network for multistream gaze estimation. The eye region isolation is performed with a U-Net style network which we train using a synthetic dataset that contains eye region masks for the visible eyeball and the iris region. The synthetic dataset used in this stage is procured using the UnityEyes simulator, and consists of 80,000 eye images. Successive to training, the eye region isolation network is then transferred to the real domain for generating masks for the real-world eye images. In order to successfully make the transfer, we exploit domain randomization in the training process, which allows for the synthetic images to benefit from a larger variance with the help of augmentations that resemble artifacts. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#31283;&#23450;&#24615;&#26469;&#20272;&#35745;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21442;&#25968;&#36890;&#36807;&#22122;&#22768;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;&#36755;&#20986;&#19982;&#21407;&#22987;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2205.13340</link><description>&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning with Noise Stability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.13340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#31283;&#23450;&#24615;&#26469;&#20272;&#35745;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21442;&#25968;&#36890;&#36807;&#22122;&#22768;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;&#36755;&#20986;&#19982;&#21407;&#22987;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#65292;&#30001;&#20110;&#27169;&#22411;&#25512;&#26029;&#30340;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#36873;&#25321;&#36807;&#31243;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#20511;&#21161;&#29305;&#27530;&#30340;&#23398;&#20064;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#23398;&#20064;&#65289;&#25110;&#36741;&#21161;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#22797;&#26434;&#19988;&#20302;&#25928;&#30340;&#27969;&#31243;&#65292;&#20351;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#31283;&#23450;&#24615;&#26469;&#20272;&#35745;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#30340;&#26032;&#31639;&#27861;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#27169;&#22411;&#21442;&#25968;&#36890;&#36807;&#22122;&#22768;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;&#27979;&#37327;&#36755;&#20986;&#19982;&#21407;&#22987;&#35266;&#27979;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#23567;&#39640;&#26031;&#22122;&#22768;&#29702;&#35770;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#36873;&#25321;&#20855;&#26377;&#22823;&#32780;&#22810;&#26679;&#30340;&#26799;&#24230;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation for unlabeled data is crucial to active learning. With a deep neural network employed as the backbone model, the data selection process is highly challenging due to the potential over-confidence of the model inference. Existing methods resort to special learning fashions (e.g. adversarial) or auxiliary models to address this challenge. This tends to result in complex and inefficient pipelines, which would render the methods impractical. In this work, we propose a novel algorithm that leverages noise stability to estimate data uncertainty. The key idea is to measure the output derivation from the original observation when the model parameters are randomly perturbed by noise. We provide theoretical analyses by leveraging the small Gaussian noise theory and demonstrate that our method favors a subset with large and diverse gradients. Our method is generally applicable in various tasks, including computer vision, natural language processing, and structural data analy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#24207;&#21015;&#25171;&#20998;&#35268;&#21017;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#27010;&#29575;&#39044;&#27979;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2112.08217</link><description>&lt;p&gt;
&#36890;&#36807;&#25171;&#20998;&#35268;&#21017;&#26368;&#23567;&#21270;&#23454;&#29616;&#29983;&#25104;&#32593;&#32476;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Generative Networks via Scoring Rule Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.08217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#24207;&#21015;&#25171;&#20998;&#35268;&#21017;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#27010;&#29575;&#39044;&#27979;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#39044;&#27979;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#25552;&#20379;&#26410;&#26469;&#32467;&#26524;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#25171;&#20998;&#35268;&#21017;&#19982;&#23454;&#38469;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#36716;&#25442;&#28508;&#22312;&#21464;&#37327;&#30340;&#25277;&#26679;&#26469;&#21442;&#25968;&#21270;&#39640;&#32500;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#12290;&#29983;&#25104;&#32593;&#32476;&#36890;&#24120;&#22312;&#23545;&#25239;&#24615;&#26694;&#26550;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#27979;&#24207;&#21015;&#25171;&#20998;&#35268;&#21017;&#22312;&#35760;&#24405;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#35757;&#32451;&#29983;&#25104;&#32593;&#32476;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#24120;&#35268;&#35780;&#20272;&#39044;&#27979;&#31995;&#32479;&#30340;&#26041;&#24335;&#30456;&#19968;&#33268;&#12290;&#23545;&#20110;&#26576;&#20123;&#25171;&#20998;&#35268;&#21017;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;&#23545;&#25239;&#30340;&#26368;&#23567;&#21270;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#32780;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#20302;&#20272;&#65292;&#20174;&#32780;&#22312;&#27010;&#29575;&#39044;&#27979;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#29983;&#25104;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting relies on past observations to provide a probability distribution for a future outcome, which is often evaluated against the realization using a scoring rule. Here, we perform probabilistic forecasting with generative neural networks, which parametrize distributions on high-dimensional spaces by transforming draws from a latent variable. Generative networks are typically trained in an adversarial framework. In contrast, we propose to train generative networks to minimize a predictive-sequential (or prequential) scoring rule on a recorded temporal sequence of the phenomenon of interest, which is appealing as it corresponds to the way forecasting systems are routinely evaluated. Adversarial-free minimization is possible for some scoring rules; hence, our framework avoids the cumbersome hyperparameter tuning and uncertainty underestimation due to unstable adversarial training, thus unlocking reliable use of generative networks in probabilistic forecasting. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21518;&#26524;&#20026;&#23548;&#21521;&#30340;&#35774;&#35745;&#20844;&#24179;&#31639;&#27861;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#20915;&#31574;&#30340;&#21518;&#26524;&#65292;&#32780;&#38750;&#20165;&#20165;&#36861;&#27714;&#22312;&#31181;&#26063;&#25110;&#24615;&#21035;&#32676;&#20307;&#38388;&#30340;&#22343;&#34913;&#12290;</title><link>https://arxiv.org/abs/2109.08792</link><description>&lt;p&gt;
&#23398;&#20250;&#20844;&#24179;&#65306;&#19968;&#31181;&#23545;&#20844;&#27491;&#20915;&#31574;&#30340;&#21518;&#26524;&#20027;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to be Fair: A Consequentialist Approach to Equitable Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21518;&#26524;&#20026;&#23548;&#21521;&#30340;&#35774;&#35745;&#20844;&#24179;&#31639;&#27861;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#20915;&#31574;&#30340;&#21518;&#26524;&#65292;&#32780;&#38750;&#20165;&#20165;&#36861;&#27714;&#22312;&#31181;&#26063;&#25110;&#24615;&#21035;&#32676;&#20307;&#38388;&#30340;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#31639;&#27861;&#20844;&#24179;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20027;&#35201;&#38598;&#20013;&#20110;&#22312;&#31181;&#26063;&#25110;&#24615;&#21035;&#32676;&#20307;&#38388;&#23454;&#29616;&#20915;&#31574;&#12289;&#32467;&#26524;&#25110;&#38169;&#35823;&#29575;&#30340;&#22343;&#34913;&#12290;&#20030;&#20010;&#20363;&#23376;&#65292;&#32771;&#34385;&#19968;&#20010;&#20551;&#35774;&#30340;&#25919;&#24220;&#20849;&#20139;&#20986;&#34892;&#39033;&#30446;&#65292;&#20026;&#21363;&#23558;&#19978;&#24237;&#30340;&#20302;&#25910;&#20837;&#20154;&#32676;&#25552;&#20379;&#20132;&#36890;&#34917;&#21161;&#12290;&#36981;&#24490;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26681;&#25454;&#27599;&#32654;&#20803;&#20272;&#35745;&#30340;&#27835;&#30103;&#25928;&#26524;&#26368;&#39640;&#20026;&#26631;&#20934;&#20998;&#37197;&#20056;&#36710;&#65292;&#21516;&#26102;&#38480;&#21046;&#21508;&#20010;&#31181;&#26063;&#32676;&#20307;&#30340;&#25903;&#20986;&#30456;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#24573;&#35270;&#20102;&#27492;&#31867;&#32422;&#26463;&#30340;&#19979;&#28216;&#21518;&#26524;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#20260;&#23475;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#36317;&#31163;&#27861;&#24237;&#26356;&#36828;&#65292;&#23454;&#34892;&#24179;&#31561;&#25903;&#20986;&#24517;&#28982;&#24847;&#21619;&#30528;&#25552;&#20379;&#30340;&#24635;&#20056;&#36710;&#27425;&#25968;&#20943;&#23569;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22810;&#20154;&#22240;&#32570;&#24109;&#24237;&#23457;&#32780;&#21463;&#21040;&#24809;&#32602;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#20844;&#24179;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#20915;&#31574;&#30340;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an attempt to make algorithms fair, the machine learning literature has largely focused on equalizing decisions, outcomes, or error rates across race or gender groups. To illustrate, consider a hypothetical government rideshare program that provides transportation assistance to low-income people with upcoming court dates. Following this literature, one might allocate rides to those with the highest estimated treatment effect per dollar, while constraining spending to be equal across race groups. That approach, however, ignores the downstream consequences of such constraints, and, as a result, can induce unexpected harms. For instance, if one demographic group lives farther from court, enforcing equal spending would necessarily mean fewer total rides provided, and potentially more people penalized for missing court. Here we present an alternative framework for designing equitable algorithms that foregrounds the consequences of decisions. In our approach, one first elicits stakeholder
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#32500;&#24230;&#22312;&#32447;&#20915;&#31574;&#30340;&#38543;&#26426;&#20302;&#31209;&#24352;&#37327;&#36172;&#21338;&#31639;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#26377;&#19978;&#19979;&#25991;&#21644;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2007.15788</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#32500;&#24230;&#22312;&#32447;&#20915;&#31574;&#30340;&#38543;&#26426;&#20302;&#31209;&#24352;&#37327;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Low-rank Tensor Bandits for Multi-dimensional Online Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.15788
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#32500;&#24230;&#22312;&#32447;&#20915;&#31574;&#30340;&#38543;&#26426;&#20302;&#31209;&#24352;&#37327;&#36172;&#21338;&#31639;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#26377;&#19978;&#19979;&#25991;&#21644;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#24230;&#22312;&#32447;&#20915;&#31574;&#22312;&#22312;&#32447;&#25512;&#33616;&#21644;&#25968;&#23383;&#33829;&#38144;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#20915;&#31574;&#26159;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#23454;&#20307;&#30340;&#36873;&#25321;&#30340;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#20302;&#31209;&#24352;&#37327;&#36172;&#21338;&#31639;&#27861;&#65292;&#19968;&#31867;&#20854;&#22343;&#20540;&#25910;&#30410;&#21487;&#34920;&#31034;&#20026;&#20302;&#31209;&#24352;&#37327;&#30340;&#36172;&#21338;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#20917;&#65292;&#21363;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24352;&#37327;&#36172;&#21338;&#21644;&#26377;&#19978;&#19979;&#25991;&#30340;&#24352;&#37327;&#36172;&#21338;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#20013;&#65292;&#24179;&#21488;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#26368;&#20339;&#20915;&#31574;&#65292;&#21363;&#30495;&#23454;&#22238;&#25253;&#24352;&#37327;&#30340;&#26368;&#22823;&#26465;&#30446;&#12290;&#22312;&#31532;&#20108;&#31181;&#24773;&#20917;&#20013;&#65292;&#24352;&#37327;&#30340;&#26576;&#20123;&#27169;&#24335;&#26159;&#19978;&#19979;&#25991;&#65292;&#20854;&#20313;&#27169;&#24335;&#26159;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20339;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#65306;&#24352;&#37327;&#28040;&#38500;&#21644;&#24352;&#37327;&#26102;&#20195;&#36138;&#23146;&#31639;&#27861;&#65292;&#29992;&#20110;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24352;&#37327;&#36172;&#21338;&#65292;&#24182;&#20026;&#23427;&#20204;&#25512;&#23548;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#19982;&#29616;&#26377;&#30340;&#31454;&#20105;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional online decision making plays a crucial role in many real applications such as online recommendation and digital marketing. In these problems, a decision at each time is a combination of choices from different types of entities. To solve it, we introduce stochastic low-rank tensor bandits, a class of bandits whose mean rewards can be represented as a low-rank tensor. We consider two settings, tensor bandits without context and tensor bandits with context. In the first setting, the platform aims to find the optimal decision with the highest expected reward, a.k.a, the largest entry of true reward tensor. In the second setting, some modes of the tensor are contexts and the rest modes are decisions, and the goal is to find the optimal decision given the contextual information. We propose two learning algorithms tensor elimination and tensor epoch-greedy for tensor bandits without context, and derive finite-time regret bounds for them. Comparing with existing competitive m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36816;&#34892;&#26102;&#26412;&#22320;&#40065;&#26834;&#24615;&#39564;&#35777;&#26469;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2002.03339</link><description>&lt;p&gt;
&#36890;&#36807;&#36816;&#34892;&#26102;&#26412;&#22320;&#40065;&#26834;&#24615;&#39564;&#35777;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Input Validation for Neural Networks via Runtime Local Robustness Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2002.03339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36816;&#34892;&#26102;&#26412;&#22320;&#40065;&#26834;&#24615;&#39564;&#35777;&#26469;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#40065;&#26834;&#24615;&#39564;&#35777;&#21487;&#20197;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#29305;&#23450;&#36755;&#20837;&#30340;&#25200;&#21160;&#22312;&#19968;&#23450;&#36317;&#31163;&#20869;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#36317;&#31163;&#31216;&#20026;&#40065;&#26834;&#24615;&#21322;&#24452;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27491;&#30830;&#20998;&#31867;&#30340;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#21322;&#24452;&#35201;&#27604;&#38169;&#35823;&#20998;&#31867;&#30340;&#36755;&#20837;&#65288;&#21253;&#25324;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#24378;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26679;&#26412;&#65289;&#35201;&#22823;&#24471;&#22810;&#12290;&#21478;&#19968;&#20010;&#35266;&#23519;&#26159;&#65292;&#27491;&#30830;&#20998;&#31867;&#30340;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#21322;&#24452;&#36890;&#24120;&#31526;&#21512;&#27491;&#24577;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36816;&#34892;&#26102;&#26412;&#22320;&#40065;&#26834;&#24615;&#39564;&#35777;&#26469;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#25252;&#31070;&#32463;&#32593;&#32476;&#20813;&#21463;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local robustness verification can verify that a neural network is robust wrt. any perturbation to a specific input within a certain distance. We call this distance Robustness Radius. We observe that the robustness radii of correctly classified inputs are much larger than that of misclassified inputs which include adversarial examples, especially those from strong adversarial attacks. Another observation is that the robustness radii of correctly classified inputs often follow a normal distribution. Based on these two observations, we propose to validate inputs for neural networks via runtime local robustness verification. Experiments show that our approach can protect neural networks from adversarial examples and improve their accuracies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#30446;&#26631;&#20989;&#25968;&#19981;&#21305;&#37197;&#65288;OFM&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#23481;&#26131;&#23548;&#33268;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#21644;&#37325;&#26500;&#19982;&#32858;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36741;&#21161;&#30446;&#26631;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#20276;&#38543;&#23545;&#35937;&#65288;UCO&#65289;&#65292;&#36890;&#36807;&#26680;&#20989;&#25968;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#21046;&#23450;&#32858;&#31867;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2001.07026</link><description>&lt;p&gt;
&#21033;&#29992;&#24352;&#37327;&#26680;&#20943;&#23569;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#30446;&#26631;&#20989;&#25968;&#19981;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Leveraging tensor kernels to reduce objective function mismatch in deep clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.07026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#30446;&#26631;&#20989;&#25968;&#19981;&#21305;&#37197;&#65288;OFM&#65289;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#23481;&#26131;&#23548;&#33268;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#21644;&#37325;&#26500;&#19982;&#32858;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36741;&#21161;&#30446;&#26631;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#20276;&#38543;&#23545;&#35937;&#65288;UCO&#65289;&#65292;&#36890;&#36807;&#26680;&#20989;&#25968;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#21046;&#23450;&#32858;&#31867;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20989;&#25968;&#19981;&#21305;&#37197;&#65288;OFM&#65289;&#25351;&#30340;&#26159;&#19968;&#20010;&#30446;&#26631;&#30340;&#20248;&#21270;&#23545;&#21478;&#19968;&#20010;&#30446;&#26631;&#30340;&#20248;&#21270;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;OFM&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#26082;&#20250;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#65292;&#21448;&#20250;&#23548;&#33268;&#37325;&#26500;&#30446;&#26631;&#21644;&#32858;&#31867;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;OFM&#12290;&#20026;&#20102;&#20943;&#23569;&#19981;&#21305;&#37197;&#65292;&#21516;&#26102;&#20445;&#25345;&#36741;&#21161;&#30446;&#26631;&#30340;&#32467;&#26500;&#20445;&#25345;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#29992;&#20110;&#28145;&#24230;&#32858;&#31867;&#30340;&#36741;&#21161;&#30446;&#26631;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#20276;&#38543;&#23545;&#35937;&#65288;UCO&#65289;&#12290;UCOs&#20381;&#36182;&#20110;&#26680;&#20989;&#25968;&#65292;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#19978;&#21046;&#23450;&#32858;&#31867;&#30446;&#26631;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#20013;&#38388;&#34920;&#31034;&#21487;&#20197;&#21253;&#25324;&#38500;&#29305;&#24449;&#32500;&#24230;&#20043;&#22806;&#30340;&#20854;&#20182;&#32500;&#24230;&#65292;&#20363;&#22914;&#31354;&#38388;&#25110;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#31616;&#21333;&#22320;&#23558;&#20854;&#21521;&#37327;&#21270;&#24182;&#24212;&#29992;&#21521;&#37327;&#26680;&#23545;&#27492;&#31867;&#38382;&#39064;&#24182;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective Function Mismatch (OFM) occurs when the optimization of one objective has a negative impact on the optimization of another objective. In this work we study OFM in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of OFM between the reconstruction and clustering objectives. To reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the Unsupervised Companion Objectives (UCOs). The UCOs rely on a kernel function to formulate a clustering objective on intermediate representations in the network. Generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. We therefore argue that the na\"ive approach of vectorizing and applying a vector kernel is suboptimal for such 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15292</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#33258;&#36866;&#24212;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15292
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#30693;&#22359;&#32467;&#26500;&#19979;&#30340;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#22359;&#31232;&#30095;&#20449;&#21495;&#37325;&#26500;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26159;&#29616;&#26377;&#26041;&#27861;LOP-$\ell_2$/$\ell_1$&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#22312;&#38750;&#21487;&#36870;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#65292;&#32780;LOP-$\ell_2$/$\ell_1$&#19981;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#22823;&#20102;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26356;&#21152;&#28789;&#27963;&#21644;&#24378;&#22823;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.10386</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;
&lt;/p&gt;
&lt;p&gt;
Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#65288;ACS&#65289;&#26159;&#19968;&#31181;&#39592;&#31185;&#24613;&#30151;&#65292;&#30001;&#32908;&#32905;&#38388;&#23460;&#20869;&#30340;&#21387;&#21147;&#21319;&#39640;&#24341;&#36215;&#65292;&#23548;&#33268;&#27704;&#20037;&#32452;&#32455;&#25439;&#20260;&#24182;&#26368;&#32456;&#33268;&#27515;&#12290;ACS&#30340;&#35786;&#26029;&#20027;&#35201;&#20381;&#36182;&#24739;&#32773;&#25253;&#21578;&#30340;&#30151;&#29366;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#19978;&#19981;&#21487;&#38752;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20405;&#20837;&#24615;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#34917;&#20805;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#12289;&#23458;&#35266;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;ACS&#35786;&#26029;&#26041;&#27861;&#12290;&#35813;&#35774;&#22791;&#36890;&#36807;&#19968;&#20010;&#20351;&#29992;&#36148;&#22312;&#30382;&#32932;&#19978;&#30340;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#65288;FSR&#65289;&#30340;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;ACS&#12290;&#26368;&#32456;&#35786;&#26029;&#32467;&#26524;&#36890;&#36807;&#34013;&#29273;&#20197;&#23454;&#26102;&#26041;&#24335;&#20256;&#36755;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#39564;&#35777;&#35786;&#26029;&#32467;&#26524;&#65292;&#21019;&#24314;&#20102;&#19968;&#32452;&#21253;&#21547;FSR&#27979;&#37327;&#21644;&#30456;&#24212;&#30340;&#27169;&#25311;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#65292;&#19982;&#20405;&#20837;&#24615;&#30340;&#37329;&#26631;&#20934;&#25345;&#24179;&#12290;&#35813;&#35774;&#22791;&#22312;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#21253;&#25324;&#20934;&#30830;&#24230;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#20854;&#29992;&#20110;&#32929;&#31080;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31867;&#20284;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04632</link><description>&lt;p&gt;
&#32929;&#31080;&#25968;&#25454;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#22235;&#20803;&#25968;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hypercomplex neural network in time series forecasting of stock data. (arXiv:2401.04632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#20854;&#29992;&#20110;&#32929;&#31080;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31867;&#20284;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26550;&#26500;&#12290;&#23427;&#20204;&#30340;&#21306;&#21035;&#22312;&#20110;&#36755;&#20837;&#23618;&#21253;&#21547;&#21367;&#31215;&#23618;&#12289;LSTM&#23618;&#25110;&#22235;&#20803;&#25968;4D&#20195;&#25968;&#30340;&#36229;&#22797;&#25968;&#23618;&#12290;&#36755;&#20837;&#26159;&#22235;&#20010;&#30456;&#20851;&#30340;&#32929;&#31080;&#24066;&#22330;&#26102;&#38388;&#24207;&#21015;&#65292;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20248;&#21270;&#19982;&#26550;&#26500;&#31867;&#21035;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#65292;&#27604;&#36739;&#20102;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#21035;&#20869;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#26550;&#26500;&#30456;&#20284;&#30340;MAE&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#35201;&#23569;&#24471;&#22810;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#27604;&#20854;&#20182;&#27979;&#35797;&#30340;&#26550;&#26500;&#26356;&#24555;&#22320;&#23398;&#20064;&#21644;&#22788;&#29702;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04305</link><description>&lt;p&gt;
&#25512;&#36827;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#65306;&#29992;&#20449;&#24687;&#35770;&#30452;&#35273;&#32479;&#19968;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Advancing Deep Active Learning &amp; Data Subset Selection: Unifying Principles with Information-Theory Intuitions. (arXiv:2401.04305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#36890;&#36807;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#12290;&#20027;&#21160;&#23398;&#20064;&#25552;&#39640;&#20102;&#26631;&#31614;&#25928;&#29575;&#65292;&#32780;&#20027;&#21160;&#37319;&#26679;&#21017;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26631;&#31614;&#33719;&#21462;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#35757;&#32451;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#8220;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#8221;&#20197;&#22806;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#32570;&#20047;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#21407;&#21017;&#22522;&#30784;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#35770;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30446;&#26631;&#21450;&#20854;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21147;&#27714;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin 
&lt;/p&gt;</description></item><item><title>Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.</title><link>http://arxiv.org/abs/2401.04301</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;Transformer&#36807;&#24230;&#24179;&#28369;&#30340;&#30495;&#30456;&#12299;
&lt;/p&gt;
&lt;p&gt;
Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04301
&lt;/p&gt;
&lt;p&gt;
Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#20250;&#36880;&#28176;&#36807;&#24230;&#24179;&#28369;&#36755;&#20837;&#25968;&#25454;&#65292;&#38477;&#20302;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#23384;&#22312;&#36825;&#20010;&#32570;&#38519;&#30340;&#24773;&#20917;&#19979;&#65292;Transformer&#26159;&#22914;&#20309;&#21462;&#24471;&#36825;&#20123;&#25104;&#21151;&#30340;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20107;&#23454;&#19978;Transformer&#24182;&#19981;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;&#30456;&#21453;&#65292;Transformer&#26159;&#21542;&#36807;&#24230;&#24179;&#28369;&#21462;&#20915;&#20110;&#20854;&#26356;&#26032;&#26041;&#31243;&#30340;&#29305;&#24449;&#35889;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#20102;&#20043;&#21069;&#20851;&#20110;&#36807;&#24230;&#24179;&#28369;&#21644;&#30456;&#20851;&#29616;&#35937;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;Transformer&#27169;&#22411;&#20855;&#26377;&#28385;&#36275;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#26465;&#20214;&#30340;&#27880;&#24847;&#21147;&#21644;&#26435;&#37325;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23545;Transformer&#26356;&#26032;&#26041;&#31243;&#30340;&#26435;&#37325;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20351;&#20854;&#21487;&#20197;&#25511;&#21046;&#20854;&#35889;&#29305;&#24615;&#65292;&#30830;&#20445;&#19981;&#20250;&#21457;&#29983;&#36807;&#24230;&#24179;&#28369;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.17816</link><description>&lt;p&gt;
Local Discovery by Partitioning: &#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs. (arXiv:2310.17816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17816
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#65292;&#36890;&#36807;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#31639;&#27861;&#65288;LDP&#65289;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;LDP&#26681;&#25454;&#19982;&#26333;&#20809;-&#32467;&#26524;&#23545;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#23558;&#21464;&#37327;&#38598;&#21512;Z&#36827;&#34892;&#20998;&#21306;&#65292;&#24182;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#26377;&#38480;&#20808;&#39564;&#30693;&#35782;&#19979;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;{X,Y}&#30340;&#26333;&#20809;-&#32467;&#26524;&#23545;&#21644;&#19968;&#20010;&#26410;&#30693;&#22240;&#26524;&#32467;&#26500;&#30340;&#21464;&#37327;&#38598;&#21512;Z&#65292;&#23616;&#37096;&#20998;&#21306;&#21457;&#29616;&#65288;LDP&#65289;&#31639;&#27861;&#23558;Z&#21010;&#20998;&#25104;&#19982;{X,Y}&#30456;&#20851;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#20219;&#24847;Z&#30340;8&#20010;&#31351;&#20030;&#19988;&#20114;&#19981;&#37325;&#22797;&#30340;&#20998;&#21306;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#21306;&#20998;&#28151;&#28102;&#22240;&#32032;&#21644;&#20854;&#20182;&#21464;&#37327;&#31867;&#22411;&#12290;LDP&#30340;&#21160;&#26426;&#26159;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#35782;&#21035;&#65292;&#20294;&#36991;&#20813;&#20102;&#33258;&#21160;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#39044;&#22788;&#29702;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;LDP&#23545;&#20110;&#28385;&#36275;&#36275;&#22815;&#22270;&#24418;&#26465;&#20214;&#30340;&#20219;&#20309;Z&#37117;&#36820;&#22238;&#19968;&#20010;&#26377;&#25928;&#30340;&#35843;&#25972;&#38598;&#12290;&#22312;&#26356;&#24378;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#21306;&#26631;&#31614;&#30340;&#28176;&#36817;&#27491;&#30830;&#24615;&#12290;&#24635;&#29420;&#31435;&#24615;&#27979;&#35797;&#22312;|Z|&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#20108;&#27425;&#30340;&#65292;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#27425;&#20108;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23545;&#29702;&#35770;&#20445;&#35777;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the problem of automated covariate selection under limited prior knowledge. Given an exposure-outcome pair {X,Y} and a variable set Z of unknown causal structure, the Local Discovery by Partitioning (LDP) algorithm partitions Z into subsets defined by their relation to {X,Y}. We enumerate eight exhaustive and mutually exclusive partitions of any arbitrary Z and leverage this taxonomy to differentiate confounders from other variable types. LDP is motivated by valid adjustment set identification, but avoids the pretreatment assumption commonly made by automated covariate selection methods. We provide theoretical guarantees that LDP returns a valid adjustment set for any Z that meets sufficient graphical conditions. Under stronger conditions, we prove that partition labels are asymptotically correct. Total independence tests is worst-case quadratic in |Z|, with sub-quadratic runtimes observed empirically. We numerically validate our theoretical guarantees on synthetic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.15903</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15903
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#22349;&#32553;&#65288;NC&#65289;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLab&#65289;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#23616;&#38480;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;NC&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#29305;&#24449;&#21464;&#24322;&#24615;&#20026;&#38646;&#65292;&#65288;ii&#65289;&#29305;&#24449;&#22343;&#20540;&#38598;&#21512;&#26500;&#25104;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#65288;iii&#65289;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#25910;&#32553;&#21040;&#29305;&#24449;&#22343;&#20540;&#20056;&#20197;&#26576;&#20010;&#32553;&#25918;&#22240;&#23376;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30740;&#31350;&#25512;&#24191;&#21040;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#23384;&#22312;&#24191;&#20041;NC&#29616;&#35937;&#12290;&#22312;&#33258;&#28982;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65288;UFM&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#20998;&#31867;&#22120;&#21482;&#26174;&#31034;&#20986;&#30456;&#21516;&#30340;ETF&#20960;&#20309;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#22349;&#32553;&#21040;&#22810;&#37325;&#24615;&#20026;1&#30340;&#29305;&#24449;&#31867;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.13576</link><description>&lt;p&gt;
&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#25112;&#30053;&#20915;&#31574;&#21040;&#29983;&#29289;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#36824;&#24418;&#24335;&#21270;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#25490;&#38500;&#20250;&#24341;&#20837;&#24490;&#29615;&#30340;&#36793;&#30340;&#39640;&#25928;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20351;&#24471;&#22312;DAG&#31354;&#38388;&#20013;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#31163;&#25955;&#25628;&#32034;&#21644;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#26159;&#32452;&#21512;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11009</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#32463;&#20856;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#21551;&#21457;&#24335;&#24230;&#37327;&#34987;&#36873;&#25321;&#20026;&#22312;&#19982;&#38142;&#36335;&#24418;&#25104;&#30456;&#20851;&#30340;&#22522;&#26412;&#22240;&#32032;&#19978;&#19982;&#20043;&#30456;&#20851;&#33391;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#20248;&#21183;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#20197;&#21450;&#25429;&#25417;&#20505;&#36873;&#38142;&#36335;&#20013;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#8220;&#23545;&#21521;&#32534;&#30721;&#8221;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#21521;&#32534;&#30721;&#24448;&#24448;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#26412;&#22240;&#32032;&#26469;&#20998;&#31867;&#25152;&#26377;&#38142;&#36335;&#12290;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#27491;&#30830;&#20998;&#31867;&#21487;&#33021;&#30001;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#21508;&#31181;&#19981;&#21516;&#38142;&#36335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#30340;&#20248;&#21270;&#31574;&#30053;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08833</link><description>&lt;p&gt;
&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#30340;&#20248;&#21270;&#31574;&#30053;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#19982;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30456;&#20851;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#30340;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;$ \widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$&#65292;&#21644;&#19968;&#20010;&#19979;&#30028;&#65292;$\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$&#12290;&#22312;&#36825;&#20123;&#34920;&#36798;&#24335;&#20013;&#65292;$|S|$&#21644;$|A|$&#20998;&#21035;&#34920;&#31034;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#21183;&#65292;$t_{\text{mix}}$&#20316;&#20026;&#24635;&#21464;&#24322;&#28151;&#21512;&#26102;&#38388;&#30340;&#32479;&#19968;&#19978;&#38480;&#65292;$\epsilon$&#34920;&#31034;&#35823;&#24046;&#23481;&#24525;&#24230;&#12290;&#22240;&#27492;&#65292;$t_{\text{mix}}$&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26174;&#30528;&#30340;&#24046;&#36317;&#38656;&#35201;&#22635;&#34917;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24314;&#31435;&#19968;&#20010;&#20248;&#21270;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#65292;&#26377;&#25928;&#22320;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#19979;&#30028;&#12290;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#31639;&#27861;&#24605;&#24819;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of $\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea
&lt;/p&gt;</description></item><item><title>FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07048</link><description>&lt;p&gt;
FedMFS: &#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#30340;&#32852;&#37030;&#22810;&#27169;&#24577;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07048
&lt;/p&gt;
&lt;p&gt;
FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20165;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#35775;&#38382;&#12289;&#20405;&#29359;&#25110;&#27844;&#38706;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#21512;&#20316;&#12290;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#32452;&#21512;&#21644;&#34701;&#21512;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#24341;&#36215;&#30340;&#38382;&#39064;&#65307;&#65288;&#20108;&#65289;&#35774;&#35745;&#19968;&#31181;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#26368;&#22823;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedMFS&#65292;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Shapley&#20540;&#26469;&#37327;&#21270;&#27599;&#20010;&#27169;&#24577;&#30340;&#36129;&#29486;&#21644;&#27169;&#24577;&#27169;&#22411;&#22823;&#23567;&#26469;&#34913;&#37327;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#20415;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05955</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#29992;&#20110;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Quality-Diversity approaches for constrained optimization problems with mixed continuous, discrete and categorical variables. (arXiv:2310.05955v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#21464;&#37327;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#25552;&#20379;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#36890;&#24120;&#28041;&#21450;&#21040;&#20351;&#29992;&#36153;&#26102;&#30340;&#27169;&#25311;&#20195;&#30721;&#26469;&#39044;&#27979;&#24453;&#35774;&#35745;&#31995;&#32479;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#35774;&#35745;&#65292;&#36825;&#20123;&#20195;&#30721;&#32463;&#24120;&#23884;&#20837;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#26469;&#25552;&#20379;&#26368;&#20339;&#35774;&#35745;&#26041;&#26696;&#65292;&#21516;&#26102;&#28385;&#36275;&#35774;&#35745;&#32422;&#26463;&#26465;&#20214;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#35774;&#35745;&#31354;&#38388;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#19968;&#32452;&#20855;&#26377;&#22810;&#26679;&#24615;&#24615;&#36136;&#30340;&#26368;&#20248;&#35299;&#38598;&#65292;&#20197;&#35780;&#20272;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#36890;&#24120;&#28041;&#21450;&#21040;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#30340;&#35774;&#35745;&#21464;&#37327;&#65292;&#20197;&#32771;&#34385;&#22312;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25216;&#26415;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20998;&#31867;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex engineering design problems, such as those involved in aerospace, civil, or energy engineering, require the use of numerically costly simulation codes in order to predict the behavior and performance of the system to be designed. To perform the design of the systems, these codes are often embedded into an optimization process to provide the best design while satisfying the design constraints. Recently, new approaches, called Quality-Diversity, have been proposed in order to enhance the exploration of the design space and to provide a set of optimal diversified solutions with respect to some feature functions. These functions are interesting to assess trade-offs. Furthermore, complex engineering design problems often involve mixed continuous, discrete, and categorical design variables allowing to take into account technological choices in the optimization problem. In this paper, a new Quality-Diversity methodology based on mixed continuous, discrete and categorical Bayesian opti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.17016</link><description>&lt;p&gt;
&#20855;&#26377;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#22343;&#20809;&#28369;&#24230;&#30340;&#26080;&#21442;&#22238;&#24402;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26080;&#20998;&#24067;&#38480;&#21046;&#19979;&#30340;&#32479;&#19968;&#25910;&#25947;&#30028;&#38480;&#21644;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#26080;&#20998;&#24067;&#38480;&#21046;&#30340;&#24179;&#22343;&#20809;&#28369;&#24230;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#30001;Ashlagi&#31561;&#20154;&#65288;2021&#65289;&#25552;&#20986;&#65292;&#29992;&#20110;&#34913;&#37327;&#20989;&#25968;&#30456;&#23545;&#20110;&#20219;&#24847;&#26410;&#30693;&#28508;&#22312;&#20998;&#24067;&#30340;"&#26377;&#25928;"&#20809;&#28369;&#24230;&#12290;&#26368;&#36817;&#30340;Hanneke&#31561;&#20154;&#65288;2023&#65289;&#30340;&#30740;&#31350;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#24179;&#22343;&#20809;&#28369;&#20989;&#25968;&#30340;&#32039;&#23494;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#25928;&#21487;&#23454;&#29616;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30446;&#21069;&#22312;&#26222;&#36941;&#26080;&#20559;&#65288;&#21363;&#26377;&#22122;&#22768;&#65289;&#24773;&#20917;&#19979;&#23578;&#32570;&#20047;&#31867;&#20284;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23436;&#20840;&#22635;&#34917;&#20102;&#36825;&#20123;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#26080;&#20559;&#35774;&#32622;&#20013;&#30340;&#24179;&#22343;&#20809;&#28369;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#20998;&#24067;&#19968;&#33268;&#25910;&#25947;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25152;&#24471;&#21040;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20197;&#25968;&#25454;&#30340;&#20869;&#22312;&#20960;&#20309;&#24418;&#29366;&#20026;&#22522;&#30784;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#20840;&#26377;&#30028;&#24230;&#37327;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07138</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#30450;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20174;&#28151;&#21512;&#20449;&#21495;&#20013;&#20998;&#31163;&#20986;&#28304;&#20449;&#21495;&#21644;&#28151;&#21512;&#31995;&#32479;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#28151;&#21512;&#31995;&#32479;&#21644;&#28304;&#20449;&#21495;&#20570;&#20986;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#28151;&#21512;&#30340;BSS&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#28982;&#29305;&#24449;&#23376;&#31354;&#38388;&#19987;&#38376;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23436;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#24378;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36755;&#20837;&#35299;&#30721;&#25104;&#22810;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#29420;&#32534;&#30721;&#31354;&#38388;&#65292;&#28982;&#21518;&#22312;&#35299;&#30721;&#22120;&#20869;&#37325;&#26032;&#28151;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#37325;&#26500;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#65292;&#21363;&#23631;&#34109;&#38500;&#19968;&#20010;&#32534;&#30721;&#22806;&#30340;&#25152;&#26377;&#32534;&#30721;&#65292;&#20351;&#24471;&#35299;&#30721;&#22120;&#33021;&#22815;&#20272;&#35745;&#28304;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32534;&#30721;&#20043;&#38388;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of blind source separation (BSS) involves separating sources from a mixture without prior knowledge of the sources or the mixing system. This is a challenging problem that often requires making restrictive assumptions about both the mixing system and the sources. In this paper, we propose a novel method for addressing BSS of non-linear mixtures by leveraging the natural feature subspace specialization ability of multi-encoder autoencoders with fully self-supervised learning without strong priors. During the training phase, our method unmixes the input into the separate encoding spaces of the multi-encoder network and then remixes these representations within the decoder for a reconstruction of the input. Then to perform source inference, we introduce a novel encoding masking technique whereby masking out all but one of the encodings enables the decoder to estimate a source signal. To this end, we also introduce a so-called pathway separation loss that encourages sparsity betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.01592</link><description>&lt;p&gt;
&#22823;&#23610;&#24230;&#21644;&#26080;&#31351;&#23485;&#24230;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21202;&#35753;&#28436;&#35762;
&lt;/p&gt;
&lt;p&gt;
Les Houches Lectures on Deep Learning at Large &amp; Infinite Width. (arXiv:2309.01592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#28436;&#35762;&#26159;&#22312;2022&#24180;&#21202;&#35753;&#22799;&#23395;&#23398;&#26657;&#32479;&#35745;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#19978;&#23637;&#31034;&#30340;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#12290;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#35762;&#24072;&#20204;&#35752;&#35770;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65307;&#35757;&#32451;&#36807;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32447;&#24615;&#27169;&#22411;&#65292;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#20123;&#32852;&#31995;&#22312;&#26080;&#31351;&#23485;&#24230;&#30340;&#26497;&#38480;&#19979;&#20986;&#29616;&#65307;&#20197;&#21450;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;</title><link>http://arxiv.org/abs/2308.06686</link><description>&lt;p&gt;
MDB&#65306;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06686
&lt;/p&gt;
&lt;p&gt;
MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#24320;&#21457;&#32773;&#38656;&#35201;&#33021;&#22815;&#31995;&#32479;&#22320;&#35843;&#35797;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MDB&#65292;&#19968;&#20010;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#35843;&#35797;&#26694;&#26550;&#12290;MDB&#36890;&#36807;&#23558;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#34920;&#36798;&#24615;&#26597;&#35810;&#30340;&#24037;&#20855;&#12290;&#26597;&#35810;&#21487;&#37325;&#29992;&#19988;&#26131;&#20110;&#20462;&#25913;&#65292;&#20351;&#24471;&#35843;&#35797;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#20197;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#20559;&#24046;&#21457;&#29616;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#25968;&#25454;&#22635;&#20805;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;MDB&#22312;&#33258;&#21160;&#39550;&#39542;&#35270;&#39057;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#30103;&#35760;&#24405;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#39640;10&#20493;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;40%&#30340;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#21457;&#32773;&#33021;&#22815;&#25104;&#21151;&#26500;&#24314;&#22797;&#26434;&#26597;&#35810;&#26469;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#38598;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;NP-hard&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#20197;&#20855;&#26377;&#39640;&#39044;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.11607</link><description>&lt;p&gt;
&#21033;&#29992;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#25214;&#21040;&#26368;&#20248;&#30340;&#22810;&#26679;&#29305;&#24449;&#38598;
&lt;/p&gt;
&lt;p&gt;
Finding Optimal Diverse Feature Sets with Alternative Feature Selection. (arXiv:2307.11607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#38598;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#30340;NP-hard&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#20197;&#20855;&#26377;&#39640;&#39044;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#33719;&#21462;&#23567;&#22411;&#12289;&#21487;&#35299;&#37322;&#19988;&#39640;&#31934;&#24230;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#29305;&#24449;&#38598;&#65292;&#36825;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#20363;&#22914;&#65292;&#29992;&#25143;&#21487;&#33021;&#23545;&#23547;&#25214;&#20855;&#26377;&#30456;&#20284;&#39044;&#27979;&#36136;&#37327;&#20294;&#25552;&#20379;&#19981;&#21516;&#25968;&#25454;&#35299;&#37322;&#30340;&#26367;&#20195;&#29305;&#24449;&#38598;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;&#23450;&#20041;&#20102;&#26367;&#20195;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#26367;&#20195;&#30340;&#25968;&#37327;&#21644;&#24046;&#24322;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#24182;&#23637;&#31034;&#20102;&#20854;NP-hard&#24615;&#36136;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#20256;&#32479;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#20316;&#20026;&#30446;&#26631;&#38598;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;30&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#26367;&#20195;&#29305;&#24449;&#36873;&#25321;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26367;&#20195;&#29305;&#24449;&#38598;&#30830;&#23454;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#24433;&#21709;&#36825;&#19968;&#32467;&#26524;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models. Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios. For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data. In this article, we introduce alternative feature selection and formalize it as an optimization problem. In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives. Next, we analyze the complexity of this optimization problem and show NP-hardness. Further, we discuss how to integrate conventional feature-selection methods as objectives. Finally, we evaluate alternative feature selection with 30 classification datasets. We observe that alternative feature sets may indeed have high prediction quality, and we analyze several factors influencing this ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20309;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#30340;&#22240;&#23376;&#21270;&#21464;&#20998;&#25512;&#26029;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.11018</link><description>&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65306;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#20351;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Amortized Variational Inference: When and Why?. (arXiv:2307.11018v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20309;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#30340;&#22240;&#23376;&#21270;&#21464;&#20998;&#25512;&#26029;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26399;&#21464;&#20998;&#25512;&#26029;&#65288;A-VI&#65289;&#26159;&#19968;&#31181;&#36817;&#20284;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#38590;&#20197;&#35745;&#31639;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;A-VI&#30340;&#23450;&#20041;&#29305;&#28857;&#26159;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25512;&#26029;&#20989;&#25968;&#65292;&#23558;&#27599;&#20010;&#35266;&#23519;&#26144;&#23556;&#21040;&#20854;&#23616;&#37096;&#28508;&#21464;&#37327;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#19982;&#26356;&#20256;&#32479;&#30340;&#20998;&#35299;&#65288;&#25110;&#22343;&#22330;&#65289;&#21464;&#20998;&#25512;&#26029;&#65288;F-VI&#65289;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#30452;&#25509;&#23398;&#20064;&#27599;&#20010;&#28508;&#21464;&#37327;&#30340;&#36817;&#20284;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;A-VI&#29992;&#20316;&#21152;&#36895;&#23616;&#37096;&#28508;&#21464;&#37327;&#25512;&#26029;&#30340;&#35745;&#31639;&#25216;&#24039;&#12290;&#26412;&#25991;&#30740;&#31350;A-VI&#20316;&#20026;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#30340;&#19968;&#31181;&#36890;&#29992;&#26367;&#20195;&#26041;&#27861;&#12290;&#30001;&#20110;&#20998;&#26399;&#23478;&#26063;&#26159;&#20998;&#35299;&#23478;&#26063;&#30340;&#23376;&#38598;&#65292;A-VI&#26080;&#27861;&#20135;&#29983;&#27604;F-VI&#26368;&#20248;&#35299;&#26356;&#20302;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#36817;&#20284;&#20540;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#29702;&#35770;&#38382;&#39064;&#26159;&#21051;&#30011;A-VI&#20309;&#26102;&#20173;&#28982;&#36798;&#21040;F-VI&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Amortized variational inference (A-VI) is a method for approximating the intractable posterior distributions that arise in probabilistic models. The defining feature of A-VI is that it learns a global inference function that maps each observation to its local latent variable's approximate posterior. This stands in contrast to the more classical factorized (or mean-field) variational inference (F-VI), which directly learns the parameters of the approximating distribution for each latent variable. In deep generative models, A-VI is used as a computational trick to speed up inference for local latent variables. In this paper, we study A-VI as a general alternative to F-VI for approximate posterior inference. A-VI cannot produce an approximation with a lower Kullback-Leibler divergence than F-VI's optimal solution, because the amortized family is a subset of the factorized family. Thus a central theoretical problem is to characterize when A-VI still attains F-VI's optimal solution. We deri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.08533</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#22788;&#29702;&#19982;&#32447;&#24615;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#25968;&#25454;&#22788;&#29702;&#26469;&#25552;&#21462;&#38544;&#34255;&#30340;&#34920;&#24449;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#31361;&#30772;&#65292;&#20294;&#21364;&#20197;&#22823;&#30005;&#23376;&#35745;&#31639;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#20026;&#20102;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20809;&#23398;&#24102;&#23485;&#30340;&#20248;&#21183;&#21644;&#20809;&#23398;&#20114;&#36830;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#22312;&#32570;&#20047;&#20302;&#21151;&#29575;&#20809;&#23398;&#38750;&#32447;&#24615;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#22312;&#20110;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#65292;&#32780;&#19981;&#20381;&#36182;&#30005;&#23376;&#20803;&#20214;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#21487;&#20197;&#21516;&#26102;&#20197;&#20302;&#20809;&#21151;&#29575;&#21512;&#25104;&#21487;&#32534;&#31243;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#21033;&#29992;&#25955;&#23556;&#21183;&#33021;&#65288;&#30001;&#25968;&#25454;&#34920;&#31034;&#65289;&#19982;&#25955;&#23556;&#22330;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22810;&#27425;&#25955;&#23556;&#36827;&#34892;&#25968;&#25454;&#37325;&#22797;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.08433</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#28216;&#36208;&#21040;&#22270;&#24418;&#24555;&#36305;&#65306;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#19978;&#20855;&#26377;&#20302;&#24310;&#36831;&#30340;&#33410;&#28857;&#23884;&#20837;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20986;&#27969;&#24335;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#65292;&#35745;&#31639;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#20197;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#22522;&#30784;&#30340;&#21160;&#24577;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#23454;&#20307;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#38543;&#26102;&#38388;&#28436;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#32771;&#34385;&#36825;&#20123;&#21160;&#24577;&#22240;&#32032;&#65292;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20805;&#20998;&#21457;&#25381;&#20854;&#28508;&#21147;&#12290;&#20197;&#21069;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20391;&#37325;&#20110;&#25277;&#26679;k-&#36339;&#37051;&#22495;&#65292;&#31867;&#20284;&#20110;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#38543;&#26426;&#28216;&#36208;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#26102;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#20302;&#24310;&#36831;&#25512;&#26029;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#19988;&#19981;&#36866;&#29992;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#24418;&#24555;&#36305;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65288;CTDGs&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#19982;&#39640;&#24310;&#36831;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#12289;&#20302;&#24310;&#36831;&#30340;&#36817;&#20284;&#38543;&#26426;&#28216;&#36208;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#20165;&#21333;&#36339;&#25805;&#20316;&#35745;&#31639;&#24635;&#32467;&#22810;&#36339;&#20449;&#24687;&#30340;&#26102;&#38388;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06887</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-Task Representation Learning by Two-Layer ReLU Neural Networks. (arXiv:2307.06887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06887
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#26159;&#31070;&#32463;&#32593;&#32476;&#23454;&#38469;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#22914;&#20309;&#20197;&#21450;&#20026;&#20309;&#21457;&#29983;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20248;&#21270;&#30340;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#25110;&#38543;&#26426;&#29305;&#24449;&#33539;&#20363;&#20013;&#24494;&#19981;&#36275;&#36947;&#30340;&#29305;&#24449;&#23398;&#20064;&#30340;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#32463;&#24120;&#22320;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#20123;&#20808;&#21069;&#30340;&#20998;&#26512;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#31616;&#21333;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26368;&#24120;&#35265;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning, i.e. extracting meaningful representations of data, is quintessential to the practical success of neural networks trained with gradient descent, yet it is notoriously difficult to explain how and why it occurs. Recent theoretical studies have shown that shallow neural networks optimized on a single task with gradient-based methods can learn meaningful features, extending our understanding beyond the neural tangent kernel or random feature regime in which negligible feature learning occurs. But in practice, neural networks are increasingly often trained on {\em many} tasks simultaneously with differing loss functions, and these prior analyses do not generalize to such settings. In the multi-task learning setting, a variety of studies have shown effective feature learning by simple linear models. However, multi-task learning via {\em nonlinear} models, arguably the most common learning paradigm in practice, remains largely mysterious. In this work, we present the first 
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;DNN&#39564;&#35777;&#30340;&#19968;&#31181;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#23454;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#25913;&#36827;&#29616;&#26377;&#25628;&#32034;&#24037;&#20855;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#23454;&#29616;&#21033;&#29992;&#20102;Imandra&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;</title><link>http://arxiv.org/abs/2307.06299</link><description>&lt;p&gt;
&#38754;&#21521;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#21487;&#35777;&#26126;&#23457;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards a Certified Proof Checker for Deep Neural Network Verification. (arXiv:2307.06299v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06299
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;DNN&#39564;&#35777;&#30340;&#19968;&#31181;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#23454;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#25913;&#36827;&#29616;&#26377;&#25628;&#32034;&#24037;&#20855;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#23454;&#29616;&#21033;&#29992;&#20102;Imandra&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20445;&#35777;&#30340;&#38656;&#27714;&#12290;&#21487;&#20197;&#20351;&#29992;&#30001;&#39564;&#35777;&#31038;&#21306;&#24320;&#21457;&#30340;&#24037;&#20855;&#26469;&#35777;&#26126;DNN&#30340;&#36825;&#20123;&#23433;&#20840;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#26412;&#36523;&#23481;&#26131;&#20986;&#29616;&#23454;&#29616;&#38169;&#35823;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#39564;&#35777;&#22120;&#20250;&#20135;&#29983;&#21487;&#20197;&#30001;&#21487;&#20449;&#26816;&#26597;&#22120;&#26816;&#26597;&#30340;&#32467;&#26524;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;DNN&#39564;&#35777;&#30340;&#26032;&#22411;&#35777;&#26126;&#26816;&#26597;&#22120;&#30340;&#23454;&#29616;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#26356;&#22823;&#30340;&#21487;&#39564;&#35777;&#24615;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;Imandra&#65288;&#19968;&#31181;&#24037;&#19994;&#32423;&#30340;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#30340;&#20004;&#20010;&#20851;&#38190;&#33021;&#21147;&#65306;&#23545;&#26080;&#38480;&#31934;&#24230;&#23454;&#25968;&#31639;&#26415;&#30340;&#25903;&#25345;&#21644;&#20854;&#24418;&#24335;&#21270;&#39564;&#35777;&#22522;&#30784;&#35774;&#26045;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#22312;Imandra&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#35268;&#23450;&#20102;&#20854;&#27491;&#30830;&#24615;&#23646;&#24615;&#21644;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep neural networks (DNNs) have led to their adoption in safety-critical systems, which in turn has heightened the need for guaranteeing their safety. These safety properties of DNNs can be proven using tools developed by the verification community. However, these tools are themselves prone to implementation bugs and numerical stability problems, which make their reliability questionable. To overcome this, some verifiers produce proofs of their results which can be checked by a trusted checker. In this work, we present a novel implementation of a proof checker for DNN verification. It improves on existing implementations by offering numerical stability and greater verifiability. To achieve this, we leverage two key capabilities of Imandra, an industrial theorem prover: its support of infinite precision real arithmetic and its formal verification infrastructure. So far, we have implemented a proof checker in Imandra, specified its correctness properties and start
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.00668</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Active Sensing with Predictive Coding and Uncertainty Minimization. (arXiv:2307.00668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20027;&#21160;&#24863;&#30693;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#21644;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#30340;&#20840;&#27969;&#31243;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20219;&#20309;&#20219;&#21153;&#26080;&#20851;&#21644;&#20869;&#22312;&#39537;&#21160;&#30340;&#24773;&#22659;&#19979;&#24212;&#29992;&#20110;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#22522;&#30784;&#30340;&#36716;&#25442;&#20998;&#24067;&#24182;&#37325;&#24314;&#29615;&#22659;&#30340;&#31354;&#38388;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#20027;&#21160;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#20027;&#21160;&#37319;&#26679;&#20854;&#35270;&#35273;&#29615;&#22659;&#20197;&#33719;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26500;&#24314;&#26080;&#30417;&#30563;&#34920;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20027;&#21160;&#37319;&#26679;&#21644;&#39640;&#25928;&#20998;&#31867;&#24863;&#30693;&#22330;&#26223;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#30340;&#36755;&#20837;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#23398;&#20064;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#26356;&#20302;&#30340;&#21442;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an end-to-end procedure for embodied exploration based on two biologically inspired computations: predictive coding and uncertainty minimization. The procedure can be applied to any exploration setting in a task-independent and intrinsically driven manner. We first demonstrate our approach in a maze navigation task and show that our model is capable of discovering the underlying transition distribution and reconstructing the spatial features of the environment. Second, we apply our model to the more complex task of active vision, where an agent must actively sample its visual environment to gather information. We show that our model is able to build unsupervised representations that allow it to actively sample and efficiently categorize sensory scenes. We further show that using these representations as input for downstream classification leads to superior data efficiency and learning speed compared to other baselines, while also maintaining lower parameter complexity. Final
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#37319;&#29992;U-Net&#21644;pretrained SAM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#38024;&#23545;&#20083;&#33146;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#65292;&#36827;&#34892;&#32959;&#30244;&#21306;&#22495;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.12510</link><description>&lt;p&gt;
&#22522;&#20110;U-Net&#21644;Segment Anything Model&#30340;&#20083;&#33146;&#32959;&#30244;&#22312;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#30340;&#23545;&#27604;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Segment Anything Model and U-Net for Breast Tumor Detection in Ultrasound and Mammography Images. (arXiv:2306.12510v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12510
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#37319;&#29992;U-Net&#21644;pretrained SAM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#38024;&#23545;&#20083;&#33146;&#36229;&#22768;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#65292;&#36827;&#34892;&#32959;&#30244;&#21306;&#22495;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#22312;&#20083;&#33146;&#36229;&#22768;&#65288;BUS&#65289;&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#35782;&#21035;&#21644;&#25551;&#32472;&#32959;&#30244;&#21306;&#22495;&#30340;&#31639;&#27861;&#12290;&#35813;&#25216;&#26415;&#37319;&#29992;&#20102;&#20004;&#31181;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;U-Net&#21644;pretrained SAM&#65292;&#29992;&#20110;&#32959;&#30244;&#20998;&#21106;&#12290;U-Net&#27169;&#22411;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#21033;&#29992;&#20854;&#28145;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20174;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;pretrained SAM&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#24182;&#29983;&#25104;&#20998;&#21106;&#32467;&#26524;&#12290;&#35780;&#20272;&#22312;&#19981;&#21516;&#33391;&#24615;&#21644;&#24694;&#24615;&#32959;&#30244;&#30340;&#27880;&#37322;&#32959;&#30244;&#21306;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;U-Net&#27169;&#22411;&#33021;&#22815;&#22312;BUS&#21644;&#20083;&#33146;X&#32447;&#22270;&#20687;&#20013;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#20998;&#21106;&#20083;&#33146;&#32959;&#30244;&#65292;&#19988;&#20248;&#20110;pretrained SAM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, the main objective is to develop an algorithm capable of identifying and delineating tumor regions in breast ultrasound (BUS) and mammographic images. The technique employs two advanced deep learning architectures, namely U-Net and pretrained SAM, for tumor segmentation. The U-Net model is specifically designed for medical image segmentation and leverages its deep convolutional neural network framework to extract meaningful features from input images. On the other hand, the pretrained SAM architecture incorporates a mechanism to capture spatial dependencies and generate segmentation results. Evaluation is conducted on a diverse dataset containing annotated tumor regions in BUS and mammographic images, covering both benign and malignant tumors. This dataset enables a comprehensive assessment of the algorithm's performance across different tumor types. Results demonstrate that the U-Net model outperforms the pretrained SAM architecture in accurately identifying and segment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06064</link><description>&lt;p&gt;
&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning for Combinatorial Optimisation. (arXiv:2306.06064v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22312;TSP&#23454;&#20363;&#35757;&#32451;&#20043;&#21069;&#65292;&#23558;&#31070;&#32463;&#27169;&#22411;&#29992;&#30456;&#20851;&#31639;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#23454;&#29616;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;NP&#38590;/&#23436;&#20840;&#32452;&#21512;&#38382;&#39064;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36229;&#36234;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20854;&#38271;&#26399;&#30446;&#26631;&#26159;&#36890;&#36807;&#23398;&#20064;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26356;&#20248;&#35299;&#26469;&#36229;&#36234;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#32780;&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26159;&#32463;&#24120;&#34987;&#36825;&#20123;&#26041;&#27861;&#30596;&#20934;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35299;&#20915;TSP&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38382;&#39064;&#22266;&#26377;&#30340;&#8220;&#31639;&#27861;&#8221;&#26412;&#36136;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#35774;&#35745;&#29992;&#20110;TSP&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#24120;&#24120;&#21033;&#29992;&#35832;&#22914;&#26597;&#25214;&#26368;&#23567;&#29983;&#25104;&#26641;&#20043;&#31867;&#30340;&#25104;&#29087;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;TSP&#38382;&#39064;&#30340;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#23545;TSP&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#20043;&#21069;&#65292;&#22312;&#30456;&#20851;&#31639;&#27861;&#19978;&#23545;&#25105;&#20204;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;TSP&#38382;&#39064;&#30340;&#35299;&#20915;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. The Travelling Salesman Problem (TSP) is a prominent combinatorial optimisation problem often targeted by such approaches. However, current neural-based methods for solving TSP often overlook the inherent "algorithmic" nature of the problem. In contrast, heuristics designed for TSP frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of TSP problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on TSP instances. Our results demonstrate that, using this learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04848</link><description>&lt;p&gt;
&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21644;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#35299;&#37322;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#12290;&#37319;&#26679;&#22120;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#30452;&#35273;&#19978;&#19982;&#25237;&#24433;&#26377;&#20851;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#65292;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#36817;&#20284;&#31561;&#20215;&#20110;&#27491;&#20132;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21435;&#22122;&#36817;&#20284;&#20110;&#23398;&#20064;&#25237;&#24433;&#12290;&#26412;&#25991;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#23558;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#35299;&#37322;&#20026;&#24212;&#29992;&#20110;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#20989;&#25968;&#30340;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#21435;&#22122;&#22120;&#25237;&#24433;&#35823;&#24046;&#30340;&#31616;&#21333;&#20551;&#35774;&#65292;&#25552;&#20379;DDIM&#65288;Denoising Diffusion Implicit Models&#65289;&#37319;&#26679;&#22120;&#30340;&#31616;&#21333;&#25910;&#25947;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#29702;&#35770;&#32467;&#26524;&#30340;&#27934;&#35265;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23545;DDIM&#30340;&#20004;&#20010;&#31616;&#21333;&#20462;&#25913;&#30340;&#26032;&#37319;&#26679;&#22120;&#12290;&#20165;&#38656;&#35201;5-10&#20010;&#20989;&#25968;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#37319;&#26679;&#22120;&#23601;&#33021;&#22312;&#39044;&#35757;&#32451;&#30340;CIFAR-10&#21644;CelebA&#27169;&#22411;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;FID&#24471;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...</title><link>http://arxiv.org/abs/2306.03589</link><description>&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#26368;&#27969;&#34892;&#30340;GNN&#31867;&#21035;&#26159;&#36890;&#36807;&#30456;&#37051;&#33410;&#28857;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#25805;&#20316;&#30340;&#65292;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#12290;&#37492;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20102;&#35299;MPNN&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#26080;&#20449;&#24687;&#33410;&#28857;&#29305;&#24449;&#30340;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;MPNN&#20801;&#35768;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20132;&#20114;&#27700;&#24179;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#35813;&#27979;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#29305;&#24615;&#65292;&#21363;&#25152;&#35859;&#30340;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#65292;&#35813;&#25928;&#24212;&#34987;&#35266;&#23519;&#21040;&#26159;&#24403;&#22823;&#37327;&#30340;&#20449;&#24687;&#32858;&#21512;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26102;&#21457;&#29983;&#30340;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01896</link><description>&lt;p&gt;
&#24212;&#23545;&#25345;&#32493;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tackling Unbounded State Spaces in Continuing Task Reinforcement Learning. (arXiv:2306.01896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25345;&#32493;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#23545;&#26080;&#30028;&#29366;&#24577;&#31354;&#38388;&#65292;&#26088;&#22312;&#40723;&#21169;&#20195;&#29702;&#22120;&#23398;&#20064;&#31283;&#23450;&#24615;&#21644;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22806;&#25512;&#19988;&#24378;&#28872;&#20381;&#36182;&#21608;&#26399;&#24615;&#37325;&#32622;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#24605;&#24819;&#30340;&#22870;&#21169;&#22609;&#24418;&#26041;&#27861;&#65292;&#20197;&#40723;&#21169;&#20195;&#29702;&#39318;&#20808;&#23398;&#20064;&#31283;&#23450;&#24615;&#65288;&#21363;&#23454;&#29616;&#26377;&#30028;&#25104;&#26412;&#65289;&#65292;&#28982;&#21518;&#20877;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#20943;&#23569;&#20102;&#20195;&#29702;&#22120;&#30340;&#21457;&#25955;&#29575;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning (RL) algorithms have been successfully applied to many tasks, their inability to extrapolate and strong reliance on episodic resets inhibits their applicability to many real-world settings. For instance, in stochastic queueing problems, the state space can be unbounded and the agent may have to learn online without the system ever being reset to states the agent has seen before. In such settings, we show that deep RL agents can diverge into unseen states from which they can never recover due to the lack of resets, especially in highly stochastic environments. Towards overcoming this divergence, we introduce a Lyapunov-inspired reward shaping approach that encourages the agent to first learn to be stable (i.e. to achieve bounded cost) and then to learn to be optimal. We theoretically show that our reward shaping technique reduces the rate of divergence of the agent and empirically find that it prevents it. We further combine our reward shaping approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#65292;&#35813;&#23398;&#20064;&#29575;&#20351;FTRL&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#20854;&#20013;SPA-sparse&#31639;&#27861;&#21487;&#36866;&#24212;&#20110;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;SPA-game-dependency&#31639;&#27861;&#21487;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;BOBW&#31639;&#27861;&#21017;&#26159;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#30340;&#36866;&#24212;&#24615;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17301</link><description>&lt;p&gt;
&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#36319;&#38543;&#27491;&#21017;&#21270;&#39046;&#34966;&#65306;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#30340;&#24182;&#23384;
&lt;/p&gt;
&lt;p&gt;
Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds. (arXiv:2305.17301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#65292;&#35813;&#23398;&#20064;&#29575;&#20351;FTRL&#20855;&#26377;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#20854;&#20013;SPA-sparse&#31639;&#27861;&#21487;&#36866;&#24212;&#20110;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;SPA-game-dependency&#31639;&#27861;&#21487;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;BOBW&#31639;&#27861;&#21017;&#26159;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#30340;&#36866;&#24212;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#36866;&#24212;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#26159;&#25193;&#23637;&#31639;&#27861;&#36866;&#29992;&#24615;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#36319;&#38543;&#27491;&#21017;&#21270;&#39046;&#34966;&#36817;&#24180;&#26469;&#25104;&#20026;&#33719;&#21462;&#28120;&#27760;&#27861;&#20013;&#21508;&#31181;&#31867;&#22411;&#36866;&#24212;&#24615;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#24191;&#36825;&#31181;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#20026;FTRL&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#31216;&#20026;&#31283;&#23450;&#24615;&#24809;&#32602;&#33258;&#36866;&#24212;&#65288;SPA&#65289;&#23398;&#20064;&#29575;&#12290;&#35813;&#23398;&#20064;&#29575;&#20135;&#29983;&#30340;&#36951;&#25022;&#30028;&#20849;&#21516;&#21462;&#20915;&#20110;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#24809;&#32602;&#65292;&#20854;&#20013;FTRL&#30340;&#36951;&#25022;&#36890;&#24120;&#34987;&#20998;&#35299;&#12290;&#20973;&#20511;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20010;&#20855;&#26377;&#19977;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#30340;&#31639;&#27861;&#65306;&#31232;&#30095;&#24615;&#12289;&#28216;&#25103;&#20381;&#36182;&#24615;&#21644;&#26368;&#20339;&#19990;&#30028;&#65288;BOBW&#65289;&#12290;&#31232;&#30095;&#24615;&#32463;&#24120;&#20986;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#31232;&#30095;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;$k$-arms&#20551;&#23450;&#20107;&#20808;&#24050;&#30693;&#31232;&#30095;&#32423;&#21035;$s \leq k$&#65292;&#32780;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#36890;&#24120;&#19981;&#26159;&#24773;&#20917;&#12290;&#20026;&#20102;&#36866;&#24212;&#26410;&#30693;&#30340;&#31232;&#30095;&#32423;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SPA-sparse&#65292;&#35813;&#31639;&#27861;&#26174;&#31034;&#27604;&#29616;&#26377;&#31232;&#30095;&#31639;&#27861;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#12290;&#28216;&#25103;&#20381;&#36182;&#24615;&#26159;&#21478;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#65292;&#24403;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#28216;&#25103;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#21363;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SPA-game-dependency&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#25152;&#29609;&#30340;&#28216;&#25103;&#33258;&#36866;&#24212;&#22320;&#25913;&#21464;&#20854;&#34892;&#20026;&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#20855;&#26377;&#31232;&#30095;&#24615;&#21448;&#20855;&#26377;&#28216;&#25103;&#20381;&#36182;&#24615;&#36866;&#24212;&#24615;&#30340;BOBW&#31639;&#27861;&#65292;&#24182;&#26174;&#31034;&#23427;&#27604;&#20165;&#38598;&#20013;&#20110;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-Regularized-Leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called Stability-Penalty-Adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and Best-of-Both-Worlds (BOBW). Sparsity frequently appears in real-world problems. However, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \leq k$ is known in advance, which is often not the case in real-world scenarios. To ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.17148</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Differentially private low-dimensional representation of high-dimensional data. (arXiv:2305.17148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#39640;&#25928;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#65307;&#19982;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#20351;&#29992;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#36807;&#31243;&#36991;&#20813;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#25935;&#24863;&#20449;&#24687;&#30340;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#22788;&#20110;&#39640;&#32500;&#31354;&#38388;&#20013;&#26102;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#20302;&#32500;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#22312;Wasserstein&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#26159;&#20351;&#29992;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#31934;&#24230;&#30028;&#38480;&#30340;&#31169;&#26377;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#36807;&#31243;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#19982;&#20351;&#29992;Davis-Kahan&#23450;&#29702;&#36827;&#34892;&#26631;&#20934;&#25200;&#21160;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31169;&#26377;PCA&#20998;&#26512;&#19981;&#38656;&#35201;&#20551;&#35774;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private synthetic data provide a powerful mechanism to enable data analysis while protecting sensitive information about individuals. However, when the data lie in a high-dimensional space, the accuracy of the synthetic data suffers from the curse of dimensionality. In this paper, we propose a differentially private algorithm to generate low-dimensional synthetic data efficiently from a high-dimensional dataset with a utility guarantee with respect to the Wasserstein distance. A key step of our algorithm is a private principal component analysis (PCA) procedure with a near-optimal accuracy bound that circumvents the curse of dimensionality. Different from the standard perturbation analysis using the Davis-Kahan theorem, our analysis of private PCA works without assuming the spectral gap for the sample covariance matrix.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15984</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#20272;&#35745;&#30340;&#21160;&#24577;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation. (arXiv:2305.15984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;HyperCATE&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#26435;&#37325;&#20849;&#20139;&#30340;&#26041;&#24335;&#23454;&#29616;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#29616;&#26377;CATE&#23398;&#20064;&#22120;&#20013;&#30340;&#26377;&#20559;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#26377;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#23398;&#20064;&#32773;&#32570;&#20047;&#31471;&#21040;&#31471;&#27835;&#30103;&#20449;&#24687;&#20849;&#20139;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#20998;&#21106;&#21040;&#28508;&#22312;&#32467;&#26524;&#20989;&#25968;&#20013;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#30340;&#26377;&#20559;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;CATE&#23398;&#20064;&#22120;&#65292;&#20419;&#36827;&#27835;&#30103;&#32452;&#20043;&#38388;&#30340;&#21160;&#24577;&#31471;&#21040;&#31471;&#20449;&#24687;&#20849;&#20139;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#8220;&#36229;&#32593;&#32476;&#8221;&#30340;&#8220;&#36719;&#26435;&#37325;&#20849;&#20139;&#8221;&#65292;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#12289;&#26356;&#24555;&#35757;&#32451;&#21644;&#25913;&#36827;&#32467;&#26524;&#31561;&#20248;&#28857;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;CATE&#23398;&#20064;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;HyperCATE&#8221;&#30340;&#26032;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;CATE&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24120;&#29992;CATE&#23398;&#20064;&#22120;&#30340;HyperCATE&#29256;&#26412;&#65292;&#24182;&#22312;IHDP&#12289;ACIC-2016&#21644;Twins&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing heterogeneous treatment effects learners, also known as conditional average treatment effects (CATE) learners, lack a general mechanism for end-to-end inter-treatment information sharing, and data have to be split among potential outcome functions to train CATE learners which can lead to biased estimates with limited observational datasets. To address this issue, we propose a novel deep learning-based framework to train CATE learners that facilitates dynamic end-to-end information sharing among treatment groups. The framework is based on \textit{soft weight sharing} of \textit{hypernetworks}, which offers advantages such as parameter efficiency, faster training, and improved results. The proposed framework complements existing CATE learners and introduces a new class of uncertainty-aware CATE learners that we refer to as \textit{HyperCATE}. We develop HyperCATE versions of commonly used CATE learners and evaluate them on IHDP, ACIC-2016, and Twins benchmarks. Our experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#26041;&#27861;SADCBO&#65292;&#36890;&#36807;&#23545;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#20195;&#20215;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14120</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#24773;&#22659;&#21464;&#37327;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-aware learning of relevant contextual variables within Bayesian optimization. (arXiv:2305.14120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#26041;&#27861;SADCBO&#65292;&#36890;&#36807;&#23545;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#20195;&#20215;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#36125;&#21494;&#26031;&#20248;&#21270;(CBO)&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#38024;&#23545;&#35774;&#35745;&#21464;&#37327;&#20248;&#21270;&#40657;&#30418;&#26114;&#36149;&#30340;&#35780;&#20272;&#20989;&#25968;&#65292;&#24182;&#21516;&#26102;&#26377;&#25928;&#22320;&#25972;&#21512;&#20851;&#20110;&#29615;&#22659;&#30340;&#30456;&#20851;&#24773;&#22659;&#20449;&#24687;&#65292;&#22914;&#23454;&#39564;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24773;&#22659;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#19981;&#19968;&#23450;&#26159;&#39044;&#20808;&#24050;&#30693;&#30340;&#12290;&#27492;&#22806;&#65292;&#26377;&#26102;&#36824;&#21487;&#20197;&#26368;&#20248;&#21270;&#24773;&#22659;&#21464;&#37327;&#26412;&#36523;&#65292;&#36825;&#26159;&#24403;&#21069;CBO&#31639;&#27861;&#26410;&#32771;&#34385;&#30340;&#35774;&#32622;&#12290;&#20248;&#21270;&#24773;&#22659;&#21464;&#37327;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#36825;&#24341;&#20986;&#20102;&#30830;&#23450;&#19968;&#20010;&#26368;&#23567;&#30456;&#20851;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20195;&#20215;&#24863;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;BO&#20219;&#21153;&#26469;&#26500;&#26550;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#24773;&#22659;BO (SADCBO) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29305;&#23450;&#36755;&#20837;&#28857;&#21518;&#39564;&#20195;&#29702;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#23398;&#20064;&#24773;&#22659;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26469;&#26368;&#23567;&#21270;&#20248;&#21270;&#30340;&#20195;&#20215;&#12290;SADCBO&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual Bayesian Optimization (CBO) is a powerful framework for optimizing black-box, expensive-to-evaluate functions with respect to design variables, while simultaneously efficiently integrating relevant contextual information regarding the environment, such as experimental conditions. However, in many practical scenarios, the relevance of contextual variables is not necessarily known beforehand. Moreover, the contextual variables can sometimes be optimized themselves, a setting that current CBO algorithms do not take into account. Optimizing contextual variables may be costly, which raises the question of determining a minimal relevant subset. In this paper, we frame this problem as a cost-aware model selection BO task and address it using a novel method, Sensitivity-Analysis-Driven Contextual BO (SADCBO). We learn the relevance of context variables by sensitivity analysis of the posterior surrogate model at specific input points, whilst minimizing the cost of optimization by lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2304.08242</link><description>&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#24102;&#26377;&#25991;&#26412;&#36793;&#30340;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08242
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#20132;&#20114;&#23548;&#33268;&#29992;&#25143;&#20849;&#20139;&#20854;&#20182;&#20154;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#33258;&#28982;&#22320;&#30001;&#23558;&#20010;&#20307;&#19982;&#33410;&#28857;&#20851;&#32852;&#21644;&#20132;&#25442;&#30340;&#25991;&#26412;&#23450;&#20041;&#20026;&#36793;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#24322;&#26500;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#33410;&#28857;&#32858;&#31867;&#20026;&#21516;&#31867;&#32676;&#32452;&#20197;&#21450;&#21576;&#29616;&#21487;&#29702;&#35299;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Deep-LPTM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#27010;&#29575;&#27169;&#22411;&#26469;&#25551;&#36848;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;Deep-LPTM&#20801;&#35768;&#22312;&#20004;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#26500;&#24314;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#21442;&#25968;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;IC2L&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#23646;&#24615;&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#31639;&#27861;&#23398;&#20064;&#21487;&#35777;&#26126;&#20855;&#26377;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.08944</link><description>&lt;p&gt;
&#36890;&#36807;ERM&#23454;&#29616;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#21487;&#39564;&#35777;&#65288;&#22810;&#65289;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifiable (Multi)Robustness Against Patch Attacks Using ERM. (arXiv:2303.08944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#36148;&#29255;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#31639;&#27861;&#23398;&#20064;&#21487;&#35777;&#26126;&#20855;&#26377;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36148;&#29255;&#25915;&#20987;&#65292;&#21363;&#22312;&#27979;&#35797;&#26102;&#23545;&#27979;&#35797;&#22270;&#20687;&#36827;&#34892;&#36148;&#29255;&#26893;&#20837;&#65292;&#20197;&#35825;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#20851;&#27880;&#26368;&#36817;&#38024;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#19968;&#31181;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;Patch-Cleanser&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#35201;&#27714;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#8220;&#20004;&#20010;&#33945;&#29256;&#27491;&#30830;&#24615;&#8221;&#23646;&#24615;&#65292;&#24847;&#21619;&#30528;&#39044;&#27979;&#27169;&#22411;&#22312;&#20219;&#20309;&#26102;&#20505;&#29992;&#20219;&#24847;&#20004;&#20010;&#31354;&#30333;&#33945;&#29256;&#26367;&#25442;&#22270;&#20687;&#37096;&#20998;&#26102;&#37117;&#24212;&#27491;&#30830;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ERM&#65288;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65289;&#31639;&#27861;&#21487;&#20197;&#35777;&#26126;&#22810;&#31181;&#33945;&#29256;&#19979;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#22810;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#20984;&#26494;&#24347;&#21644;&#40065;&#26834;&#20248;&#21270;&#19982;&#22522;&#20110;&#36793;&#30028;&#30340;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#28385;&#36275;&#19968;&#23450;&#22823;&#23567;&#21644;&#20301;&#32622;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#20005;&#26684;&#30340;&#35777;&#26126;&#22810;&#40065;&#26834;&#24615;&#23545;&#36148;&#29255;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#36148;&#29255;&#25915;&#20987;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider patch attacks, where at test-time an adversary manipulates a test image with a patch in order to induce a targeted misclassification. We consider a recent defense to patch attacks, Patch-Cleanser (Xiang et al. [2022]). The Patch-Cleanser algorithm requires a prediction model to have a ``two-mask correctness'' property, meaning that the prediction model should correctly classify any image when any two blank masks replace portions of the image. Xiang et al. learn a prediction model to be robust to two-mask operations by augmenting the training set with pairs of masks at random locations of training images and performing empirical risk minimization (ERM) on the augmented dataset.  However, in the non-realizable setting when no predictor is perfectly correct on all two-mask operations on all images, we exhibit an example where ERM fails. To overcome this challenge, we propose a different algorithm that provably learns a predictor robust to all two-mask operations using an ERM orac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#20851;&#32858;&#31867;&#20013;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#24212;&#21508;&#31181;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#65292;&#21516;&#26102;&#20855;&#26377;&#36866;&#24212;&#24615;&#28789;&#27963;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.10295</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#30456;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Correlation Clustering with Active Learning of Pairwise Similarities. (arXiv:2302.10295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#20851;&#32858;&#31867;&#20013;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#24212;&#21508;&#31181;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#65292;&#21516;&#26102;&#20855;&#26377;&#36866;&#24212;&#24615;&#28789;&#27963;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#22788;&#29702;&#27491;&#36127;&#30456;&#20284;&#24615;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25104;&#23545;&#30456;&#20284;&#24615;&#19981;&#20107;&#20808;&#32473;&#20986;&#65292;&#24517;&#39035;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#26597;&#35810;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#22810;&#31181;&#20248;&#21183;&#65292;&#20363;&#22914;&#65292;&#29992;&#25143;/&#27880;&#37322;&#32773;&#21487;&#20197;&#25552;&#20379;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#12289;&#36866;&#24212;&#20219;&#20309;&#30456;&#20851;&#32858;&#31867;&#31639;&#27861;&#21644;&#26597;&#35810;&#31574;&#30053;&#20197;&#21450;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#20998;&#26512;&#20102;&#19968;&#20123;&#36866;&#21512;&#36825;&#31181;&#35774;&#32622;&#30340;&#26032;&#30340;&#26597;&#35810;&#31574;&#30053;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#21644;&#25152;&#25552;&#20986;&#30340;&#26597;&#35810;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a well-known unsupervised learning setting that deals with positive and negative pairwise similarities. In this paper, we study the case where the pairwise similarities are not given in advance and must be queried in a cost-efficient way. Thereby, we develop a generic active learning framework for this task that benefits from several advantages, e.g., flexibility in the type of feedback that a user/annotator can provide, adaptation to any correlation clustering algorithm and query strategy, and robustness to noise. In addition, we propose and analyze a number of novel query strategies suited to this setting. We demonstrate the effectiveness of our framework and the proposed query strategies via several experimental studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#26368;&#32456;&#35780;&#20272;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.11989</link><description>&lt;p&gt;
&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Practical Differentially Private Hyperparameter Tuning with Subsampling. (arXiv:2301.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23376;&#37319;&#26679;&#23454;&#29616;&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#26368;&#32456;&#35780;&#20272;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#25972;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#36890;&#36807;&#36229;&#21442;&#25968;&#20540;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20165;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#38543;&#26426;&#23376;&#38598;&#24182;&#23558;&#26368;&#20339;&#20540;&#22806;&#25512;&#21040;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#26469;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#35813;&#26041;&#27861;&#30340; Renyi &#24046;&#20998;&#38544;&#31169;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#19981;&#25439;&#22833;&#25152;&#36873;&#36229;&#21442;&#25968;&#30340;&#26368;&#32456;&#35780;&#20272;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#22987;&#32456;&#23454;&#29616;&#26356;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tuning the hyperparameters of differentially private (DP) machine learning (ML) algorithms often requires use of sensitive data and this may leak private information via hyperparameter values. Recently, Papernot and Steinke (2022) proposed a certain class of DP hyperparameter tuning algorithms, where the number of random search samples is randomized itself. Commonly, these algorithms still considerably increase the DP privacy parameter $\varepsilon$ over non-tuned DP ML model training and can be computationally heavy as evaluating each hyperparameter candidate requires a new training run. We focus on lowering both the DP bounds and the computational cost of these methods by using only a random subset of the sensitive data for the hyperparameter tuning and by extrapolating the optimal values to a larger dataset. We provide a R\'enyi differential privacy analysis for the proposed method and experimentally show that it consistently leads to better privacy-utility trade-off than the baseli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item></channel></rss>