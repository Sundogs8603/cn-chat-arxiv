<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.01568</link><description>&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) &#65288;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#20808;&#39564;&#30340;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65288;A-CMDP&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ACRL&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#35777;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#21051;&#31454;&#20105;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;A-CMDP&#65289;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21463;&#38480;&#21046;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#30740;&#31350;&#26088;&#22312;&#22312;&#38543;&#26426;&#21160;&#24577;&#20013;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#32422;&#26463;&#26399;&#26395;&#25104;&#26412;&#65292;&#20294;&#26159;&#29305;&#23450;&#24773;&#33410;&#20013;&#30340;&#25104;&#26412;&#20173;&#28982;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;&#30456;&#21453;&#65292;A-CMDP&#30340;&#30446;&#26631;&#26159;&#22312;&#20219;&#24847;&#19968;&#36718;&#30340;&#20219;&#20309;&#24773;&#33410;&#20013;&#65292;&#20248;&#21270;&#26399;&#26395;&#22238;&#25253;&#21516;&#26102;&#20445;&#35777;&#26377;&#30028;&#30340;&#25104;&#26412;&#65292;&#24182;&#38024;&#23545;&#31574;&#30053;&#20808;&#39564;&#36827;&#34892;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Anytime-Competitive Reinforcement Learning&#65288;ACRL&#65289;&#65292;&#23427;&#21487;&#20197;&#35777;&#26126;&#22320;&#20445;&#35777;&#20102;&#20219;&#24847;&#26102;&#21051;&#30340;&#25104;&#26412;&#32422;&#26463;&#12290;&#36951;&#25022;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#31574;&#30053;&#22312;&#20219;&#24847;&#30340;&#31454;&#20105;&#24615;&#32422;&#26463;&#19979;&#28176;&#36817;&#22320;&#19982;&#26368;&#20248;&#22238;&#25253;&#30456;&#21305;&#37197;&#12290;&#22312;&#30899;&#26234;&#33021;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;ACRL&#30340;&#22238;&#25253;&#24615;&#33021;&#21644;&#25104;&#26412;&#32422;&#26463;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.01544</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65306;&#36890;&#36807;&#27979;&#37327;&#34928;&#20943;&#26469;&#20462;&#21098;LLM&#32452;&#20214;&#24182;&#20248;&#21270;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#21387;&#32553;&#21518;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;DTM&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#28145;&#20837;&#27934;&#23519;&#65292;&#24182;&#19988;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;DTM&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#20462;&#21098;&#25481;&#36229;&#36807;90%&#30340;LLM&#32452;&#20214;&#21644;&#37327;&#21270;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#22823;&#23567;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#30340;&#26377;&#25928;&#37096;&#32626;&#21644;LLM&#21387;&#32553;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21387;&#32553;LLM&#30340;&#26041;&#27861;&#65292;&#21363;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;DTM&#65289;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25351;&#26631;&#22914;&#22256;&#24785;&#24230;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#23616;&#38480;&#24615;&#12290;DTM&#20851;&#27880;&#20196;&#29260;&#30340;&#24046;&#24322;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#24494;&#22937;&#20043;&#22788;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#25439;&#23475;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#21644;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;DTM&#36824;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#31532;&#19968;&#20010;&#19981;&#21516;&#30340;&#20196;&#29260;&#25351;&#26631;&#65288;FDTM&#65289;&#22312;&#27169;&#22411;&#31232;&#30095;&#21270;&#20013;&#26174;&#31034;&#65292;&#36229;&#36807;90%&#30340;&#25152;&#26377;&#32452;&#20214;&#21487;&#20197;&#20462;&#21098;&#25481;&#12290;&#23545;&#20110;&#37327;&#21270;&#65292;FDTM&#34920;&#26126;&#36229;&#36807;80%&#30340;&#21442;&#25968;&#21487;&#20197;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.00797</link><description>&lt;p&gt;
&#28436;&#21270;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#65306;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#24335;&#30740;&#31350;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;(SIS)&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#21463;&#25968;&#20540;&#38543;&#26426;&#31215;&#20998;&#22120;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;ResNet&#26550;&#26500;&#65292;&#35782;&#21035;&#20986;&#19968;&#20010;&#21442;&#25968;&#30456;&#20851;&#30340;&#22522;&#20110;&#29289;&#29702;&#24847;&#20041;&#30340;&#31895;&#31890;&#24230;&#22343;&#22330;&#21464;&#37327;&#30340;&#26377;&#25928;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(eSDE)&#12290;&#25105;&#20204;&#22522;&#20110;eSDE&#30340;&#30830;&#23450;&#20559;&#31227;&#39033;&#26500;&#24314;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26377;&#25928;&#20998;&#23700;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#22343;&#22330;SIS&#27169;&#22411;&#30340;&#20998;&#23700;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#28436;&#21270;&#32593;&#32476;&#30340;&#26377;&#25928;SIS&#21160;&#21147;&#23398;&#20013;&#30340;&#27425;&#20020;&#30028;Hopf&#20998;&#23700;&#65292;&#23427;&#24341;&#36215;&#20102;&#20020;&#30028;&#36716;&#25240;&#34892;&#20026;&#65307;&#36825;&#34920;&#29616;&#20026;&#22823;&#24133;&#24230;&#30340;&#38598;&#20307;&#25391;&#33633;&#65292;&#23427;&#20204;&#20174;(&#22122;&#22768;&#30340;)&#22266;&#23450;&#29366;&#24577;&#30340;&#37051;&#22495;&#20013;&#33258;&#21457;&#22320;&#12289;&#32597;&#35265;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#22797;&#30340;&#26292;&#21147;&#27169;&#25311;&#21644;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#25968;&#23398;&#24037;&#20855;&#30740;&#31350;&#20102;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#32958;&#32454;&#32990;&#30284;&#30340;&#30149;&#29702;&#20122;&#22411;&#65292;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#22312;&#26415;&#21069;&#20570;&#20986;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2311.00567</link><description>&lt;p&gt;
&#22522;&#20110;CT&#22270;&#20687;&#30340;&#32958;&#32454;&#32990;&#30284;&#30149;&#29702;&#20998;&#31867;&#30340;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images. (arXiv:2311.00567v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#32958;&#32454;&#32990;&#30284;&#30340;&#30149;&#29702;&#20122;&#22411;&#65292;&#20026;&#25918;&#23556;&#31185;&#21307;&#24072;&#22312;&#26415;&#21069;&#20570;&#20986;&#35786;&#26029;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#20415;&#24110;&#21161;&#25918;&#23556;&#31185;&#21307;&#24072;&#22312;&#26415;&#21069;&#21306;&#20998;&#22522;&#20110;CT&#22270;&#20687;&#30340;&#32958;&#32454;&#32990;&#30284;&#30149;&#29702;&#20122;&#22411;&#12290;&#26041;&#27861;&#65306;&#25910;&#38598;&#20102;&#26469;&#33258; Center 1 &#30340; 668 &#20363;&#36830;&#32493;&#24739;&#32773;&#30340;&#30149;&#29702;&#35777;&#23454;&#30340;&#32958;&#32454;&#32990;&#30284;&#25968;&#25454;&#12290;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#29992;&#20110;&#23558;&#32958;&#32454;&#32990;&#30284;&#20122;&#22411;&#20998;&#31867;&#20026;&#36879;&#26126;&#32454;&#32990;&#30284; (ccRCC)&#12289;&#20083;&#22836;&#29366;&#32454;&#32990;&#30284; (pRCC) &#21644;&#21457;&#33394;&#32454;&#32990;&#30284; (chRCC)&#12290; Center 2 &#30340; 78 &#20363;&#22806;&#37096;&#39564;&#35777;&#24739;&#32773;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#65306;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#20013;&#65292;&#27169;&#22411;&#23545; ccRCC&#12289;pRCC &#21644; chRCC &#30340;&#20998;&#31867;&#30340;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215; (AUC) &#20998;&#21035;&#20026; 0.868 (95% CI: 0.826-0.923)&#12289;0.846 (95% CI: 0.812-0.886) &#21644; 0.839 (95% CI: 0.802-0.88)&#12290;&#22312;&#22806;&#37096;&#39564;&#35777;&#38598;&#20013;&#65292;AUC&#20026;...
&lt;/p&gt;
&lt;p&gt;
Objectives To develop and validate a deep learning-based diagnostic model incorporating uncertainty estimation so as to facilitate radiologists in the preoperative differentiation of the pathological subtypes of renal cell carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients, pathologically proven RCC, were retrospectively collected from Center 1. By using five-fold cross-validation, a deep learning model incorporating uncertainty estimation was developed to classify RCC subtypes into clear cell RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external validation set of 78 patients from Center 2 further evaluated the model's performance. Results In the five-fold cross-validation, the model's area under the receiver operating characteristic curve (AUC) for the classification of ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI: 0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external validation set, the A
&lt;/p&gt;</description></item><item><title>MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00334</link><description>&lt;p&gt;
MetisFL:&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#24037;&#20316;&#27969;&#30340;&#23604;&#23596;&#24182;&#34892;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00334
&lt;/p&gt;
&lt;p&gt;
MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#31995;&#32479;&#36890;&#24120;&#30001;&#20004;&#20010;&#26680;&#24515;&#22788;&#29702;&#23454;&#20307;&#32452;&#25104;:&#32852;&#37030;&#25511;&#21046;&#22120;&#21644;&#23398;&#20064;&#22120;&#12290;&#25511;&#21046;&#22120;&#36127;&#36131;&#31649;&#29702;&#22312;&#23398;&#20064;&#22120;&#20043;&#38388;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#31243;&#65292;&#23398;&#20064;&#22120;&#36127;&#36131;&#22312;&#20854;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#32852;&#37030;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#26102;&#65292;FL&#31995;&#32479;&#23545;&#21442;&#19982;&#23398;&#20064;&#22120;&#30340;&#35745;&#31639;&#36164;&#28304;&#25110;&#25968;&#25454;&#27809;&#26377;&#25511;&#21046;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;FL&#31995;&#32479;&#26469;&#20419;&#36827;FL&#24037;&#20316;&#27969;&#30340;&#24320;&#21457;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#22823;&#22810;&#25968;&#24573;&#35270;&#20102;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MetisFL&#30340;&#26032;&#22411;FL&#31995;&#32479;&#65292;&#20854;&#20013;&#32852;&#37030;&#25511;&#21046;&#22120;&#26159;&#31532;&#19968;&#31561;&#20844;&#27665;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;</title><link>http://arxiv.org/abs/2310.16945</link><description>&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection&#65288;CATE&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#22240;&#26524;Q&#38598;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#26159;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#29992;&#20110;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21033;&#20110;&#20855;&#26377;&#21452;&#37325;&#40065;&#26834;&#24615;&#36136;&#30340;&#20195;&#29702;&#25439;&#22833;&#24230;&#37327;&#21644;&#27169;&#22411;&#38598;&#25104;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#30452;&#25509;&#24212;&#29992;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20250;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#20027;&#35201;CATE&#38598;&#25104;&#26041;&#27861;&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#30340;Q&#38598;&#25104;&#30340;&#26032;&#30340;CATE&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;Q&#38598;&#25104;&#22312;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#30340;&#36951;&#25022;&#29575;&#19978;&#36798;&#21040;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20540;&#20026;$\frac{\log(M)}{n}$&#65288;&#20854;&#20013;$M$&#20026;&#27169;&#22411;&#25968;&#65292;$n$&#20026;&#26679;&#26412;&#25968;&#65289;&#65292;&#21152;&#19978;&#39640;&#38454;&#20272;&#35745;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2310.11864</link><description>&lt;p&gt;
VQ-NeRF: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#21453;&#23556;&#20998;&#35299;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11864
&lt;/p&gt;
&lt;p&gt;
VQ-NeRF&#26159;&#19968;&#20010;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#35299;&#21644;&#32534;&#36753;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#12290;&#36890;&#36807;&#23558;&#36830;&#32493;&#26448;&#26009;&#31163;&#25955;&#21270;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#22122;&#22768;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#21270;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VQ-NeRF&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;3D&#22330;&#26223;&#20013;&#30340;&#21453;&#23556;&#22330;&#36827;&#34892;&#20998;&#35299;&#21644;&#32534;&#36753;&#12290;&#20256;&#32479;&#30340;&#31070;&#32463;&#21453;&#23556;&#22330;&#20165;&#20351;&#29992;&#36830;&#32493;&#34920;&#31034;&#26469;&#24314;&#27169;3D&#22330;&#26223;&#65292;&#23613;&#31649;&#29616;&#23454;&#20013;&#30340;&#29289;&#20307;&#36890;&#24120;&#30001;&#31163;&#25955;&#26448;&#26009;&#32452;&#25104;&#12290;&#36825;&#31181;&#32570;&#20047;&#31163;&#25955;&#21270;&#21487;&#33021;&#23548;&#33268;&#26448;&#26009;&#20998;&#35299;&#22122;&#22768;&#21644;&#22797;&#26434;&#30340;&#26448;&#26009;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#36830;&#32493;&#20998;&#25903;&#21644;&#19968;&#20010;&#31163;&#25955;&#20998;&#25903;&#12290;&#36830;&#32493;&#20998;&#25903;&#25353;&#29031;&#20256;&#32479;&#27969;&#31243;&#39044;&#27979;&#20998;&#35299;&#30340;&#26448;&#26009;&#65292;&#32780;&#31163;&#25955;&#20998;&#25903;&#20351;&#29992;VQ&#26426;&#21046;&#23558;&#36830;&#32493;&#26448;&#26009;&#37327;&#21270;&#20026;&#21333;&#29420;&#30340;&#26448;&#26009;&#12290;&#36890;&#36807;&#31163;&#25955;&#21270;&#26448;&#26009;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#65292;&#24182;&#29983;&#25104;&#31163;&#25955;&#26448;&#26009;&#30340;&#20998;&#21106;&#22320;&#22270;&#12290;&#21487;&#20197;&#36890;&#36807;&#28857;&#20987;&#20998;&#21106;&#32467;&#26524;&#30340;&#30456;&#24212;&#21306;&#22495;&#26469;&#36731;&#26494;&#36873;&#25321;&#29305;&#23450;&#26448;&#26009;&#36827;&#34892;&#36827;&#19968;&#27493;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11689</link><description>&lt;p&gt;
&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#25913;&#36827;LLMs&#20013;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#25105;&#35780;&#20272;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#65292;&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25552;&#39640;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#20173;&#28982;&#38480;&#20110;&#20854;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#36873;&#25321;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#22312;LLMs&#19981;&#30830;&#23450;&#26102;&#20351;&#20854;&#36991;&#20813;&#39044;&#27979;&#32780;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#35780;&#20272;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#21442;&#25968;&#25928;&#29575;&#35843;&#25972;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#24182;&#25913;&#36827;&#20854;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#22312;CoQA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;AUACC&#20174;91.23%&#25552;&#39640;&#21040;92.63%&#65292;&#24182;&#23558;AURO
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08660</link><description>&lt;p&gt;
&#22312;&#19981;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;RL&#31574;&#30053;&#65306;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36895;&#29575;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#21151;&#29575;&#25511;&#21046;&#12289;&#27874;&#26463;&#25104;&#24418;&#21644;&#24178;&#25200;&#28040;&#38500;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#22522;&#31449;&#19982;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#36890;&#20449;&#30340;&#24773;&#20917;&#12290;&#30001;&#20110;&#31351;&#20030;&#25628;&#32034;&#30340;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20197;&#20854;&#38590;&#20197;&#20934;&#30830;&#24314;&#27169;&#30340;&#34892;&#20026;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;RL&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#20415;&#20195;&#29702;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22833;&#36133;&#30340;&#39640;&#25104;&#26412;&#65292;&#23558;&#31639;&#27861;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#25506;&#32034;&#21644;&#23398;&#20064;&#26159;&#19981;&#26126;&#26234;&#30340;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25511;&#21046;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#19968;&#31181;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#26469;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#32676;&#35770;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#24182;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.03530</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#65292;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#24341;&#23548;&#20102;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#22312;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#26469;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35268;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#32676;&#35770;&#35777;&#26126;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;&#36825;&#19968;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#24182;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#23545;&#31216;&#24615;&#21644;&#20960;&#20309;&#24615;&#32771;&#34385;&#20026;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#20013;&#65292;&#20294;&#26159;&#20855;&#20307;&#30340;&#32534;&#30721;&#35268;&#21017;&#36824;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#36890;&#36807;&#20851;&#27880;&#25968;&#25454;-&#21442;&#25968;&#22495;&#19978;&#30340;&#32852;&#21512;&#32676;&#19981;&#21464;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#35268;&#21017;&#65292;&#20174;&#25968;&#25454;&#22495;&#19978;&#30340;&#32676;&#20316;&#29992;&#20013;&#25214;&#21040;&#21442;&#25968;&#22495;&#19978;&#30340;&#21452;&#37325;&#32676;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30001;&#32852;&#21512;&#19981;&#21464;&#20989;&#25968;&#23548;&#20986;&#30340;&#24191;&#20041;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;Schur&#24341;&#29702;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#26222;&#36941;&#24615;&#23450;&#29702;&#30340;&#26032;&#30340;&#32676;&#35770;&#35777;&#26126;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#26222;&#36941;&#24615;&#23450;&#29702;&#26159;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#35777;&#26126;&#30340;&#65292;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#36924;&#36817;&#29702;&#35770;&#30340;&#32676;&#35770;&#26041;&#38754;&#65292;&#23558;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#19982;&#25277;&#35937;&#35843;&#21644;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;&#25105;&#20204;&#21033;&#29992;&#32676;&#35770;&#35770;&#35777;&#35777;&#26126;&#20102;&#36825;&#20123;DNN&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03529</link><description>&lt;p&gt;
&#28145;&#24230;&#33034;&#27874;&#21464;&#25442;&#65306;&#20351;&#29992;Koopman&#31639;&#23376;&#35777;&#26126;&#20102;&#24418;&#24335;&#28145;&#24230;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks. (arXiv:2310.03529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03529
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;&#25105;&#20204;&#21033;&#29992;&#32676;&#35770;&#35770;&#35777;&#35777;&#26126;&#20102;&#36825;&#20123;DNN&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23545;&#25968;&#25454;&#31354;&#38388;&#19978;&#30340;&#32676;&#20316;&#29992;&#26469;&#35782;&#21035;DNN&#20869;&#37096;&#30340;&#38544;&#34255;&#23618;&#65292;&#24182;&#23558;DNN&#26500;&#24314;&#20026;&#30456;&#23545;&#20110;Koopman&#31639;&#23376;&#30340;&#21452;&#22768;&#21464;&#25442;&#65292;Koopman&#31639;&#23376;&#26159;&#32676;&#20316;&#29992;&#30340;&#32447;&#24615;&#34920;&#31034;&#12290;&#22522;&#20110;&#32676;&#35770;&#35770;&#35777;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;Schur&#24341;&#29702;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;DNN&#26222;&#36866;&#24615;&#30340;&#31616;&#21333;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify hidden layers inside a DNN with group actions on the data space, and formulate the DNN as a dual voice transform with respect to Koopman operator, a linear representation of the group action. Based on the group theoretic arguments, particularly by using Schur's lemma, we show a simple proof of the universality of those DNNs.
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.00023</link><description>&lt;p&gt;
De-SaTE&#65306;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#30340;&#21435;&#22122;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#20026;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#20379;&#30005;&#21040;&#25512;&#21160;&#30005;&#21160;&#27773;&#36710;&#21644;&#25903;&#25345;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#26377;&#25928;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#20934;&#30830;&#39044;&#27979;&#20854;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#36825;&#26159;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#30340;&#33021;&#37327;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#32463;&#36807;&#35757;&#32451;&#26469;&#22788;&#29702;&#30005;&#27744;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22122;&#22768;&#31867;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23567;&#27874;&#21435;&#22122;&#22120;&#26469;&#29983;&#25104;&#32534;&#30721;/&#20998;&#35299;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#19987;&#29992;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;NASA&#21644;CALCE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#22810;&#31181;&#22122;&#22768;&#27169;&#24335;&#19979;&#30340;&#24191;&#27867;&#20581;&#24247;&#25351;&#26631;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25253;&#21578;&#30340;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#65292;&#21487;&#20197;&#20026;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16521</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#65292;&#21487;&#20197;&#20026;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19982;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#23427;&#21033;&#29992;&#21382;&#21490;&#24739;&#32773;&#36712;&#36857;&#25968;&#25454;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20849;&#21516;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#26465;&#20214;&#21270;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#35757;&#32451;&#29983;&#25104;&#19982;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#20303;&#38498;&#31958;&#23615;&#30149;&#24739;&#32773;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#21644;&#34880;&#31958;&#39044;&#27979;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#20165;&#21407;&#22987;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#29983;&#29289;&#23398;&#24615;&#21035;&#12289;&#20108;&#20540;&#21270;&#24180;&#40836;&#21644;&#33258;&#25253;&#31181;&#26063;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11373</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#23398;&#20064;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#21450;&#20445;&#25252;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness. (arXiv:2309.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#20165;&#21407;&#22987;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#29983;&#29289;&#23398;&#24615;&#21035;&#12289;&#20108;&#20540;&#21270;&#24180;&#40836;&#21644;&#33258;&#25253;&#31181;&#26063;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#24739;&#32773;&#38544;&#31169;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21307;&#30103;&#25968;&#25454;&#20013;&#27809;&#26377;&#26126;&#30830;&#21253;&#21547;&#31181;&#26063;&#20449;&#24687;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#21307;&#30103;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#33258;&#25253;&#30340;&#31181;&#26063;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#35782;&#21035;&#30340;&#31243;&#24230;&#27809;&#26377;&#20102;&#35299;&#65292;&#20063;&#32570;&#20047;&#24320;&#21457;&#27169;&#22411;&#20197;&#26368;&#23567;&#31243;&#24230;&#21463;&#21040;&#36825;&#20123;&#20449;&#24687;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#20165;&#21407;&#22987;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36824;&#26377;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#37117;&#21487;&#20197;&#34987;&#35757;&#32451;&#29992;&#26469;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#20854;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#21487;&#36798;&#21040;0.851&#65288;&#23545;&#29983;&#29289;&#23398;&#24615;&#21035;&#65289;&#12289;0.869&#65288;&#23545;&#20108;&#20540;&#21270;&#24180;&#40836;&#65289;&#21644;0.810&#65288;&#23545;&#33258;&#25253;&#31181;&#26063;&#65289;&#12290;&#36825;&#31181;&#39640;&#39044;&#27979;&#24615;&#33021;&#21487;&#20197;&#25193;&#23637;&#21040;&#24191;&#27867;&#30340;&#20849;&#30149;&#22240;&#32032;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#27169;&#22411;&#34987;&#35299;&#37322;&#26102;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08534</link><description>&lt;p&gt;
&#26397;&#30528;&#20351;&#29992;&#26356;&#23569;&#26631;&#27880;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#21644;&#21482;&#26377;&#23569;&#37327;&#31867;&#21035;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20248;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#23454;&#29616;&#32676;&#20307;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#23569;&#25968;&#32676;&#20307;&#19978;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29305;&#24449;&#20877;&#36171;&#26435;(DFR)&#25216;&#26415;&#36890;&#36807;&#31616;&#21333;&#30340;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32676;&#20307;&#20445;&#25252;&#24615;&#33021;&#65292;&#20294;&#23427;&#38656;&#35201;&#20445;&#30041;&#32676;&#20307;&#21644;&#31867;&#21035;&#30340;&#26631;&#27880;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32676;&#20307;&#24179;&#34913;&#30340;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#35201;&#27714;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#27809;&#26377;&#32676;&#20307;&#26631;&#27880;&#65288;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#65289;&#65292;&#21482;&#26377;&#23569;&#37327;&#30340;&#31867;&#21035;&#26631;&#27880;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#20986;&#20154;&#24847;&#26009;&#22320;&#26377;&#25928;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21363;&#20351;&#20877;&#36171;&#26435;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#19968;&#23567;&#37096;&#20998;&#26368;&#24046;&#32676;&#20307;&#25968;&#25454;&#65292;&#26368;&#21518;&#19968;&#23618;&#20877;&#35757;&#32451;&#20173;&#28982;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26368;&#24046;&#32676;&#20307;&#20934;&#30830;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#36890;&#36807;&#20445;&#30041;&#19968;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#26469;&#37325;&#26032;&#35757;&#32451;&#26368;&#21518;&#19968;&#23618;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#25110;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;ERM&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we i
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.03179</link><description>&lt;p&gt;
SLiMe: &#20687;&#25105;&#19968;&#26679;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03179
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;SLiMe&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#21644;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#65292;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#21363;&#21487;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;SD&#65289;&#65292;&#22312;&#35832;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#23545;&#24212;&#21644;3D&#24418;&#29366;&#29983;&#25104;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20986;SLiMe&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#26631;&#27880;&#26679;&#26412;&#23545;&#22270;&#20687;&#36827;&#34892;&#20219;&#24847;&#32454;&#31890;&#24230;&#30340;&#20998;&#21106;&#12290;SLiMe&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24352;&#35757;&#32451;&#22270;&#20687;&#21450;&#20854;&#20998;&#21106;&#25513;&#33180;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SD&#20808;&#39564;&#20013;&#25552;&#21462;&#27880;&#24847;&#21147;&#22270;&#65292;&#21253;&#25324;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#8220;&#21152;&#26435;&#32047;&#31215;&#33258;&#27880;&#24847;&#21147;&#22270;&#8221;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#25552;&#21462;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20248;&#21270;&#31283;&#23450;&#25193;&#25955;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#20351;&#24471;&#27599;&#20010;&#23884;&#20837;&#21482;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20998;&#21106;&#21306;&#22495;&#12290;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#28982;&#21518;&#22312;&#27880;&#24847;&#21147;&#22270;&#20013;&#31361;&#20986;&#26174;&#31034;&#20998;&#21106;&#21306;&#22495;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#20998;&#21106;&#22270;&#12290;&#36825;&#20351;&#24471;SLiMe&#21487;&#20197;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#24314;&#27169;&#35823;&#24046;&#65292;&#22312;&#20165;&#20351;&#29992;25%&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11630</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning. (arXiv:2308.11630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#24314;&#27169;&#35823;&#24046;&#65292;&#22312;&#20165;&#20351;&#29992;25%&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#35757;&#32451;Mach-Zehnder&#24178;&#28041;&#20202;&#32593;&#29366;&#20809;&#23398;&#30697;&#38453;&#20056;&#27861;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#39564;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#20174;&#19981;&#22826;&#20934;&#30830;&#30340;&#35299;&#26512;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#35299;&#26512;&#27169;&#22411;&#25110;&#29420;&#31435;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24314;&#27169;&#35823;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#21644;&#38598;&#25104;&#24179;&#22343;&#65292;&#25105;&#20204;&#22312;&#20165;&#20351;&#29992;25%&#21487;&#29992;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#23545;&#30001;&#20809;&#23376;&#33455;&#29255;&#23454;&#29616;&#30340;&#30697;&#38453;&#26435;&#37325;&#30340;&#23567;&#20110;1 dB&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve &lt; 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.05345</link><description>&lt;p&gt;
RTLLM&#65306;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;RTL&#29983;&#25104;&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05345
&lt;/p&gt;
&lt;p&gt;
RTLLM&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24037;&#20316;&#20013;&#30446;&#26631;&#35774;&#35745;&#31616;&#21333;&#19988;&#35268;&#27169;&#23567;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#35774;&#35745;&#36136;&#37327;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#37319;&#29992;LLMs&#36827;&#34892;&#25935;&#25463;&#30828;&#20214;&#35774;&#35745;&#65292;&#20363;&#22914;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#24037;&#20316;&#20013;&#65292;&#30446;&#26631;&#35774;&#35745;&#37117;&#30456;&#23545;&#31616;&#21333;&#19988;&#35268;&#27169;&#36739;&#23567;&#65292;&#24182;&#30001;&#20316;&#32773;&#33258;&#24049;&#25552;&#20986;&#65292;&#36825;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;LLMs&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#35774;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#35780;&#20272;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#30340;&#35774;&#35745;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RTLLM&#30340;&#24320;&#28304;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#35774;&#35745;RTL&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#35774;&#35745;RTL&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19977;&#20010;&#28176;&#36827;&#30446;&#26631;&#65292;&#21363;&#35821;&#27861;&#30446;&#26631;&#12289;&#21151;&#33021;&#30446;&#26631;&#21644;&#35774;&#35745;&#36136;&#37327;&#30446;&#26631;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#33258;&#21160;&#25552;&#20379;&#23545;&#20219;&#20309;&#32473;&#23450;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#65292;&#24179;&#34913;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#65292;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2308.01436</link><description>&lt;p&gt;
&#30005;&#21147;&#24066;&#22330;&#30340;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#65292;&#24179;&#34913;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#65292;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#36880;&#28176;&#28183;&#36879;&#21040;&#36816;&#33829;&#35268;&#21010;&#20013;&#65292;&#20294;&#20854;&#22266;&#26377;&#30340;&#39044;&#27979;&#35823;&#24046;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#30005;&#21147;&#20215;&#26684;&#12290;&#26412;&#25991;&#32771;&#23519;&#20102;&#39044;&#27979;&#35823;&#24046;&#22914;&#20309;&#20256;&#25773;&#21040;&#30005;&#21147;&#20215;&#26684;&#20013;&#65292;&#25581;&#31034;&#20102;&#25317;&#25380;&#30005;&#21147;&#31995;&#32479;&#20013;&#26174;&#33879;&#30340;&#23450;&#20215;&#35823;&#24046;&#21450;&#20854;&#31354;&#38388;&#24046;&#24322;&#12290;&#20026;&#20102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20248;&#21270;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#23618;&#20013;&#12290;&#36890;&#36807;&#27492;&#23618;&#30340;&#21306;&#20998;&#65292;&#21487;&#20197;&#22312;&#39044;&#27979;&#35823;&#24046;&#21644;&#23450;&#20215;&#35823;&#24046;&#20043;&#38388;&#24179;&#34913;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#12290;&#35813;&#23618;&#38544;&#24335;&#22320;&#20248;&#21270;&#20844;&#24179;&#24615;&#65292;&#24182;&#25511;&#21046;&#31995;&#32479;&#20013;&#20215;&#26684;&#35823;&#24046;&#30340;&#31354;&#38388;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#21644;&#30701;&#26399;&#30005;&#21147;&#24066;&#22330;&#28165;&#31639;&#20013;&#30340;&#20215;&#26684;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
&lt;/p&gt;</description></item><item><title>ODTlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#21644;&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.15691</link><description>&lt;p&gt;
ODTlearn: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#21253;
&lt;/p&gt;
&lt;p&gt;
ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15691
&lt;/p&gt;
&lt;p&gt;
ODTlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#21644;&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ODTLearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;(MIO)&#26694;&#26550;&#30340;&#39640;&#39118;&#38505;&#39044;&#27979;&#21644;&#22788;&#26041;&#20219;&#21153;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#21253;&#30340;&#24403;&#21069;&#29256;&#26412;&#25552;&#20379;&#20102;&#23398;&#20064;&#26368;&#20248;&#20998;&#31867;&#26641;&#12289;&#20844;&#24179;&#26368;&#20248;&#20998;&#31867;&#26641;&#12289;&#40065;&#26834;&#26368;&#20248;&#20998;&#31867;&#26641;&#21644;&#20174;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#26368;&#20248;&#22788;&#26041;&#26641;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35813;&#21253;&#20197;&#20415;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#65292;&#24403;&#24341;&#20837;&#26032;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#38382;&#39064;&#31867;&#12289;&#37325;&#26500;&#31574;&#30053;&#21644;&#35299;&#20915;&#31639;&#27861;&#26102;&#65292;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#12290;&#20026;&#27492;&#65292;&#35813;&#21253;&#36981;&#24490;&#38754;&#21521;&#23545;&#35937;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25903;&#25345;&#21830;&#19994;(Gurobi)&#21644;&#24320;&#28304;(COIN-OR branch and cut)&#27714;&#35299;&#22120;&#12290;&#21253;&#30340;&#25991;&#26723;&#21644;&#35814;&#32454;&#29992;&#25143;&#25351;&#21335;&#21487;&#20197;&#22312;https://d3m-research-group.github.io/odtlearn/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
ODTLearn is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in Aghaei et al. (2019) and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#23454;&#29616;&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#26102;&#38388;&#30028;&#38480;&#21644;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2307.12897</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#23454;&#29616;&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#26102;&#38388;&#30028;&#38480;&#21644;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36172;&#21338;&#20248;&#21270;&#20013;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#38656;&#35201;&#22312;&#34892;&#21160;&#36873;&#25321;&#26041;&#38754;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#65292;&#36824;&#38656;&#35201;&#22312;&#27169;&#22411;&#36873;&#25321;&#26041;&#38754;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#23558;&#19981;&#21516;&#27169;&#22411;&#35270;&#20026;&#19987;&#23478;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36951;&#25022;&#26041;&#38754;&#19982;&#27169;&#22411;&#25968;&#37327;$M$&#30340;&#35268;&#27169;&#65288;$\text{poly}M$&#65289;&#21576;&#19981;&#33391;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26377;&#21033;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26469;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#32473;&#22312;&#32447;&#23398;&#20064;&#32773;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20986;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#65288;$\log M$&#65289;&#22312;&#36951;&#25022;&#26041;&#38754;&#23545;$M$&#20381;&#36182;&#24615;&#30340;ALEXP&#12290;ALEXP&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#20219;&#24847;&#20445;&#35777;&#65292;&#24182;&#19988;&#26082;&#19981;&#38656;&#35201;&#23545;&#26102;&#38388;&#30028;$n$&#20855;&#26377;&#30693;&#35782;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#30340;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Lasso&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#22343;&#21248;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#22312;&#32447;&#23398;&#20064;&#21644;&#39640;&#32500;&#32479;&#35745;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10317</link><description>&lt;p&gt;
FedBug: &#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10317
&lt;/p&gt;
&lt;p&gt;
FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20026;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#24615;&#65292;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#24182;&#19982;&#24444;&#27492;&#21457;&#25955;&#65292;&#36825;&#34987;&#31216;&#20026;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedBug&#65288;&#20855;&#26377;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20943;&#36731;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;FedBug&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#26381;&#21153;&#22120;&#20998;&#21457;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#36328;&#23458;&#25143;&#31471;&#23545;&#40784;&#30340;&#21442;&#32771;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#19978;&#65292;FedBug&#20174;&#20923;&#32467;&#25972;&#20010;&#27169;&#22411;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#35299;&#20923;&#23618;&#65292;&#20174;&#36755;&#20837;&#23618;&#21040;&#36755;&#20986;&#23618;&#12290;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#35757;&#32451;&#35299;&#20923;&#30340;&#26032;&#23618;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20998;&#31163;&#36229;&#24179;&#38754;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;FedBug
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20108;&#27425;&#35268;&#21010;&#36755;&#20837;&#35268;&#27169;&#21644;&#35299;&#20915;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.07735</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nearly-Linear Time Algorithm for Structured Support Vector Machines. (arXiv:2307.07735v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32467;&#26500;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20108;&#27425;&#35268;&#21010;&#36755;&#20837;&#35268;&#27169;&#21644;&#35299;&#20915;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#35268;&#21010;&#26159;&#20984;&#20248;&#21270;&#39046;&#22495;&#20013;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#21487;&#20197;&#34920;&#31034;&#20026;&#20108;&#27425;&#35268;&#21010;&#65292;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30427;&#34892;&#20043;&#21069;&#65292;&#32447;&#24615;SVM&#26159;&#36807;&#21435;&#19977;&#21313;&#24180;&#26469;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#30340;&#36755;&#20837;&#35268;&#27169;&#20026;&#920;(n^2)&#65288;&#20854;&#20013;n&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#65289;&#65292;&#22240;&#27492;&#35299;&#20915;&#35813;&#38382;&#39064;&#38656;&#35201;&#937;(n^2)&#30340;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;SVM&#20135;&#29983;&#30340;&#20108;&#27425;&#35268;&#21010;&#30340;&#36755;&#20837;&#35268;&#27169;&#20026;O(n)&#65292;&#36825;&#20351;&#24471;&#35774;&#35745;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;&#20004;&#20010;&#37325;&#35201;&#30340;SVM&#31867;&#21035;&#26159;&#20855;&#26377;&#20302;&#31209;&#26680;&#22240;&#24335;&#20998;&#35299;&#21644;&#20302;&#26641;&#23485;&#35268;&#27169;&#30340;&#31243;&#24207;&#12290;&#20302;&#26641;&#23485;&#20984;&#20248;&#21270;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65288;&#20363;&#22914;&#32447;&#24615;&#35268;&#21010;[Dong, Lee and Ye 2021]&#21644;&#21322;&#23450;&#35268;&#21010;[Gu and Song 2022]&#65289;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#26159;&#26159;&#21542;&#23384;&#22312;&#25509;&#36817;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating.  In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.01231</link><description>&lt;p&gt;
&#23545;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;(ER)&#26159;&#35782;&#21035;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#25351;&#21521;&#30456;&#21516;&#23454;&#20307;&#30340;&#35760;&#24405;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#35299;&#20915;ER&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21305;&#37197;&#38454;&#27573;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#23545;&#23454;&#39564;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#36827;&#34892;&#26816;&#26597;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#20010;&#24050;&#24314;&#31435;&#25968;&#25454;&#38598;&#30340;&#38590;&#24230;&#21644;&#36866;&#29992;&#24615;&#65306;&#20004;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#28041;&#21450;&#26032;&#30340;&#32447;&#24615;&#24230;&#37327;&#21644;&#29616;&#26377;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20197;&#21450;&#20004;&#31181;&#23454;&#38469;&#26041;&#27861;&#65306;&#26368;&#20339;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#21305;&#37197;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#20339;&#23398;&#20064;&#21305;&#37197;&#22120;&#21644;&#23436;&#32654;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#25968;&#25454;&#38598;&#37117;&#25552;&#20986;&#20102;&#30456;&#24403;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#26799;&#24230;&#65292;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#30340;&#21464;&#25442;&#21644;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16830</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37319;&#26679;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16830
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#26799;&#24230;&#65292;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#30340;&#21464;&#25442;&#21644;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#21644;&#26377;&#25928;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#20559;&#24046;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#19981;&#38656;&#35201;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25110;&#35745;&#31639;&#20869;&#37096;&#32593;&#32476;&#21442;&#25968;&#30340;&#26799;&#24230;&#26469;&#35757;&#32451;&#32593;&#32476;&#12290;&#37319;&#26679;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#24605;&#24819;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#36755;&#20837;&#21644;&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#23545;&#27973;&#23618;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25968;&#25454;&#26080;&#20851;&#30340;&#20998;&#24067;&#65292;&#22914;&#27491;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26500;&#36896;&#30340;&#37319;&#26679;&#32593;&#32476;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#26041;&#26696;&#23545;&#21018;&#20307;&#21464;&#25442;&#21644;&#36755;&#20837;&#25968;&#25454;&#30340;&#32553;&#25918;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35768;&#22810;&#24120;&#29992;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#19981;&#20877;&#38656;&#35201;&#12290;&#23545;&#20110;&#24052;&#40857;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37319;&#26679;&#27973;&#23618;&#32593;&#32476;&#30340;L^2&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#20943;&#23567;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex
&lt;/p&gt;</description></item><item><title>OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.16045</link><description>&lt;p&gt;
OpenNDD:&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16045
&lt;/p&gt;
&lt;p&gt;
OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;(NDDs)&#26159;&#19968;&#32452;&#39640;&#24739;&#30149;&#29575;&#30340;&#38556;&#30861;&#65292;&#34920;&#29616;&#20986;&#20020;&#24202;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#30340;NDDs&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#21644;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#65289;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;NDDs&#35786;&#26029;&#24182;&#27809;&#26377;&#21487;&#38752;&#30340;&#29983;&#29702;&#26631;&#24535;&#29289;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#24515;&#29702;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26234;&#33021;&#36741;&#21161;&#35786;&#26029;&#26469;&#38450;&#27490;&#35823;&#35786;&#21644;&#28431;&#35786;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19982;&#38543;&#21518;&#30340;&#30456;&#24212;&#27835;&#30103;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;NDDs&#31579;&#26597;&#21644;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#36825;&#26159;&#22312;&#35813;&#39046;&#22495;&#20013;&#39318;&#27425;&#24212;&#29992;&#24320;&#25918;&#24615;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#36807;&#21435;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13948</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#65306;&#20171;&#32461;&#26131;&#20110;&#20351;&#29992;&#30340;PurpleAirSF&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#32473;&#30740;&#31350;&#20154;&#21592;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20840;&#38754;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;PurpleAir&#32593;&#32476;&#20013;&#25910;&#38598;&#32780;&#26469;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#21508;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26032;&#22411;&#39044;&#27979;&#27169;&#22411;&#12289;&#30740;&#31350;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#20197;&#21450;&#35843;&#26597;&#20854;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#30340;&#24433;&#21709;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;PurpleAirSF&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#32463;&#20856;&#21644;&#29616;&#20195;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#21046;&#23450;&#24314;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13761</link><description>&lt;p&gt;
CeBed: &#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20449;&#36947;&#20272;&#35745;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20844;&#27491;&#21644;&#29616;&#23454;&#30340;&#27604;&#36739;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#24037;&#20855;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#24211;&#65289;&#38459;&#30861;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20449;&#36947;&#20272;&#35745;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24314;&#31435;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#65292;&#32479;&#19968;&#20102;&#20960;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CeBed&#65288;&#20449;&#36947;&#20272;&#35745;&#27979;&#35797;&#24179;&#21488;&#65289;&#65292;&#21253;&#25324;&#28085;&#30422;&#21508;&#31181;&#31995;&#32479;&#27169;&#22411;&#21644;&#20256;&#25773;&#26465;&#20214;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21313;&#20010;&#28145;&#24230;&#21644;&#20256;&#32479;&#30340;&#22522;&#32447;&#23454;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#20248;&#21270;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz&#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#26174;&#24335;&#20248;&#21270;&#21644;&#38544;&#24335;&#20248;&#21270;&#26159;&#20004;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.09338</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20248;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding Optimization of Deep Learning. (arXiv:2306.09338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#20248;&#21270;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz&#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#26174;&#24335;&#20248;&#21270;&#21644;&#38544;&#24335;&#20248;&#21270;&#26159;&#20004;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#29702;&#35770;&#65292;&#20027;&#35201;&#20851;&#27880;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#31561;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#27169;&#22411;&#34920;&#31034;&#33021;&#21147;&#38477;&#20302;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#39640;&#26799;&#24230;&#27969;&#21644;&#23545;&#32593;&#32476;Lipschitz &#24120;&#25968;&#26045;&#21152;&#32422;&#26463;&#31561;&#25514;&#26045;&#26469;&#20998;&#26512;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#24403;&#21069;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#26174;&#24335;&#20248;&#21270;&#26041;&#27861;&#21644;&#38544;&#24335;&#20248;&#21270;&#26041;&#27861;&#12290;&#26174;&#24335;&#20248;&#21270;&#26041;&#27861;&#28041;&#21450;&#30452;&#25509;&#25805;&#20316;&#20248;&#21270;&#22120;&#21442;&#25968;&#65292;&#21253;&#25324;&#26435;&#37325;&#12289;&#26799;&#24230;&#12289;&#23398;&#20064;&#29575;&#21644;&#26435;&#37325;&#34928;&#20943;&#31561;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38544;&#24335;&#20248;&#21270;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#22686;&#24378;&#32593;&#32476;&#27169;&#22359;&#65288;&#22914;&#27531;&#24046;&#24555;&#25463;&#26041;&#24335;&#12289;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#27880;&#24847;&#26426;&#21046;&#21644;&#28608;&#27963;&#65289;&#26469;&#25913;&#21892;&#32593;&#32476;&#25972;&#20307;&#24418;&#21183;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26356;&#22909;&#22320;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth a
&lt;/p&gt;</description></item><item><title>&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.08698</link><description>&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08698
&lt;/p&gt;
&lt;p&gt;
&#36328;&#22269;&#21644;&#26102;&#38388;&#30340;&#31038;&#20250;&#21160;&#33633;&#30456;&#21464;&#30740;&#31350;&#25506;&#32034;&#20102;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#26159;&#21542;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#24490;&#29615;&#30456;&#21464;&#65292;&#24182;&#35777;&#26126;&#20102;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26222;&#36941;&#26426;&#21046;&#21487;&#33021;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#21464;&#26159;&#22797;&#26434;&#31995;&#32479;&#20013;&#31361;&#21457;&#36716;&#21464;&#30340;&#29305;&#24449;&#65292;&#23613;&#31649;&#22312;&#29289;&#29702;&#21644;&#33258;&#28982;&#31185;&#23398;&#20013;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#22312;&#31038;&#20250;&#31995;&#32479;&#20013;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#38598;&#20307;&#31038;&#20250;&#21160;&#33633;&#30340;&#21160;&#21147;&#23398;&#26159;&#21542;&#21487;&#20197;&#34987;&#21512;&#29702;&#22320;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#24490;&#29615;&#30456;&#21464;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#20855;&#26377;&#21487;&#27979;&#37327;&#21644;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23439;&#35266;&#27700;&#24179;&#30340;&#31038;&#20250;&#21160;&#33633;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21253;&#25324;1946&#24180;&#33267;2017&#24180;&#22312;&#20869;&#30340;170&#20010;&#22269;&#23478;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20854;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#23439;&#35266;&#30456;&#21464;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#20102;&#20840;&#29699;&#21508;&#22269;&#31038;&#20250;&#21160;&#33633;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#26222;&#36941;&#30340;&#26426;&#21046;&#21487;&#33021;&#28508;&#22312;&#22320;&#25903;&#25745;&#30528;&#31038;&#20250;&#21160;&#33633;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#34920;&#26469;&#34913;&#37327;&#19968;&#20010;&#22269;&#23478;&#30340;&#31038;&#20250;&#21160;&#33633;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.06874</link><description>&lt;p&gt;
VillanDiffusion: &#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;VillanDiffusion&#65292;&#19968;&#20010;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#28085;&#30422;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65292;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#23398;&#20064;&#21487;&#36870;&#30340;&#25439;&#22351;&#36807;&#31243;&#12290;&#23427;&#20204;&#26159;&#35768;&#22810;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#20027;&#24178;&#65292;&#20363;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#26377;&#26465;&#20214;&#29983;&#25104;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22522;&#26412;&#26080;&#26465;&#20214;DM&#65288;&#20363;&#22914;DDPM&#21644;DDIM&#65289;&#26131;&#21463;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#30001;&#20110;&#24694;&#24847;&#23884;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#27169;&#24335;&#32780;&#35302;&#21457;&#30340;&#36755;&#20986;&#25805;&#32437;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65288;VillanDiffusion&#65289;&#65292;&#20197;&#25193;&#23637;&#24403;&#21069;&#30340;DM&#21518;&#38376;&#20998;&#26512;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#20027;&#27969;&#30340;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;DM&#65288;&#22522;&#20110;&#21435;&#22122;&#21644;&#22522;&#20110;&#35780;&#20998;&#65289;&#65292;&#20197;&#21450;&#21508;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#37319;&#26679;&#22120;&#36827;&#34892;&#25972;&#20307;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#20415;&#20110;&#23545;&#19981;&#21516;DM&#37197;&#32622;&#36827;&#34892;&#21518;&#38376;&#20998;&#26512;&#65292;&#24182;&#20026;&#22522;&#20110;&#23383;&#24149;&#30340;DM&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00074</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#26657;&#20934;&#29992;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20010;&#20154;&#20559;&#22909;&#30340;&#32622;&#20449;&#24230;&#26500;&#36896;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#32622;&#20449;&#24230;&#23545;&#20110;&#20915;&#31574;&#32773;&#20449;&#20219;&#20915;&#31574;&#30340;&#19981;&#20934;&#30830;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#23427;&#36890;&#24120;&#25552;&#20379;&#26631;&#31614;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#20915;&#31574;&#32773;&#24212;&#20351;&#29992;&#32622;&#20449;&#24230;&#20540;&#26469;&#26657;&#20934;&#23545;&#39044;&#27979;&#30340;&#20449;&#20219;&#31243;&#24230;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#32463;&#24120;&#35748;&#20026;&#32622;&#20449;&#24230;&#20540;&#24212;&#23545;&#39044;&#27979;&#26631;&#31614;&#19982;&#23454;&#38469;&#26631;&#31614;&#21305;&#37197;&#30340;&#27010;&#29575;&#36827;&#34892;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22810;&#26465;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#38590;&#20197;&#20351;&#29992;&#36825;&#20123;&#32622;&#20449;&#24230;&#20540;&#24456;&#22909;&#22320;&#30830;&#23450;&#20309;&#26102;&#20449;&#20219;&#39044;&#27979;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#39318;&#20808;&#26159;&#29702;&#35299;&#20026;&#20160;&#20040;&#65292;&#28982;&#21518;&#30740;&#31350;&#22914;&#20309;&#26500;&#24314;&#26356;&#26377;&#29992;&#30340;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#35748;&#20026;&#65292;&#22312;&#24191;&#27867;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20013;&#65292;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#29702;&#24615;&#20915;&#31574;&#32773;&#36890;&#24120;&#38590;&#20197;&#20351;&#29992;&#20197;&#19978;&#32622;&#20449;&#24230;&#20540;&#21457;&#29616;&#26368;&#20339;&#20915;&#31574;&#25919;&#31574;&#8212;&#8212;&#26368;&#20339;&#30340;&#20915;&#31574;&#32773;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#35810;&#38382;&#20915;&#31574;&#32773;&#20182;&#20204;&#22312;&#25152;&#38754;&#20020;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#19978;&#30340;&#20010;&#20154;&#20559;&#22909;&#30340;&#26032;&#26041;&#27861;&#26469;&#26500;&#36896;&#32622;&#20449;&#24230;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#30340;&#32622;&#20449;&#24230;&#20540;&#27604;&#20351;&#29992;&#26631;&#20934;&#32622;&#20449;&#24230;&#24230;&#37327;&#23548;&#33268;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#29992;&#20110;&#20449;&#21495;&#20247;&#22810;&#20294;&#21464;&#24322;&#24615;&#36739;&#22823;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#65292;&#33021;&#33258;&#36866;&#24212;&#35843;&#25972;&#24133;&#24230;&#12289;&#28388;&#27874;&#22120;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters &#23454;&#29616;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#19988;&#26174;&#33879;&#25552;&#21319;&#20449;&#21495;&#20998;&#31867;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18831</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270;&#30340;&#29983;&#29289;&#20449;&#21495;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Convolutional Monge Mapping Normalization for learning on biosignals. (arXiv:2305.18831v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#29992;&#20110;&#20449;&#21495;&#20247;&#22810;&#20294;&#21464;&#24322;&#24615;&#36739;&#22823;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#65292;&#33021;&#33258;&#36866;&#24212;&#35843;&#25972;&#24133;&#24230;&#12289;&#28388;&#27874;&#22120;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters &#23454;&#29616;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#19988;&#26174;&#33879;&#25552;&#21319;&#20449;&#21495;&#20998;&#31867;&#21644;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20449;&#21495;&#19982;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#33041;&#30005;&#22270; (EEG) &#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#25968;&#25454;&#22312;&#21463;&#35797;&#32773;&#12289;&#20250;&#35805;&#21644;&#30828;&#20214;&#35774;&#22791;&#19978;&#30340;&#21464;&#24322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21367;&#31215;&#33945;&#26085;&#26144;&#23556;&#24402;&#19968;&#21270; (CMMN)&#65292;&#20854;&#26680;&#24515;&#26159;&#26681;&#25454;&#35757;&#32451;&#25968;&#25454;&#20272;&#35745;Wasserstein barycenter&#65292;&#36890;&#36807;&#28388;&#27874;&#20449;&#21495;&#20197;&#36866;&#24212;&#20854;&#21151;&#29575;&#35889;&#23494;&#24230; (PSD)&#12290;CMMN &#22522;&#20110;&#26032;&#30340;&#38381;&#24335;&#35299;&#65292;&#25552;&#20379;&#20102;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#21644; barycenters&#65292;&#24182;&#25552;&#20379;&#20102;&#20010;&#20307;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#21151;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#30561;&#30496; EEG &#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;CMMN &#22312;&#36866;&#24212;&#21463;&#35797;&#32773;&#12289;&#20250;&#35805;&#29978;&#33267;&#22312;&#20351;&#29992;&#19981;&#21516;&#30828;&#20214;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#65292;&#37117;&#33021;&#24102;&#26469;&#26174;&#33879;&#19988;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#19988;&#20854;&#24615;&#33021;&#25552;&#21319;&#19982;&#25968;&#20540;&#23494;&#38598;&#30340;&#22495;&#36866;&#24212; (DA) &#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18450</link><description>&lt;p&gt;
GBG++&#65306;&#20998;&#31867;&#30340;&#24555;&#36895;&#21644;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#12289;&#31283;&#20581;&#12289;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#39063;&#31890;&#35745;&#31639;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#39063;&#31890;&#29699;&#35745;&#31639;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39063;&#31890;&#29699;&#29983;&#25104;&#65288;GBG&#65289;&#21644;&#22522;&#20110;&#39063;&#31890;&#29699;&#65288;GB&#65289;&#30340;&#22810;&#31890;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GBG&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;k&#22343;&#20540;&#25110;k&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;GB&#30340;&#20998;&#31867;&#22120;&#20165;&#21333;&#21521;&#32771;&#34385;GB&#30340;&#20960;&#20309;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#35268;&#21017;&#65292;&#32780;&#24573;&#35270;&#20102;GB&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31283;&#23450;&#30340;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;GBG&#65288;GBG++&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;GBG++&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#20998;&#21106;&#27599;&#20010;GB&#26102;&#35745;&#31639;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#20013;&#24515;&#21040;&#26410;&#20998;&#21106;&#26679;&#26412;&#30340;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#20013;&#24515;&#24182;&#35745;&#31639;&#23427;&#21040;&#25152;&#26377;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB, instead of randomly selecting the center and calculating the distances between it to all samples. Moreover, an outlier detection method is introduced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.18427</link><description>&lt;p&gt;
GRD: &#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#29983;&#25104;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#22870;&#21169;&#20877;&#20998;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#36879;&#35270;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#36129;&#29486;&#65292;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#26694;&#26550;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#21738;&#20123;&#29366;&#24577;-&#34892;&#21160;&#23545;&#24212;&#35813;&#23545;&#26410;&#26469;&#30340;&#20998;&#27493;&#22870;&#21169;&#36127;&#36131;&#12290;Return Decomposition&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#35266;&#27979;&#24207;&#21015;&#20013;&#30340;&#22870;&#21169;&#26469;&#20445;&#25345;&#31574;&#30053;&#19981;&#21464;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20197;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26500;&#24314;&#22870;&#21169;&#20877;&#20998;&#37197;&#65292;&#20294;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#22240;&#26524;&#36879;&#35270;&#26469;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#36129;&#29486;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#36820;&#22238;&#20998;&#35299;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#22312;&#36820;&#22238;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25551;&#36848;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22522;&#20110;&#36712;&#36857;&#30340;&#38271;&#26399;&#22238;&#25253;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#36820;&#22238;&#20998;&#35299;&#65288;GRD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24310;&#36831;&#22870;&#21169;&#22330;&#26223;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GRD&#39318;&#20808;&#30830;&#23450;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#21644;&#22240;&#26524;&#20851;&#31995;&#65292;&#28982;&#21518;&#21033;&#29992;&#30830;&#23450;&#30340;&#22240;&#26524;&#27169;&#22411;&#35745;&#31639;&#21487;&#35266;&#27979;&#22870;&#21169;&#30340;&#26399;&#26395;&#65292;&#36827;&#32780;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;StEik&#26469;&#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24418;&#29366;&#32454;&#33410;&#34920;&#31034;&#21644;&#20248;&#21270;&#25928;&#26524;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;INR&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18414</link><description>&lt;p&gt;
StEik: &#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#26356;&#32454;&#33268;&#24418;&#29366;&#34920;&#31034;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation. (arXiv:2305.18414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;StEik&#26469;&#31283;&#23450;&#31070;&#32463;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24418;&#29366;&#32454;&#33410;&#34920;&#31034;&#21644;&#20248;&#21270;&#25928;&#26524;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;INR&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#24418;&#29366;&#30340;&#26032;&#35265;&#35299;&#21644;&#26032;&#33539;&#24335;&#65288;StEik&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;INR&#20013;&#29992;&#20110;&#26045;&#21152;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#32422;&#26463;&#30340;&#27969;&#34892;&#30340;Eikonal Loss&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#20174;&#35299;&#26512;&#19978;&#35777;&#26126;&#65292;&#38543;&#30528;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#30340;&#22686;&#24378;&#65292;&#20248;&#21270;&#26041;&#27861;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#36924;&#36817;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#32780;PDE&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#22312;&#29616;&#26377;&#30340;&#32593;&#32476;&#20248;&#21270;&#20013;&#21487;&#33021;&#34920;&#29616;&#20026;&#37325;&#26500;&#34920;&#38754;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;/&#25110;&#25910;&#25947;&#21040;&#27425;&#20248;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#20174;&#35299;&#26512;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20854;&#20182;&#28155;&#21152;&#21040;&#25439;&#22833;&#20013;&#30340;&#26415;&#35821;(&#24403;&#21069;&#22312;&#25991;&#29486;&#20013;&#29992;&#20110;&#20854;&#20182;&#30446;&#30340;)&#26469;&#28040;&#38500;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39033;&#21487;&#33021;&#36807;&#24230;&#35268;&#21017;&#21270;&#34920;&#38754;&#65292;&#38450;&#27490;&#32454;&#33410;&#30340;&#31934;&#32454;&#24418;&#29366;&#34920;&#31034;&#12290;&#22522;&#20110;&#36830;&#32493;&#26497;&#38480;&#30340;&#31867;&#20284;PDE&#29702;&#35770;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#31283;&#23450;&#20248;&#21270;&#21644;&#25429;&#25417;&#31934;&#32454;&#30340;&#24418;&#29366;&#32454;&#33410;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;3D&#24418;&#29366;&#25968;&#25454;&#38598;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;INR&#26041;&#27861;&#65292;&#22312;&#24418;&#29366;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present new insights and a novel paradigm (StEik) for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.15889</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#24322;&#36136;&#24615;&#37327;&#21270;&#21644;&#23545;&#27604;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Feature Heterogeneity Distance(FHD)&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;Contrastive Convergence for Domain Generalization (CCDG) &#26469;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20960;&#20010;&#28304;&#22495;&#35757;&#32451;&#20986;&#23545;&#26410;&#35265;&#36807;&#30446;&#26631;&#22495;&#36827;&#34892;&#26377;&#25928;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#39046;&#22495;&#26631;&#31614;-&#21363;&#27599;&#20010;&#25968;&#25454;&#28857;&#26469;&#33258;&#21738;&#20010;&#22495;&#33258;&#28982;&#23384;&#22312;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;DG&#31639;&#27861;&#23558;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#30417;&#30563;&#20449;&#24687;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39046;&#22495;&#20043;&#38388;&#32570;&#20047;&#24322;&#36136;&#24615;&#65292;&#21363;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21407;&#22987;&#30340;&#22495;&#26631;&#31614;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;Feature Heterogeneity Distance(FHD)&#26469;&#34913;&#37327;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#27169;&#24335;CCDG&#65292;&#29992;&#20110;&#23547;&#25214;&#26368;&#20339;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#27867;&#21270;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;FHD&#24230;&#37327;&#26631;&#20934;&#21644;CCDG&#27169;&#24335;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.15616</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#21487;&#36870;&#21644;&#19981;&#21487;&#36870;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#65292;&#36825;&#35299;&#37322;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#32467;&#26500;&#20801;&#35768;&#35757;&#32451;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32780;&#19981;&#20250;&#36807;&#24230;&#20809;&#28369;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29289;&#29702;&#30340;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#65292;&#23613;&#31649;&#21487;&#36870;&#65288;&#20363;&#22914;&#21704;&#23494;&#39039;&#65289;&#21644;&#19981;&#21487;&#36870;&#65288;&#20363;&#22914;&#25193;&#25955;&#65289;&#29616;&#35937;&#30340;&#25104;&#21151;&#23454;&#20363;&#20135;&#29983;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#26426;&#21046;&#25130;&#28982;&#30456;&#21453;&#65292;&#24182;&#19988;&#30001;&#20110;&#32463;&#39564;&#19978;&#30340;&#31163;&#24320;&#25968;&#23398;&#29702;&#35770;&#32780;&#20986;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#32467;&#26500;&#20445;&#25345;&#22522;&#20110;&#25324;&#21495;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26032;&#22411;GNN&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#29702;&#35770;&#19978;&#34987;&#35777;&#26126;&#35201;&#20040;&#20445;&#25345;&#33021;&#37327;&#65292;&#35201;&#20040;&#22312;&#28145;&#24230;&#22686;&#21152;&#26102;&#20135;&#29983;&#27491;&#30340;&#32791;&#25955;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#37324;&#20351;&#29992;&#30340;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26694;&#26550;&#20801;&#35768;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#23558;&#24403;&#21069;&#26550;&#26500;&#20013;&#30340;&#31163;&#24320;&#29702;&#35770;&#20869;&#23481;&#25918;&#22312;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#26356;&#22909;&#22320;&#38416;&#26126;&#20102;&#21487;&#36870;&#24615;&#21644;&#19981;&#21487;&#36870;&#24615;&#22312;&#32593;&#32476;&#24615;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.
&lt;/p&gt;</description></item><item><title>ParticleWNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#24369;&#24418;&#24335;&#19979;&#27714;&#35299;PDE&#12290;&#23427;&#37319;&#29992;DNN&#20316;&#20026;&#35797;&#39564;&#31354;&#38388;&#65292;&#29992;&#30001;&#31890;&#23376;&#20026;&#20013;&#24515;&#30340;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#32039;&#23494;&#25903;&#25345;&#30340;&#20989;&#25968;&#26500;&#25104;&#30340;&#27979;&#35797;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;R&#33258;&#36866;&#24212;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#26131;&#20110;&#25193;&#23637;&#21644;&#24182;&#34892;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.12433</link><description>&lt;p&gt;
ParticleWNN&#65306;&#19968;&#31181;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ParticleWNN: a Novel Neural Networks Framework for Solving Partial Differential Equations. (arXiv:2305.12433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12433
&lt;/p&gt;
&lt;p&gt;
ParticleWNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#24369;&#24418;&#24335;&#19979;&#27714;&#35299;PDE&#12290;&#23427;&#37319;&#29992;DNN&#20316;&#20026;&#35797;&#39564;&#31354;&#38388;&#65292;&#29992;&#30001;&#31890;&#23376;&#20026;&#20013;&#24515;&#30340;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#32039;&#23494;&#25903;&#25345;&#30340;&#20989;&#25968;&#26500;&#25104;&#30340;&#27979;&#35797;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;R&#33258;&#36866;&#24212;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#19988;&#26131;&#20110;&#25193;&#23637;&#21644;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24191;&#27867;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;Particle Weak-form&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;ParticleWNN&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#24369;&#24418;&#24335;&#30340;PDE&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#35797;&#39564;&#31354;&#38388;&#36873;&#25321;&#20026;DNN&#30340;&#31354;&#38388;&#65292;&#27979;&#35797;&#31354;&#38388;&#30001;&#32039;&#23494;&#25903;&#25345;&#22312;&#26497;&#23567;&#21306;&#22495;&#20869;&#30340;&#20989;&#25968;&#26500;&#25104;&#65292;&#36825;&#20123;&#20989;&#25968;&#30340;&#20013;&#24515;&#26159;&#31890;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;R&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#33258;&#36866;&#24212;&#20462;&#25913;&#21306;&#22495;&#30340;&#21322;&#24452;&#12290;ParticleWNN&#32487;&#25215;&#20102;&#24369;/&#21464;&#20998;&#20844;&#24335;&#30340;&#20248;&#28857;&#65292;&#20363;&#22914;&#35201;&#27714;&#36739;&#23569;&#30340;&#35299;&#30340;&#27491;&#21017;&#24615;&#21644;&#35745;&#31639;&#31215;&#20998;&#30340;&#23569;&#37327;&#31215;&#20998;&#28857;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27979;&#35797;&#20989;&#25968;&#30340;&#29305;&#27530;&#26500;&#36896;&#65292;ParticleWNN&#20801;&#35768;&#26412;&#22320;&#35757;&#32451;&#32593;&#32476;&#12289;&#24182;&#34892;&#23454;&#29616;&#21644;&#20165;&#22312;&#26497;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31215;&#20998;&#35745;&#31639;&#12290;&#35813;&#26694;&#26550;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#29616;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been widely used to solve partial differential equations (PDEs) in recent years. In this work, a novel deep learning-based framework named Particle Weak-form based Neural Networks (ParticleWNN) is developed for solving PDEs in the weak form. In this framework, the trial space is chosen as the space of DNNs, and the test space is constructed by functions compactly supported in extremely small regions whose centers are particles. To train the neural networks, an R-adaptive strategy is designed to adaptively modify the radius of regions during training. The ParticleWNN inherits the advantages of weak/variational formulation, such as requiring less regularity of the solution and a small number of quadrature points for computing the integrals. Moreover, due to the special construction of the test functions, the ParticleWNN allows local training of networks, parallel implementation, and integral calculations only in extremely small regions. The framework is p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.12082</link><description>&lt;p&gt;
SneakyPrompt&#65306;&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#35813;&#26694;&#26550;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#65292;&#22914;Stable Diffusion&#21644;DALL$\cdot$E 2&#31561;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#26159;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#20363;&#22914;&#19982;&#26292;&#21147;&#21644;&#25104;&#20154;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#37096;&#32626;&#25152;&#35859;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#22522;&#20110;&#25991;&#26412;&#25110;&#22270;&#20687;&#29305;&#24449;&#38459;&#27490;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#27492;&#31867;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#21487;&#33021;&#32469;&#36807;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#25163;&#21160;&#23436;&#25104;&#24182;&#19987;&#38376;&#38024;&#23545;Stable Diffusion&#23448;&#26041;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;Stable Diffusion&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#32469;&#36807;&#27604;&#29575;&#20165;&#20026;23.51&#65285;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33258;&#21160;&#25915;&#20987;&#26694;&#26550;SneakyPrompt&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#29616;&#23454;&#19990;&#30028;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#25628;&#32034;&#22791;&#36873;&#20196;&#29260;&#26469;&#32469;&#36807;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2305.11650</link><description>&lt;p&gt;
&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#21305;&#37197;&#21435;&#22122;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#8216;&#22024;&#26434;&#8217;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#24178;&#20928;&#30340;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBMs&#65289;&#20026;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;EBMs &#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#29992;&#20110;&#21487;&#25193;&#23637; EBM &#35757;&#32451;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#65288;DSM&#65289;&#26041;&#27861;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23548;&#33268;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#21040;&#8220;&#22024;&#26434;&#8221;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65306;&#65288;&#20266;&#65289;Gibbs&#37319;&#26679;&#19982;&#21160;&#37327;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#32463;&#36807;DSM&#35757;&#32451;&#33391;&#22909;&#30340;&#8220;&#22024;&#26434;&#8221;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#22522;&#30784;&#8220;&#24178;&#20928;&#8221;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30456;&#20851;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03626</link><description>&lt;p&gt;
&#40065;&#26834;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340;&#21487;&#39564;&#35777;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39564;&#35777;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#26131;&#20110;&#39564;&#35777;&#30340;&#38480;&#21046;&#27169;&#22411;&#31867;&#26469;&#35299;&#20915;&#20915;&#31574;&#26641;&#38598;&#25104;&#30340; NP-hard &#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#35774;&#35745;&#20986;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21487;&#20197;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#65292;&#32780;&#19988;&#20173;&#20445;&#25345;&#30528;&#35813;&#39046;&#22495;&#26368;&#22909;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#65292;&#23545;&#20110;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159; NP-hard &#65292;&#22240;&#27492;&#23545;&#20110;&#29305;&#23450;&#30340;&#36755;&#20837;&#26469;&#35828;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#21463;&#38480;&#20915;&#31574;&#26641;&#38598;&#25104;&#65292;&#31216;&#20026; large-spread &#38598;&#25104;&#65292;&#20854;&#20801;&#35768;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#23433;&#20840;&#39564;&#35777;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39564;&#35777;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20513;&#23548;&#35757;&#32451;&#36825;&#31181;&#26131;&#20110;&#39564;&#35777;&#30340;&#21463;&#38480;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064; large-spread &#20915;&#31574;&#26641;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#30410;&#22788;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;&#23433;&#20840;&#39564;&#35777;&#12290;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31639;&#27861;&#35757;&#32451;&#30340; large-spread &#38598;&#25104;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20351;&#29992;&#26631;&#20934;&#21322;&#23450;&#32534;&#31243;&#27714;&#35299;&#22120;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
&lt;/p&gt;</description></item><item><title>MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09402</link><description>&lt;p&gt;
MixPro&#65306;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09402
&lt;/p&gt;
&lt;p&gt;
MixPro&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#36755;&#20837;&#21644;&#27169;&#26495;&#36827;&#34892;&#28151;&#21512;&#26469;&#25552;&#39640;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36890;&#36807;&#23558;&#36755;&#20837;&#19982;&#27169;&#26495;&#32452;&#21512;&#36215;&#26469;&#65292;&#23558;&#19979;&#28216;&#20219;&#21153;&#37325;&#26500;&#20026;&#22635;&#31354;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#28982;&#32780;&#65292;&#20351;&#29992;&#26377;&#38480;&#30340;&#27169;&#26495;&#21644;&#25991;&#26412;&#20173;&#28982;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#30340;&#26041;&#27861;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixPro&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26631;&#35760;&#32423;&#12289;&#21477;&#23376;&#32423;&#21644;&#26102;&#20195;&#32423;&#30340;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#21407;&#22987;&#36755;&#20837;&#25991;&#26412;&#21644;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#23569;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MixPro&#20248;&#20110;&#20854;&#20182;&#22686;&#24378;&#22522;&#32447;&#65292;&#30456;&#27604;&#22686;&#24378;&#21069;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;5.08%&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
&lt;/p&gt;</description></item><item><title>FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02725</link><description>&lt;p&gt;
FMG-Net&#21644;W-Net&#65306;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02725
&lt;/p&gt;
&lt;p&gt;
FMG-Net&#21644;W-Net&#26159;&#20004;&#31181;&#21463;&#22810;&#37325;&#32593;&#26684;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38754;&#20020;&#30340;&#32454;&#33410;&#29305;&#24449;&#21644;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#33021;&#22815;&#25552;&#39640;&#32959;&#30244;&#20998;&#21106;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#23545;&#20110;&#31934;&#30830;&#21644;&#26377;&#25928;&#30340;&#21307;&#30103;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20173;&#38754;&#20020;&#30528;&#22788;&#29702;&#32454;&#31890;&#24230;&#29305;&#24449;&#21644;&#22270;&#20687;&#23610;&#24230;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#21106;&#20219;&#21153;&#20013;&#29305;&#21035;&#26126;&#26174;&#65292;&#20363;&#22914;BraTS&#22810;&#26631;&#31614;&#33041;&#32959;&#30244;&#20998;&#21106;&#25361;&#25112;&#36187;&#20013;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#31934;&#30830;&#22320;&#20998;&#21106;&#19981;&#21516;&#30340;&#32959;&#30244;&#20122;&#32452;&#20998;&#65292;&#22312;&#22823;&#23567;&#21644;&#24418;&#29366;&#19978;&#37117;&#26377;&#26174;&#33879;&#21464;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20063;&#20250;&#20135;&#29983;&#37325;&#22823;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26550;&#26500;&#65292;FMG-Net&#21644;W-Net&#65292;&#23427;&#20204;&#23558;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#20960;&#20309;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#21407;&#29702;&#32435;&#20837;CNN&#20013;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FMG-Net&#21644;W-Net&#37117;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;U-Net&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#20013;&#30340;tum&#30340;&#31934;&#24230;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.01203</link><description>&lt;p&gt;
&#22522;&#20110;&#20934;&#24230;&#37327;&#23398;&#20064;&#30340;&#26368;&#20248;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#65307;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30446;&#26631;&#36798;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31216;&#20026;&#20934;&#24230;&#37327;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20934;&#24230;&#37327;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#65292;&#21033;&#29992;&#20934;&#24230;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#26368;&#20248;&#20215;&#20540;&#20989;&#25968;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;QRL&#30340;&#30446;&#26631;&#26159;&#19987;&#38376;&#20026;&#20934;&#24230;&#37327;&#35774;&#35745;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#22312;&#31163;&#25955;&#21270;&#30340;MountainCar&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;QRL&#30340;&#24615;&#36136;&#20197;&#21450;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#30446;&#26631;&#36798;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;QRL&#36824;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#37319;&#26679;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13525</link><description>&lt;p&gt;
&#20113;&#35745;&#31639;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#23545;&#20110;&#31649;&#29702;&#20113;&#25968;&#25454;&#20013;&#24515;&#24182;&#20445;&#35777;&#23458;&#25143;&#26368;&#20302;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#24314;&#27169;&#26410;&#26469;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#30001;&#20110;&#36164;&#28304;&#36807;&#24230;&#20998;&#37197;&#32780;&#24102;&#26469;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36164;&#28304;&#38656;&#27714;&#30340;&#20998;&#24067;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24773;&#26223;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36807;&#31243;&#26159;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#37197;&#32622;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27493;&#39588;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23558;&#21452;&#21464;&#37327;&#27169;&#22411;&#19982;&#20854;&#21333;&#21464;&#37327;&#23545;&#24212;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#24433;&#21709;QoS&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.07271</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#21363;&#25554;&#21363;&#29992;&#25311;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#26041;&#27861;&#26159;&#19968;&#31867;&#39640;&#25928;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;ISTA&#25110;ADMM&#65289;&#65292;&#23558;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#21644;&#28145;&#24230;&#21435;&#22122;&#22120;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#20005;&#26684;&#30340;&#38480;&#21046;&#65292;&#22914;&#38750;&#25193;&#24352;&#24615;&#25110;&#20005;&#26684;&#20984;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36817;&#31471;&#21435;&#22122;&#22120;&#26045;&#21152;&#30456;&#23545;&#36739;&#36731;&#30340;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#22823;&#22823;&#21152;&#36895;&#25910;&#25947;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#21435;&#22122;&#22120;&#29305;&#21035;&#21442;&#25968;&#21270;&#20026;&#26799;&#24230;&#27493;&#39588;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25311;&#29275;&#39039;PnP&#31639;&#27861;&#30340;&#22266;&#23450;&#28857;&#34920;&#24449;&#20026;&#21487;&#33021;&#38750;&#20984;&#20989;&#25968;&#30340;&#20020;&#30028;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06532</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.
&lt;/p&gt;
&lt;p&gt;
&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#20854;&#33021;&#22815;&#29420;&#31435;&#20110;&#38382;&#39064;&#32467;&#26500;&#21644;&#38382;&#39064;&#39046;&#22495;&#36827;&#34892;&#25628;&#32034;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#20197;&#36866;&#24212;&#35299;&#20915;&#30446;&#26631;&#38382;&#39064;&#12290;&#25163;&#21160;&#35843;&#25972;&#36807;&#31243;&#21487;&#33021;&#26159;&#36153;&#21147;&#30340;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#20197;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#20351;&#39640;&#24615;&#33021;&#31639;&#27861;&#23545;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#29992;&#65307;&#36890;&#36807;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#36827;&#34892;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#23454;&#29616;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06234</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#24212;&#25968;&#25454;&#20013;&#36827;&#34892;&#26368;&#20248;&#21644;&#31169;&#23494;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#23454;&#29616;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new spectral estimation algorithm that is efficient and accurate, achieving the minimax optimal error bound (modulo a log factor) under mild sampling conditions, and enjoys optimal sample complexity for top-$K$ recovery.
&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#26159;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#20570;&#20986;&#27010;&#29575;&#20915;&#31574;&#30340;&#23398;&#31185;&#65292;&#20855;&#26377;&#25945;&#32946;&#27979;&#35797;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#20108;&#20803;&#21453;&#24212;&#25968;&#25454;&#30340;Rasch&#27169;&#22411;&#26159;IRT&#20013;&#26368;&#22522;&#26412;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#23454;&#38469;&#24847;&#20041;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;Nguyen&#21644;Zhang&#65288;2022&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#20004;&#31181;&#37325;&#35201;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35889;&#31639;&#27861;&#30340;&#31934;&#32454;&#36880;&#39033;&#35823;&#24046;&#30028;&#38480;&#65292;&#34917;&#20805;&#20102;&#20182;&#20204;&#24037;&#20316;&#20013;&#30340;&#8220;&#24179;&#22343;&#35823;&#24046;&#8221;$\ell_2$&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#65292;&#35889;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65288;&#27169;&#38500;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#22312;&#31934;&#32454;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35889;&#31639;&#27861;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#65288;&#20363;&#22914;&#65292;&#20174;&#25209;&#20934;/&#19981;&#25209;&#20934;&#21453;&#24212;&#25968;&#25454;&#20013;&#35782;&#21035;&#26368;&#20339;&#30340;$K$&#20010;&#39033;&#30446;&#65289;&#65292;&#35299;&#37322;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item response theory (IRT) is the study of how people make probabilistic decisions, with diverse applications in education testing, recommendation systems, among others. The Rasch model of binary response data, one of the most fundamental models in IRT, remains an active area of research with important practical significance. Recently, Nguyen and Zhang (2022) proposed a new spectral estimation algorithm that is efficient and accurate. In this work, we extend their results in two important ways. Firstly, we obtain a refined entrywise error bound for the spectral algorithm, complementing the `average error' $\ell_2$ bound in their work. Notably, under mild sampling conditions, the spectral algorithm achieves the minimax optimal error bound (modulo a log factor). Building on the refined analysis, we also show that the spectral algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g., identifying the best $K$ items from approval/disapproval response data), explaining the empir
&lt;/p&gt;</description></item><item><title>DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04878</link><description>&lt;p&gt;
DeepGD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04878
&lt;/p&gt;
&lt;p&gt;
DeepGD&#26159;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#30446;&#26631;&#40657;&#30418;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36755;&#20837;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#27979;&#35797;DNN&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#29983;&#25104;&#25110;&#25506;&#32034;&#22823;&#35268;&#27169;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;DNN&#27979;&#35797;&#31070;&#35861;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#24037;&#20316;&#26469;&#26631;&#35760;&#27979;&#35797;&#25968;&#25454;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#19987;&#23478;&#20197;&#30830;&#20445;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepGD&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#27169;&#22411;&#30340;&#40657;&#30418;&#22810;&#30446;&#26631;&#27979;&#35797;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#39640;&#38169;&#35823;&#26292;&#38706;&#33021;&#21147;&#30340;&#27979;&#35797;&#36755;&#20837;&#26469;&#38477;&#20302;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36873;&#25321;&#20855;&#26377;&#39640;&#19981;&#30830;&#23450;&#24615;&#20998;&#25968;&#30340;&#27979;&#35797;&#36755;&#20837;&#20197;&#23613;&#21487;&#33021;&#35302;&#21457;&#26356;&#22810;&#30340;&#35823;&#39044;&#27979;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#25581;&#31034;DNN&#27169;&#22411;&#20013;&#19981;&#21516;&#32570;&#38519;&#30340;&#27010;&#29575;&#26469;&#22686;&#21152;&#27979;&#35797;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
&lt;/p&gt;</description></item><item><title>&#21464;&#35843;&#31070;&#32463;ODEs &#65288;MoNODEs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31070;&#32463;ODE&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19981;&#21464;&#30340;&#35843;&#21046;&#21464;&#37327;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#31561;&#26041;&#38754;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.13262</link><description>&lt;p&gt;
&#21464;&#35843;&#31070;&#32463;ODEs
&lt;/p&gt;
&lt;p&gt;
Modulated Neural ODEs. (arXiv:2302.13262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13262
&lt;/p&gt;
&lt;p&gt;
&#21464;&#35843;&#31070;&#32463;ODEs &#65288;MoNODEs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31070;&#32463;ODE&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19981;&#21464;&#30340;&#35843;&#21046;&#21464;&#37327;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#31561;&#26041;&#38754;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23398;&#20064;&#20219;&#24847;&#36712;&#36857;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NODE&#26041;&#27861;&#20165;&#36890;&#36807;&#21021;&#22987;&#29366;&#24577;&#20540;&#25110;&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;&#26356;&#26032;&#26469;&#25429;&#25417;&#36712;&#36857;&#38388;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#35843;&#31070;&#32463;ODEs&#65288;MoNODEs&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#21160;&#21147;&#23398;&#29366;&#24577;&#19982;&#22522;&#30784;&#38745;&#24577;&#21464;&#21270;&#22240;&#32032;&#20998;&#24320;&#24182;&#25913;&#36827;&#29616;&#26377;NODE&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#8220;&#26102;&#38388;&#19981;&#21464;&#35843;&#21046;&#21464;&#37327;&#8221;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#21040;&#22235;&#31181;&#29616;&#26377;&#30340;NODE&#21464;&#20307;&#20013;&#12290;&#25105;&#20204;&#22312;&#25391;&#33633;&#31995;&#32479;&#12289;&#35270;&#39057;&#21644;&#20154;&#31867;&#34892;&#36208;&#36712;&#36857;&#19978;&#23545;MoNODE&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20854;&#20013;&#27599;&#20010;&#36712;&#36857;&#37117;&#20855;&#26377;&#36712;&#36857;&#29305;&#23450;&#30340;&#35843;&#21046;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22987;&#32456;&#25552;&#39640;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#26032;&#30340;&#21160;&#24577;&#21442;&#25968;&#21270;&#24182;&#36827;&#34892;&#36828;&#26399;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#35843;&#21046;&#21464;&#37327;&#30340;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce $\textit{time-invariant modulator variables}$ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are infor
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11975</link><description>&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#31526;&#21495;&#38899;&#20048;&#36890;&#24120;&#19982;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#38899;&#20048;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#21363;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#26631;&#35760;&#12290;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#38899;&#20048;&#21487;&#20197;&#30001;&#21516;&#26102;&#23384;&#22312;&#30340;&#36712;&#36947;&#65292;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#21516;&#26102;&#38899;&#31526;&#32452;&#25104;&#12290;&#30446;&#21069;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#35760;&#21270;&#20381;&#36182;&#20110;&#25551;&#36848;&#38899;&#31526;&#23646;&#24615;&#21644;&#26102;&#38388;&#20107;&#20214;&#30340;&#23567;&#22411;&#26631;&#35760;&#23383;&#20856;&#65292;&#23548;&#33268;&#26631;&#35760;&#24207;&#21015;&#30456;&#24403;&#38271;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#20351;&#29992;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#24182;&#23884;&#20837;&#25110;&#32452;&#21512;&#26631;&#35760;&#26469;&#20943;&#23569;&#25972;&#20307;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20854;&#26174;&#33879;&#20943;&#23567;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#19982;&#26356;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26356;&#32039;&#23494;&#22320;&#30028;&#23450;Transformer&#32534;&#30721;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21516;&#26102;&#26159;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#19968;&#38454;&#36923;&#36753;&#21464;&#20307;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2301.10743</link><description>&lt;p&gt;
&#23545;Transformer&#32534;&#30721;&#22120;&#34920;&#36798;&#33021;&#21147;&#30340;&#26356;&#32039;&#23494;&#30028;&#23450;
&lt;/p&gt;
&lt;p&gt;
Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26356;&#32039;&#23494;&#22320;&#30028;&#23450;Transformer&#32534;&#30721;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21516;&#26102;&#26159;&#19979;&#38480;&#21644;&#19978;&#38480;&#30340;&#19968;&#38454;&#36923;&#36753;&#21464;&#20307;&#30340;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#31995;&#32479;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#26377;&#28508;&#21147;&#25581;&#31034;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#28982;&#32780;&#23545;&#20110;Transformer&#26469;&#35828;&#36825;&#20173;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;Bhattamishra&#31561;&#20154;&#24050;&#32463;&#34920;&#26126;&#65292;Transformer&#32534;&#30721;&#22120;&#33267;&#23569;&#19982;&#19968;&#31181;&#29305;&#23450;&#30340;&#35745;&#25968;&#26426;&#21516;&#31561;&#34920;&#36798;&#33021;&#21147;&#65292;&#32780;Merrill&#21644;Sabharwal&#21017;&#34920;&#26126;&#22266;&#23450;&#31934;&#24230;&#30340;Transformer&#32534;&#30721;&#22120;&#21482;&#33021;&#35782;&#21035;&#32479;&#19968;&#30340;$TC^0$&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#35748;&#20855;&#26377;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#21464;&#20307;&#65292;&#26082;&#26159;&#22266;&#23450;&#31934;&#24230;Transformer&#32534;&#30721;&#22120;&#30340;&#19978;&#30028;&#65292;&#20063;&#26159;Transformer&#32534;&#30721;&#22120;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#23558;&#36825;&#20123;&#32467;&#26524;&#32852;&#31995;&#36215;&#26469;&#24182;&#21152;&#20197;&#21152;&#24378;&#12290;&#36825;&#20351;&#25105;&#20204;&#27604;&#20197;&#21069;&#26356;&#25509;&#36817;&#20934;&#30830;&#21051;&#30011;Transformer&#32534;&#30721;&#22120;&#21487;&#35782;&#21035;&#30340;&#35821;&#35328;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2212.11146</link><description>&lt;p&gt;
HTR&#27169;&#22411;&#35757;&#32451;&#30340;&#25361;&#25112;&#65306;&#12298;&#25968;&#23383;&#26102;&#20195;&#30340;&#26723;&#26696;&#20445;&#25252;&#35745;&#21010;&#12299;&#39033;&#30446;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Transkribus&#24179;&#21488;&#25913;&#36827;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#24615;&#33021;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21253;&#25324;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#30340;&#21512;&#20316;&#24615;&#36136;&#21644;&#25968;&#25454;&#20998;&#20139;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#20889;&#35782;&#21035;&#25216;&#26415;&#30340;&#20986;&#29616;&#20026;&#25991;&#21270;&#36951;&#20135;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#29616;&#22312;&#38656;&#35201;&#21453;&#24605;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#30340;&#32463;&#39564;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#33258;2018&#24180;&#20197;&#26469;&#20351;&#29992;Transkribus&#24179;&#21488;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#65288;HTR&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#36716;&#24405;17&#19990;&#32426;&#30340;&#27861;&#35821;&#25163;&#20889;&#25991;&#26412;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#21019;&#24314;&#36716;&#24405;&#21327;&#35758;&#12289;&#23436;&#25972;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#30830;&#23450;&#26368;&#20339;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;HTR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23558;&#25152;&#26377;&#36825;&#20123;&#20803;&#32032;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#23558;&#21333;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#39640;20%&#20197;&#19978;&#65288;&#36798;&#21040;&#23383;&#31526;&#38169;&#35823;&#29575;&#20302;&#20110;5%&#65289;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;HTR&#24179;&#21488;&#65288;&#22914;Transkribus&#65289;&#30340;&#21512;&#20316;&#24615;&#36136;&#20197;&#21450;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20998;&#20139;&#20854;&#25968;&#25454;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#23545;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#25361;&#25112;&#30340;&#24555;&#36895;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#21644;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.05402</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#28789;&#27963;&#32465;&#23450;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#38543;&#26426;&#19968;&#38454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model. (arXiv:2212.05402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#23545;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#25361;&#25112;&#30340;&#24555;&#36895;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#21644;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMM)&#26159;&#19968;&#31181;&#22522;&#20110;&#26680;&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#12290;&#38543;&#30528;&#25968;&#25454;&#28304;&#30340;&#21095;&#22686;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#65292;&#22312;&#38754;&#23545;&#39640;&#32500;&#21644;&#27969;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#30340;&#23494;&#24230;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#39640;&#26031;&#32452;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22312;&#32447;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#38454;&#38543;&#26426;&#20248;&#21270;&#26469;&#22788;&#29702;GMM&#25152;&#38754;&#20020;&#30340;&#39640;&#32500;&#27969;&#25968;&#25454;&#21644;&#22797;&#26434;&#23494;&#24230;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28789;&#27963;&#32465;&#23450;&#22240;&#23376;&#20998;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#65292;&#20445;&#25345;&#27491;&#20132;&#24615;&#65292;&#24182;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25968;&#20540;&#20248;&#21270;&#19968;&#36215;&#20351;&#29992;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Models (GMM) are one of the most potent parametric density estimators based on the kernel model that finds application in many scientific domains. In recent years, with the dramatic enlargement of data sources, typical machine learning algorithms, e.g. Expectation Maximization (EM), encounters difficulty with high-dimensional and streaming data. Moreover, complicated densities often demand a large number of Gaussian components. This paper proposes a fast online parameter estimation algorithm for GMM by using first-order stochastic optimization. This approach provides a framework to cope with the challenges of GMM when faced with high-dimensional streaming data and complex densities by leveraging the flexibly-tied factorization of the covariance matrix. A new stochastic Manifold optimization algorithm that preserves the orthogonality is introduced and used along with the well-known Euclidean space numerical optimization. Numerous empirical results on both synthetic and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;</title><link>http://arxiv.org/abs/2212.01382</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#32500;&#24230;&#30340;&#21521;&#37327;&#20540;&#22870;&#21169;&#19978;&#21516;&#26102;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#21463;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#26399;&#26395;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#38024;&#23545;&#21521;&#37327;&#30340;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38750;&#32447;&#24615;&#20844;&#24179;&#31119;&#21033;&#20989;&#25968;&#12290;&#20854;&#20013;&#19968;&#20010;&#32463;&#20856;&#30340;&#20363;&#23376;&#26159;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65292;&#25110;&#32773;&#20960;&#20309;&#24179;&#22343;&#25968;&#65292;&#20854;&#23545;&#25968;&#21464;&#25442;&#20063;&#34987;&#31216;&#20026;&#27604;&#20363;&#20844;&#24179;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26399;&#26395;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#36827;&#34892;&#36817;&#20284;&#26368;&#20248;&#21270;&#20063;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Q-learning&#25913;&#36827;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20248;&#21270;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21487;&#25910;&#25947;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#21442;&#25968;&#21270;&#19981;&#21487;&#30693;&#19988;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21644;&#20998;&#24067;&#36716;&#25442;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.01258</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#27169;&#22411;&#21442;&#25968;&#21270;&#19981;&#21487;&#30693;&#19988;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21644;&#20998;&#24067;&#36716;&#25442;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#38480;&#26080;&#27861;&#35299;&#37322;&#24433;&#21709;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30001;&#20110;&#36825;&#20123;&#30028;&#38480;&#36890;&#24120;&#23545;&#25152;&#26377;&#21442;&#25968;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#23427;&#20204;&#23481;&#26131;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#26080;&#27861;&#32771;&#34385;&#21040;&#21021;&#22987;&#21270;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#20256;&#36755;&#35299;&#37322;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#20381;&#36182;&#20110;&#25968;&#25454;&#31354;&#38388;&#20013;&#39044;&#27979;&#20989;&#25968;&#30340;&#23616;&#37096;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#24615;&#30340;&#23454;&#20363;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23545;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36828;&#23567;&#20110;&#21442;&#25968;&#25968;&#37327;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#19968;&#20123;&#23567;&#30340;&#20462;&#25913;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#19978;&#21487;&#20197;&#33719;&#24471;&#21152;&#36895;&#30340;&#36895;&#29575;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#36716;&#25442;&#19979;&#20855;&#26377;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#20998;&#26512;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#32467;&#26524;&#26174;&#31034;&#30028;&#38480;&#20540;&#26159;
&lt;/p&gt;
&lt;p&gt;
Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14750</link><description>&lt;p&gt;
&#38024;&#23545;&#20117;&#27979;&#25968;&#25454;&#30340;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#34892;&#19994;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#38075;&#20117;&#25968;&#25454;&#20026;&#20117;&#27573;&#25552;&#20379;&#34920;&#31034;&#24418;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#20027;&#35201;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#20851;&#27880;&#20110;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#21363;&#20272;&#35745;&#20117;&#27573;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#24076;&#26395;&#22312;&#19981;&#20351;&#29992;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#19982;&#26377;&#30417;&#30563;&#33539;&#24335;&#30456;&#21453;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#25968;&#25454;&#38656;&#35201;&#24456;&#23569;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#29616;&#20170;&#65292;&#22823;&#22810;&#25968;SSL&#26041;&#27861;&#35201;&#20040;&#26159;&#23545;&#27604;&#30340;&#65292;&#35201;&#20040;&#26159;&#38750;&#23545;&#27604;&#30340;&#12290;&#23545;&#27604;&#26041;&#27861;&#20351;&#30456;&#20284;&#30340;&#65288;&#27491;&#65289;&#23545;&#35937;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#25509;&#36817;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#65288;&#36127;&#65289;&#23545;&#35937;&#19982;&#20043;&#36317;&#31163;&#12290;&#30001;&#20110;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#27491;&#36127;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#38750;&#23545;&#27604;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#27492;&#31867;&#26631;&#27880;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#23481;&#26131;&#35782;&#21035;&#30340;&#30456;&#20284;&#23545;&#35937;&#23545;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning problem in the oil &amp; gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#20110;&#29616;&#26377;&#30340;&#20998;&#21106;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#32780;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2206.13086</link><description>&lt;p&gt;
RankSEG:&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#25490;&#24207;&#30340;&#20998;&#21106;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RankSEG: A Consistent Ranking-based Framework for Segmentation. (arXiv:2206.13086v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#20110;&#29616;&#26377;&#30340;&#20998;&#21106;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#32780;&#21487;&#33021;&#23548;&#33268;&#30340;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#26412;&#39046;&#22495;&#65292;&#23427;&#23558;&#26631;&#31614;&#20998;&#37197;&#32473;&#27599;&#20010;&#20687;&#32032;/&#29305;&#24449;&#65292;&#20197;&#20174;&#22270;&#20687;/&#25991;&#26412;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#20026;&#20102;&#35780;&#20272;&#20998;&#21106;&#24615;&#33021;&#65292;&#20351;&#29992;Dice&#21644;IoU&#25351;&#26631;&#26469;&#34913;&#37327;&#23454;&#38469;&#20540;&#21644;&#39044;&#27979;&#20998;&#21106;&#20043;&#38388;&#30340;&#37325;&#21472;&#31243;&#24230;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19982;Dice/IoU&#25351;&#26631;&#30456;&#20851;&#30340;&#20998;&#21106;&#29702;&#35770;&#22522;&#30784;&#65292;&#21253;&#25324;&#31867;&#27604;&#20110;&#20998;&#31867;&#30340;&#36125;&#21494;&#26031;&#35268;&#21017;&#21644;Dice-/IoU-&#26657;&#20934;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#26694;&#26550;&#23545;&#20110;Dice/IoU&#25351;&#26631;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25490;&#24207;&#26694;&#26550;&#65292;&#21363;RankDice/RankIoU&#65292;&#21463;&#36125;&#21494;&#26031;&#20998;&#21106;&#35268;&#21017;&#30340;&#25554;&#20837;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19977;&#20010;&#20351;&#29992;GPU&#24182;&#34892;&#25191;&#34892;&#30340;&#25968;&#23383;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel exe
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item></channel></rss>