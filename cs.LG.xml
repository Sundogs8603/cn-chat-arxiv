<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#26041;&#31243;&#24314;&#27169;&#65292;&#24182;&#24212;&#29992;&#20110;&#32467;&#26500;&#30340;&#24367;&#26354;&#21018;&#24230;&#22238;&#24402;&#12289;&#21709;&#24212;&#25554;&#20540;&#21644;&#27010;&#29575;&#25512;&#26029;&#12290;&#35813;&#27169;&#22411;&#22312;&#24748;&#33218;&#26753;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#29992;&#20110;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02894</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#29992;&#20110;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#20803;&#32032;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Gaussian process model for Euler-Bernoulli beam elements. (arXiv:2308.02894v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#26041;&#31243;&#24314;&#27169;&#65292;&#24182;&#24212;&#29992;&#20110;&#32467;&#26500;&#30340;&#24367;&#26354;&#21018;&#24230;&#22238;&#24402;&#12289;&#21709;&#24212;&#25554;&#20540;&#21644;&#27010;&#29575;&#25512;&#26029;&#12290;&#35813;&#27169;&#22411;&#22312;&#24748;&#33218;&#26753;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#29992;&#20110;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#27431;&#25289;-&#20271;&#21162;&#21033;&#26753;&#26041;&#31243;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#20197;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#24418;&#24335;&#30340;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#19979;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22238;&#24402;&#32467;&#26500;&#30340;&#24367;&#26354;&#21018;&#24230;&#30340;&#35299;&#26512;&#20540;&#12289;&#25554;&#20540;&#21709;&#24212;&#20197;&#21450;&#23545;&#28508;&#22312;&#29289;&#29702;&#37327;&#36827;&#34892;&#27010;&#29575;&#25512;&#26029;&#12290;&#35813;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#30340;&#24748;&#33218;&#26753;&#19978;&#65292;&#35780;&#20272;&#20102;&#22238;&#24402;&#30340;&#24367;&#26354;&#21018;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#39044;&#27979;&#36136;&#37327;&#21463;&#27979;&#37327;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22238;&#24402;&#24471;&#21040;&#30340;&#27010;&#29575;&#21018;&#24230;&#20998;&#24067;&#65292;&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#32972;&#26223;&#19979;&#65292;&#37319;&#29992;&#39532;&#27663;&#36317;&#31163;&#26469;&#25512;&#26029;&#32467;&#26500;&#31995;&#32479;&#20013;&#21487;&#33021;&#30340;&#25439;&#20260;&#20301;&#32622;&#21644;&#33539;&#22260;&#12290;&#20026;&#20102;&#39564;&#35777;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#65292;&#20351;&#29992;&#27979;&#37327;&#30340;&#24322;&#36136;&#25968;&#25454;&#38598;&#26469;&#26356;&#26032;&#20551;&#23450;&#30340;&#35299;&#26512;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A physics-informed machine learning model, in the form of a multi-output Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given appropriate datasets, the model can be used to regress the analytical value of the structure's bending stiffness, interpolate responses, and make probabilistic inferences on latent physical quantities. The developed model is applied on a numerically simulated cantilever beam, where the regressed bending stiffness is evaluated and the influence measurement noise on the prediction quality is investigated. Further, the regressed probabilistic stiffness distribution is used in a structural health monitoring context, where the Mahalanobis distance is employed to reason about the possible location and extent of damage in the structural system. To validate the developed framework, an experiment is conducted and measured heterogeneous datasets are used to update the assumed analytical structural model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23545;&#22810;&#20010;&#31363;&#21548;&#32773;&#30340;&#23433;&#20840;&#36890;&#20449;&#30340;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;Deep-JSCC&#65289;&#26041;&#27861;&#30340;&#25512;&#24191;&#12290;&#36890;&#36807;&#25512;&#24191;&#38544;&#31169;&#28431;&#26007;&#21644;&#31363;&#21548;&#20449;&#36947;&#32534;&#30721;&#30340;&#24605;&#24819;&#65292;&#21051;&#30011;&#20102;&#21512;&#27861;&#33410;&#28857;&#22270;&#20687;&#24674;&#22797;&#19982;&#20449;&#24687;&#27844;&#28431;&#32473;&#31363;&#21548;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.02892</link><description>&lt;p&gt;
&#22810;&#20010;&#31363;&#21548;&#32773;&#19979;&#30340;&#23433;&#20840;&#28145;&#24230; JSCC
&lt;/p&gt;
&lt;p&gt;
Secure Deep-JSCC Against Multiple Eavesdroppers. (arXiv:2308.02892v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23545;&#22810;&#20010;&#31363;&#21548;&#32773;&#30340;&#23433;&#20840;&#36890;&#20449;&#30340;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;Deep-JSCC&#65289;&#26041;&#27861;&#30340;&#25512;&#24191;&#12290;&#36890;&#36807;&#25512;&#24191;&#38544;&#31169;&#28431;&#26007;&#21644;&#31363;&#21548;&#20449;&#36947;&#32534;&#30721;&#30340;&#24605;&#24819;&#65292;&#21051;&#30011;&#20102;&#21512;&#27861;&#33410;&#28857;&#22270;&#20687;&#24674;&#22797;&#19982;&#20449;&#24687;&#27844;&#28431;&#32473;&#31363;&#21548;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23545;&#22810;&#20010;&#31363;&#21548;&#32773;&#30340;&#23433;&#20840;&#36890;&#20449;&#30340;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#65288;Deep-JSCC&#65289;&#26041;&#27861;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#20540;&#34928;&#33853;&#20449;&#36947;&#19978;&#25269;&#24481;&#22810;&#20010;&#31363;&#21548;&#32773;&#30340;&#23433;&#20840;&#36890;&#20449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31363;&#21548;&#32773;&#21512;&#35851;&#21644;&#38750;&#21512;&#35851;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#21512;&#35851;&#31574;&#30053;&#65292;&#31363;&#21548;&#32773;&#20849;&#20139;&#20854;&#36923;&#36753;&#20197;&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#20849;&#21516;&#25512;&#27979;&#31169;&#26377;&#23646;&#24615;&#65292;&#32780;&#23545;&#20110;&#38750;&#21512;&#35851;&#35774;&#32622;&#65292;&#20182;&#20204;&#29420;&#33258;&#34892;&#21160;&#12290;&#30446;&#26631;&#26159;&#38450;&#27490;&#31363;&#21548;&#32773;&#25512;&#27979;&#26377;&#20851;&#20256;&#36755;&#22270;&#20687;&#30340;&#31169;&#20154;&#65288;&#25935;&#24863;&#65289;&#20449;&#24687;&#65292;&#21516;&#26102;&#20197;&#26368;&#23567;&#22833;&#30495;&#23558;&#22270;&#20687;&#20256;&#36882;&#32473;&#21512;&#27861;&#25509;&#25910;&#32773;&#12290;&#36890;&#36807;&#25512;&#24191;&#38544;&#31169;&#28431;&#26007;&#21644;&#31363;&#21548;&#20449;&#36947;&#32534;&#30721;&#30340;&#24605;&#24819;&#65292;&#21051;&#30011;&#20102;&#21512;&#27861;&#33410;&#28857;&#22270;&#20687;&#24674;&#22797;&#19982;&#20449;&#24687;&#27844;&#28431;&#32473;&#31363;&#21548;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20445;&#23494;&#28431;&#26007;&#26694;&#26550;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In this paper, a generalization of deep learning-aided joint source channel coding (Deep-JSCC) approach to secure communications is studied. We propose an end-to-end (E2E) learning-based approach for secure communication against multiple eavesdroppers over complex-valued fading channels. Both scenarios of colluding and non-colluding eavesdroppers are studied. For the colluding strategy, eavesdroppers share their logits to collaboratively infer private attributes based on ensemble learning method, while for the non-colluding setup they act alone. The goal is to prevent eavesdroppers from inferring private (sensitive) information about the transmitted images, while delivering the images to a legitimate receiver with minimum distortion. By generalizing the ideas of privacy funnel and wiretap channel coding, the trade-off between the image recovery at the legitimate node and the information leakage to the eavesdroppers is characterized. To solve this secrecy funnel framework, we implement 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#21151;&#29575;&#25511;&#21046;&#30340;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#27861;&#23454;&#29616;&#20102;&#31169;&#23494;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#20445;&#25252;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2308.02881</link><description>&lt;p&gt;
&#31169;&#23494;&#32852;&#37030;&#23398;&#20064;&#30340;&#21160;&#24577;&#21151;&#29575;&#25511;&#21046;&#19982;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation. (arXiv:2308.02881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02881
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#21151;&#29575;&#25511;&#21046;&#30340;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#27861;&#23454;&#29616;&#20102;&#31169;&#23494;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#20445;&#25252;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#21442;&#25968;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#21151;&#29575;&#25511;&#21046;&#30340;&#38750;&#30456;&#24178;&#31354;&#20013;&#35745;&#31639;&#26041;&#26696;&#12290;&#36793;&#32536;&#35774;&#22791;&#36890;&#36807;&#28608;&#27963;&#20004;&#20010;&#30456;&#37051;&#30340;&#27491;&#20132;&#39057;&#20998;&#22810;&#36335;&#22797;&#29992;&#65288;OFDM&#65289;&#23376;&#36733;&#27874;&#26469;&#20256;&#36755;&#26412;&#22320;&#38543;&#26426;&#26799;&#24230;&#30340;&#31526;&#21495;&#65292;&#24182;&#21033;&#29992;&#23376;&#36733;&#27874;&#19978;&#30340;&#33021;&#37327;&#32047;&#31215;&#33719;&#24471;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#22810;&#25968;&#25237;&#31080;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21151;&#29575;&#25511;&#21046;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25269;&#28040;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;&#20540;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25972;&#20010;&#26041;&#26696;&#21487;&#20197;&#20943;&#36731;&#26102;&#38388;&#21516;&#27493;&#35823;&#24046;&#12289;&#20449;&#36947;&#34928;&#33853;&#21644;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#26696;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#35777;&#26126;&#20063;&#34987;&#37325;&#26032;&#25512;&#23548;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
To further preserve model weight privacy and improve model performance in Federated Learning (FL), FL via Over-the-Air Computation (AirComp) scheme based on dynamic power control is proposed. The edge devices (EDs) transmit the signs of local stochastic gradients by activating two adjacent orthogonal frequency division multi-plexing (OFDM) subcarriers, and majority votes (MVs) at the edge server (ES) are obtained by exploiting the energy accumulation on the subcarriers. Then, we propose a dynamic power control algorithm to further offset the biased aggregation of the MV aggregation values. We show that the whole scheme can mitigate the impact of the time synchronization error, channel fading and noise. The theoretical convergence proof of the scheme is re-derived.
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02877</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#20803;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning in healthcare: A survey. (arXiv:2308.02877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02877
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#36866;&#24403;&#22320;&#35299;&#20915;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#25968;&#37327;&#19981;&#36275;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#29305;&#28857;&#20351;&#20803;&#23398;&#20064;&#25104;&#20026;&#22312;&#21508;&#31181;&#21307;&#30103;&#29615;&#22659;&#20013;&#24320;&#21457;&#26377;&#24433;&#21709;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#36873;&#25321;&#65292;&#36825;&#20123;&#29615;&#22659;&#20013;&#21487;&#29992;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#65292;&#24182;&#19988;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20063;&#19981;&#21516;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20102;&#35299;&#23427;&#22914;&#20309;&#20197;&#21450;&#22312;&#21738;&#20123;&#26041;&#38754;&#21487;&#20197;&#35299;&#20915;&#20851;&#38190;&#30340;&#21307;&#30103;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20998;&#20026;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20004;&#22823;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#35774;&#35745;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#65292;&#26088;&#22312;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#36890;&#36807;&#35774;&#35745;&#30495;&#23454;&#30340;&#30707;&#21270;&#28860;&#21378;&#21333;&#20803;&#25512;&#29702;&#20256;&#24863;&#22120;&#36827;&#34892;&#28436;&#31034;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02872</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Data-Based Design of Multi-Model Inferential Sensors. (arXiv:2308.02872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#35774;&#35745;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#65292;&#26088;&#22312;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#36890;&#36807;&#35774;&#35745;&#30495;&#23454;&#30340;&#30707;&#21270;&#28860;&#21378;&#21333;&#20803;&#25512;&#29702;&#20256;&#24863;&#22120;&#36827;&#34892;&#28436;&#31034;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25512;&#29702;&#65288;&#36719;&#65289;&#20256;&#24863;&#22120;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#24037;&#19994;&#36807;&#31243;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#36890;&#24120;&#26159;&#35774;&#35745;&#31616;&#21333;&#32447;&#24615;&#25512;&#29702;&#20256;&#24863;&#22120;&#26102;&#30340;&#20027;&#35201;&#38480;&#21046;&#65292;&#30001;&#20110;&#38656;&#35201;&#20445;&#25345;&#32447;&#24615;&#32467;&#26500;&#65292;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#20256;&#24863;&#22120;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#26159;&#19968;&#20010;&#30452;&#25509;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#35774;&#35745;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#20026;&#20102;&#28436;&#31034;&#25152;&#24320;&#21457;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#30707;&#21270;&#28860;&#21378;&#21333;&#20803;&#8212;&#8212;&#30495;&#31354;&#21152;&#27668;&#20307;&#27833;&#21152;&#27682;&#35013;&#32622;&#30340;&#25512;&#29702;&#20256;&#24863;&#22120;&#12290;&#23558;&#22810;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#30340;&#24615;&#33021;&#19982;&#21508;&#31181;&#21333;&#27169;&#22411;&#25512;&#29702;&#20256;&#24863;&#22120;&#20197;&#21450;&#28860;&#21378;&#20013;&#24403;&#21069;&#65288;&#21442;&#32771;&#65289;&#20256;&#24863;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29616;&#26377;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with the problem of inferential (soft) sensor design. The nonlinear character of industrial processes is usually the main limitation to designing simple linear inferential sensors with sufficient accuracy. In order to increase the inferential sensor predictive performance and yet to maintain its linear structure, multi-model inferential sensors represent a straightforward option. In this contribution, we propose two novel approaches for the design of multi-model inferential sensors aiming to mitigate some drawbacks of the state-of-the-art approaches. For a demonstration of the developed techniques, we design inferential sensors for a Vacuum Gasoil Hydrogenation unit, which is a real-world petrochemical refinery unit. The performance of the multi-model inferential sensor is compared against various single-model inferential sensors and the current (referential) inferential sensor used in the refinery. The results show substantial improvements over the state-of-the-art de
&lt;/p&gt;</description></item><item><title>&#23558;&#31070;&#32463;&#36807;&#31243;&#65288;NPs&#65289;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23454;&#29616;&#20102;&#27169;&#22411;&#23545;&#20110;&#26410;&#30693;&#20869;&#23481;&#30340;&#35748;&#30693;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02866</link><description>&lt;p&gt;
NP-SemiSeg: &#24403;&#31070;&#32463;&#36807;&#31243;&#36935;&#35265;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation. (arXiv:2308.02866v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02866
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#36807;&#31243;&#65288;NPs&#65289;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23454;&#29616;&#20102;&#27169;&#22411;&#23545;&#20110;&#26410;&#30693;&#20869;&#23481;&#30340;&#35748;&#30693;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#32467;&#26524;&#30340;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#28041;&#21450;&#22312;&#35757;&#32451;&#26102;&#20026;&#26080;&#26631;&#31614;&#22270;&#20687;&#20998;&#37197;&#36880;&#20687;&#32032;&#26631;&#31614;&#12290;&#22312;&#26080;&#27861;&#21450;&#26102;&#25110;&#25104;&#26412;&#39640;&#26114;&#22320;&#25910;&#38598;&#36880;&#20687;&#32032;&#26631;&#31614;&#30340;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#24403;&#21069;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#36890;&#36807;&#20174;&#27169;&#22411;&#36755;&#20986;&#30340;&#31867;&#21035;&#27010;&#29575;&#20998;&#24067;&#20013;&#39044;&#27979;&#27599;&#20010;&#20687;&#32032;&#30340;&#20266;&#26631;&#31614;&#26469;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#20250;&#23548;&#33268;&#20998;&#21106;&#32467;&#26524;&#19981;&#20339;&#65292;&#23545;&#20110;&#21307;&#30103;&#22270;&#20687;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31561;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#26469;&#35828;&#65292;&#21487;&#33021;&#20135;&#29983;&#36830;&#38145;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#27169;&#22411;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;&#26159;&#24456;&#37325;&#35201;&#30340;&#65292;&#36825;&#20027;&#35201;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#36807;&#31243;&#65288;NPs&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;NPs&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If the predicted probability distribution is incorrect, however, this leads to poor segmentation results, which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#26579;&#33394;&#26631;&#20934;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.02851</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Stain Normalisation in Histopathology. (arXiv:2308.02851v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#22312;&#26579;&#33394;&#26631;&#20934;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#26356;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#25913;&#21892;&#20020;&#24202;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#39640;&#27700;&#24179;&#30340;&#35270;&#35273;&#21464;&#24322;&#24615;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#26579;&#33394;&#26631;&#20934;&#21270;&#26088;&#22312;&#22312;&#19981;&#25913;&#21464;&#22270;&#20687;&#32467;&#26500;&#20869;&#23481;&#30340;&#24773;&#20917;&#19979;&#26631;&#20934;&#21270;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#26579;&#33394;&#26631;&#20934;&#21270;&#30340;&#19981;&#21516;&#25216;&#26415;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#20248;&#20110;&#38750;&#29983;&#25104;&#26041;&#27861;&#65292;&#20294;&#20195;&#20215;&#26159;&#26356;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#23545;&#26579;&#33394;&#26631;&#20934;&#21270;&#26356;&#22909;&#65292;&#19981;&#21516;&#30340;GAN&#21644;&#38750;GAN&#26041;&#27861;&#30456;&#20114;&#20043;&#38388;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#35777;&#26126;&#20102;&#21333;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;&#26080;&#27861;&#24674;&#22797;&#31232;&#30095;&#21521;&#37327;&#65292;&#20294;&#36890;&#36807;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#31283;&#23450;&#19988;&#31934;&#30830;&#22320;&#24674;&#22797;&#20219;&#24847;&#31232;&#30095;&#31243;&#24230;&#30340;&#21521;&#37327;&#65292;&#27492;&#22806;&#36824;&#25512;&#24191;&#21040;&#20102;&#20854;&#20182;&#24674;&#22797;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02836</link><description>&lt;p&gt;
&#20351;&#29992;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#27491;&#40784;&#27425;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks. (arXiv:2308.02836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#23610;&#24230;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#35777;&#26126;&#20102;&#21333;&#38544;&#34255;&#23618;ReLU&#32593;&#32476;&#26080;&#27861;&#24674;&#22797;&#31232;&#30095;&#21521;&#37327;&#65292;&#20294;&#36890;&#36807;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#31283;&#23450;&#19988;&#31934;&#30830;&#22320;&#24674;&#22797;&#20219;&#24847;&#31232;&#30095;&#31243;&#24230;&#30340;&#21521;&#37327;&#65292;&#27492;&#22806;&#36824;&#25512;&#24191;&#21040;&#20102;&#20854;&#20182;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;ReLU&#32593;&#32476;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#30001;&#20110;&#32447;&#24615;&#20851;&#31995;&#24102;&#26469;&#30340;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#23545;&#20110;&#36825;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#37325;&#26500;&#20989;&#25968;f&#26159;&#27491;&#40784;&#27425;&#20989;&#25968;&#65292;&#21363;&#28385;&#36275;&#23545;&#20110;&#25152;&#26377;&#38750;&#36127;&#30340;&#955;&#65292;&#26377;f(&#955;x) = &#955;f(x)&#12290;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#36825;&#20010;&#26465;&#20214;&#36716;&#21270;&#20026;&#22312;&#32593;&#32476;&#20013;&#19981;&#32771;&#34385;&#20559;&#32622;&#39033;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20174;&#23569;&#37327;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#31232;&#30095;&#21521;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#21333;&#38544;&#34255;&#23618;&#30340;ReLU&#32593;&#32476;&#26080;&#27861;&#24674;&#22797;1-&#31232;&#30095;&#21521;&#37327;&#65292;&#21363;&#20351;&#26159;&#36817;&#20284;&#24674;&#22797;&#65292;&#32780;&#19988;&#19981;&#35770;&#32593;&#32476;&#30340;&#23485;&#24230;&#22914;&#20309;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20004;&#20010;&#38544;&#34255;&#23618;&#65292;&#21487;&#20197;&#20197;&#31283;&#23450;&#30340;&#26041;&#24335;&#36817;&#20284;&#22320;&#24674;&#22797;&#20219;&#24847;&#31934;&#24230;&#30340;&#12289;&#20219;&#24847;&#31232;&#30095;&#31243;&#24230;&#20026;s&#30340;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#21253;&#25324;&#20302;&#31209;&#30697;&#38453;&#24674;&#22797;&#21644;&#30456;&#20301;&#24674;&#22797;&#22312;&#20869;&#30340;&#26356;&#24191;&#27867;&#30340;&#24674;&#22797;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#23545;&#19968;&#33324;&#27491;&#40784;&#27425;&#20989;&#25968;&#30340;&#36924;&#36817;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative $\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20855;&#26377;&#21160;&#24577;&#24615;&#30340;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#27169;&#22411;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20123;&#23616;&#38480;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#33021;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#35813;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38480;&#21046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02820</link><description>&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Financial Index Tracking. (arXiv:2308.02820v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20855;&#26377;&#21160;&#24577;&#24615;&#30340;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#27169;&#22411;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20123;&#23616;&#38480;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#33021;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#35813;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38480;&#21046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#21160;&#24577;&#24418;&#24335;&#30340;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22522;&#20110;&#25910;&#30410;&#30340;&#36319;&#36394;&#35823;&#24046;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#35813;&#27169;&#22411;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#19981;&#20165;&#38480;&#20110;&#20215;&#26684;&#30340;&#24066;&#22330;&#20449;&#24687;&#21464;&#37327;&#30340;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#32771;&#34385;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#31561;&#12290;&#35813;&#27169;&#22411;&#36824;&#24341;&#20837;&#20102;&#29616;&#37329;&#27880;&#20837;&#25110;&#25552;&#21462;&#30340;&#26032;&#30340;&#20915;&#31574;&#21464;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Banach&#19981;&#21160;&#28857;&#36845;&#20195;&#27714;&#35299;&#25237;&#36164;&#32452;&#21512;&#20877;&#24179;&#34913;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#23454;&#36341;&#20013;&#25351;&#23450;&#20026;&#20132;&#26131;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20132;&#26131;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#25968;&#25454;&#38480;&#21046;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#30340;Chimney&#28779;&#28798;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#36830;&#36143;&#19988;&#32467;&#26500;&#33391;&#22909;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02810</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#39044;&#27979;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A generative model for surrogates of spatial-temporal wildfire nowcasting. (arXiv:2308.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#30340;Chimney&#28779;&#28798;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#36830;&#36143;&#19988;&#32467;&#26500;&#33391;&#22909;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20840;&#29699;&#37326;&#28779;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#23545;&#23454;&#26102;&#28779;&#21183;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#22914;&#20803;&#32990;&#33258;&#21160;&#26426;&#21644;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#28779;&#21183;&#20256;&#25773;&#27169;&#25311;&#65292;&#20294;&#32791;&#26102;&#19988;&#35745;&#31639;&#22797;&#26434;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28779;&#21183;&#39044;&#27979;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#29305;&#23450;&#20110;&#26576;&#20010;&#22320;&#21306;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#19981;&#21516;&#29983;&#24577;&#21306;&#22495;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#26368;&#36817;&#30340;&#19968;&#27425;&#22823;&#35268;&#27169;&#37326;&#28779;&#20107;&#20214; - Chimney&#28779;&#28798;&#30340;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#29983;&#25104;&#20102;&#36830;&#36143;&#19988;&#26377;&#32467;&#26500;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, ta
&lt;/p&gt;</description></item><item><title>MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02804</link><description>&lt;p&gt;
MiAMix: &#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02804
&lt;/p&gt;
&lt;p&gt;
MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#36807;&#25311;&#21512;&#20381;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#21313;&#20998;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#31574;&#30053;&#65292;&#20294;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#22312;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;MiAMix&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#12290;MiAMix&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#28151;&#21512;&#25513;&#27169;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26174;&#33879;&#24615;&#20449;&#24687;&#65292;&#32780;MiAMix&#30340;&#35774;&#35745;&#20063;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23545;MiAMix&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#22270;&#20687;&#22522;&#20934;&#21644;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
&lt;/p&gt;</description></item><item><title>OBESEYE&#26159;&#19968;&#27454;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;AI&#30340;&#21487;&#29702;&#35299;&#30340;&#32933;&#32982;&#31649;&#29702;&#39278;&#39135;&#25512;&#33616;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20010;&#20307;&#25152;&#38656;&#30340;&#33829;&#20859;&#29289;&#36136;&#37327;&#65292;&#23545;&#32933;&#32982;&#31649;&#29702;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.02796</link><description>&lt;p&gt;
OBESEYE: &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#30340;AI&#30340;&#21487;&#29702;&#35299;&#30340;&#32933;&#32982;&#31649;&#29702;&#39278;&#39135;&#25512;&#33616;&#22120;
&lt;/p&gt;
&lt;p&gt;
OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI. (arXiv:2308.02796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02796
&lt;/p&gt;
&lt;p&gt;
OBESEYE&#26159;&#19968;&#27454;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;AI&#30340;&#21487;&#29702;&#35299;&#30340;&#32933;&#32982;&#31649;&#29702;&#39278;&#39135;&#25512;&#33616;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20010;&#20307;&#25152;&#38656;&#30340;&#33829;&#20859;&#29289;&#36136;&#37327;&#65292;&#23545;&#32933;&#32982;&#31649;&#29702;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32933;&#32982;&#26159;&#35768;&#22810;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25668;&#20837;&#36229;&#36807;&#36523;&#20307;&#38656;&#27714;&#21644;&#32570;&#20047;&#36866;&#24403;&#30340;&#27963;&#21160;&#12290;&#22240;&#27492;&#65292;&#20445;&#25345;&#20581;&#24247;&#38656;&#35201;&#20581;&#24247;&#30340;&#39278;&#39135;&#35745;&#21010;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24739;&#26377;&#20849;&#30149;&#30340;&#24739;&#32773;&#12290;&#20294;&#26159;&#65292;&#35201;&#30830;&#23450;&#27599;&#31181;&#33829;&#20859;&#29289;&#36136;&#30340;&#30830;&#20999;&#25968;&#37327;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#33829;&#20859;&#29289;&#36136;&#30340;&#38656;&#27714;&#26681;&#25454;&#36523;&#20307;&#21644;&#30142;&#30149;&#30340;&#24773;&#20917;&#32780;&#21464;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#19968;&#20010;&#20154;&#20445;&#25345;&#20581;&#24247;&#25152;&#38656;&#30340;&#33829;&#20859;&#29289;&#36136;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65306;&#32447;&#24615;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;LightGBM&#22312;&#27969;&#20307;&#21644;3&#31181;&#20854;&#20182;&#20027;&#35201;&#24494;&#37327;&#33829;&#20859;&#32032;&#65288;&#30899;&#27700;&#21270;&#21512;&#29289;&#12289;&#34507;&#30333;&#36136;&#12289;&#33026;&#32938;&#65289;&#30340;&#28040;&#32791;&#39044;&#27979;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#22312;&#27969;&#20307;&#39044;&#27979;&#20013;&#33719;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#22312;&#30899;&#27700;&#21270;&#21512;&#29289;&#39044;&#27979;&#20013;&#65292;&#20351;&#29992;LightGBM&#22312;&#34507;&#30333;&#36136;&#21644;&#33026;&#32938;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#21644;&#20302;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#39278;&#39135;&#25512;&#33616;&#22120;&#20250;&#23545;&#32933;&#32982;&#31649;&#29702;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obesity, the leading cause of many non-communicable diseases, occurs mainly for eating more than our body requirements and lack of proper activity. So, being healthy requires heathy diet plans, especially for patients with comorbidities. But it is difficult to figure out the exact quantity of each nutrient because nutrients requirement varies based on physical and disease conditions. In our study we proposed a novel machine learning based system to predict the amount of nutrients one individual requires for being healthy. We applied different machine learning algorithms: linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid and 3 other major micronutrients: carbohydrate, protein, fat consumption prediction. We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction. We believe our diet recommender s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#23547;&#25214;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#26410;&#35265;&#38754;&#37096;&#22270;&#20687;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.02784</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Contrastive Regression for Estimation of Eye Gaze. (arXiv:2308.02784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#23547;&#25214;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#26410;&#35265;&#38754;&#37096;&#22270;&#20687;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#23545;&#20154;&#26426;&#30028;&#38754;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#24320;&#21457;&#30524;&#29699;&#25511;&#21046;&#31995;&#32479;&#24050;&#25104;&#20026;&#24517;&#35201;&#12290;&#30524;&#29699;&#27880;&#35270;&#20316;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#65292;&#26159;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22522;&#20110;&#22806;&#35266;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#23436;&#20840;&#21463;&#21040;&#26631;&#35760;&#30340;&#30524;&#29699;&#27880;&#35270;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#20250;&#24433;&#21709;&#24615;&#33021;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#26041;&#21521;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#26631;&#35760;&#30340;&#30524;&#29699;&#27880;&#35270;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#26159;&#30475;&#19981;&#35265;&#30340;&#20154;&#33080;&#22270;&#20687;&#20063;&#33021;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#26082;&#26368;&#22823;&#21270;&#20102;&#30456;&#20284;&#22270;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23884;&#20837;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#22238;&#24402;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#20854;&#20182;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GRU&#31639;&#27861;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02778</link><description>&lt;p&gt;
&#20174;&#33041;&#30005;&#22270;&#20013;&#25581;&#31034;&#24773;&#32490;&#65306;&#19968;&#31181;&#22522;&#20110;GRU&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Emotions from EEG: A GRU-Based Approach. (arXiv:2308.02778v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GRU&#31639;&#27861;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35745;&#31639;&#20013;&#26368;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#19968;&#26159;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30340;&#31867;&#22411;&#65292;&#30475;&#23427;&#26159;&#21542;&#33021;&#22815;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#26469;&#39044;&#27979;&#24773;&#32490;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#38598;&#21253;&#25324;&#38745;&#24687;&#20013;&#24615;&#25968;&#25454;&#20197;&#21450;&#25509;&#21463;&#24341;&#21457;&#24555;&#20048;&#12289;&#20013;&#24615;&#21644;&#28040;&#26497;&#24773;&#32490;&#30340;&#21050;&#28608;&#30340;&#20154;&#30340;&#33041;&#30005;&#22270;&#35760;&#24405;&#12290;&#20026;&#20102;&#36827;&#34892;&#26368;&#20339;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#36857;&#21435;&#38500;&#12289;&#24102;&#36890;&#28388;&#27874;&#22120;&#21644;&#24402;&#19968;&#21270;&#26041;&#27861;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#22312;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;100%&#30340;&#20934;&#30830;&#29575;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;GRU&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GRU&#27169;&#22411;&#30340;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#20998;&#31867;&#22120;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23545;&#28151;&#28102;&#30697;&#38453;&#36827;&#34892;&#30340;&#35814;&#32454;&#30740;&#31350;&#25581;&#31034;&#20102;&#26377;&#20851;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#31934;&#20934;&#30340;&#24773;&#32490;&#35782;&#21035;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most important study areas in affective computing is emotion identification using EEG data. In this study, the Gated Recurrent Unit (GRU) algorithm, which is a type of Recurrent Neural Networks (RNNs), is tested to see if it can use EEG signals to predict emotional states. Our publicly accessible dataset consists of resting neutral data as well as EEG recordings from people who were exposed to stimuli evoking happy, neutral, and negative emotions. For the best feature extraction, we pre-process the EEG data using artifact removal, bandpass filters, and normalization methods. With 100% accuracy on the validation set, our model produced outstanding results by utilizing the GRU's capacity to capture temporal dependencies. When compared to other machine learning techniques, our GRU model's Extreme Gradient Boosting Classifier had the highest accuracy. Our investigation of the confusion matrix revealed insightful information about the performance of the model, enabling precise em
&lt;/p&gt;</description></item><item><title>Dataopsy&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#27969;&#21160;&#30340;&#21487;&#35270;&#21270;&#25506;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#26597;&#35810;&#38613;&#21051;&#65288;AQS&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#38754;&#21487;&#35270;&#21270;&#26597;&#35810;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#21512;&#24320;&#22987;&#65292;&#36880;&#27493;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.02764</link><description>&lt;p&gt;
Dataopsy: &#21487;&#25193;&#23637;&#21644;&#27969;&#21160;&#30340;&#21487;&#35270;&#21270;&#25506;&#32034;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#32858;&#21512;&#26597;&#35810;&#38613;&#21051;
&lt;/p&gt;
&lt;p&gt;
Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting. (arXiv:2308.02764v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02764
&lt;/p&gt;
&lt;p&gt;
Dataopsy&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#27969;&#21160;&#30340;&#21487;&#35270;&#21270;&#25506;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#26597;&#35810;&#38613;&#21051;&#65288;AQS&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#38754;&#21487;&#35270;&#21270;&#26597;&#35810;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#21512;&#24320;&#22987;&#65292;&#36880;&#27493;&#25506;&#32034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#32500;&#25968;&#25454;&#30340;&#20998;&#38754;&#21487;&#35270;&#21270;&#26597;&#35810;&#25216;&#26415;&#65292;&#31216;&#20026;&#32858;&#21512;&#26597;&#35810;&#38613;&#21051;&#65288;AQS&#65289;&#12290;&#20316;&#20026;&#19968;&#31181;&#8220;&#22825;&#29983;&#21487;&#25193;&#23637;&#8221;&#30340;&#26597;&#35810;&#25216;&#26415;&#65292;AQS&#20174;&#19968;&#20010;&#34920;&#31034;&#25972;&#20010;&#25968;&#25454;&#38598;&#32858;&#21512;&#30340;&#21333;&#20010;&#35270;&#35273;&#26631;&#35760;&#24320;&#22987;&#21487;&#35270;&#21270;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#36880;&#27493;&#25506;&#32034;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25805;&#20316;&#32553;&#20889;&#20026;P6&#65306;&#26530;&#36724;&#65288;&#22522;&#20110;&#23646;&#24615;&#23545;&#32858;&#21512;&#36827;&#34892;&#20998;&#38754;&#65289;&#65292;&#20998;&#21106;&#65288;&#23558;&#20998;&#38754;&#24067;&#23616;&#22312;&#31354;&#38388;&#20013;&#65289;&#65292;&#31397;&#35270;&#65288;&#20351;&#29992;&#32858;&#21512;&#35270;&#35273;&#34920;&#31034;&#26597;&#30475;&#23376;&#38598;&#65289;&#65292;&#22534;&#31215;&#65288;&#21512;&#24182;&#20004;&#20010;&#25110;&#22810;&#20010;&#23376;&#38598;&#65289;&#65292;&#25237;&#24433;&#65288;&#23558;&#23376;&#38598;&#25552;&#21462;&#21040;&#26032;&#30340;&#22522;&#36136;&#65289;&#65292;&#21644;&#20462;&#21098;&#65288;&#20002;&#24323;&#24403;&#21069;&#19981;&#24863;&#20852;&#36259;&#30340;&#32858;&#21512;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;Dataopsy&#26469;&#39564;&#35777;AQS&#65292;&#23427;&#26159;AQS&#30340;&#21407;&#22411;&#23454;&#29616;&#65292;&#26088;&#22312;&#22312;&#26700;&#38754;&#21644;&#35302;&#25720;&#24335;&#31227;&#21160;&#35774;&#22791;&#19978;&#23454;&#29616;&#27969;&#30021;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#19977;&#20010;&#24212;&#29992;&#31034;&#20363;&#26469;&#23637;&#31034;AQS&#21644;Dataopsy&#12290;
&lt;/p&gt;
&lt;p&gt;
We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a "born scalable" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#23545;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#23849;&#28291;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#20013;&#38388;&#38544;&#34255;&#23618;&#37117;&#20250;&#20986;&#29616;&#26576;&#31181;&#31243;&#24230;&#30340;&#31070;&#32463;&#23849;&#28291;&#65292;&#32780;&#23849;&#28291;&#31243;&#24230;&#36890;&#24120;&#19982;&#23618;&#30340;&#28145;&#24230;&#21576;&#27491;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#31867;&#20869;&#26041;&#24046;&#30340;&#20943;&#23567;&#20027;&#35201;&#21457;&#29983;&#22312;&#36739;&#27973;&#30340;&#23618;&#20013;&#65292;&#22841;&#35282;&#20998;&#31163;&#37327;&#38543;&#38544;&#34255;&#23618;&#28145;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2308.02760</link><description>&lt;p&gt;
&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks. (arXiv:2308.02760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#23545;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#38388;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#23849;&#28291;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#20013;&#38388;&#38544;&#34255;&#23618;&#37117;&#20250;&#20986;&#29616;&#26576;&#31181;&#31243;&#24230;&#30340;&#31070;&#32463;&#23849;&#28291;&#65292;&#32780;&#23849;&#28291;&#31243;&#24230;&#36890;&#24120;&#19982;&#23618;&#30340;&#28145;&#24230;&#21576;&#27491;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#31867;&#20869;&#26041;&#24046;&#30340;&#20943;&#23567;&#20027;&#35201;&#21457;&#29983;&#22312;&#36739;&#27973;&#30340;&#23618;&#20013;&#65292;&#22841;&#35282;&#20998;&#31163;&#37327;&#38543;&#38544;&#34255;&#23618;&#28145;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#23545;&#20998;&#31867;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#32456;&#38544;&#34255;&#23618;&#20013;&#31867;&#21035;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#25551;&#36848;&#12290;&#36825;&#20010;&#25551;&#36848;&#25581;&#31034;&#20102;&#36825;&#20123;&#32593;&#32476;&#22914;&#20309;&#22312;&#35757;&#32451;&#36229;&#36807;&#38646;&#35757;&#32451;&#35823;&#24046;&#26102;&#23398;&#20064;&#29305;&#24449;&#21644;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#65288;NC&#65289;&#21482;&#22312;&#36825;&#20123;&#32593;&#32476;&#30340;&#26368;&#32456;&#23618;&#20013;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#20013;&#38388;&#38544;&#34255;&#23618;&#20013;&#65288;NC&#65289;&#30340;&#20986;&#29616;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#12289;&#28608;&#27963;&#20989;&#25968;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#32593;&#32476;&#30340;&#22823;&#22810;&#25968;&#20013;&#38388;&#38544;&#34255;&#23618;&#20013;&#20250;&#20986;&#29616;&#26576;&#31181;&#31243;&#24230;&#30340;&#65288;NC&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#38544;&#34255;&#23618;&#30340;&#23849;&#28291;&#31243;&#24230;&#36890;&#24120;&#19982;&#35813;&#23618;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#21576;&#27491;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#65306;&#65288;1&#65289;&#26679;&#26412;&#20013;&#30340;&#20960;&#20046;&#25152;&#26377;&#31867;&#20869;&#26041;&#24046;&#30340;&#20943;&#23567;&#21457;&#29983;&#22312;&#26356;&#27973;&#30340;&#23618;&#20013;&#65292;&#65288;2&#65289;&#22841;&#35282;&#20998;&#31163;&#37327;&#38543;&#38544;&#34255;&#23618;&#28145;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separa
&lt;/p&gt;</description></item><item><title>DaMSTF&#26159;&#19968;&#31181;&#39046;&#22495;&#36866;&#24212;&#30340;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#24182;&#20445;&#30041;&#22256;&#38590;&#30340;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2308.02753</link><description>&lt;p&gt;
DaMSTF: &#39046;&#22495;&#23545;&#25239;&#23398;&#20064;&#22686;&#24378;&#30340;&#39046;&#22495;&#33258;&#25105;&#35757;&#32451;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation. (arXiv:2308.02753v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02753
&lt;/p&gt;
&lt;p&gt;
DaMSTF&#26159;&#19968;&#31181;&#39046;&#22495;&#36866;&#24212;&#30340;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#24182;&#20445;&#30041;&#22256;&#38590;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#20316;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20266;&#26631;&#31614;&#65292;&#33258;&#25105;&#35757;&#32451;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#20266;&#23454;&#20363;&#26469;&#24341;&#23548;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20266;&#26631;&#31614;&#30340;&#39044;&#27979;&#38169;&#35823;&#65288;&#26631;&#31614;&#22122;&#22768;&#65289;&#25361;&#25112;&#20102;&#33258;&#25105;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#21487;&#38752;&#30340;&#20266;&#23454;&#20363;&#65292;&#21363;&#20855;&#26377;&#39640;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#20266;&#23454;&#20363;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#26631;&#31614;&#22122;&#22768;&#65292;&#20294;&#23481;&#26131;&#38169;&#36807;&#22256;&#38590;&#30340;&#20363;&#23376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36866;&#24212;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#23545;&#25239;&#23398;&#20064;&#22686;&#24378;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#65288;DaMSTF&#65289;&#12290;&#39318;&#20808;&#65292;DaMSTF&#36890;&#36807;&#20803;&#23398;&#20064;&#20272;&#35745;&#27599;&#20010;&#20266;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21516;&#26102;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#21644;&#20445;&#30041;&#22256;&#38590;&#30340;&#20363;&#23376;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20803;&#26500;&#36896;&#22120;&#26469;&#26500;&#36896;&#20803;&#39564;&#35777;&#22534;&#26632;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation se
&lt;/p&gt;</description></item><item><title>NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02751</link><description>&lt;p&gt;
NeRFs: &#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02751
&lt;/p&gt;
&lt;p&gt;
NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#35270;&#22270;&#21512;&#25104;&#25110;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#31561;&#38382;&#39064;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#20063;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;NeRFs&#36890;&#36807;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#35270;&#22270;&#30456;&#20851;&#36752;&#23556;&#21644;&#20307;&#31215;&#23494;&#24230;&#31561;&#20307;&#31215;&#21442;&#25968;&#65292;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#20307;&#31215;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#65292;&#27599;&#24180;&#26377;&#25968;&#21315;&#31687;&#35770;&#25991;&#22312;&#20854;&#22522;&#30784;&#19978;&#25193;&#23637;&#25110;&#30456;&#20851;&#30740;&#31350;&#65292;&#22810;&#20301;&#20316;&#32773;&#21644;&#32593;&#31449;&#25552;&#20379;&#27010;&#36848;&#21644;&#35843;&#30740;&#65292;&#24182;&#26377;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#21019;&#19994;&#20844;&#21496;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;NeRFs&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#38271;&#36798;&#19977;&#21313;&#24180;&#30340;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#26368;&#32456;&#24341;&#20986;NeRFs&#35770;&#25991;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;AMD Versal ACAP&#26550;&#26500;&#30340;&#24322;&#26500;&#35745;&#31639;&#33021;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#33258;&#23450;&#20041;&#30828;&#20214;&#27169;&#22359;&#21644;&#35774;&#35745;&#36816;&#34892;&#26102;&#20869;&#26680;&#26144;&#23556;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;GNN&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;&#35813;&#26041;&#27861;&#22312;VCK5000 ACAP&#24179;&#21488;&#19978;&#30340;&#23454;&#29616;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02749</link><description>&lt;p&gt;
&#21033;&#29992;Versal&#20307;&#31995;&#32467;&#26500;&#30340;&#29255;&#19978;&#24322;&#26500;&#24615;&#21152;&#36895;GNN&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration. (arXiv:2308.02749v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;AMD Versal ACAP&#26550;&#26500;&#30340;&#24322;&#26500;&#35745;&#31639;&#33021;&#21147;&#65292;&#36890;&#36807;&#24320;&#21457;&#33258;&#23450;&#20041;&#30828;&#20214;&#27169;&#22359;&#21644;&#35774;&#35745;&#36816;&#34892;&#26102;&#20869;&#26680;&#26144;&#23556;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;GNN&#25512;&#29702;&#30340;&#21152;&#36895;&#12290;&#35813;&#26041;&#27861;&#22312;VCK5000 ACAP&#24179;&#21488;&#19978;&#30340;&#23454;&#29616;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20013;&#37117;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#36890;&#36807;&#21033;&#29992;&#36755;&#20837;&#22270;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#39030;&#28857;&#29305;&#24449;&#21644;GNN&#35745;&#31639;&#20013;&#30340;&#20013;&#38388;&#25968;&#25454;&#65292;&#21487;&#20197;&#21152;&#36895;GNN&#25512;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#21160;&#24577;&#31232;&#30095;&#24615;&#21033;&#29992;&#65292;&#25105;&#20204;&#21033;&#29992;AMD Versal ACAP&#26550;&#26500;&#30340;&#24322;&#26500;&#35745;&#31639;&#33021;&#21147;&#26469;&#21152;&#36895;GNN&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30828;&#20214;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#22312;&#21487;&#32534;&#31243;&#36923;&#36753;&#65288;PL&#65289;&#19978;&#25191;&#34892;&#35745;&#31639;&#26680;&#24515;&#30340;&#31232;&#30095;&#21407;&#35821;&#65292;&#24182;&#20351;&#29992;AI&#24341;&#25806;&#65288;AIE&#65289;&#39640;&#25928;&#35745;&#31639;&#23494;&#38598;&#22411;&#21407;&#35821;&#12290;&#20026;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36816;&#34892;&#26102;&#20869;&#26680;&#26144;&#23556;&#31574;&#30053;&#65292;&#26681;&#25454;&#25968;&#25454;&#31232;&#30095;&#24615;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#20219;&#21153;&#32473;PL&#21644;AIE&#12290;&#22312;VCK5000 ACAP&#24179;&#21488;&#19978;&#30340;&#23454;&#29616;&#30456;&#27604;&#20110;CPU&#12289;GPU&#12289;ACAP&#21644;&#20854;&#20182;&#33258;&#23450;&#20041;GNN&#21152;&#36895;&#22120;&#30340;&#26368;&#26032;&#23454;&#29616;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared 
&lt;/p&gt;</description></item><item><title>SABRE&#26159;&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.02747</link><description>&lt;p&gt;
SABRE:&#24378;&#40065;&#26834;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SABRE: Robust Bayesian Peer-to-Peer Federated Learning. (arXiv:2308.02747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02747
&lt;/p&gt;
&lt;p&gt;
SABRE&#26159;&#19968;&#31181;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#32858;&#21512;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#20855;&#22791;&#40065;&#26834;&#24615;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SABRE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#40065;&#26834;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24050;&#30693;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;BayP2PFL&#65289;&#30340;&#25239;&#25915;&#20987;&#24615;&#65292;&#38543;&#21518;&#35777;&#26126;&#20102;BayP2PFL&#22312;&#38754;&#23545;&#36825;&#20123;&#25915;&#20987;&#26102;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SABRE&#32858;&#21512;&#26041;&#27861;&#26469;&#20811;&#26381;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;SABRE&#22312;&#38750;IID&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#35201;&#27714;&#22823;&#22810;&#25968;&#33391;&#24615;&#33410;&#28857;&#22810;&#20110;&#21463;&#25439;&#33410;&#28857;&#65292;&#24182;&#19988;&#22312;&#33391;&#24615;&#29615;&#22659;&#19979;&#29978;&#33267;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;&#25105;&#20204;&#20174;&#21435;&#20013;&#24515;&#21270;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#23545;&#25968;&#25454;/&#27169;&#22411;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23545;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#36827;&#34892;&#30340;&#27010;&#24565;&#35777;&#26126;&#35780;&#20272;&#26174;&#31034;&#20102;SABRE&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#20248;&#20110;&#29616;&#26377;&#26694;&#26550;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
&lt;/p&gt;</description></item><item><title>&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#26469;&#35299;&#20915;&#33258;&#35757;&#32451;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#26102;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02746</link><description>&lt;p&gt;
&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#33258;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification. (arXiv:2308.02746v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02746
&lt;/p&gt;
&lt;p&gt;
&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#26469;&#35299;&#20915;&#33258;&#35757;&#32451;&#22312;&#22823;&#39046;&#22495;&#36716;&#31227;&#26102;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#36328;&#39046;&#22495;&#36866;&#24212;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#33258;&#35757;&#32451;&#36890;&#36807;&#20174;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#20013;&#29983;&#25104;&#20266;&#26679;&#26412;&#65292;&#24182;&#36845;&#20195;&#22312;&#20266;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#22312;&#30446;&#26631;&#22495;&#19978;&#26368;&#23567;&#21270;Gibbs&#29109;&#12290;&#28982;&#32780;&#65292;Gibbs&#29109;&#23545;&#39044;&#27979;&#35823;&#24046;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#27492;&#24403;&#39046;&#22495;&#36716;&#31227;&#36739;&#22823;&#26102;&#65292;&#33258;&#35757;&#32451;&#24448;&#24448;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20803;-Tsallis-&#29109;&#26368;&#23567;&#21270;&#65288;MTEM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24212;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#23454;&#20363;&#33258;&#36866;&#24212;Tsallis&#29109;&#12290;&#20026;&#20102;&#38477;&#20302;MTEM&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#25216;&#26415;&#26469;&#36817;&#20284;&#20803;&#23398;&#20064;&#20013;&#28041;&#21450;&#30340;&#20108;&#38454;&#23548;&#25968;&#12290;&#20026;&#20102;&#39640;&#25928;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36864;&#28779;&#37319;&#26679;&#26426;&#21046;&#26469;&#25506;&#32034;&#27169;&#22411;&#30340;&#39044;&#27979;&#27010;&#29575;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;m&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26410;&#30693;&#32479;&#35745;&#37327;&#30340;&#38750;&#31283;&#24577;&#26080;&#32447;&#32593;&#32476;&#20013;&#36827;&#34892;&#35843;&#24230;&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26368;&#22823;&#26435;&#37325;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#28369;&#21160;&#31383;&#21475;&#19978;&#30028;&#32622;&#20449;&#24230;&#26469;&#23398;&#20064;&#20449;&#36947;&#30340;&#29366;&#24577;&#12290;&#22312;&#20551;&#35774;&#24179;&#22343;&#26381;&#21153;&#29575;&#30340;&#21464;&#24322;&#24615;&#28201;&#21644;&#30340;&#21069;&#25552;&#19979;&#65292;&#35813;&#31639;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.02734</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26410;&#30693;&#32479;&#35745;&#37327;&#30340;&#38750;&#31283;&#24577;&#26080;&#32447;&#32593;&#32476;&#20013;&#36827;&#34892;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics. (arXiv:2308.02734v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26410;&#30693;&#32479;&#35745;&#37327;&#30340;&#38750;&#31283;&#24577;&#26080;&#32447;&#32593;&#32476;&#20013;&#36827;&#34892;&#35843;&#24230;&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26368;&#22823;&#26435;&#37325;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#28369;&#21160;&#31383;&#21475;&#19978;&#30028;&#32622;&#20449;&#24230;&#26469;&#23398;&#20064;&#20449;&#36947;&#30340;&#29366;&#24577;&#12290;&#22312;&#20551;&#35774;&#24179;&#22343;&#26381;&#21153;&#29575;&#30340;&#21464;&#24322;&#24615;&#28201;&#21644;&#30340;&#21069;&#25552;&#19979;&#65292;&#35813;&#31639;&#27861;&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26080;&#32447;&#32593;&#32476;&#30340;&#20986;&#29616;&#65292;&#24102;&#26469;&#20102;&#23545;&#35774;&#35745;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#30340;&#26032;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#26102;&#21464;&#21160;&#24577;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#24191;&#20041;&#24178;&#25200;&#32422;&#26463;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#26377;&#25928;&#35843;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#24179;&#22343;&#21040;&#36798;&#29575;&#21644;&#24179;&#22343;&#26381;&#21153;&#29575;&#26159;&#26410;&#30693;&#21644;&#38750;&#31283;&#24577;&#30340;&#12290;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#29616;&#20195;&#32593;&#32476;&#20013;&#26080;&#32447;&#36890;&#20449;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#23454;&#38469;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;MW-UCB&#65292;&#29992;&#20110;&#24191;&#20041;&#26080;&#32447;&#32593;&#32476;&#35843;&#24230;&#65292;&#23427;&#22522;&#20110;&#26368;&#22823;&#26435;&#37325;&#31574;&#30053;&#65292;&#24182;&#21033;&#29992;&#28369;&#21160;&#31383;&#21475;&#19978;&#30028;&#32622;&#20449;&#24230;&#26469;&#23398;&#20064;&#38750;&#31283;&#24577;&#19979;&#30340;&#20449;&#36947;&#32479;&#35745;&#37327;&#12290;&#22312;&#23545;&#24179;&#22343;&#26381;&#21153;&#29575;&#30340;&#21464;&#24322;&#24615;&#25552;&#20986;&#28201;&#21644;&#20551;&#35774;&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MW-UCB&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21482;&#35201;&#22312;&#20219;&#20309;&#26102;&#38388;&#27573;&#20869;&#24179;&#22343;&#26381;&#21153;&#29575;&#30340;&#24635;&#21464;&#21270;&#20197;&#27425;&#32447;&#24615;&#22686;&#38271;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#35777;&#26126;MW-UCB&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#30340;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#21387;&#21147;&#31227;&#21160;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21387;&#21147;&#39044;&#27979;&#20013;&#26631;&#31614;&#20027;&#35266;&#24615;&#21644;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02731</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#20010;&#24615;&#21270;&#21387;&#21147;&#31227;&#21160;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Personalization of Stress Mobile Sensing using Self-Supervised Learning. (arXiv:2308.02731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#21387;&#21147;&#31227;&#21160;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21387;&#21147;&#39044;&#27979;&#20013;&#26631;&#31614;&#20027;&#35266;&#24615;&#21644;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#21147;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#21508;&#31181;&#20581;&#24247;&#38382;&#39064;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#35760;&#24405;&#30340;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#36827;&#34892;&#21387;&#21147;&#39044;&#27979;&#26159;&#31227;&#21160;&#24863;&#30693;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#22240;&#20026;&#23454;&#26102;&#30340;&#21387;&#21147;&#39044;&#27979;&#21487;&#20197;&#20351;&#25968;&#23383;&#24178;&#39044;&#22312;&#21387;&#21147;&#21457;&#29983;&#26102;&#31435;&#21363;&#20570;&#20986;&#21453;&#24212;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#35768;&#22810;&#24515;&#29702;&#21644;&#29983;&#29702;&#30151;&#29366;&#65292;&#27604;&#22914;&#24515;&#24459;&#19981;&#40784;&#12290;&#30382;&#30005;&#27963;&#21160;&#65288;EDA&#65289;&#36890;&#24120;&#29992;&#20110;&#27979;&#37327;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21387;&#21147;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#26631;&#31614;&#30340;&#20027;&#35266;&#24615;&#21644;&#31232;&#30095;&#24615;&#12289;&#22823;&#37327;&#30340;&#29305;&#24449;&#31354;&#38388;&#12289;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#31614;&#20197;&#21450;&#29305;&#24449;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#21644;&#20027;&#35266;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#20351;&#29992;&#65306;&#20026;&#27599;&#20010;&#29992;&#25143;&#35757;&#32451;&#21333;&#29420;&#30340;&#21387;&#21147;&#39044;&#27979;&#27169;&#22411;&#12290;&#20026;&#20102;&#35753;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27599;&#20010;&#20010;&#20307;&#22522;&#32447;&#29983;&#29289;&#20449;&#21495;&#27169;&#24335;&#30340;&#26102;&#38388;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#21644;ReLU&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#22312;&#32534;&#30721;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#35821;&#35328;&#19978;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;PIRL&#31639;&#27861;&#12290;&#20351;&#29992;ReLU&#32593;&#32476;&#21487;&#20197;&#23558;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#36716;&#21270;&#20026;&#20855;&#26377;if-then-else&#32467;&#26500;&#12289;&#32447;&#24615;&#21464;&#25442;&#21644;PID&#25805;&#20316;&#31561;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.02729</link><description>&lt;p&gt;
&#20351;&#29992;Actor-Critic&#31639;&#27861;&#21644;ReLU&#32593;&#32476;&#32508;&#21512;&#32534;&#31243;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks. (arXiv:2308.02729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#21644;ReLU&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#22312;&#32534;&#30721;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#35821;&#35328;&#19978;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;PIRL&#31639;&#27861;&#12290;&#20351;&#29992;ReLU&#32593;&#32476;&#21487;&#20197;&#23558;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#36716;&#21270;&#20026;&#20855;&#26377;if-then-else&#32467;&#26500;&#12289;&#32447;&#24615;&#21464;&#25442;&#21644;PID&#25805;&#20316;&#31561;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21270;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;(PIRL)&#36890;&#36807;&#20154;&#31867;&#21487;&#35835;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#26469;&#32534;&#30721;&#31574;&#30053;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#31639;&#27861;&#26469;&#22788;&#29702;&#22312;&#31243;&#24207;&#21270;&#31574;&#30053;&#31354;&#38388;&#20013;&#32570;&#20047;&#26799;&#24230;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;PIRL&#31639;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#23548;&#24341;&#22312;&#31243;&#24207;&#21270;&#31354;&#38388;&#20013;&#25628;&#32034;&#30340;&#8220;&#31070;&#35861;&#8221;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32534;&#30721;&#31243;&#24207;&#21270;&#31574;&#30053;&#30340;&#35821;&#35328;&#19978;&#65292;&#36825;&#26679;&#30340;PIRL&#29305;&#23450;&#31639;&#27861;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#36825;&#26159;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#30452;&#25509;&#33719;&#24471;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#21033;&#29992;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#26012;&#29575;&#20915;&#31574;&#26641;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23558;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#36716;&#21270;&#20026;&#31243;&#24207;&#21270;&#31574;&#30053;&#12290;&#36825;&#31181;&#20174;ReLU&#32593;&#32476;&#36716;&#21270;&#20351;&#25105;&#20204;&#33021;&#22815;&#32508;&#21512;&#32534;&#30721;&#20102;if-then-else&#32467;&#26500;&#12289;&#36755;&#20837;&#20540;&#30340;&#32447;&#24615;&#21464;&#25442;&#21644;PID&#25805;&#20316;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#29305;&#24449;&#21644;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20462;&#25913;&#21253;&#25324;&#22686;&#24378;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#21644;&#35774;&#35745;&#21487;&#38450;&#27490;&#39044;&#27979;&#26497;&#30701;&#29255;&#27573;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#23545;&#20110;&#25552;&#39640;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#25928;&#26524;&#26377;&#23454;&#38469;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02723</link><description>&lt;p&gt;
&#20026;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#25913;&#36827;&#35856;&#27874;&#25935;&#24863;&#24615;&#21644;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction. (arXiv:2308.02723v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#29305;&#24449;&#21644;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20462;&#25913;&#21253;&#25324;&#22686;&#24378;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#21644;&#35774;&#35745;&#21487;&#38450;&#27490;&#39044;&#27979;&#26497;&#30701;&#29255;&#27573;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#23545;&#20110;&#25552;&#39640;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#25928;&#26524;&#26377;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;&#26059;&#24459;&#25552;&#21462;&#27169;&#22411;&#20381;&#36182;&#20110;&#37325;&#26032;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#20551;&#35774;&#30340;&#36755;&#20837;&#29305;&#24449;&#20462;&#25913;&#21644;&#35757;&#32451;&#30446;&#26631;&#20462;&#25913;&#12290;&#39318;&#20808;&#65292;&#38899;&#39057;&#25968;&#25454;&#30340;&#39057;&#35889;&#22270;&#20013;&#30340;&#35856;&#27874;&#22312;&#39057;&#29575;&#36724;&#19978;&#36805;&#36895;&#34928;&#20943;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;z-transform&#20462;&#25913;&#20102;&#32852;&#21512;&#39057;&#29575;&#21644;&#21608;&#26399;&#24615;(CFP)&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#26497;&#30701;&#26102;&#38271;&#30340;&#20154;&#22768;&#21644;&#38750;&#20154;&#22768;&#29255;&#27573;&#24182;&#19981;&#24120;&#35265;&#12290;&#20026;&#20102;&#30830;&#20445;&#26356;&#31283;&#23450;&#30340;&#26059;&#24459;&#36718;&#24275;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#38450;&#27490;&#27169;&#22411;&#39044;&#27979;&#36825;&#20123;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20462;&#25913;&#24212;&#29992;&#20110;&#22810;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;MSNet&#12289;FTANet&#21644;&#26032;&#24341;&#20837;&#30340;&#27169;&#22411;PianoNet&#65292;&#35813;&#27169;&#22411;&#26159;&#20174;&#38050;&#29748;&#36716;&#24405;&#32593;&#32476;&#25913;&#32534;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#20462;&#25913;&#22312;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#19978;&#20855;&#26377;&#23454;&#35777;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extrac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28082;&#20307;&#30340;&#31896;&#24230;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#35270;&#35273;&#19978;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#21160;&#24577;&#31896;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.02715</link><description>&lt;p&gt;
AI&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#22312;&#27969;&#20307;&#23646;&#24615;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fluid Property Prediction Leveraging AI and Robotics. (arXiv:2308.02715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#28082;&#20307;&#30340;&#31896;&#24230;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#35270;&#35273;&#19978;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#21160;&#24577;&#31896;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35270;&#35273;&#19978;&#25512;&#27979;&#28082;&#20307;&#23646;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#28082;&#20307;&#30340;&#34892;&#20026;&#21644;&#26816;&#27979;&#37117;&#21313;&#20998;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#35270;&#35273;&#20449;&#24687;&#20013;&#25512;&#26029;&#28082;&#20307;&#23646;&#24615;&#23545;&#20110;&#33258;&#20027;&#27969;&#20307;&#22788;&#29702;&#31995;&#32479;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#24456;&#23481;&#26131;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#20165;&#36890;&#36807;&#35270;&#35273;&#39044;&#27979;&#28082;&#20307;&#23646;&#24615;&#21487;&#20197;&#21152;&#24555;&#27969;&#20307;&#34920;&#24449;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#39564;&#29615;&#22659;&#20013;&#33410;&#30465;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#35270;&#35273;&#26041;&#27861;&#26469;&#20272;&#35745;&#31896;&#24230;&#65292;&#21033;&#29992;&#28082;&#20307;&#25391;&#33633;&#34892;&#20026;&#19982;&#31896;&#24230;&#30452;&#25509;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;3D&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35270;&#39057;&#20013;&#19981;&#21516;&#28082;&#20307;&#25391;&#33633;&#27169;&#24335;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20511;&#21161;&#36825;&#20123;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#35270;&#39057;&#20013;&#30452;&#35266;&#22320;&#25512;&#26029;&#28082;&#20307;&#30340;&#31867;&#21035;&#25110;&#28082;&#20307;&#30340;&#21160;&#24577;&#31896;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#37325;&#26500;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#23547;&#25214;&#26368;&#20339;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02714</link><description>&lt;p&gt;
&#25506;&#32034;&#31232;&#30095;&#24674;&#22797;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#36136;&#37327;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution. (arXiv:2308.02714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#37325;&#26500;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#23547;&#25214;&#26368;&#20339;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23383;&#20856;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20174;&#39640;&#20998;&#36776;&#29575;&#21644;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#23545;&#23398;&#20064;&#19968;&#23545;&#32806;&#21512;&#23383;&#20856;&#65292;&#20351;&#24471;&#23545;&#24212;&#30340;&#22270;&#20687;&#23545;&#22312;&#30001;&#32806;&#21512;&#23383;&#20856;&#34920;&#31034;&#26102;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#21521;&#37327;&#65292;&#20174;&#32780;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#24674;&#22797;&#12290;&#36825;&#20123;&#23383;&#20856;&#21487;&#20197;&#29992;&#20110;&#26681;&#25454;&#31232;&#30095;&#24674;&#22797;&#20174;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#22270;&#20687;&#20013;&#37325;&#26500;&#23545;&#24212;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22359;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25105;&#20204;&#20351;&#29992;&#30340;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#23545;&#37325;&#26500;&#22270;&#20687;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#20197;&#23547;&#25214;&#26368;&#36866;&#21512;&#27492;&#30446;&#30340;&#30340;&#31232;&#30095;&#24674;&#22797;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dictionary learning can be used for image superresolution by learning a pair of coupled dictionaries of image patches from high-resolution and low-resolution image pairs such that the corresponding pairs share the same sparse vector when represented by the coupled dictionaries. These dictionaries then can be used to to reconstruct the corresponding high-resolution patches from low-resolution input images based on sparse recovery. The idea is to recover the shared sparse vector using the low-resolution dictionary and then multiply it by the high-resolution dictionary to recover the corresponding high-resolution image patch. In this work, we study the effect of the sparse recovery algorithm that we use on the quality of the reconstructed images. We offer empirical experiments to search for the best sparse recovery algorithm that can be used for this purpose.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#21644;&#31163;&#25955;&#20540;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#19978;&#35745;&#31639;&#22240;&#26524;&#26597;&#35810;&#30340;&#30028;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#21098;&#26041;&#27861;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#26356;&#22823;&#35268;&#27169;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#30028;&#38480;&#65292;&#22312;&#29305;&#27530;&#38382;&#39064;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#30028;&#38480;&#65292;&#24182;&#23558;&#26041;&#27861;&#25193;&#23637;&#21040;&#20998;&#25968;&#32447;&#24615;&#35268;&#21010;&#36827;&#34892;&#35745;&#31639;</title><link>http://arxiv.org/abs/2308.02709</link><description>&lt;p&gt;
&#21487;&#20280;&#32553;&#30340;&#22240;&#26524;&#30028;&#38480;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Scalable Computation of Causal Bounds. (arXiv:2308.02709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#21644;&#31163;&#25955;&#20540;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#19978;&#35745;&#31639;&#22240;&#26524;&#26597;&#35810;&#30340;&#30028;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#21098;&#26041;&#27861;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#26356;&#22823;&#35268;&#27169;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#30028;&#38480;&#65292;&#22312;&#29305;&#27530;&#38382;&#39064;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#30028;&#38480;&#65292;&#24182;&#23558;&#26041;&#27861;&#25193;&#23637;&#21040;&#20998;&#25968;&#32447;&#24615;&#35268;&#21010;&#36827;&#34892;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#23384;&#22312;&#26410;&#35266;&#27979;&#28151;&#28102;&#21464;&#37327;&#21644;&#31163;&#25955;&#20540;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#22270;&#19978;&#35745;&#31639;&#22240;&#26524;&#26597;&#35810;&#30340;&#30028;&#38480;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#28385;&#36275;&#21487;&#36776;&#35782;&#24615;&#12290;&#29616;&#26377;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#29992;&#20110;&#35745;&#31639;&#36825;&#20123;&#30028;&#38480;&#30340;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#20844;&#24335;&#30001;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#22823;&#23567;&#38543;&#30528;&#22240;&#26524;&#22270;&#20013;&#30340;&#36793;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#32780;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;LP&#20844;&#24335;&#21487;&#20197;&#34987;&#26174;&#33879;&#20462;&#21098;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35745;&#31639;&#36739;&#22823;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#30340;&#30028;&#38480;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#36825;&#20010;&#20462;&#21098;&#36807;&#31243;&#20801;&#35768;&#25105;&#20204;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#19968;&#31867;&#29305;&#27530;&#38382;&#39064;&#30340;&#30028;&#38480;&#65292;&#21253;&#25324;&#22810;&#20010;&#28151;&#28102;&#22788;&#29702;&#24433;&#21709;&#32467;&#26524;&#30340;&#24191;&#20026;&#30740;&#31350;&#30340;&#38382;&#39064;&#26063;&#12290;&#25105;&#20204;&#23558;&#20462;&#21098;&#26041;&#27861;&#25193;&#23637;&#21040;&#20998;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;fractional LP&#65289;&#65292;&#36825;&#20123;&#35268;&#21010;&#20844;&#24335;&#29992;&#20110;&#35745;&#31639;&#21253;&#21547;&#26377;&#20851;&#20010;&#20307;&#30340;&#38468;&#21152;&#35266;&#27979;&#30340;&#22240;&#26524;&#26597;&#35810;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#20272;&#35745;&#20108;&#20998;&#31867;&#27169;&#22411;&#30340;FPR&#20197;&#21450;TPR&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#34429;&#28982;&#20943;&#23567;&#20102;&#24635;&#35823;&#24046;&#65292;&#21364;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#23545;FPR&#21644;TPR&#30340;&#20272;&#35745;&#20934;&#30830;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#28165;&#29702;&#35823;&#24046;&#19982;&#27169;&#22411;&#20998;&#25968;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02695</link><description>&lt;p&gt;
&#23384;&#22312;&#31867;&#26465;&#20214;&#26631;&#31614;&#22122;&#38899;&#19979;&#30340;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;FPR&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise. (arXiv:2308.02695v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#20272;&#35745;&#20108;&#20998;&#31867;&#27169;&#22411;&#30340;FPR&#20197;&#21450;TPR&#30340;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20013;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#34429;&#28982;&#20943;&#23567;&#20102;&#24635;&#35823;&#24046;&#65292;&#21364;&#19981;&#33021;&#20445;&#35777;&#27169;&#22411;&#23545;FPR&#21644;TPR&#30340;&#20272;&#35745;&#20934;&#30830;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#28165;&#29702;&#35823;&#24046;&#19982;&#27169;&#22411;&#20998;&#25968;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#39564;&#35777;&#38598;&#20013;&#23384;&#22312;&#38169;&#35823;&#26631;&#31614;&#65288;&#26631;&#31614;&#22122;&#38899;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20108;&#20998;&#31867;&#27169;&#22411;&#30340;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;/&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#30340;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#24212;&#29992;&#26159;&#27450;&#35784;&#39044;&#38450;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#20272;&#35745;FPR&#23545;&#20110;&#20445;&#25252;&#22909;&#23458;&#25143;&#30340;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26631;&#31614;&#22122;&#38899;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#28165;&#29702;&#36807;&#31243;&#20013;&#30340;&#24635;&#35823;&#24046; - &#36991;&#20813;&#28165;&#29702;&#38750;&#22122;&#38899;&#30340;&#31034;&#20363;&#65292;&#24182;&#30830;&#20445;&#28165;&#29702;&#22122;&#38899;&#31034;&#20363;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20934;&#30830;&#24615;&#24230;&#37327;&#65292;&#20294;&#19981;&#36275;&#20197;&#20445;&#35777;&#27169;&#22411;&#30340;&#30495;&#23454;FPR&#25110;TPR&#30340;&#33391;&#22909;&#20272;&#35745;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#24635;&#35823;&#24046;&#36739;&#20302;&#65292;&#20351;&#29992;&#27169;&#22411;&#30452;&#25509;&#28165;&#29702;&#33258;&#24049;&#30340;&#39564;&#35777;&#25968;&#25454;&#20063;&#20250;&#23548;&#33268;&#20302;&#20272;&#12290;&#36825;&#34920;&#26126;&#65292;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#36861;&#27714;&#19981;&#20165;&#20943;&#23569;&#24635;&#35823;&#24046;&#65292;&#36824;&#38656;&#35201;&#23547;&#27714;&#20351;&#28165;&#29702;&#35823;&#24046;&#19982;&#27169;&#22411;&#20998;&#25968;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the false-/ true-positive-rate (FPR/TPR) for a binary classification model when there are incorrect labels (label noise) in the validation set. Our motivating application is fraud prevention where accurate estimates of FPR are critical to preserving the experience for good customers, and where label noise is highly asymmetric. Existing methods seek to minimize the total error in the cleaning process - to avoid cleaning examples that are not noise, and to ensure cleaning of examples that are. This is an important measure of accuracy but insufficient to guarantee good estimates of the true FPR or TPR for a model, and we show that using the model to directly clean its own validation data leads to underestimates even if total error is low. This indicates a need for researchers to pursue methods that not only reduce total error but also seek to de-correlate cleaning error with model scores.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20107;&#21518;&#20998;&#26512;&#21457;&#29616;&#35813;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#27963;&#21160;&#21306;&#29305;&#24449;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.02682</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#19982;&#20107;&#21518;&#27880;&#24847;&#21147;&#30340;&#25805;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting. (arXiv:2308.02682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20107;&#21518;&#20998;&#26512;&#21457;&#29616;&#35813;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#27963;&#21160;&#21306;&#29305;&#24449;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#30424;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#20107;&#21518;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#27599;&#23567;&#26102;&#30340;&#20840;&#30424;&#32447;&#35270;&#30913;&#22270;&#20687;&#65292;&#24182;&#36873;&#25321;&#20108;&#36827;&#21046;&#39044;&#27979;&#27169;&#24335;&#26469;&#39044;&#27979;&#26410;&#26469;24&#23567;&#26102;&#20869;&#21457;&#29983;&#8805;M1.0&#32423;&#32768;&#26001;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26679;&#26412;&#21152;&#26435;&#26469;&#24212;&#23545;&#22266;&#26377;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25216;&#33021;&#32479;&#35745;&#21644;Heidke&#25216;&#33021;&#35780;&#20998;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#36817;&#28176;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#27880;&#24847;&#21147;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23558;&#26799;&#24230;&#20449;&#21495;&#21457;&#36865;&#21040;&#36755;&#20837;&#29305;&#24449;&#19978;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36127;&#25285;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#20107;&#21518;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#35299;&#37322;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;(i) &#23548;&#21521;&#28176;&#21464;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#65292;(ii) &#28145;&#24230;Shapley&#21152;&#24615;&#35299;&#37322;&#65292;&#20197;&#21450;(iii) &#32508;&#21512;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#30424;&#22826;&#38451;&#32768;&#26001;&#30340;&#39044;&#27979;&#19982;&#19982;&#27963;&#21160;&#21306;&#30456;&#20851;&#30340;&#29305;&#24449;&#30456;&#21563;&#21512;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1) &#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20840;&#30424;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a post hoc analysis of a deep learning-based full-disk solar flare prediction model. We used hourly full-disk line-of-sight magnetogram images and selected binary prediction mode to predict the occurrence of $\geq$M1.0-class flares within 24 hours. We leveraged custom data augmentation and sample weighting to counter the inherent class-imbalance problem and used true skill statistic and Heidke skill score as evaluation metrics. Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods: (i) Guided Gradient-weighted Class Activation Mapping, (ii) Deep Shapley Additive Explanations, and (iii) Integrated Gradients. Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are: (1) We demonstrate that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20449;&#36151;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27169;&#22411;&#65292;&#36890;&#36807;&#20511;&#37492;&#20132;&#21449;&#24615;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#22312;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#23130;&#23035;&#29366;&#20917;&#12289;&#21333;&#20146;&#29366;&#24577;&#21644;&#23376;&#22899;&#25968;&#37327;&#31561;&#22810;&#20010;&#20132;&#21449;&#31038;&#20250;&#31867;&#21035;&#30340;&#24433;&#21709;&#19979;&#65292;&#20449;&#36151;&#20998;&#37197;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2308.02680</link><description>&lt;p&gt;
&#20449;&#36151;&#20013;&#30340;&#20844;&#24179;&#27169;&#22411;&#65306;&#20132;&#21449;&#27495;&#35270;&#21644;&#19981;&#24179;&#31561;&#30340;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Fair Models in Credit: Intersectional Discrimination and the Amplification of Inequity. (arXiv:2308.02680v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20449;&#36151;&#39046;&#22495;&#20013;&#30340;&#20844;&#24179;&#27169;&#22411;&#65292;&#36890;&#36807;&#20511;&#37492;&#20132;&#21449;&#24615;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#22312;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#23130;&#23035;&#29366;&#20917;&#12289;&#21333;&#20146;&#29366;&#24577;&#21644;&#23376;&#22899;&#25968;&#37327;&#31561;&#22810;&#20010;&#20132;&#21449;&#31038;&#20250;&#31867;&#21035;&#30340;&#24433;&#21709;&#19979;&#65292;&#20449;&#36151;&#20998;&#37197;&#20013;&#23384;&#22312;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#36151;&#24314;&#27169;&#20013;&#65292;&#26032;&#25968;&#25454;&#26469;&#28304;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#19981;&#20844;&#24179;&#20915;&#31574;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#20915;&#31574;&#20381;&#36182;&#20110;&#21463;&#20445;&#25252;&#30340;&#29305;&#24449;&#65288;&#22914;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#24180;&#40836;&#65289;&#25110;&#20854;&#20182;&#31038;&#20250;&#32463;&#27982;&#21644;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12290;&#20316;&#32773;&#22312;&#23567;&#39069;&#20449;&#36151;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#22312;&#20449;&#29992;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#19981;&#24179;&#31561;&#30340;&#22256;&#38590;&#22312;&#24369;&#21183;&#32676;&#20307;&#20013;&#26356;&#20026;&#20984;&#26174;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#22810;&#20010;&#20132;&#21449;&#31038;&#20250;&#31867;&#21035;&#23450;&#20041;&#30340;&#32676;&#20307;&#20043;&#38388;&#30340;&#20449;&#36151;&#20998;&#37197;&#19981;&#24179;&#31561;&#38750;&#24120;&#20102;&#35299;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20132;&#21449;&#24615;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#25353;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#23130;&#23035;&#29366;&#20917;&#12289;&#21333;&#20146;&#29366;&#24577;&#21644;&#23376;&#22899;&#25968;&#37327;&#20132;&#21449;&#30340;&#27700;&#24179;&#19981;&#24179;&#31561;&#22312;&#20449;&#36151;&#33719;&#24471;&#20013;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#21033;&#29992;&#35199;&#29677;&#29273;&#23567;&#39069;&#20449;&#36151;&#24066;&#22330;&#30340;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#22810;&#20803;&#21270;&#30340;&#29616;&#23454;&#21644;&#20132;&#21449;&#36523;&#20221;&#22914;&#20309;&#22609;&#36896;&#20449;&#36151;&#20998;&#37197;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing usage of new data sources and machine learning (ML) technology in credit modeling raises concerns with regards to potentially unfair decision-making that rely on protected characteristics (e.g., race, sex, age) or other socio-economic and demographic data. The authors demonstrate the impact of such algorithmic bias in the microfinance context. Difficulties in assessing credit are disproportionately experienced among vulnerable groups, however, very little is known about inequities in credit allocation between groups defined, not only by single, but by multiple and intersecting social categories. Drawing from the intersectionality paradigm, the study examines intersectional horizontal inequities in credit access by gender, age, marital status, single parent status and number of children. This paper utilizes data from the Spanish microfinance market as its context to demonstrate how pluralistic realities and intersectional identities can shape patterns of credit allocation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02668</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20027;&#23548;&#33539;&#24335;&#26159;&#20381;&#36182;&#20110;&#23436;&#20840;&#24102;&#27880;&#37322;&#30340;&#35757;&#32451;&#22270;&#20687;&#65292;&#36825;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20381;&#36182;&#24182;&#25552;&#39640;&#32467;&#26524;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20197;&#38480;&#21046;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#39062;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#26174;&#33879;&#25913;&#36827;&#24072;&#29983;&#33976;&#39311;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;(i)&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#25913;&#36827;&#20102;&#33976;&#39311;&#26041;&#27861;&#65292;(ii)&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23454;&#20363;&#20998;&#21106;&#26550;&#26500;&#12289;&#20027;&#24178;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#20043;&#21069;&#21482;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#26469;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#39044;&#28903;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#23548;&#24072;&#27169;&#22411;&#30340;&#25351;&#23548;&#22312;&#39044;&#28903;&#38454;&#27573;&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#30340;&#33976;&#39311;&#26041;&#27861;&#22312;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#20840;&#29699;&#27969;&#26143;&#38632;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20114;&#21160;&#30340;&#32593;&#39029;&#24179;&#21488;&#65292;&#35753;&#20844;&#20247;&#21442;&#19982;&#27969;&#26143;&#25506;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27969;&#26143;&#38632;&#30340;&#21457;&#29616;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02664</link><description>&lt;p&gt;
&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#22788;&#29702;&#21644;&#21457;&#29616;&#32676;&#20307;&#22806;&#21253;&#29992;&#20110;&#27969;&#26143;&#38632;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping. (arXiv:2308.02664v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#20840;&#29699;&#27969;&#26143;&#38632;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20114;&#21160;&#30340;&#32593;&#39029;&#24179;&#21488;&#65292;&#35753;&#20844;&#20247;&#21442;&#19982;&#27969;&#26143;&#25506;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27969;&#26143;&#38632;&#30340;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;NASA&#33258;2010&#24180;&#24320;&#22987;&#36164;&#21161;&#30340;&#20840;&#29699;&#20840;&#22825;&#20505;&#27969;&#26143;&#35266;&#27979;(CAMS)&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23545;&#26469;&#33258;16&#20010;&#22269;&#23478;&#12289;&#21271;&#21322;&#29699;&#21644;&#21335;&#21322;&#29699;&#30340;&#22810;&#22320;&#20302;&#20809;&#35270;&#39057;&#25668;&#20687;&#26426;&#25506;&#27979;&#21040;&#30340;&#27969;&#26143;&#36712;&#36857;&#36827;&#34892;&#19977;&#35282;&#27979;&#37327;&#65292;&#21046;&#22270;&#20840;&#29699;&#27969;&#26143;&#38632;&#12290;&#35813;&#39033;&#30446;&#30340;&#20351;&#21629;&#26159;&#39564;&#35777;&#12289;&#21457;&#29616;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#26143;&#38632;&#38477;&#20020;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#26045;&#22522;&#20110;&#20113;&#30340;&#33258;&#21160;&#21270;AI&#27969;&#31243;&#21644;&#25913;&#21892;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#23558;&#20844;&#20247;&#21442;&#19982;&#21040;&#30417;&#27979;&#27969;&#26143;&#25506;&#27979;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#21457;&#29616;&#29575;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#27963;&#21160;&#23398;&#20064;&#21644;AI&#27969;&#31243;&#33258;&#21160;&#21270;&#36827;&#34892;&#25968;&#25454;&#25668;&#21462;&#12289;&#22788;&#29702;&#21644;&#27934;&#23519;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#20114;&#21160;&#30340;Web&#38376;&#25143;&#65288;NASA&#27969;&#26143;&#38632;&#38376;&#25143;&#65289;&#65292;&#29992;&#20110;&#20419;&#36827;&#27969;&#26143;&#28304;&#22320;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;&#30446;&#21069;&#65292;CAMS&#24050;&#32463;&#21457;&#29616;&#20102;&#36229;&#36807;200&#20010;&#26032;&#30340;&#27969;&#26143;&#38632;
&lt;/p&gt;
&lt;p&gt;
The Cameras for Allsky Meteor Surveillance (CAMS) project, funded by NASA starting in 2010, aims to map our meteor showers by triangulating meteor trajectories detected in low-light video cameras from multiple locations across 16 countries in both the northern and southern hemispheres. Its mission is to validate, discover, and predict the upcoming returns of meteor showers. Our research aimed to streamline the data processing by implementing an automated cloud-based AI-enabled pipeline and improve the data visualization to improve the rate of discoveries by involving the public in monitoring the meteor detections. This article describes the process of automating the data ingestion, processing, and insight generation using an interpretable Active Learning and AI pipeline. This work also describes the development of an interactive web portal (the NASA Meteor Shower portal) to facilitate the visualization of meteor radiant maps. To date, CAMS has discovered over 200 new meteor showers and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#32479;&#19968;&#35270;&#35282;&#20986;&#21457;&#65292;&#31995;&#32479;&#24615;&#22320;&#24635;&#32467;&#20102;28&#20010;&#21464;&#37327;&#21464;&#25442;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.02652</link><description>&lt;p&gt;
&#23545;&#29983;&#25104;&#24314;&#27169;&#30340;&#21464;&#37327;&#21464;&#25442;&#20844;&#24335;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Change of Variable Formulas for Generative Modeling. (arXiv:2308.02652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#32479;&#19968;&#35270;&#35282;&#20986;&#21457;&#65292;&#31995;&#32479;&#24615;&#22320;&#24635;&#32467;&#20102;28&#20010;&#21464;&#37327;&#21464;&#25442;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#37327;&#21464;&#25442;&#65288;CoV&#65289;&#20844;&#24335;&#36890;&#36807;&#21487;&#35745;&#31639;&#38597;&#21487;&#27604;&#34892;&#21015;&#24335;&#30340;&#23398;&#20064;&#21464;&#25442;&#23558;&#22797;&#26434;&#30340;&#27010;&#29575;&#23494;&#24230;&#31616;&#21270;&#20026;&#31616;&#21333;&#30340;&#24418;&#24335;&#65292;&#26159;&#26368;&#22823;&#20284;&#28982;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#25512;&#26029;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#27169;&#22411;&#36873;&#25321;&#31561;&#39046;&#22495;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#34429;&#28982;&#24050;&#32463;&#20026;&#22810;&#31181;&#27169;&#22411;&#31867;&#22411;&#23548;&#20986;&#20102;CoV&#20844;&#24335;&#65292;&#20294;&#36825;&#20123;&#20449;&#24687;&#25955;&#24067;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25991;&#29486;&#20013;&#12290;&#26412;&#25991;&#20174;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#32479;&#19968;&#35270;&#35282;&#20986;&#21457;&#65292;&#23545;28&#20010;CoV&#20844;&#24335;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#30740;&#31350;&#65292;&#23558;&#30475;&#20284;&#19981;&#21516;&#30340;&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#21576;&#29616;&#20986;&#26469;&#65292;&#24378;&#35843;&#20102;&#25991;&#29486;&#20013;&#19981;&#24120;&#28165;&#26970;&#30340;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change-of-variables (CoV) formulas allow to reduce complicated probability densities to simpler ones by a learned transformation with tractable Jacobian determinant. They are thus powerful tools for maximum-likelihood learning, Bayesian inference, outlier detection, model selection, etc. CoV formulas have been derived for a large variety of model types, but this information is scattered over many separate works. We present a systematic treatment from the unifying perspective of encoder/decoder architectures, which collects 28 CoV formulas in a single place, reveals interesting relationships between seemingly diverse methods, emphasizes important distinctions that are not always clear in the literature, and identifies surprising gaps for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20272;&#35745;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26356;&#20934;&#30830;&#21644;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2308.02636</link><description>&lt;p&gt;
&#20174;&#25299;&#25169;&#23398;&#20013;&#23398;&#20064;&#65306;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#23431;&#23449;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure. (arXiv:2308.02636v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20272;&#35745;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26356;&#20934;&#30830;&#21644;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#21253;&#21547;&#30528;&#26377;&#20851;&#22522;&#30784;&#23431;&#23449;&#21442;&#25968;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#34429;&#28982;&#25345;&#32493;&#21516;&#35843;&#21487;&#20197;&#25552;&#21462;&#36825;&#31181;&#25299;&#25169;&#20449;&#24687;&#65292;&#20294;&#22914;&#20309;&#26368;&#20339;&#22320;&#20174;&#36825;&#20010;&#24037;&#20855;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#25345;&#32493;&#22270;&#20687;&#26144;&#23556;&#21040;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#32780;&#31934;&#30830;&#22320;&#20272;&#35745;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02632</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27169;&#25311;FMCW&#38647;&#36798;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22522;&#20110;&#23556;&#32447;&#36861;&#36394;&#65292;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#19988;&#19981;&#33021;&#32771;&#34385;&#32972;&#26223;&#22122;&#22768;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;FMCW&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;16&#20010;&#21516;&#26102;&#30340;&#33033;&#20914;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#24320;&#21457;&#29992;&#20110;&#22788;&#29702;&#38647;&#36798;&#25968;&#25454;&#65288;&#28388;&#27874;&#21644;&#32858;&#31867;&#65289;&#30340;&#31639;&#27861;&#12290;&#36825;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#19981;&#21487;&#22797;&#29616;&#30340;&#19981;&#23384;&#22312;&#25110;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25705;&#25176;&#36710;&#30340;&#38647;&#36798;&#27979;&#37327;&#25968;&#25454;&#23545;GAN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#29983;&#25104;&#25705;&#25176;&#36710;&#30452;&#32447;&#34892;&#39542;&#30340;&#21512;&#25104;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#20102;&#25705;&#25176;&#36710;&#30340;&#36317;&#31163;&#21644;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#37325;&#24314;&#25216;&#26415;&#65288;PHiRec&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;MRI&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20256;&#25773;MR&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02631</link><description>&lt;p&gt;
&#21152;&#36895;MRI&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction. (arXiv:2308.02631v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#27010;&#29575;&#37325;&#24314;&#25216;&#26415;&#65288;PHiRec&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;MRI&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#37325;&#24314;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20256;&#25773;MR&#37325;&#24314;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#25216;&#26415;&#22312;&#39640;&#24230;&#21152;&#36895;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20063;&#34987;&#30693;&#36947;&#22312;&#24847;&#22806;&#22833;&#36133;&#21644;&#20135;&#29983;&#34394;&#26500;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22914;&#26524;&#37325;&#24314;&#30452;&#25509;&#29992;&#20110;&#23454;&#26102;&#27835;&#30103;&#25351;&#23548;&#25110;&#33258;&#21160;&#25552;&#21462;&#20020;&#24202;&#21442;&#25968;&#65288;&#22914;&#36890;&#36807;&#20998;&#21106;&#65289;&#65292;&#36825;&#23601;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23558;&#26159;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#23433;&#20840;&#20351;&#29992;&#36825;&#39033;&#25216;&#26415;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#37325;&#24314;&#25216;&#26415;&#65288;PHiRec&#65289;&#65292;&#22522;&#20110;&#26465;&#20214;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#20197;&#21450;&#27604;&#20960;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#26356;&#20026;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20256;&#25773;MR&#37325;&#24314;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI reconstruction techniques based on deep learning have led to unprecedented reconstruction quality especially in highly accelerated settings. However, deep learning techniques are also known to fail unexpectedly and hallucinate structures. This is particularly problematic if reconstructions are directly used for downstream tasks such as real-time treatment guidance or automated extraction of clinical paramters (e.g. via segmentation). Well-calibrated uncertainty quantification will be a key ingredient for safe use of this technology in clinical practice. In this paper we propose a novel probabilistic reconstruction technique (PHiRec) building on the idea of conditional hierarchical variational autoencoders. We demonstrate that our proposed method produces high-quality reconstructions as well as uncertainty quantification that is substantially better calibrated than several strong baselines. We furthermore demonstrate how uncertainties arising in the MR econstruction can be propagate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;&#30340;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.02622</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring. (arXiv:2308.02622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;&#30340;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDGs&#65289;&#26159;&#32852;&#21512;&#22269;&#25552;&#20986;&#30340;&#26088;&#22312;&#40723;&#21169;&#26377;&#21161;&#20110;&#20445;&#38556;&#20154;&#31867;&#32321;&#33635;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#25919;&#31574;&#21644;&#27963;&#21160;&#12290;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#19968;&#20010;&#20844;&#21496;&#19982;&#27599;&#20010;17&#20010;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#35774;&#35745;&#20102;SDG&#26694;&#26550;&#25552;&#20379;&#20998;&#25968;&#12290;&#36825;&#31181;&#35780;&#20998;&#33021;&#22815;&#23545;&#28508;&#22312;&#24314;&#31435;&#21253;&#23481;&#21644;&#21487;&#25345;&#32493;&#32463;&#27982;&#30340;&#25237;&#36164;&#36827;&#34892;&#19968;&#33268;&#30340;&#35780;&#20272;&#12290;&#30001;&#20110;&#27492;&#31867;&#26694;&#26550;&#38656;&#35201;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#21019;&#24314;&#21644;&#32500;&#25252;&#23427;&#20204;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19981;&#21516;&#32593;&#32476;&#26469;&#28304;&#21644;&#19982;&#19968;&#32452;&#20844;&#21496;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#25968;&#25454;&#35757;&#32451;&#21644;&#37096;&#32626;&#20102;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting score
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#39640;&#38454;&#26631;&#37327;&#30340;&#24425;&#33394;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#20256;&#32479;&#30340;&#20108;&#38454;&#30697;&#38453;&#27169;&#22411;&#21040;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#30697;&#38453;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#25193;&#23637;&#20102;&#19968;&#20123;&#24120;&#29992;&#30340;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24425;&#33394;&#22270;&#20687;&#34917;&#20840;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02621</link><description>&lt;p&gt;
&#20351;&#29992;&#24191;&#20041;&#30697;&#38453;&#34917;&#20840;&#22312;&#39640;&#38454;&#26377;&#38480;&#32500;&#20195;&#25968;&#20013;&#24674;&#22797;&#24425;&#33394;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra. (arXiv:2308.02621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#39640;&#38454;&#26631;&#37327;&#30340;&#24425;&#33394;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#20256;&#32479;&#30340;&#20108;&#38454;&#30697;&#38453;&#27169;&#22411;&#21040;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#30697;&#38453;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#25193;&#23637;&#20102;&#19968;&#20123;&#24120;&#29992;&#30340;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24425;&#33394;&#22270;&#20687;&#34917;&#20840;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#26377;&#32570;&#22833;&#26465;&#30446;&#30340;&#24425;&#33394;&#22270;&#20687;&#34917;&#20840;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#39640;&#38454;&#26631;&#37327;&#30340;&#24674;&#22797;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20108;&#38454;&#30697;&#38453;&#27169;&#22411;&#25193;&#23637;&#20026;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#30697;&#38453;&#31561;&#25928;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;t-&#30697;&#38453;&#8221;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20687;&#32032;&#37051;&#22495;&#25193;&#23637;&#31574;&#30053;&#26469;&#34920;&#24449;&#23616;&#37096;&#20687;&#32032;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#8220;t-&#30697;&#38453;&#8221;&#27169;&#22411;&#23558;&#19968;&#20123;&#24120;&#29992;&#30340;&#30697;&#38453;&#21644;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#25193;&#23637;&#21040;&#23427;&#20204;&#30340;&#39640;&#38454;&#29256;&#26412;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;&#30340;&#21508;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#24191;&#20041;&#30697;&#38453;&#34917;&#20840;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#31639;&#27861;&#19982;&#23427;&#20204;&#30340;&#20302;&#38454;&#24352;&#37327;&#21644;&#20256;&#32479;&#30697;&#38453;&#23545;&#24212;&#29289;&#30456;&#27604;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the accuracy of color image completion with missing entries, we present a recovery method based on generalized higher-order scalars. We extend the traditional second-order matrix model to a more comprehensive higher-order matrix equivalent, called the "t-matrix" model, which incorporates a pixel neighborhood expansion strategy to characterize the local pixel constraints. This "t-matrix" model is then used to extend some commonly used matrix and tensor completion algorithms to their higher-order versions. We perform extensive experiments on various algorithms using simulated data and algorithms on simulated data and publicly available images and compare their performance. The results show that our generalized matrix completion model and the corresponding algorithm compare favorably with their lower-order tensor and conventional matrix counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#30340;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#20248;&#21270;&#36710;&#36742;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.02614</link><description>&lt;p&gt;
&#36710;&#36742;&#25511;&#21046;&#65306;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;
&lt;/p&gt;
&lt;p&gt;
Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning. (arXiv:2308.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#30340;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#20248;&#21270;&#36710;&#36742;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20154;&#21475;&#22686;&#38271;&#21644;&#36947;&#36335;&#19978;&#36710;&#36742;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#31649;&#29702;&#20132;&#36890;&#24182;&#30830;&#20445;&#23433;&#20840;&#24050;&#25104;&#20026;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24320;&#21457;&#36710;&#36742;&#26234;&#33021;&#25511;&#21046;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#36710;&#36742;&#30896;&#25758;&#36991;&#20813;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#21033;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;FDRL&#65289;&#25216;&#26415;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20986;&#34892;&#24310;&#35823;&#21644;&#25552;&#39640;&#36710;&#36742;&#30340;&#24179;&#22343;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23545;&#26412;&#22320;&#27169;&#22411;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#21644;&#20840;&#29699;&#27169;&#22411;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;FDDPG&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#20248;&#21270;&#36710;&#36742;&#30896;&#25758;&#36991;&#20813;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FDDPG&#31639;&#27861;&#22312;&#26377;&#25928;&#25511;&#21046;&#36710;&#36742;&#21644;&#39044;&#38450;&#30896;&#25758;&#26041;&#38754;&#20248;&#20110;DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and prev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02613</link><description>&lt;p&gt;
&#29992;SyntHIR&#23454;&#29616;&#20114;&#25805;&#20316;&#24615;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;CDSS&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools. (arXiv:2308.02613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#24739;&#32773;&#26085;&#24535;&#21644;&#20581;&#24247;&#30331;&#35760;&#26469;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26377;&#24456;&#22823;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#23454;&#26045;CDSS&#24037;&#20855;&#65292;&#38656;&#35201;&#23558;&#35813;&#24037;&#20855;&#38598;&#25104;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#22312;&#29992;&#20110;&#23384;&#20648;&#21644;&#31649;&#29702;&#24739;&#32773;&#25968;&#25454;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21512;&#35268;&#27861;&#35268;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#33719;&#24471;&#23545;EHR&#31995;&#32479;&#30340;&#24517;&#35201;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21644;&#20351;&#29992;CDSS&#24037;&#20855;&#24320;&#21457;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#30340;&#20307;&#31995;&#26550;&#26500;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#22312;&#19968;&#20010;&#31216;&#20026;SyntHIR&#30340;&#31995;&#32479;&#20013;&#23454;&#29616;&#12290;SyntHIR&#31995;&#32479;&#20351;&#29992;Fast Healthcare Interoperability Resources (FHIR)&#26631;&#20934;&#36827;&#34892;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#65292;&#20351;&#29992;Gretel&#26694;&#26550;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;Microsoft Azure FHIR&#26381;&#21153;&#22120;&#20316;&#20026;&#22522;&#20110;FHIR&#30340;EHR&#31995;&#32479;&#65292;&#20197;&#21450;&#20351;&#29992;SMART on FHIR&#26694;&#26550;&#36827;&#34892;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;CDSS&#24037;&#20855;&#26469;&#23637;&#31034;SyntHIR&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#35745;&#31639;&#21368;&#36733;&#30340;&#24310;&#36831;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;&#21040;&#36793;&#32536;&#21333;&#20803;&#65292;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#27599;&#36742;&#36710;&#30340;&#26368;&#20339;&#21368;&#36733;&#36873;&#39033;&#12290;</title><link>http://arxiv.org/abs/2308.02603</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#30340;&#35745;&#31639;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles. (arXiv:2308.02603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#36710;&#36742;&#20013;&#35745;&#31639;&#21368;&#36733;&#30340;&#24310;&#36831;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;&#21040;&#36793;&#32536;&#21333;&#20803;&#65292;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#36873;&#25321;&#27599;&#36742;&#36710;&#30340;&#26368;&#20339;&#21368;&#36733;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36710;&#36742;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#20219;&#21153;&#21368;&#36733;&#21040;&#36947;&#36335;&#36793;&#30340;&#21333;&#20803;(RSU)&#65292;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#22312;&#29289;&#32852;&#32593;&#36710;&#36742;(IoV)&#20013;&#21487;&#20197;&#20943;&#36731;&#36710;&#36733;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation
&lt;/p&gt;</description></item><item><title>Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.</title><link>http://arxiv.org/abs/2308.02599</link><description>&lt;p&gt;
&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators. (arXiv:2308.02599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02599
&lt;/p&gt;
&lt;p&gt;
Branched Latent Neural Operators (BLNOs) are introduced to learn input-output maps for encoding complex physical processes. BLNOs utilize interpretable latent outputs to enhance learned dynamics and overcome the curse of dimensionality, while also showing excellent generalization properties with small training datasets and short training times. The partial connection structure reduces the number of tunable parameters. BLNOs are proven effective in a challenging biophysically detailed test case.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#25903;&#28508;&#22312;&#31070;&#32463;&#31639;&#23376;&#65288;BLNOs&#65289;&#26469;&#23398;&#20064;&#32534;&#30721;&#22797;&#26434;&#29289;&#29702;&#36807;&#31243;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#12290;BLNO&#30001;&#19968;&#20010;&#31616;&#21333;&#32039;&#20945;&#30340;&#21069;&#39304;&#37096;&#20998;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#65292;&#35813;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#23558;&#19981;&#21516;&#22266;&#26377;&#35282;&#33394;&#30340;&#36755;&#20837;&#36827;&#34892;&#35299;&#31163;&#65292;&#20363;&#22914;&#23558;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#24863;&#20852;&#36259;&#30340;&#36890;&#29992;&#39046;&#22495;&#12290;BLNO&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#36755;&#20986;&#22686;&#24378;&#20102;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#22312;&#21333;&#20010;&#22788;&#29702;&#22120;&#19978;&#20351;&#29992;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#27979;&#35797;&#38454;&#27573;&#37319;&#29992;&#30340;&#31163;&#25955;&#21270;&#26041;&#24335;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#21487;&#27604;&#24615;&#12290;&#27492;&#22806;&#65292;&#37096;&#20998;&#36830;&#25509;&#22312;&#20840;&#36830;&#25509;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#26174;&#33879;&#20943;&#23569;&#20102;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BLNO&#22312;&#28041;&#21450;&#29983;&#29289;&#29289;&#29702;&#32454;&#33410;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed elect
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#65292;&#26088;&#22312;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02597</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36164;&#28304;&#39640;&#25928;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#65306;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#25552;&#39640;&#24739;&#32773;&#29983;&#23384;&#29575;
&lt;/p&gt;
&lt;p&gt;
Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries. (arXiv:2308.02597v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02597
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#65292;&#26088;&#22312;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#29305;&#21035;&#26159;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#12289;&#21335;&#20122;&#21644;&#21335;&#32654;&#30340;&#20083;&#33146;&#30284;&#24739;&#32773;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#23548;&#33268;&#20840;&#29699;&#27515;&#20129;&#29575;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#30001;&#20110;&#21463;&#35757;&#30149;&#29702;&#23398;&#23478;&#20005;&#37325;&#30701;&#32570;&#32780;&#23548;&#33268;&#30340;&#35786;&#26029;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#20174;&#32780;&#23548;&#33268;&#35786;&#26029;&#26102;&#36739;&#22823;&#27604;&#20363;&#30340;&#26202;&#26399;&#34920;&#29616;&#12290;&#21021;&#22987;&#30151;&#29366;&#21457;&#23637;&#21040;&#35786;&#26029;&#25509;&#21463;&#26102;&#38388;&#21487;&#33021;&#38271;&#36798;15&#20010;&#26376;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#30340;&#21307;&#30103;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#35786;&#26029;&#31995;&#32479;&#65292;&#26082;&#33021;&#36798;&#21040;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#65292;&#21448;&#33021;&#20445;&#35777;&#35745;&#31639;&#25928;&#29575;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#22522;&#20110;MobileNetV2&#30340;&#35786;&#26029;&#27169;&#22411;&#22312;&#35786;&#26029;&#20934;&#30830;&#24230;&#12289;&#27169;&#22411;&#27867;&#21270;&#24615;&#21644;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#36229;&#36807;&#20102;&#26356;&#22797;&#26434;&#30340;VGG16&#12289;ResNet50&#21644;ResNet101&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.02588</link><description>&lt;p&gt;
&#29992;&#24494;&#31505;&#25581;&#31034;&#24085;&#37329;&#26862;&#30149;&#65306;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31579;&#26597;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unmasking Parkinson's Disease with Smile: An AI-enabled Screening Framework. (arXiv:2308.02588v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#21487;&#38752;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26377;&#38480;&#30340;&#20020;&#24202;&#25252;&#29702;&#36164;&#28304;&#65292;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#30340;&#35786;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#24494;&#34920;&#24773;&#30340;&#26368;&#22823;&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;PD&#31579;&#26597;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;1,059&#21517;&#29420;&#31435;&#21442;&#19982;&#32773;&#30340;3,871&#20010;&#35270;&#39057;&#65292;&#20854;&#20013;&#21253;&#25324;256&#21517;&#33258;&#25253;PD&#24739;&#32773;&#12290;&#36825;&#20123;&#24405;&#20687;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65292;&#21253;&#25324;&#22810;&#20010;&#22269;&#23478;&#30340;&#21442;&#19982;&#32773;&#23478;&#20013;&#12289;&#19968;&#23478;&#35786;&#25152;&#21644;&#19968;&#20010;&#32654;&#22269;&#30340;PD&#25252;&#29702;&#26426;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#38754;&#37096;&#26631;&#24535;&#21644;&#34892;&#21160;&#21333;&#20301;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19982;PD&#30340;&#19968;&#20010;&#20027;&#35201;&#30151;&#29366;Hypomimia&#65288;&#38754;&#37096;&#34920;&#24773;&#20943;&#23569;&#65289;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#19968;&#32452;AI&#27169;&#22411;&#22312;&#20445;&#30041;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#65292;&#24182;&#19988;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20154;&#32676;&#23376;&#32452;&#19978;&#26080;&#21487;&#26816;&#27979;&#30340;&#20559;&#35265;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#20165;&#36890;&#36807;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#23601;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#21644;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#30333;&#20869;&#38556;&#25163;&#26415;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#22810;&#31867;&#22810;&#26631;&#31614;&#26465;&#20214;&#19979;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.02587</link><description>&lt;p&gt;
&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#32597;&#35265;&#30333;&#20869;&#38556;&#25163;&#26415;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models. (arXiv:2308.02587v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#30333;&#20869;&#38556;&#25163;&#26415;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#23545;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#22810;&#31867;&#22810;&#26631;&#31614;&#26465;&#20214;&#19979;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#20869;&#38556;&#25163;&#26415;&#26159;&#19968;&#20010;&#32463;&#24120;&#36827;&#34892;&#30340;&#38656;&#35201;&#33258;&#21160;&#21270;&#21644;&#20808;&#36827;&#36741;&#21161;&#31995;&#32479;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#27880;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31995;&#32479;&#30340;&#25968;&#25454;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#20063;&#21253;&#21547;&#25163;&#26415;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#30333;&#20869;&#38556;&#25163;&#26415;&#35270;&#39057;&#25968;&#25454;&#65292;&#38024;&#23545;&#39044;&#35757;&#32451;&#19979;&#28216;&#24037;&#20855;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#26368;&#24046;&#30340;&#38454;&#27573;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#24179;&#34913;&#20250;&#20351;&#20998;&#31867;&#22120;&#22312;&#23569;&#25968;&#34987;&#20302;&#20272;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#22797;&#26434;&#30340;&#22810;&#31867;&#22810;&#26631;&#31614;&#26465;&#20214;&#65288;&#22914;&#25163;&#26415;&#38454;&#27573;&#21644;&#25163;&#26415;&#24037;&#20855;&#32452;&#21512;&#65289;&#21512;&#25104;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30830;&#35748;&#21512;&#25104;&#30340;&#26679;&#26412;&#26174;&#31034;&#20986;&#20998;&#31867;&#22120;&#35782;&#21035;&#30340;&#24037;&#20855;&#12290;&#36825;&#20123;&#26679;&#26412;&#24456;&#38590;&#19982;&#30495;&#23454;&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.02585</link><description>&lt;p&gt;
&#23558;&#20195;&#29702;&#31574;&#30053;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#65306;&#36890;&#36807;&#21452;&#23618;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Aligning Agent Policy with Externalities: Reward Design via Bilevel RL. (arXiv:2308.02585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#22806;&#37096;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#65292;&#19978;&#23618;&#23398;&#20064;&#36866;&#24403;&#30340;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#22312;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#30340;&#24320;&#22987;&#22788;&#20551;&#35774;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#22312;&#36825;&#31181;&#22266;&#23450;&#22870;&#21169;&#33539;&#24335;&#19979;&#30340;&#23398;&#20064;&#20013;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#37325;&#35201;&#30340;&#31574;&#30053;&#20248;&#21270;&#32771;&#34385;&#22240;&#32032;&#65292;&#27604;&#22914;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#21644;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#31038;&#20250;&#31119;&#21033;&#12289;&#21487;&#25345;&#32493;&#24615;&#25110;&#24066;&#22330;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#21462;&#30340; emergent &#34892;&#20026;&#21644;&#21487;&#33021;&#19981;&#23545;&#40784;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25968;&#23398;&#21270;&#22320;&#27010;&#25324;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#19982;&#36825;&#31181;&#22806;&#22312;&#24615;&#23545;&#40784;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#19982;&#22996;&#25176;-&#20195;&#29702;&#26694;&#26550;&#30456;&#32852;&#31995;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22996;&#25176;&#20154;&#22312;&#19978;&#23618;&#30830;&#23450;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#20195;&#29702;&#20154;&#22312;&#19979;&#23618;&#35299;&#20915;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#19978;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#19968;&#20010;&#19982;&#26356;&#24191;&#27867;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#36866;&#24403;&#22870;&#21169;&#21442;&#25968;&#21270;&#65292;&#19979;&#23618;&#20219;&#21153;&#26159;&#23398;&#20064;&#20195;&#29702;&#20154;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#25511;&#22320;&#19979;&#40657;&#23458;&#35770;&#22363;&#26469;&#26816;&#27979;&#37326;&#22806;&#28431;&#27934;&#30340;&#21033;&#29992;&#12290;&#36890;&#36807;&#23545;CrimeBB&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36807;&#28388;&#24341;&#29992;CVE&#30340;&#20027;&#39064;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;Proof-of-Concept&#12289;&#27494;&#22120;&#21270;&#25110;&#28431;&#27934;&#21033;&#29992;&#12290;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#21487;&#20197;&#36798;&#21040;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#22823;&#20110;0.99&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#27494;&#22120;&#21270;&#21644;&#28431;&#27934;&#21033;&#29992;&#20043;&#38388;&#24615;&#36136;&#24046;&#24322;&#30340;insights&#65292;&#20197;&#21450;&#23545;&#40657;&#23458;&#31038;&#21306;&#21033;&#28070;&#21644;&#20854;&#20182;&#30456;&#20851;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.02581</link><description>&lt;p&gt;
&#22320;&#19979;&#32593;&#32476;&#35770;&#22363;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#28857;&#30340;&#26041;&#27861;&#65306;&#22902;&#27833;&#25735;&#21462;
&lt;/p&gt;
&lt;p&gt;
Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums. (arXiv:2308.02581v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#25511;&#22320;&#19979;&#40657;&#23458;&#35770;&#22363;&#26469;&#26816;&#27979;&#37326;&#22806;&#28431;&#27934;&#30340;&#21033;&#29992;&#12290;&#36890;&#36807;&#23545;CrimeBB&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36807;&#28388;&#24341;&#29992;CVE&#30340;&#20027;&#39064;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;Proof-of-Concept&#12289;&#27494;&#22120;&#21270;&#25110;&#28431;&#27934;&#21033;&#29992;&#12290;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#21487;&#20197;&#36798;&#21040;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#22823;&#20110;0.99&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#27494;&#22120;&#21270;&#21644;&#28431;&#27934;&#21033;&#29992;&#20043;&#38388;&#24615;&#36136;&#24046;&#24322;&#30340;insights&#65292;&#20197;&#21450;&#23545;&#40657;&#23458;&#31038;&#21306;&#21033;&#28070;&#21644;&#20854;&#20182;&#30456;&#20851;&#26041;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#30417;&#25511;&#22320;&#19979;&#40657;&#23458;&#35770;&#22363;&#26469;&#26816;&#27979;&#37326;&#22806;&#28431;&#27934;&#30340;&#21033;&#29992;&#12290;&#35752;&#35770;&#37326;&#22806;&#28431;&#27934;&#21033;&#29992;&#30340;&#24086;&#23376;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#33258;&#21160;&#22788;&#29702;&#24086;&#23376;&#21644;&#20027;&#39064;&#65292;&#24182;&#26681;&#25454;&#20854;&#20869;&#23481;&#35302;&#21457;&#35686;&#25253;&#12290;&#20026;&#20102;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;CrimeBB&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#22810;&#20010;&#22320;&#19979;&#35770;&#22363;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#36807;&#28388;&#24341;&#29992;CVE&#30340;&#20027;&#39064;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;Proof-of-Concept&#12289;&#27494;&#22120;&#21270;&#25110;&#28431;&#27934;&#21033;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65292;&#25105;&#20204;&#25351;&#20986;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#26469;&#35828;&#65292;&#21487;&#20197;&#36798;&#21040;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#22823;&#20110;0.99&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27494;&#22120;&#21270;&#21644;&#28431;&#27934;&#21033;&#29992;&#20043;&#38388;&#30340;&#24615;&#36136;&#24046;&#24322;&#25552;&#20379;&#20102;insights&#65292;&#20363;&#22914;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#36755;&#20986;&#65292;&#24182;&#20998;&#26512;&#20102;&#40657;&#23458;&#31038;&#21306;&#30340;&#21033;&#28070;&#21644;&#20854;&#20182;&#30456;&#20851;&#26041;&#38754;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#22320;&#19979;&#40657;&#23458;&#35770;&#22363;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#28857;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, ou
&lt;/p&gt;</description></item><item><title>PDS-Net is a novel framework for QoS prediction that effectively reduces errors resulting from noise data by utilizing a probabilistic space and a condition-based multitasking loss function.</title><link>http://arxiv.org/abs/2308.02580</link><description>&lt;p&gt;
Probabilistic Deep Supervision Network: &#19968;&#31181;&#25239;&#22122;&#22768;&#30340;QoS&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction. (arXiv:2308.02580v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02580
&lt;/p&gt;
&lt;p&gt;
PDS-Net is a novel framework for QoS prediction that effectively reduces errors resulting from noise data by utilizing a probabilistic space and a condition-based multitasking loss function.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;QoS&#65288;&#26381;&#21153;&#36136;&#37327;&#65289;&#30340;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#30693;&#30340;QoS&#20540;&#21487;&#20197;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QoS&#39044;&#27979;&#25216;&#26415;&#22312;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#65288;&#22914;&#34394;&#20551;&#20301;&#32622;&#20449;&#24687;&#25110;&#34394;&#25311;&#32593;&#20851;&#65289;&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QoS&#39044;&#27979;&#26694;&#26550;&#8212;&#8212;&#27010;&#29575;&#28145;&#24230;&#30417;&#30563;&#32593;&#32476;&#65288;PDS-Net&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;PDS-Net&#21033;&#29992;&#22522;&#20110;&#39640;&#26031;&#30340;&#27010;&#29575;&#31354;&#38388;&#30417;&#30563;&#20013;&#38388;&#23618;&#65292;&#24182;&#23398;&#20064;&#24050;&#30693;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;PDS-Net&#37319;&#29992;&#22522;&#20110;&#26465;&#20214;&#30340;&#22810;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#26469;&#35782;&#21035;&#20855;&#26377;&#22122;&#22768;&#25968;&#25454;&#30340;&#23545;&#35937;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#23545;&#35937;&#30340;&#27010;&#29575;&#31354;&#38388;&#19982;&#30495;&#23454;&#26631;&#31614;&#27010;&#29575;&#31354;&#38388;&#20043;&#38388;&#30340;Kullback-Leibler&#36317;&#31163;&#65292;&#30452;&#25509;&#23545;&#20174;&#27010;&#29575;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#28145;&#24230;&#29305;&#24449;&#36827;&#34892;&#30417;&#30563;&#12290;&#22240;&#27492;&#65292;PDS-Net&#26377;&#25928;&#20943;&#23569;&#20102;&#22240;&#20256;&#25773;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADRNet&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#30340;&#24191;&#20041;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#22823;&#22411;&#20844;&#24320;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#20808;&#21069;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#30340;&#24191;&#27867;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02571</link><description>&lt;p&gt;
ADRNet&#65306;&#32467;&#21512;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25968;&#25454;&#30340;&#24191;&#20041;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#29992;&#20110;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ADRNet: A Generalized Collaborative Filtering Framework Combining Clinical and Non-Clinical Data for Adverse Drug Reaction Prediction. (arXiv:2308.02571v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADRNet&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#30340;&#24191;&#20041;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#22823;&#22411;&#20844;&#24320;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#20808;&#21069;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#30340;&#24191;&#27867;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#65288;ADR&#65289;&#30340;&#39044;&#27979;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#33647;&#29289;&#21457;&#29616;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#38477;&#20302;&#24739;&#32773;&#27515;&#20129;&#29575;&#24182;&#22686;&#24378;&#33647;&#29289;&#23433;&#20840;&#24615;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#26377;&#25928;&#39044;&#27979;&#33647;&#29289;-ADR&#21457;&#29983;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#26377;&#25928;&#21033;&#29992;&#38750;&#20020;&#24202;&#25968;&#25454;&#65292;&#21363;&#26377;&#20851;&#33647;&#29289;&#30340;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#20449;&#24687;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#38454;&#27573;&#24456;&#23569;&#24314;&#31435;&#20869;&#23481;&#20998;&#26512;&#21644;&#32431;&#21327;&#21516;&#36807;&#28388;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#22810;&#26631;&#31614;ADR&#30340;&#39044;&#27979;&#24418;&#24335;&#21270;&#20026;&#33647;&#29289;-ADR&#21327;&#21516;&#36807;&#28388;&#38382;&#39064;&#65292;&#24182;&#19988;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#22312;&#20004;&#20010;&#22823;&#22411;&#20844;&#24320;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20808;&#21069;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#24191;&#27867;&#22522;&#20934;&#32467;&#26524;&#30340;&#24037;&#20316;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#38750;&#20020;&#24202;&#25968;&#25454;&#30340;&#26131;&#33719;&#24471;&#30340;&#33647;&#29289;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADRNet&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20020;&#24202;&#21644;&#38750;&#20020;&#24202;&#25968;&#25454;&#30340;&#24191;&#20041;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adverse drug reaction (ADR) prediction plays a crucial role in both health care and drug discovery for reducing patient mortality and enhancing drug safety. Recently, many studies have been devoted to effectively predict the drug-ADRs incidence rates. However, these methods either did not effectively utilize non-clinical data, i.e., physical, chemical, and biological information about the drug, or did little to establish a link between content-based and pure collaborative filtering during the training phase. In this paper, we first formulate the prediction of multi-label ADRs as a drug-ADR collaborative filtering problem, and to the best of our knowledge, this is the first work to provide extensive benchmark results of previous collaborative filtering methods on two large publicly available clinical datasets. Then, by exploiting the easy accessible drug characteristics from non-clinical data, we propose ADRNet, a generalized collaborative filtering framework combining clinical and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02570</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#23398;&#20064;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;NER
&lt;/p&gt;
&lt;p&gt;
Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(MNER)&#38754;&#20020;&#30340;&#25361;&#25112;&#20027;&#35201;&#26377;&#20004;&#26041;&#38754;: (1) &#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;; (2) &#21305;&#37197;&#23454;&#20307;&#19982;&#22270;&#20687;&#20013;&#20854;&#20851;&#32852;&#30340;&#29289;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#24212;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;BGA-MNER&#21253;&#25324;&#38024;&#23545;&#20004;&#31181;&#27169;&#24577;&#20013;&#30340;&#23454;&#20307;&#26174;&#33879;&#20869;&#23481;&#30340;\texttt{&#22270;&#20687;&#21040;&#25991;&#26412;}&#21644;\texttt{&#25991;&#26412;&#21040;&#22270;&#20687;}&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#21452;&#21521;&#37325;&#24314;&#30446;&#26631;&#26469;&#23545;&#40784;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22312;&#30452;&#25509;&#32780;&#24378;&#22823;&#30340;&#32422;&#26463;&#19979;&#23454;&#29616;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36890;&#24120;&#21253;&#21547;&#19981;&#21305;&#37197;&#30340;&#32452;&#20214;&#65292;&#23545;&#20110;&#29983;&#25104;&#26469;&#35828;&#26159;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38454;&#27573;&#24615;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#25552;&#21462;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our met
&lt;/p&gt;</description></item><item><title>&#21152;&#26435;&#22810;&#32423;&#29305;&#24449;&#20998;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#24191;&#21578;&#30340;CTR&#21644;&#23433;&#35013;&#39044;&#27979;&#65292;&#36890;&#36807;&#24037;&#31243;&#21270;&#29305;&#23450;&#29305;&#24449;&#21644;&#20849;&#20139;&#29305;&#24449;&#30340;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02568</link><description>&lt;p&gt;
&#21152;&#26435;&#22810;&#32423;&#29305;&#24449;&#20998;&#35299;&#22312;&#24212;&#29992;&#31243;&#24207;&#24191;&#21578;&#30340;CTR&#21644;&#23433;&#35013;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weighted Multi-Level Feature Factorization for App ads CTR and installation prediction. (arXiv:2308.02568v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02568
&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#22810;&#32423;&#29305;&#24449;&#20998;&#35299;&#26041;&#27861;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#24191;&#21578;&#30340;CTR&#21644;&#23433;&#35013;&#39044;&#27979;&#65292;&#36890;&#36807;&#24037;&#31243;&#21270;&#29305;&#23450;&#29305;&#24449;&#21644;&#20849;&#20139;&#29305;&#24449;&#30340;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#22242;&#38431;ISISTANITOS&#22312;ACM RecSys Challenge 2023&#20013;&#25152;&#20351;&#29992;&#30340;&#26041;&#27861;&#27010;&#36848;&#12290;&#35813;&#31454;&#36187;&#30001;ShareChat&#32452;&#32455;&#65292;&#28041;&#21450;&#39044;&#27979;&#29992;&#25143;&#28857;&#20987;&#24212;&#29992;&#31243;&#24207;&#24191;&#21578;&#21644;/&#25110;&#23433;&#35013;&#24212;&#29992;&#31243;&#24207;&#30340;&#27010;&#29575;&#65292;&#20197;&#25913;&#21892;&#28145;&#24230;&#28431;&#26007;&#20248;&#21270;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#29992;&#25143;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#28857;&#20987;&#21644;&#23433;&#35013;&#30340;&#27010;&#29575;&#25512;&#26029;&#20026;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#19968;&#32452;&#20849;&#20139;&#29305;&#24449;&#24037;&#31243;&#21270;&#20102;&#19968;&#32452;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;&#21152;&#26435;&#22810;&#32423;&#29305;&#24449;&#20998;&#35299;&#65292;&#22240;&#20026;&#23427;&#32771;&#34385;&#21040;&#19981;&#21516;&#32423;&#21035;&#29305;&#24449;&#30340;&#20132;&#20114;&#65292;&#20854;&#20013;&#32423;&#21035;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#30456;&#20851;&#12290;&#32473;&#23450;&#20219;&#21153;&#30340;&#39044;&#27979;&#26159;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#19978;&#32452;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#20849;&#20139;&#29305;&#24449;&#26469;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#39046;&#22495;&#30340;&#27604;&#36187;&#20013;&#33719;&#24471;&#20102;11&#21517;&#21644;55&#30340;&#24635;&#20307;&#24471;&#20998;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#32593;&#22336;&#21457;&#24067;&#20102;&#28304;&#20195;&#30721;&#65306;https://githu
&lt;/p&gt;
&lt;p&gt;
This paper provides an overview of the approach we used as team ISISTANITOS for the ACM RecSys Challenge 2023. The competition was organized by ShareChat, and involved predicting the probability of a user clicking an app ad and/or installing an app, to improve deep funnel optimization and a special focus on user privacy. Our proposed method inferring the probabilities of clicking and installing as two different, but related tasks. Hence, the model engineers a specific set of features for each task and a set of shared features. Our model is called Weighted Multi-Level Feature Factorization because it considers the interaction of different order features, where the order is associated to the depth in a neural network. The prediction for a given task is generated by combining the task specific and shared features on the different levels. Our submission achieved the 11 rank and overall score of 55 in the competition academia-track final results. We release our source code at: https://githu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65292;&#20855;&#26377;&#20248;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#25216;&#26415;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02560</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#26631;&#35760;&#21040;&#39640;&#20445;&#30495;&#38899;&#39057;&#65306;&#20351;&#29992;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion. (arXiv:2308.02560v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65292;&#20855;&#26377;&#20248;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#25216;&#26415;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#34920;&#31034;&#65288;&#22914;mel&#39057;&#35889;&#12289;MFCC&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#38899;&#39057;&#12290;&#26368;&#36817;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#26681;&#25454;&#39640;&#24230;&#21387;&#32553;&#30340;&#34920;&#31034;&#21512;&#25104;&#38899;&#39057;&#27874;&#24418;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#24403;&#26465;&#20214;&#19981;&#23436;&#32654;&#26102;&#65292;&#26131;&#20110;&#20135;&#29983;&#21487;&#21548;&#21040;&#30340;&#20266;&#24433;&#12290;&#21478;&#19968;&#31181;&#24314;&#27169;&#26041;&#27861;&#26159;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#29992;&#20316;&#35821;&#38899;&#27169;&#22411;&#65288;&#25110;&#22522;&#20110;mel&#39057;&#35889;&#30340;&#26465;&#20214;&#27169;&#22411;&#65289;&#25110;&#29983;&#25104;&#30456;&#23545;&#36739;&#20302;&#37319;&#26679;&#29575;&#30340;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20445;&#30495;&#22810;&#39057;&#24102;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#20302;&#27604;&#29305;&#29575;&#30340;&#31163;&#25955;&#34920;&#31034;&#29983;&#25104;&#20219;&#20309;&#31867;&#22411;&#30340;&#38899;&#39057;&#24418;&#24335;&#65288;&#22914;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#29615;&#22659;&#22768;&#38899;&#65289;&#12290;&#22312;&#30456;&#21516;&#30340;&#27604;&#29305;&#29575;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24863;&#30693;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation cod
&lt;/p&gt;</description></item><item><title>DLSIA&#26159;&#19968;&#31181;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21152;&#36895;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.02559</link><description>&lt;p&gt;
DLSIA: &#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DLSIA: Deep Learning for Scientific Image Analysis. (arXiv:2308.02559v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02559
&lt;/p&gt;
&lt;p&gt;
DLSIA&#26159;&#19968;&#31181;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21152;&#36895;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DLSIA&#65288;Deep Learning for Scientific Image Analysis&#65289;&#30340;&#22522;&#20110;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#35813;&#24211;&#20026;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#23450;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#19979;&#28216;&#25968;&#25454;&#22788;&#29702;&#25110;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#29615;&#22659;&#12290;DLSIA&#21253;&#21547;&#26131;&#20110;&#20351;&#29992;&#30340;&#26550;&#26500;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21487;&#35843;&#30340;U-Net&#21644;&#21442;&#25968;&#31934;&#31616;&#30340;&#28151;&#21512;&#23610;&#24230;&#31264;&#23494;&#32593;&#32476;&#65288;MSDNets&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20351;&#29992;&#38543;&#26426;&#22270;&#21644;&#31232;&#30095;&#36830;&#25509;&#29983;&#25104;&#30340;&#31232;&#30095;&#28151;&#21512;&#23610;&#24230;&#32593;&#32476;&#65288;SMSNets&#65289;&#12290;&#38543;&#30528;&#23454;&#39564;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;DLSIA&#25552;&#20379;&#20102;&#26131;&#20110;&#35775;&#38382;&#30340;CNN&#26500;&#24314;&#21644;&#25277;&#35937;CNN&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#31185;&#23398;&#23478;&#33021;&#22815;&#23450;&#21046;&#20182;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21152;&#36895;&#21457;&#29616;&#65292;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#24182;&#25512;&#21160;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DLSIA (Deep Learning for Scientific Image Analysis), a Python-based machine learning library that empowers scientists and researchers across diverse scientific domains with a range of customizable convolutional neural network (CNN) architectures for a wide variety of tasks in image analysis to be used in downstream data processing, or for experiment-in-the-loop computing scenarios. DLSIA features easy-to-use architectures such as autoencoders, tunable U-Nets, and parameter-lean mixed-scale dense networks (MSDNets). Additionally, we introduce sparse mixed-scale networks (SMSNets), generated using random graphs and sparse connections. As experimental data continues to grow in scale and complexity, DLSIA provides accessible CNN construction and abstracts CNN complexities, allowing scientists to tailor their machine learning approaches, accelerate discoveries, foster interdisciplinary collaboration, and advance research in scientific image analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;DNN&#30340;&#20302;&#21151;&#32791;&#21644;&#33410;&#33021;&#23454;&#29616;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#20943;&#23569;&#31639;&#26415;&#25805;&#20316;&#25968;&#37327;&#25913;&#21892;&#20102;DNN&#30340;&#21487;&#37096;&#32626;&#24615;&#65292;&#24182;&#19988;&#19981;&#26174;&#33879;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02553</link><description>&lt;p&gt;
&#23545;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Computer Vision Techniques for Internet-of-Things Devices. (arXiv:2308.02553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;DNN&#30340;&#20302;&#21151;&#32791;&#21644;&#33410;&#33021;&#23454;&#29616;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#20943;&#23569;&#31639;&#26415;&#25805;&#20316;&#25968;&#37327;&#25913;&#21892;&#20102;DNN&#30340;&#21487;&#37096;&#32626;&#24615;&#65292;&#24182;&#19988;&#19981;&#26174;&#33879;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#35299;&#20915;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;DNN&#38656;&#35201;&#25968;&#21313;&#20159;&#30340;&#21442;&#25968;&#21644;&#25805;&#20316;&#25165;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#35201;&#27714;&#20351;&#24471;DNN&#38750;&#24120;&#20381;&#36182;&#35745;&#31639;&#12289;&#20869;&#23384;&#21644;&#33021;&#37327;&#65292;&#24182;&#19988;&#38590;&#20197;&#22312;&#20855;&#26377;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#30340;&#23567;&#22411;&#30005;&#27744;&#20379;&#30005;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#37096;&#32626;&#12290;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;DNN&#65292;&#22914;&#20132;&#36890;&#25668;&#20687;&#22836;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#33258;&#21160;&#20107;&#25925;&#26816;&#27979;&#21644;&#32039;&#24613;&#21709;&#24212;&#31561;&#24212;&#29992;&#26469;&#25552;&#39640;&#20844;&#20849;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#20302;&#21151;&#29575;&#21644;&#33410;&#33021;&#30340;DNN&#23454;&#29616;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#23454;&#29616;&#25552;&#39640;&#20102;DNN&#30340;&#21487;&#37096;&#32626;&#24615;&#32780;&#19981;&#26174;&#33879;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#25216;&#26415;&#35201;&#20040;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#65292;&#35201;&#20040;&#20943;&#23569;&#31639;&#26415;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#20998;&#20026;&#19977;&#22823;&#31867;&#65306;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#65292;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#21644;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are state-of-the-art techniques for solving most computer vision problems. DNNs require billions of parameters and operations to achieve state-of-the-art results. This requirement makes DNNs extremely compute, memory, and energy-hungry, and consequently difficult to deploy on small battery-powered Internet-of-Things (IoT) devices with limited computing resources. Deployment of DNNs on Internet-of-Things devices, such as traffic cameras, can improve public safety by enabling applications such as automatic accident detection and emergency response.Through this paper, we survey the recent advances in low-power and energy-efficient DNN implementations that improve the deployability of DNNs without significantly sacrificing accuracy. In general, these techniques either reduce the memory requirements, the number of arithmetic operations, or both. The techniques can be divided into three major categories: neural network compression, network architecture search and 
&lt;/p&gt;</description></item><item><title>qgym&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#32534;&#35793;&#30340;&#20581;&#36523;&#25151;&#65292;&#26088;&#22312;&#20248;&#21270;&#24182;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#19982;&#37327;&#23376;&#32534;&#35793;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.02536</link><description>&lt;p&gt;
qgym: &#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#32534;&#35793;&#30340;&#20581;&#36523;&#25151;
&lt;/p&gt;
&lt;p&gt;
qgym: A Gym for Training and Benchmarking RL-Based Quantum Compilation. (arXiv:2308.02536v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02536
&lt;/p&gt;
&lt;p&gt;
qgym&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#32534;&#35793;&#30340;&#20581;&#36523;&#25151;&#65292;&#26088;&#22312;&#20248;&#21270;&#24182;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#19982;&#37327;&#23376;&#32534;&#35793;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37327;&#23376;&#30005;&#36335;&#32534;&#35793;&#20026;&#29305;&#23450;&#30340;&#37327;&#23376;&#30828;&#20214;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#23384;&#22312;&#20005;&#37325;&#30340;&#30828;&#20214;&#38480;&#21046;&#12290;&#20026;&#20102;&#21457;&#25381;&#26377;&#38480;&#36164;&#28304;&#30340;&#26368;&#22823;&#20316;&#29992;&#65292;&#32534;&#35793;&#36807;&#31243;&#24212;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#25913;&#36827;&#24403;&#21069;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#20854;&#20013;&#20195;&#29702;&#19982;&#29615;&#22659;&#20132;&#20114;&#20197;&#23398;&#20064;&#22797;&#26434;&#31574;&#30053;&#20197;&#36798;&#21040;&#29305;&#23450;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;qgym&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;OpenAI gym&#27966;&#29983;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#20197;&#21450;&#19987;&#38376;&#38024;&#23545;&#37327;&#23376;&#32534;&#35793;&#30340;&#29615;&#22659;&#12290;qgym&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25277;&#35937;&#23545;&#20110;&#20004;&#20010;&#39046;&#22495;&#37117;&#26080;&#20851;&#30340;&#36807;&#31243;&#30340;&#37096;&#20998;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#30740;&#31350;&#39046;&#22495;&#19982;&#37327;&#23376;&#32534;&#35793;&#30456;&#36830;&#25509;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#22312;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;RL&#20195;&#29702;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compiling a quantum circuit for specific quantum hardware is a challenging task. Moreover, current quantum computers have severe hardware limitations. To make the most use of the limited resources, the compilation process should be optimized. To improve currents methods, Reinforcement Learning (RL), a technique in which an agent interacts with an environment to learn complex policies to attain a specific goal, can be used. In this work, we present qgym, a software framework derived from the OpenAI gym, together with environments that are specifically tailored towards quantum compilation. The goal of qgym is to connect the research fields of Artificial Intelligence (AI) with quantum compilation by abstracting parts of the process that are irrelevant to either domain. It can be used to train and benchmark RL agents and algorithms in highly customizable environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02535</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21040;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;Robusta&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19979;&#28216;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#12289;&#20998;&#24067;&#21464;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#24615;&#20851;&#38190;&#24494;&#35843;&#65288;RiFT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38750;&#20851;&#38190;&#40065;&#26834;&#27169;&#22359;&#19978;&#24494;&#35843;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#25439;&#23475;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02533</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#20851;&#38190;&#40065;&#26834;&#24615;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#25239;&#35757;&#32451;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning. (arXiv:2308.02533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#24615;&#20851;&#38190;&#24494;&#35843;&#65288;RiFT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38750;&#20851;&#38190;&#40065;&#26834;&#27169;&#22359;&#19978;&#24494;&#35843;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#25439;&#23475;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26159;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#25104;&#29087;&#25216;&#26415;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#30340;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#40065;&#26834;&#24615;&#20851;&#38190;&#24494;&#35843;&#65288;RiFT&#65289;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;RiFT&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#38750;&#20851;&#38190;&#40065;&#26834;&#27169;&#22359;&#19978;&#24494;&#35843;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#20887;&#20313;&#23481;&#37327;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22359;&#40065;&#26834;&#20851;&#38190;&#24615;&#65288;MRC&#65289;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#27169;&#22359;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26435;&#37325;&#25200;&#21160;&#19979;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#25214;&#21040;&#20855;&#26377;&#26368;&#20302;MRC&#20540;&#30340;&#27169;&#22359;&#20316;&#20026;&#38750;&#20851;&#38190;&#40065;&#26834;&#27169;&#22359;&#65292;&#24182;&#24494;&#35843;&#20854;&#26435;&#37325;&#20197;&#33719;&#24471;&#24494;&#35843;&#21518;&#30340;&#26435;&#37325;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#26435;&#37325;&#21644;&#24494;&#35843;&#21518;&#30340;&#26435;&#37325;&#20043;&#38388;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to adversarial examples, posing a significant security risk in critical applications. Adversarial Training (AT) is a well-established technique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by fine-tuning the adversarially trained model on its non-robust-critical module. To do so, we introduce module robust criticality (MRC), a measure that evaluates the significance of a given module to model robustness under worst-case weight perturbations. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module and fine-tune its weights to obtain fine-tuned weights. Subsequently, we linearly interpolate between the adversarially trained weight
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20855;&#22791;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#30456;&#24403;&#30340;&#39640;&#32423;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02522</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT and GPT-4 for Visual Programming. (arXiv:2308.02522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20855;&#22791;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#30456;&#24403;&#30340;&#39640;&#32423;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#21453;&#39304;&#21644;&#20869;&#23481;&#26469;&#26497;&#22823;&#25913;&#21892;&#35745;&#31639;&#26426;&#25945;&#32946;&#30340;&#26684;&#23616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#20123;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#31243;&#65292;&#29305;&#21035;&#26159;Python&#32534;&#31243;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#35299;&#31572;&#36825;&#20123;&#27169;&#22411;&#22312;K-8&#32534;&#31243;&#25945;&#32946;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65306;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#19982;&#23427;&#20204;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#20013;&#30456;&#24403;&#30340;&#35270;&#35273;&#32534;&#31243;&#33021;&#21147;&#65311;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#21644;GPT-4&#65292;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;MDPs&#30340;&#27010;&#29575;&#21452;&#21521;&#24314;&#27169;&#30340;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.02519</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;MDPs&#30340;&#27010;&#29575;&#21452;&#21521;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Improving Probabilistic Bisimulation for MDPs Using Machine Learning. (arXiv:2308.02519v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;MDPs&#30340;&#27010;&#29575;&#21452;&#21521;&#24314;&#27169;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26816;&#39564;&#34987;&#25552;&#35758;&#20316;&#20026;&#19968;&#31181;&#24418;&#24335;&#21270;&#39564;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#20998;&#26512;&#20851;&#38190;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#29992;&#20110;&#22797;&#26434;&#31995;&#32479;&#26102;&#65292;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#29366;&#24577;&#31354;&#38388;&#29190;&#28856;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21452;&#21521;&#26368;&#23567;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#24102;&#26631;&#35760;&#30340;&#36716;&#25442;&#31995;&#32479;&#20013;&#30340;&#29366;&#24577;&#25968;&#65292;&#26088;&#22312;&#20811;&#26381;&#19982;&#29366;&#24577;&#31354;&#38388;&#29190;&#28856;&#38382;&#39064;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;&#23545;&#20110;&#20855;&#26377;&#38543;&#26426;&#34892;&#20026;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#27010;&#29575;&#21452;&#21521;&#24314;&#27169;&#26469;&#26368;&#23567;&#21270;&#32473;&#23450;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#20855;&#26377;&#36739;&#23569;&#29366;&#24577;&#30340;&#31561;&#25928;&#24418;&#24335;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#38477;&#20302;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#38750;&#30830;&#23450;&#24615;&#34892;&#20026;&#30340;&#38543;&#26426;&#31995;&#32479;&#30340;&#27010;&#29575;&#21452;&#21521;&#24314;&#27169;&#30340;&#36845;&#20195;&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#23558;&#32473;&#23450;&#27010;&#29575;&#27169;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#20026;&#20854;&#21452;&#21521;&#24314;&#27169;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of model checking has been suggested as a formal verification technique for analyzing critical systems. However, the primary challenge in applying to complex systems is state space explosion problem. To address this issue, bisimulation minimization has emerged as a prominent method for reducing the number of states in a labeled transition system, aiming to overcome the difficulties associated with the state space explosion problem. In the case of systems exhibiting stochastic behaviors, probabilistic bisimulation is employed to minimize a given model, obtaining its equivalent form with fewer states. Recently, various techniques have been introduced to decrease the time complexity of the iterative methods used to compute probabilistic bisimulation for stochastic systems that display nondeterministic behaviors. In this paper, we propose a new technique to partition the state space of a given probabilistic model to its bisimulation classes. This technique uses the PRISM pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#37325;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;EEG&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26102;&#23384;&#22312;&#30340;&#20302;&#20449;&#22122;&#27604;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#38477;&#20302;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02515</link><description>&lt;p&gt;
&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#29305;&#24449;&#37325;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Feature Reweighting for EEG-based Motor Imagery Classification. (arXiv:2308.02515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#37325;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;EEG&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26102;&#23384;&#22312;&#30340;&#20302;&#20449;&#22122;&#27604;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#38477;&#20302;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#20998;&#31867;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#29992;&#20110;&#39044;&#27979;&#20027;&#20307;&#32930;&#20307;&#31227;&#21160;&#30340;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;MI-EEG&#20449;&#21495;&#20998;&#31867;&#30340;&#25361;&#25112;&#21253;&#25324;&#20449;&#22122;&#27604;&#20302;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;EEG&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#12290;&#22522;&#20110;CNN&#30340;&#32593;&#32476;&#35745;&#31639;&#24471;&#21040;&#30340;MI-EEG&#20449;&#21495;&#29305;&#24449;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#30001;&#22122;&#22768;&#21644;&#26080;&#20851;&#29305;&#24449;&#35745;&#31639;&#24471;&#21040;&#30340;CNN&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#20063;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26080;&#29992;&#30340;&#29305;&#24449;&#24120;&#24120;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#38477;&#20302;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#21152;&#26435;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of motor imagery (MI) using non-invasive electroencephalographic (EEG) signals is a critical objective as it is used to predict the intention of limb movements of a subject. In recent research, convolutional neural network (CNN) based methods have been widely utilized for MI-EEG classification. The challenges of training neural networks for MI-EEG signals classification include low signal-to-noise ratio, non-stationarity, non-linearity, and high complexity of EEG signals. The features computed by CNN-based networks on the highly noisy MI-EEG signals contain irrelevant information. Subsequently, the feature maps of the CNN-based network computed from the noisy and irrelevant features contain irrelevant information. Thus, many non-contributing features often mislead the neural network training and degrade the classification performance. Hence, a novel feature reweighting approach is proposed to address this issue. The proposed method gives a noise reduction mechanism named
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.02514</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#26041;&#31243;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#26041;&#31243;&#22312;&#24314;&#27169;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#28982;&#32780;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#35299;&#20915;&#20027;&#26041;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24212;&#29992;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20027;&#26041;&#31243;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#36895;&#29575;&#21442;&#25968;&#12289;&#21021;&#22987;&#26465;&#20214;&#21644;&#26102;&#38388;&#20540;&#30452;&#25509;&#26144;&#23556;&#21040;&#19982;&#36755;&#20837;&#19978;&#19979;&#25991;&#23436;&#20840;&#21305;&#37197;&#30340;&#29366;&#24577;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#36817;&#20284;&#22320;&#27714;&#35299;&#20102;&#20027;&#26041;&#31243;&#30340;&#26368;&#19968;&#33324;&#24418;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21453;&#39304;&#22870;&#21169;&#30001;&#19968;&#32452;&#21464;&#20998;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#20379;&#12290;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#24456;&#39640;&#12290;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39592;&#26894;&#39592;&#39612;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#20851;&#32852;&#26894;&#39592;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#36991;&#20813;&#20102;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21253;&#21547;&#26894;&#24339;&#26681;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.02509</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#26102;&#33410;&#28857;&#21644;&#36793;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39592;&#26894;&#39592;&#39612;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks. (arXiv:2308.02509v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39592;&#26894;&#39592;&#39612;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#20851;&#32852;&#26894;&#39592;&#30340;&#20301;&#32622;&#21644;&#26041;&#21521;&#65292;&#36991;&#20813;&#20102;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21253;&#21547;&#26894;&#24339;&#26681;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CT&#25195;&#25551;&#20013;&#33258;&#21160;&#23450;&#20301;&#21644;&#35782;&#21035;&#26894;&#39592;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#22823;&#37096;&#20998;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#26894;&#39592;&#30340;&#20301;&#32622;&#23450;&#20301;&#65292;&#24573;&#30053;&#20102;&#20854;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#20854;&#27969;&#31243;&#20013;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#21487;&#33021;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20013;&#23545;&#24322;&#24120;&#24773;&#20917;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#27969;&#31243;&#65292;&#23427;&#20351;&#29992;&#26631;&#20934;&#30340;U-Net&#36827;&#34892;&#39044;&#27979;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20851;&#32852;&#21644;&#20998;&#31867;&#20855;&#26377;&#23436;&#25972;&#26041;&#21521;&#30340;&#26894;&#39592;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26894;&#39592;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#21547;&#20102;&#19982;&#26894;&#39592;&#20307;&#30456;&#20851;&#32852;&#30340;&#26894;&#24339;&#26681;&#26816;&#27979;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22320;&#26631;&#39044;&#27979;&#12289;&#20851;&#32852;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#20851;&#32852;&#27491;&#30830;&#30340;&#26894;&#39592;&#20307;&#21644;&#26894;&#24339;&#26681;&#22320;&#26631;&#65292;&#24573;&#30053;&#35823;&#25253;&#65292;&#24182;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#12289;&#23436;&#20840;&#21487;&#35757;&#32451;&#30340;&#27969;&#31243;&#20013;&#23545;&#26894;&#39592;&#36827;&#34892;&#20998;&#31867;&#65292;&#36991;&#20813;&#20102;&#29305;&#23450;&#24212;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic vertebra localization and identification in CT scans is important for numerous clinical applications. Much progress has been made on this topic, but it mostly targets positional localization of vertebrae, ignoring their orientation. Additionally, most methods employ heuristics in their pipeline that can be sensitive in real clinical images which tend to contain abnormalities. We introduce a simple pipeline that employs a standard prediction with a U-Net, followed by a single graph neural network to associate and classify vertebrae with full orientation. To test our method, we introduce a new vertebra dataset that also contains pedicle detections that are associated with vertebra bodies, creating a more challenging landmark prediction, association and classification task. Our method is able to accurately associate the correct body and pedicle landmarks, ignore false positives and classify vertebrae in a simple, fully trainable pipeline avoiding application-specific heuristics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#38750;&#29983;&#29289;&#21307;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#21644;&#20313;&#24358;&#36317;&#31163;&#35780;&#20272;&#31867;&#20869;&#22810;&#26679;&#24615;&#65292;&#20351;&#29992;Frechet Inception&#36317;&#31163;&#35780;&#20272;&#36136;&#37327;&#12290;&#35780;&#20272;&#36825;&#20123;&#24230;&#37327;&#23545;&#20110;&#20102;&#35299;&#21512;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.02505</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#38750;&#29983;&#29289;&#21307;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting. (arXiv:2308.02505v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#38750;&#29983;&#29289;&#21307;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#21644;&#20313;&#24358;&#36317;&#31163;&#35780;&#20272;&#31867;&#20869;&#22810;&#26679;&#24615;&#65292;&#20351;&#29992;Frechet Inception&#36317;&#31163;&#35780;&#20272;&#36136;&#37327;&#12290;&#35780;&#20272;&#36825;&#20123;&#24230;&#37327;&#23545;&#20110;&#20102;&#35299;&#21512;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#25968;&#25454;&#19981;&#24179;&#34913;&#22312;&#22810;&#31181;&#25104;&#20687;&#27169;&#24577;&#20013;&#24456;&#24120;&#35265;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#20043;&#19968;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#12290;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29305;&#24449;&#23545;&#20110;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#21151;&#25928;&#38750;&#24120;&#25935;&#24863;&#12290;&#24403;&#35780;&#20272;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#19979;&#30340;&#21512;&#25104;&#22270;&#20687;&#26102;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#23545;&#24230;&#37327;&#20998;&#25968;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#30495;&#23454;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26469;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#12290;&#22810;&#23610;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#37327;&#21644;&#20313;&#24358;&#36317;&#31163;&#34987;&#29992;&#20110;&#35780;&#20272;&#31867;&#20869;&#22810;&#26679;&#24615;&#65292;&#32780;Frechet Inception&#36317;&#31163;&#29992;&#20110;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#21644;&#38750;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#30340;&#36825;&#20123;&#24230;&#37327;&#23545;&#20110;&#30740;&#31350;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#30340;&#30693;&#24773;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#23454;&#35777;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#21644;&#38750;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#21355;&#26143;&#22270;&#20687;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#22622;&#28006;&#36335;&#26031;&#20892;&#26449;&#22320;&#21306;&#30340;&#38750;&#27861;&#22403;&#22334;&#20542;&#20498;&#22330;&#25152;&#65292;&#24182;&#20026;&#30456;&#20851;&#37096;&#38376;&#25552;&#20379;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#22403;&#22334;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.02502</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#21355;&#26143;&#22270;&#20687;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#35782;&#21035;&#22622;&#28006;&#36335;&#26031;&#20892;&#26449;&#22320;&#21306;&#30340;&#22403;&#22334;&#20542;&#20498;&#22330;&#25152;
&lt;/p&gt;
&lt;p&gt;
The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery. (arXiv:2308.02502v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02502
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21355;&#26143;&#22270;&#20687;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#35782;&#21035;&#22622;&#28006;&#36335;&#26031;&#20892;&#26449;&#22320;&#21306;&#30340;&#38750;&#27861;&#22403;&#22334;&#20542;&#20498;&#22330;&#25152;&#65292;&#24182;&#20026;&#30456;&#20851;&#37096;&#38376;&#25552;&#20379;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#22403;&#22334;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22403;&#22334;&#22788;&#29702;&#26159;&#21457;&#36798;&#22269;&#23478;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#22312;&#22622;&#28006;&#36335;&#26031;&#65292;&#38750;&#27861;&#30340;&#8220;&#20081;&#20498;&#22403;&#22334;&#8221;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20892;&#26449;&#22320;&#21306;&#20960;&#20046;&#27809;&#26377;&#21512;&#27861;&#30340;&#22403;&#22334;&#22788;&#29702;&#36873;&#39033;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#36825;&#19968;&#38382;&#39064;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#20063;&#27809;&#26377;&#36275;&#22815;&#36164;&#28304;&#26469;&#35299;&#20915;&#23427;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#21355;&#26143;&#22270;&#20687;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#21542;&#29992;&#20110;&#35782;&#21035;&#22622;&#28006;&#36335;&#26031;&#20892;&#26449;&#22320;&#21306;&#30340;&#38750;&#27861;&#22403;&#22334;&#20542;&#20498;&#22330;&#25152;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23558;&#20854;&#20998;&#20026;&#21253;&#21547;&#22403;&#22334;&#21644;&#19981;&#21253;&#21547;&#22403;&#22334;&#20004;&#20010;&#31867;&#21035;&#12290;&#30001;&#20110;&#25910;&#38598;&#36825;&#31181;&#25968;&#25454;&#38598;&#30340;&#21407;&#22987;&#25968;&#37327;&#32791;&#26102;&#19988;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#20808;&#25910;&#38598;&#20102;&#19968;&#32452;&#30456;&#23545;&#36739;&#23567;&#30340;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#27880;&#37322;&#20013;&#23398;&#20064;&#20998;&#21106;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#30456;&#20851;&#24615;&#21644;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22122;&#22768;&#27880;&#37322;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02498</link><description>&lt;p&gt;
&#20174;&#22122;&#22768;&#27880;&#37322;&#20013;&#23398;&#20064;&#20998;&#21106;&#65306;&#19968;&#31181;&#31354;&#38388;&#20462;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Segment from Noisy Annotations: A Spatial Correction Approach. (arXiv:2308.02498v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#27880;&#37322;&#20013;&#23398;&#20064;&#20998;&#21106;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32534;&#30721;&#31354;&#38388;&#30456;&#20851;&#24615;&#21644;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22122;&#22768;&#27880;&#37322;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24615;&#33021;&#36890;&#24120;&#20250;&#21463;&#21040;&#22024;&#26434;&#26631;&#31614;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#27880;&#37322;&#26102;&#38388;&#21644;&#27880;&#37322;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#38656;&#27714;&#65292;&#27880;&#37322;&#24448;&#24448;&#23384;&#22312;&#38169;&#35823;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20551;&#35774;&#19981;&#21516;&#20687;&#32032;&#28857;&#19978;&#30340;&#22122;&#22768;&#26631;&#31614;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#20998;&#21106;&#26631;&#31614;&#30340;&#22122;&#22768;&#36890;&#24120;&#20855;&#26377;&#24378;&#28872;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#21644;&#26174;&#33879;&#30340;&#20998;&#24067;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#29992;&#20110;&#32534;&#30721;&#20998;&#21106;&#22122;&#22768;&#27880;&#37322;&#20013;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#21644;&#20559;&#24046;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#26631;&#31614;&#22122;&#22768;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#20462;&#27491;&#26041;&#27861;&#36880;&#27493;&#24674;&#22797;&#30495;&#23454;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#25552;&#26041;&#27861;&#27491;&#30830;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22122;&#22768;&#27880;&#37322;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators' expertise. Existing methods mostly assume noisy labels in different pixels are \textit{i.i.d}. However, segmentation label noise usually has strong spatial correlation and has prominent bias in distribution. In this paper, we propose a novel Markov model for segmentation noisy annotations that encodes both spatial correlation and bias. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#36152;&#26131;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#22269;&#38469;&#36152;&#26131;&#25968;&#25454;&#25512;&#26029;&#20135;&#21697;&#23618;&#38754;&#30340;&#20215;&#20540;&#38142;&#20851;&#31995;&#65292;&#20026;&#24212;&#23545;&#32463;&#27982;&#20013;&#26029;&#25552;&#20379;&#37325;&#35201;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.02491</link><description>&lt;p&gt;
&#22312;&#20135;&#21697;&#23618;&#38754;&#19978;&#32472;&#21046;&#20840;&#29699;&#20215;&#20540;&#38142;
&lt;/p&gt;
&lt;p&gt;
Mapping Global Value Chains at the Product Level. (arXiv:2308.02491v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#36152;&#26131;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#22269;&#38469;&#36152;&#26131;&#25968;&#25454;&#25512;&#26029;&#20135;&#21697;&#23618;&#38754;&#30340;&#20215;&#20540;&#38142;&#20851;&#31995;&#65292;&#20026;&#24212;&#23545;&#32463;&#27982;&#20013;&#26029;&#25552;&#20379;&#37325;&#35201;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#38142;&#25968;&#25454;&#23545;&#20110;&#24212;&#23545;&#32463;&#27982;&#20013;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;COVID-19&#30123;&#24773;&#21644;&#20044;&#20811;&#20848;&#25112;&#20105;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#20215;&#20540;&#38142;&#25968;&#25454;&#38598;&#65292;&#22914;&#8220;&#19990;&#30028;&#25237;&#20837;&#20135;&#20986;&#25968;&#25454;&#24211;&#8221;&#65292;&#8220;&#22269;&#38469;&#25237;&#20837;&#20135;&#20986;&#34920;&#8221;&#65292;&#8220;EXIOBASE&#8221;&#25110;&#8220;EORA&#8221;&#65292;&#32570;&#20047;&#20851;&#20110;&#20135;&#21697;&#65288;&#22914;&#25910;&#38899;&#26426;&#25509;&#25910;&#26426;&#65292;&#30005;&#35805;&#65292;&#30005;&#23481;&#22120;&#65292;&#28082;&#26230;&#26174;&#31034;&#22120;&#31561;&#65289;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#26356;&#31895;&#30053;&#30340;&#34892;&#19994;&#37096;&#38376;&#65288;&#22914;&#30005;&#22120;&#35774;&#22791;&#65292;&#30005;&#35759;&#19994;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#36152;&#26131;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#22269;&#38469;&#36152;&#26131;&#25968;&#25454;&#25512;&#26029;&#20135;&#21697;&#23618;&#38754;&#30340;&#20215;&#20540;&#38142;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24635;&#32467;&#20102;300&#22810;&#20010;&#19990;&#30028;&#21306;&#22495;&#65288;&#22914;&#32654;&#22269;&#30340;&#24030;&#65292;&#26085;&#26412;&#30340;&#21439;&#31561;&#65289;&#21644;1200&#22810;&#20010;&#20135;&#21697;&#30340;&#20986;&#21475;&#21644;&#36827;&#21475;&#30340;&#25968;&#25454;&#19978;&#65292;&#20197;&#25512;&#26029;&#20986;&#20854;&#20013;&#30340;&#36152;&#26131;&#27169;&#24335;&#20013;&#38544;&#21547;&#30340;&#20215;&#20540;&#38142;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value chain data is crucial to navigate economic disruptions, such as those caused by the COVID-19 pandemic and the war in Ukraine. Yet, despite its importance, publicly available value chain datasets, such as the ``World Input-Output Database'', ``Inter-Country Input-Output Tables'', ``EXIOBASE'' or the ``EORA'', lack detailed information about products (e.g. Radio Receivers, Telephones, Electrical Capacitors, LCDs, etc.) and rely instead on more aggregate industrial sectors (e.g. Electrical Equipment, Telecommunications). Here, we introduce a method based on machine learning and trade theory to infer product-level value chain relationships from fine-grained international trade data. We apply our method to data summarizing the exports and imports of 300+ world regions (e.g. states in the U.S., prefectures in Japan, etc.) and 1200+ products to infer value chain information implicit in their trade patterns. Furthermore, we use proportional allocation to assign the trade flow between reg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;&#26694;&#26550;IFIB&#65292;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#20107;&#20214;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#20107;&#20214;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2308.02360</link><description>&lt;p&gt;
&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intensity-free Integral-based Learning of Marked Temporal Point Processes. (arXiv:2308.02360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31215;&#20998;&#30340;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#30340;&#23398;&#20064;&#26694;&#26550;IFIB&#65292;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#20107;&#20214;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#20107;&#20214;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;MTPP&#65289;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#20026;&#26465;&#20214;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;$p^*&#65288;m&#65292;t&#65289;$&#21442;&#25968;&#21270;&#25554;&#20540;&#26102;&#38388;t&#21644;&#26631;&#35760;m&#22312;&#21382;&#21490;&#26465;&#20214;&#19979;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#39044;&#20808;&#23450;&#20041;&#24378;&#24230;&#20989;&#25968;&#12290;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#25351;&#23450;&#24378;&#24230;&#20989;&#25968;&#27491;&#30830;&#24418;&#24335;&#30340;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#21644;&#22788;&#29702;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26377;&#30740;&#31350;&#25670;&#33073;&#39044;&#23450;&#20041;&#24378;&#24230;&#20989;&#25968;&#65292;&#19968;&#20010;&#27169;&#22411;$p^*&#65288;t&#65289;$&#21644;$p^*&#65288;m&#65289;$&#20998;&#24320;&#65292;&#21478;&#19968;&#20010;&#20391;&#37325;&#20110;&#19981;&#32771;&#34385;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#39640;&#20445;&#30495;&#24230;&#30340;$p^*&#65288;m&#65292;t&#65289;$&#65292;&#36866;&#29992;&#20110;&#20107;&#20214;&#26631;&#35760;&#22312;&#22810;&#32500;&#36830;&#32493;&#31354;&#38388;&#20013;&#20855;&#26377;&#20998;&#31867;&#25110;&#25968;&#20540;&#23646;&#24615;&#30340;&#31163;&#25955;&#20107;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#26694;&#26550;IFIB&#65288;&#26080;&#24378;&#24230;&#31215;&#20998;&#21270;&#23398;&#33021;&#36807;&#31243;&#65289;&#65292;&#30452;&#25509;&#24314;&#27169;&#26465;&#20214;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;$p^*&#65288;m&#65292;t&#65289;$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#65306;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197;&#32959;&#30244;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20808;&#36827;&#30340;LLMs&#33021;&#22815;&#22788;&#29702;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#26465;&#20214;&#21644;&#21305;&#37197;&#36923;&#36753;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65292;&#24182;&#21487;&#20316;&#20026;&#20154;&#24037;&#36741;&#21161;&#31579;&#36873;&#24739;&#32773;-&#35797;&#39564;&#20505;&#36873;&#20154;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#21307;&#30103;&#20256;&#36882;&#21644;&#21457;&#29616;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#24222;&#22823;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#19981;&#21487;&#25193;&#23637;&#30340;&#25163;&#21160;&#22788;&#29702;&#65292;&#35813;&#36807;&#31243;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#32959;&#30244;&#23398;&#20026;&#37325;&#28857;&#39046;&#22495;&#65292;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20010;&#27491;&#22312;&#32654;&#22269;&#19968;&#20010;&#22823;&#22411;&#21307;&#30103;&#32593;&#32476;&#36827;&#34892;&#27979;&#35797;&#37096;&#32626;&#30340;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#31995;&#32479;&#12290;&#21021;&#27493;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65306;&#20808;&#36827;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#21487;&#20197;&#31435;&#21363;&#36830;&#25509;&#20020;&#24202;&#35797;&#39564;&#30340;&#22797;&#26434;&#30340;&#21512;&#26684;&#26465;&#20214;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#30340;&#21305;&#37197;&#36923;&#36753;&#65288;&#20363;&#22914;&#23884;&#22871;&#30340;AND/OR/NOT&#65289;&#12290;&#34429;&#28982;&#20173;&#19981;&#23436;&#32654;&#65292;LLM&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#24378;&#22522;&#20934;&#32447;&#65292;&#24182;&#21487;&#33021;&#20316;&#20026;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#36827;&#34892;&#20505;&#36873;&#24739;&#32773;-&#35797;&#39564;&#21010;&#20998;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#19968;&#20123;&#24212;&#29992;LLM&#36827;&#34892;&#31471;&#21040;&#31471;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#30340;&#37325;&#35201;&#22686;&#38271;&#39046;&#22495;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#38480;&#21046;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02029</link><description>&lt;p&gt;
&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#22312;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection. (arXiv:2308.02029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Deep Maxout&#32593;&#32476;&#29305;&#24449;&#34701;&#21512;&#21644;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24402;&#19968;&#21270;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26368;&#32456;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#20013;&#28023;&#36139;&#34880;&#26159;&#19968;&#31181;&#36951;&#20256;&#24615;&#34880;&#28082;&#30149;&#65292;&#30001;&#36951;&#20256;&#32570;&#38519;&#23548;&#33268;&#34880;&#32418;&#34507;&#30333;&#22810;&#32957;&#38142;&#30340;&#20135;&#29983;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#22320;&#21306;&#30340;&#21457;&#30149;&#39057;&#29575;&#21644;&#20849;&#20139;&#31243;&#24230;&#30340;&#20102;&#35299;&#36739;&#23569;&#12290;&#20102;&#35299;&#22320;&#20013;&#28023;&#36139;&#34880;&#21457;&#29983;&#30340;&#39057;&#29575;&#21644;&#21487;&#38752;&#31361;&#21464;&#26159;&#39044;&#38450;&#12289;&#25511;&#21046;&#21644;&#27835;&#30103;&#35745;&#21010;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;Political Tangent Search&#20248;&#21270;&#22120;&#30340;&#36716;&#31227;&#23398;&#20064;&#65288;PTSO_TL&#65289;&#22312;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#20174;&#29305;&#23450;&#25968;&#25454;&#38598;&#33719;&#21462;&#30340;&#36755;&#20837;&#25968;&#25454;&#22312;&#25968;&#25454;&#24402;&#19968;&#21270;&#38454;&#27573;&#36827;&#34892;&#20102;&#35268;&#33539;&#21270;&#12290;&#25968;&#25454;&#24402;&#19968;&#21270;&#38454;&#27573;&#21033;&#29992;&#20998;&#20301;&#25968;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#28982;&#21518;&#23558;&#25968;&#25454;&#20256;&#36882;&#32473;&#29305;&#24449;&#34701;&#21512;&#38454;&#27573;&#65292;&#22312;&#35813;&#38454;&#27573;&#21033;&#29992;Deep Maxout&#32593;&#32476;&#30340;&#21152;&#26435;&#27431;&#27663;&#36317;&#31163;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#22686;&#21152;&#25968;&#25454;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#36827;&#34892;&#22320;&#20013;&#28023;&#36139;&#34880;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#26032;&#29616;&#35937;&#65292;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#65292;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#31616;&#21270;&#35745;&#31639;&#30340;bra-ket&#31526;&#21495;&#12290;</title><link>http://arxiv.org/abs/2308.01814</link><description>&lt;p&gt;
Tensor&#31243;&#24207;IVb&#65306;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. (arXiv:2308.01814v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#26032;&#29616;&#35937;&#65292;&#36890;&#36807;&#25512;&#23548;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#65292;&#23637;&#31034;&#20102;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#31616;&#21270;&#35745;&#31639;&#30340;bra-ket&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#36234;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#24403;&#20351;&#29992;Adam&#31561;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#29616;&#35937;&#21527;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#65306;&#19982;SGD&#19968;&#26679;&#65292;&#23545;&#20110;&#21253;&#25324;Adam&#22312;&#20869;&#30340;&#19968;&#33324;&#20248;&#21270;&#22120;&#65292;&#29305;&#24449;&#23398;&#20064;&#21644;&#26680;&#34892;&#20026;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#21516;&#30340;&#20108;&#20998;&#27861; - &#23613;&#31649;&#26377;&#19968;&#31181;&#38750;&#32447;&#24615;&#30340;&#8220;&#26680;&#8221;&#27010;&#24565;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#23545;&#20110;&#20219;&#20309;&#26550;&#26500;&#30340;&#30456;&#24212;&#30340;&#8220;&#31070;&#32463;&#20999;&#32447;&#8221;&#21644;&#8220;&#26368;&#22823;&#26356;&#26032;&#8221;&#26497;&#38480;&#12290;&#19978;&#36848;&#32467;&#26524;&#30340;&#20004;&#20010;&#22522;&#30784;&#24615;&#36827;&#23637;&#26159;&#65306;1&#65289;&#19968;&#31181;&#26032;&#30340;Tensor&#31243;&#24207;&#35821;&#35328;&#65292;NEXORT&#65292;&#21487;&#20197;&#34920;&#36798;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#22914;&#20309;&#23558;&#26799;&#24230;&#22788;&#29702;&#20026;&#26356;&#26032;&#12290;2&#65289;&#24341;&#20837;bra-ket&#31526;&#21495;&#26469;&#26497;&#22823;&#22320;&#31616;&#21270;Tensor&#31243;&#24207;&#20013;&#30340;&#34920;&#36798;&#24335;&#21644;&#35745;&#31639;&#12290;&#35813;&#24037;&#20316;&#24635;&#32467;&#24182;&#27010;&#25324;&#20102;Tensor&#31243;&#24207;&#31995;&#21015;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20808;&#21069;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond stochastic gradient descent (SGD), what new phenomena emerge in wide neural networks trained by adaptive optimizers like Adam? Here we show: The same dichotomy between feature learning and kernel behaviors (as in SGD) holds for general optimizers as well, including Adam -- albeit with a nonlinear notion of "kernel." We derive the corresponding "neural tangent" and "maximal update" limits for any architecture. Two foundational advances underlie the above results: 1) A new Tensor Program language, NEXORT, that can express how adaptive optimizers process gradients into updates. 2) The introduction of bra-ket notation to drastically simplify expressions and calculations in Tensor Programs. This work summarizes and generalizes all previous results in the Tensor Programs series of papers.
&lt;/p&gt;</description></item><item><title>OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;</title><link>http://arxiv.org/abs/2308.01390</link><description>&lt;p&gt;
OpenFlamingo: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01390
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;OpenFlamingo&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#22238;&#24402;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;3B&#21040;9B&#12290; OpenFlamingo&#26159;&#19968;&#20010;&#25345;&#32493;&#21162;&#21147;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#22797;&#21046;DeepMind&#30340;Flamingo&#27169;&#22411;&#30340;&#24320;&#28304;&#29256;&#26412;&#12290;&#22312;&#19971;&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#65292;OpenFlamingo&#27169;&#22411;&#30340;&#24615;&#33021;&#20171;&#20110;&#23545;&#24212;&#30340;Flamingo&#24615;&#33021;&#30340;80%&#33267;89%&#20043;&#38388;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21644;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;https://github.com/mlfoundations/open_flamingo&#19978;&#20998;&#20139;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;</title><link>http://arxiv.org/abs/2308.01157</link><description>&lt;p&gt;
LLMs&#29702;&#35299;&#29627;&#29827;&#30418;&#27169;&#22411;&#65292;&#21457;&#29616;&#24778;&#21916;&#24182;&#25552;&#20986;&#20462;&#22797;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01157
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23558;&#22797;&#26434;&#32467;&#26524;&#20998;&#35299;&#20026;&#21333;&#19968;&#21464;&#37327;&#30340;&#22270;&#34920;&#31034;&#32452;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25972;&#20010;&#27169;&#22411;&#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#24212;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#33258;&#21160;&#23436;&#25104;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#26816;&#27979;&#19982;&#20808;&#21069;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24322;&#24120;&#65292;&#25551;&#36848;&#24322;&#24120;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#21435;&#38500;&#24322;&#24120;&#30340;&#20462;&#22797;&#24314;&#35758;&#12290;&#25105;&#20204;&#20351;&#29992;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#22810;&#20010;&#31034;&#20363;&#26469;&#35777;&#26126;LLMs&#30340;&#36825;&#20123;&#26032;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;$\texttt{TalkToEBM}$&#21253;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#36827;&#34892;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00628</link><description>&lt;p&gt;
&#20154;&#31867;-M3&#65306;&#19968;&#20010;&#29992;&#20110;&#23460;&#22806;&#22330;&#26223;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#22810;&#35270;&#35282;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23460;&#22806;&#22330;&#26223;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#21482;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#25110;&#28857;&#20113;&#65289;&#65292;&#24182;&#19988;&#22330;&#26223;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#20154;&#12290;&#25968;&#25454;&#38598;&#22522;&#30784;&#30340;&#26377;&#38480;&#33539;&#22260;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#21464;&#21270;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-M3&#65292;&#36825;&#26159;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#23460;&#22806;&#22330;&#26223;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#40065;&#26834;&#30340;&#28857;&#20113;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23460;&#22806;&#22330;&#26223;&#20013;&#22810;&#20010;&#20154;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#20934;&#30830;&#20154;&#20307;&#23450;&#20301;&#21644;&#21305;&#37197;&#27169;&#31946;&#38382;&#39064;&#65292;&#29983;&#25104;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.00086</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#39640;&#38454;CFD&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning shock capturing for High-Order CFD solvers. (arXiv:2308.00086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00086
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#27969;&#21160;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#38663;&#33633;&#25429;&#25417;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;GMM&#20256;&#24863;&#22120;&#22312;&#26816;&#27979;&#38663;&#33633;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#38656;&#35201;&#21442;&#25968;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#22312;&#22810;&#26679;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#19982;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#26377;&#26041;&#27861;&#37117;&#38598;&#25104;&#21040;&#39640;&#38454;&#21487;&#21387;&#24615;&#19981;&#36830;&#32493;Galerkin&#27714;&#35299;&#22120;&#20013;&#65292;&#20154;&#24037;&#40655;&#24615;&#21487;&#20197;&#35843;&#33410;&#20197;&#25429;&#25417;&#38663;&#33633;&#12290;&#36229;&#38899;&#36895;&#27979;&#35797;&#26696;&#20363;&#65292;&#21253;&#25324;&#39640;&#38647;&#35834;&#25968;&#65292;&#23637;&#31034;&#20102;&#20256;&#24863;&#22120;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#25928;&#26524;&#19982;&#31934;&#35843;&#30340;&#26368;&#20808;&#36827;&#20256;&#24863;&#22120;&#30456;&#24403;&#12290;%&#33410;&#28857;DG&#26041;&#27861;&#20801;&#35768;&#22312;&#20122;&#21333;&#20803;&#36890;&#37327;&#26377;&#24046;&#24322;&#30340;&#20844;&#24335;&#20013;&#36827;&#34892;&#28508;&#22312;&#24212;&#29992;&#65292;&#36229;&#38899;&#36895;&#29305;&#24449;&#26816;&#27979;&#21644;&#32593;&#26684;&#32454;&#21270;&#12290;&#36825;&#31181;&#22522;&#20110;GMM&#30340;&#20256;&#24863;&#22120;&#36866;&#29992;&#20110;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#21644;&#21508;&#31181;&#27969;&#21160;&#37197;&#32622;&#65292;&#20854;&#33258;&#36866;&#24212;&#24615;&#21644;&#26080;&#38656;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21151;&#33021;&#20351;&#20854;&#20855;&#22791;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14938</link><description>&lt;p&gt;
&#39640;&#25928;&#20114;&#21160;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#29615;&#30340;&#21306;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24320;&#29615;&#31995;&#32479;&#30340;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#38597;&#21487;&#27604;&#36793;&#30028;&#30340;&#24320;&#29615;&#21160;&#21147;&#23398;&#21253;&#21547;&#20989;&#25968;&#30340;&#26032;&#31867;&#21035;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#23545;&#20110;&#20219;&#24847;&#21160;&#21147;&#31995;&#32479;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#20989;&#25968;&#26500;&#24314;&#19968;&#20010;&#29366;&#24577;&#25968;&#26159;&#21407;&#31995;&#32479;&#20004;&#20493;&#30340;&#23884;&#20837;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#23884;&#20837;&#31995;&#32479;&#30340;&#21333;&#20010;&#36712;&#36857;&#21487;&#20197;&#25552;&#20379;&#21487;&#36798;&#38598;&#30340;&#36229;&#30697;&#24418;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#30340;&#38381;&#29615;&#23884;&#20837;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a computationally efficient framework for interval reachability of neural network controlled systems. Our approach builds upon inclusion functions for the neural network controller and the open-loop system. We observe that many state-of-the-art neural network verifiers can produce inclusion functions for neural networks. We introduce and analyze a new class of inclusion functions for the open-loop dynamics based on bounds of the function Jacobian that is particularly suitable for capturing the interactions between systems and neural network controllers. Next, for any dynamical system, we use inclusion functions to construct an embedding system with twice the number of states as the original system. We show that a single trajectory of this embedding system provides hyper-rectangular over-approximations of reachable sets. We then propose two approaches for constructing a closed-loop embedding system for a neural network controlled dynamical system that accounts 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#20174;EEG&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13181</link><description>&lt;p&gt;
&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Neural Memory Decoding with EEG Data and Representation Learning. (arXiv:2307.13181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;EEG&#25968;&#25454;&#21644;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#31070;&#32463;&#35760;&#24518;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#20174;EEG&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20174;EEG&#25968;&#25454;&#20013;&#35299;&#30721;&#35760;&#24518;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;EEG&#27874;&#24418;&#20013;&#35782;&#21035;&#20986;&#34987;&#21484;&#22238;&#30340;&#27010;&#24565;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;78.4&#65285;&#65288;&#26426;&#20250;4&#65285;&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#19982;&#26377;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#26469;&#23558;&#33041;&#27963;&#21160;&#30340;EEG&#35760;&#24405;&#26144;&#23556;&#21040;&#19968;&#20010;&#20302;&#32500;&#31354;&#38388;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#34920;&#31034;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#27010;&#24565;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#20986;&#29616;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27010;&#24565;&#37117;&#24517;&#39035;&#23384;&#22312;&#30456;&#24212;&#30340;&#21442;&#32771;EEG&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#24403;&#29992;&#25143;&#22238;&#24518;&#25991;&#26723;&#20869;&#23481;&#26102;&#25429;&#33719;EEG&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#39044;&#27979;&#25991;&#26723;&#30340;&#38142;&#25509;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item><item><title>TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08674</link><description>&lt;p&gt;
TableGPT&#65306;&#23558;&#34920;&#26684;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#32479;&#19968;&#21040;&#19968;&#20010;GPT&#20013;
&lt;/p&gt;
&lt;p&gt;
TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08674
&lt;/p&gt;
&lt;p&gt;
TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24211;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#38656;&#35201;&#20154;&#20204;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#36827;&#34892;&#20998;&#26512;&#21644;&#25805;&#20316;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#19982;&#34920;&#26684;&#20132;&#20114;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#24471;&#36825;&#31181;&#33021;&#21147;&#26356;&#21152;&#25509;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TableGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#31934;&#35843;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#29702;&#35299;&#21644;&#25805;&#20316;&#34920;&#26684;&#12290;&#23427;&#24341;&#20837;&#20102;&#19982;&#34920;&#26684;&#26080;&#32541;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25968;&#25454;&#25805;&#20316;&#65288;&#20363;&#22914;&#25554;&#20837;&#12289;&#21024;&#38500;&#12289;&#26597;&#35810;&#21644;&#20462;&#25913;&#25805;&#20316;&#65289;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#20998;&#26512;&#25253;&#21578;&#29983;&#25104;&#21644;&#33258;&#21160;&#39044;&#27979;&#12290;TableGPT&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;TableGPT&#30340;&#26680;&#24515;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#26032;&#27010;&#24565;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#24182;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#25805;&#20316;&#23545;&#34920;&#26684;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33322;&#31354;&#25110;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#27888;&#22269;&#36164;&#20135;&#20215;&#20540;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35895;&#27468;&#22320;&#22270;API&#30340;&#24433;&#20687;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24471;&#21040;&#26356;&#31934;&#30830;&#30340;&#22303;&#22320;&#20215;&#26684;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08650</link><description>&lt;p&gt;
&#21033;&#29992;&#33322;&#31354;&#25110;&#21355;&#26143;&#24433;&#20687;&#35780;&#20272;&#27888;&#22269;&#36164;&#20135;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Thailand Asset Value Estimation Using Aerial or Satellite Imagery. (arXiv:2307.08650v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33322;&#31354;&#25110;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#27888;&#22269;&#36164;&#20135;&#20215;&#20540;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35895;&#27468;&#22320;&#22270;API&#30340;&#24433;&#20687;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24471;&#21040;&#26356;&#31934;&#30830;&#30340;&#22303;&#22320;&#20215;&#26684;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#22320;&#20135;&#26159;&#27888;&#22269;&#32463;&#27982;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#26356;&#20934;&#30830;&#30340;&#22303;&#22320;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#30340;&#26085;&#30410;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#22303;&#22320;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;&#21152;&#26435;&#36136;&#37327;&#24471;&#20998;&#65288;WQS&#65289;&#65292;&#30001;&#20110;&#20381;&#36182;&#20027;&#35266;&#26631;&#20934;&#21644;&#32570;&#20047;&#23545;&#31354;&#38388;&#21464;&#37327;&#30340;&#32771;&#34385;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#35895;&#27468;&#22320;&#22270;API&#25552;&#20379;&#30340;&#33322;&#31354;&#25110;&#21355;&#26143;&#24433;&#20687;&#65292;&#22686;&#24378;&#20102;&#30001;&#21345;&#22622;&#31185;&#24681;&#21830;&#19994;&#31185;&#25216;&#38598;&#22242;&#65288;KBTG&#65289;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#22303;&#22320;&#20215;&#26684;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#36164;&#20135;&#20272;&#20540;&#27169;&#22411;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;EfficientNet&#26550;&#26500;&#30340;Siamese&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#22303;&#22320;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32422;0.81&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#20248;&#20110;&#20165;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#20855;&#26377;&#39640;&#20110;&#39044;&#23450;&#20041;&#38408;&#20540;&#30340;&#30456;&#20284;&#24615;&#35780;&#20998;&#30340;&#38468;&#36817;&#22303;&#22320;&#30340;&#35780;&#20272;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
Real estate is a critical sector in Thailand's economy, which has led to a growing demand for a more accurate land price prediction approach. Traditional methods of land price prediction, such as the weighted quality score (WQS), are limited due to their reliance on subjective criteria and their lack of consideration for spatial variables. In this study, we utilize aerial or satellite imageries from Google Map API to enhance land price prediction models from the dataset provided by Kasikorn Business Technology Group (KBTG). We propose a similarity-based asset valuation model that uses a Siamese-inspired Neural Network with pretrained EfficientNet architecture to assess the similarity between pairs of lands. By ensembling deep learning and tree-based models, we achieve an area under the ROC curve (AUC) of approximately 0.81, outperforming the baseline model that used only tabular data. The appraisal prices of nearby lands with similarity scores higher than a predefined threshold were us
&lt;/p&gt;</description></item><item><title>DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05628</link><description>&lt;p&gt;
DNAGPT&#65306;&#29992;&#20110;&#22810;&#20010;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#30340;&#36890;&#29992;&#39044;&#35757;&#32451;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05628
&lt;/p&gt;
&lt;p&gt;
DNAGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#31995;&#21015;&#30340;&#25104;&#21151;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#19968;&#33324;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#24471;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#21463;&#30410;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25506;&#32034;DNA&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;DNA&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#38656;&#27714;&#38750;&#24120;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#65292;&#22240;&#20026;DNA&#30456;&#20851;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#22914;&#24207;&#21015;&#12289;&#34920;&#36798;&#27700;&#24179;&#31561;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#36825;&#20123;&#29305;&#28857;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DNAGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#22312;9&#20010;&#29289;&#31181;&#30340;&#36229;&#36807;100&#20159;&#20010;&#30897;&#22522;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#38024;&#23545;&#20219;&#20309;DNA&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#25110;&#36755;&#20986;DNA&#24207;&#21015;&#21644;&#25968;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#26631;&#35760;&#35774;&#35745;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#20219;&#21153;&#38656;&#27714;&#26469;&#35774;&#35745;&#25552;&#31034;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#26469;&#25913;&#36827;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#20195;&#26367;&#25104;&#23545;&#20559;&#22909;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05405</link><description>&lt;p&gt;
&#25552;&#21319;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26469;&#22686;&#21152;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#26469;&#25913;&#36827;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#20195;&#26367;&#25104;&#23545;&#20559;&#22909;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#22312;&#23398;&#20064;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#20132;&#20114;&#21453;&#39304;&#65292;&#36825;&#19968;&#36807;&#31243;&#21487;&#33021;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#20998;&#25968;&#32780;&#19981;&#26159;&#25104;&#23545;&#20559;&#22909;&#65292;&#26469;&#25552;&#39640;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#39304;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20998;&#25968;&#21487;&#20197;&#20135;&#29983;&#27604;&#25104;&#23545;&#20559;&#22909;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35201;&#27714;&#25945;&#24072;&#19982;&#20195;&#29702;&#20132;&#20114;&#35780;&#20998;&#20840;&#38754;&#30340;&#36712;&#36857;&#26469;&#35757;&#32451;&#34892;&#20026;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#20154;&#31867;&#32473;&#20986;&#30340;&#19981;&#31283;&#23450;&#20998;&#25968;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#12290;&#36825;&#20351;&#24471;&#23398;&#20064;&#33539;&#24335;&#23545;&#20110;&#19981;&#23436;&#32654;&#25110;&#19981;&#21487;&#38752;&#30340;&#20998;&#25968;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#23545;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#20998;&#25968;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive reinforcement learning has shown promise in learning complex robotic tasks. However, the process can be human-intensive due to the requirement of large amount of interactive feedback. This paper presents a new method that uses scores provided by humans, instead of pairwise preferences, to improve the feedback efficiency of interactive reinforcement learning. Our key insight is that scores can yield significantly more data than pairwise preferences. Specifically, we require a teacher to interactively score the full trajectories of an agent to train a behavioral policy in a sparse reward environment. To avoid unstable scores given by human negatively impact the training process, we propose an adaptive learning scheme. This enables the learning paradigm to be insensitive to imperfect or unreliable scores. We extensively evaluate our method on robotic locomotion and manipulation tasks. The results show that the proposed method can efficiently learn near-optimal policies by adap
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#21512;&#25104;&#30340;&#32508;&#36848;&#21644;&#35752;&#35770;&#65292;&#32467;&#21512;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#21442;&#32771;&#34920;&#21644;&#24635;&#32467;&#20851;&#38190;&#35201;&#28857;&#26469;&#24041;&#22266;&#30740;&#31350;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.02106</link><description>&lt;p&gt;
SoK: &#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
SoK: Privacy-Preserving Data Synthesis. (arXiv:2307.02106v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02106
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#21512;&#25104;&#30340;&#32508;&#36848;&#21644;&#35752;&#35770;&#65292;&#32467;&#21512;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#21442;&#32771;&#34920;&#21644;&#24635;&#32467;&#20851;&#38190;&#35201;&#28857;&#26469;&#24041;&#22266;&#30740;&#31350;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20998;&#26512;&#30340;&#26222;&#21450;&#65292;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#22240;&#27492;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#20998;&#26512;&#26426;&#21046;&#30340;&#21457;&#23637;&#26085;&#30410;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#23545;&#20110;&#26032;&#20219;&#21153;&#30340;&#35774;&#35745;&#36807;&#31243;&#32321;&#29712;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#21019;&#24314;&#29702;&#35770;&#19978;&#19981;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;PPDS&#65289;&#24182;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12289;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26041;&#27861;&#30340;&#24635;&#32467;&#12290;&#22312;&#35813;&#24635;&#32467;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#34920;&#31034;&#30340;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#21407;&#21017;&#26469;&#30740;&#31350;&#22522;&#20110;DL&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#24635;&#32467;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21442;&#32771;&#34920;&#65292;&#27010;&#25324;&#20102;&#20851;&#38190;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the prevalence of data analysis grows, safeguarding data privacy has become a paramount concern. Consequently, there has been an upsurge in the development of mechanisms aimed at privacy-preserving data analyses. However, these approaches are task-specific; designing algorithms for new tasks is a cumbersome process. As an alternative, one can create synthetic data that is (ideally) devoid of private information. This paper focuses on privacy-preserving data synthesis (PPDS) by providing a comprehensive overview, analysis, and discussion of the field. Specifically, we put forth a master recipe that unifies two prominent strands of research in PPDS: statistical methods and deep learning (DL)-based methods. Under the master recipe, we further dissect the statistical methods into choices of modeling and representation, and investigate the DL-based methods by different generative modeling principles. To consolidate our findings, we provide comprehensive reference tables, distill key take
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65292;&#21033;&#29992;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01759</link><description>&lt;p&gt;
&#21482;&#38656;&#39044;&#35757;&#32451;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification. (arXiv:2307.01759v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20998;&#31867;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65292;&#21033;&#29992;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31934;&#31070;&#30142;&#30149;&#65292;&#34920;&#29616;&#20026;&#38750;&#20856;&#22411;&#30340;&#35748;&#30693;&#12289;&#24773;&#32490;&#21644;&#31038;&#20132;&#27169;&#24335;&#12290;&#21450;&#26102;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;ASD&#24739;&#32773;&#30340;&#26377;&#25928;&#24178;&#39044;&#21644;&#25913;&#21892;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;atlas&#22686;&#24378;transformer&#26694;&#26550;&#65288;METAFormer&#65289;&#29992;&#20110;ASD&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;ABIDE I&#25968;&#25454;&#38598;&#20013;&#30340;&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#65292;&#21253;&#25324;406&#21517;ASD&#24739;&#32773;&#21644;476&#21517;&#20856;&#22411;&#23545;&#29031;&#65288;TC&#65289;&#21463;&#35797;&#32773;&#12290;METAFormer&#37319;&#29992;&#20102;&#22810;atlas&#26041;&#27861;&#65292;&#20854;&#20013;&#26469;&#33258;AAL&#12289;CC200&#21644;DOS160&#22270;&#35889;&#30340;&#23637;&#24179;&#36830;&#25509;&#30697;&#38453;&#20316;&#20026;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#36755;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20174;&#36755;&#20837;&#20013;&#37325;&#24314;&#25513;&#30721;&#20540;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25110;&#29420;&#31435;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#39564;&#35777;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#24182;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a prevalent psychiatric condition characterized by atypical cognitive, emotional, and social patterns. Timely and accurate diagnosis is crucial for effective interventions and improved outcomes in individuals with ASD. In this study, we propose a novel Multi-Atlas Enhanced Transformer framework, METAFormer, ASD classification. Our framework utilizes resting-state functional magnetic resonance imaging data from the ABIDE I dataset, comprising 406 ASD and 476 typical control (TC) subjects. METAFormer employs a multi-atlas approach, where flattened connectivity matrices from the AAL, CC200, and DOS160 atlases serve as input to the transformer encoder. Notably, we demonstrate that self-supervised pretraining, involving the reconstruction of masked values from the input, significantly enhances classification performance without the need for additional or separate training data. Through stratified cross-validation, we evaluate the proposed framework and show
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01292</link><description>&lt;p&gt;
Pareto-&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PSML&#65289;&#65306;&#25351;&#32441;&#21644;&#20445;&#25252;&#25512;&#26029;&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#23558;&#26597;&#35810;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#24182;&#25351;&#23450;&#25152;&#38656;&#30340;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#31561;&#65289;&#12290;&#26381;&#21153;&#22120;&#22312;&#21518;&#31471;&#32500;&#25252;&#19968;&#32452;&#27169;&#22411;&#65288;&#27169;&#22411;&#24211;&#65289;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#25351;&#26631;&#25552;&#20379;&#26597;&#35810;&#26381;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#38544;&#34255;&#22312;&#25512;&#29702;&#26381;&#21153;&#25509;&#21475;&#32972;&#21518;&#30340;&#27169;&#22411;&#24211;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#30830;&#23450;&#20351;&#29992;&#30340;&#26159;&#21738;&#20010;&#27169;&#22411;&#12290;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#27493;&#39588;&#26469;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#37117;&#33021;&#24471;&#21040;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#27169;&#22411;&#25552;&#21462;&#21487;&#20197;&#20855;&#26377;&#20445;&#30495;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20986;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;</title><link>http://arxiv.org/abs/2306.12435</link><description>&lt;p&gt;
&#24212;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24110;&#21161;&#35786;&#26029;&#24378;&#36843;&#30151;
&lt;/p&gt;
&lt;p&gt;
Modeling T1 Resting-State MRI Variants Using Convolutional Neural Networks in Diagnosis of OCD. (arXiv:2306.12435v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20986;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#36843;&#30151;&#26159;&#19968;&#31181;&#39640;&#24230;&#20196;&#20154;&#30171;&#33510;&#30340;&#30142;&#30149;&#65292;&#20854;&#24120;&#24120;&#19982;&#21069;&#39069;&#30382;&#23618;&#21644;&#20195;&#35874;&#22411;&#35895;&#27688;&#37240;&#21463;&#20307;5(mGluR5)&#26377;&#20851;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#27979;&#37327;&#23567;&#40736;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#30340;&#20998;&#24067;&#23481;&#31215;&#27604;&#65292;&#21457;&#29616;&#35813;&#21463;&#20307;&#20449;&#21495;&#27700;&#24179;&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#26356;&#22810;&#23454;&#35777;&#25968;&#25454;&#65292;&#23578;&#26080;&#27861;&#23436;&#20840;&#39564;&#35777;mGluR5&#30340;&#21442;&#19982;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#24739;&#26377;&#31934;&#31070;&#20998;&#35010;&#30151;&#12289;&#25233;&#37057;&#30151;&#21644;&#24378;&#36843;&#30151;&#24739;&#32773;&#30340;T1&#38745;&#24687;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;(TRS-MRI)&#25195;&#25551;&#65292;&#20197;&#22238;&#31572;&#26377;&#20851;OCD&#30340;&#22240;&#26524;&#22240;&#32032;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36825;&#20123;&#30142;&#30149;&#20043;&#38388;&#30340;&#20132;&#21449;&#27604;&#36739;&#65292;&#23547;&#25214;&#29305;&#23450;&#30142;&#30149;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#21306;&#20998;&#24739;&#26377;OCD&#21644;&#20854;&#20182;&#31934;&#31070;&#38556;&#30861;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#24739;&#26377;OCD &#30340;&#24739;&#32773;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;90%&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#35786;&#26029;OCD&#30340;&#24037;&#20855;&#65292;&#20026;&#20256;&#32479;&#35786;&#26029;&#27969;&#31243;&#25552;&#20379;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obsessive-compulsive disorder (OCD) presents itself as a highly debilitating disorder. The disorder has common associations with the prefrontal cortex and the glutamate receptor known as Metabotropic Glutamate Receptor 5 (mGluR5). This receptor has been observed to demonstrate higher levels of signaling from positron emission tomography scans measured by its distribution volume ratios in mice. Despite this evidence, studies are unable to fully verify the involvement of mGluR5 as more empirical data is needed. Computational modeling methods were used as a means of validation for previous hypotheses involving mGluR5. The inadequacies in relation to the causal factor of OCD were answered by utilizing T1 resting-state magnetic resonance imaging (TRS-MRI) scans of patients suffering from schizophrenia, major depressive disorder, and obsessive-compulsive disorder. Because comorbid cases often occur within these disorders, cross-comparative abilities become necessary to find distinctive chara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoHHGN+&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#29992;&#25143;ID&#30340;&#30005;&#21830;&#32593;&#31449;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#23450;&#20041;&#30340;&#20266;&#20250;&#35805;&#20197;&#21450;&#21253;&#25324;&#20215;&#26684;&#12289;&#31867;&#21035;&#12289;&#24615;&#21035;&#21644;&#22320;&#21306;&#31561;&#29992;&#25143;&#20449;&#24687;&#65292;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10029</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#23618;&#23884;&#20837;&#21644;&#20250;&#35805;&#23646;&#24615;&#30340;&#20266;&#20250;&#35805;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Pseudo session-based recommendation with hierarchical embedding and session attributes. (arXiv:2306.10029v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoHHGN+&#65292;&#29992;&#20110;&#35299;&#20915;&#32570;&#20047;&#29992;&#25143;ID&#30340;&#30005;&#21830;&#32593;&#31449;&#25968;&#25454;&#30340;&#25512;&#33616;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#23450;&#20041;&#30340;&#20266;&#20250;&#35805;&#20197;&#21450;&#21253;&#25324;&#20215;&#26684;&#12289;&#31867;&#21035;&#12289;&#24615;&#21035;&#21644;&#22320;&#21306;&#31561;&#29992;&#25143;&#20449;&#24687;&#65292;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#26080;&#27861;&#20026;&#27599;&#20010;&#20132;&#26131;&#25968;&#25454;&#26465;&#30446;&#25552;&#20379;&#26631;&#35782;&#21495;&#65288;&#29992;&#25143;ID&#65289;&#12290;&#22240;&#20026;&#22823;&#22810;&#25968;&#25512;&#33616;&#26041;&#27861;&#20551;&#23450;&#25152;&#26377;&#25968;&#25454;&#37117;&#34987;&#20998;&#37197;&#20102;&#29992;&#25143;ID&#65292;&#25152;&#20197;&#23427;&#20204;&#19981;&#33021;&#24212;&#29992;&#20110;&#27809;&#26377;&#29992;&#25143;ID&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#30740;&#31350;&#20102;&#22522;&#20110;&#20250;&#35805;&#20449;&#24687;&#30340;&#20250;&#35805;&#25512;&#33616;&#65288;SBR&#65289;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29992;&#25143;&#30340;&#30701;&#26399;&#34892;&#20026;&#20449;&#24687;&#12290;&#24120;&#35268;&#30340;SBR&#21482;&#20351;&#29992;&#19982;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#25512;&#33616;&#65288;&#20363;&#22914;&#65292;&#22312;EC&#31449;&#28857;&#19978;&#20351;&#29992;&#39033;&#30446;ID&#65289;&#12290;&#29305;&#21035;&#26159;&#22312;EC&#32593;&#31449;&#30340;&#24773;&#20917;&#19979;&#65292;&#35760;&#24405;&#30340;&#25968;&#25454;&#21253;&#25324;&#34987;&#36141;&#20080;&#30340;&#29289;&#21697;&#21517;&#31216;&#12289;&#29289;&#21697;&#20215;&#26684;&#12289;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#20197;&#21450;&#29992;&#25143;&#30340;&#24615;&#21035;&#21644;&#22320;&#21306;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20026;&#27809;&#26377;&#29992;&#25143;ID&#21644;&#20250;&#35805;ID&#30340;EC&#32593;&#31449;&#30340;&#36141;&#20080;&#21382;&#21490;&#25968;&#25454;&#23450;&#20041;&#20102;&#20266;&#20250;&#35805;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CoHHGN+&#20250;&#35805;&#25512;&#33616;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21327;&#21516;&#23548;&#21521;&#30340;&#24322;&#26500;&#36229;&#22270;&#21644;&#20840;&#23616;&#22270;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, electronic commerce (EC) websites have been unable to provide an identification number (user ID) for each transaction data entry because of privacy issues. Because most recommendation methods assume that all data are assigned a user ID, they cannot be applied to the data without user IDs. Recently, session-based recommendation (SBR) based on session information, which is short-term behavioral information of users, has been studied. A general SBR uses only information about the item of interest to make a recommendation (e.g., item ID for an EC site). Particularly in the case of EC sites, the data recorded include the name of the item being purchased, the price of the item, the category hierarchy, and the gender and region of the user. In this study, we define a pseudo--session for the purchase history data of an EC site without user IDs and session IDs. Finally, we propose an SBR with a co-guided heterogeneous hypergraph and globalgraph network plus, called CoHHGN+. The result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#65292;&#24182;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.09780</link><description>&lt;p&gt;
&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#26041;&#27861;&#29702;&#35299;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Generative Models with Generalized Empirical Likelihoods. (arXiv:2306.09780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#65292;&#24182;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#25429;&#33719;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#26576;&#20123;&#27169;&#22411;&#31867;&#21035;&#65292;&#27604;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#31561;&#19981;&#20801;&#35768;&#31934;&#30830;&#20284;&#28982;&#24230;&#37327;&#30340;&#27169;&#22411;&#65292;&#23588;&#20854;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#35768;&#22810;&#32570;&#38519;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#30697;&#26465;&#20214;&#35268;&#23450;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21738;&#20123;&#27169;&#24335;&#34987;&#21024;&#38500;&#12289;DGM&#23384;&#22312;&#30340;&#27169;&#24335;&#19981;&#24179;&#34913;&#31243;&#24230;&#20197;&#21450;DGM&#26159;&#21542;&#36275;&#22815;&#25429;&#33719;&#31867;&#20869;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32467;&#21512;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#21644;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#30340;&#25216;&#26415;&#65292;&#19981;&#20165;&#21019;&#36896;&#20102;&#20445;&#30041;&#27599;&#20010;&#26679;&#26412;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#24067;&#27979;&#35797;&#65292;&#36824;&#21253;&#25324;&#26631;&#31614;&#20449;&#24687;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27979;&#35797;&#21487;&#20197;&#39044;&#27979;&#27169;&#24335;&#38477;&#20302;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how well a deep generative model captures a distribution of high-dimensional data remains an important open challenge. It is especially difficult for certain model classes, such as Generative Adversarial Networks and Diffusion Models, whose models do not admit exact likelihoods. In this work, we demonstrate that generalized empirical likelihood (GEL) methods offer a family of diagnostic tools that can identify many deficiencies of deep generative models (DGMs). We show, with appropriate specification of moment conditions, that the proposed method can identify which modes have been dropped, the degree to which DGMs are mode imbalanced, and whether DGMs sufficiently capture intra-class diversity. We show how to combine techniques from Maximum Mean Discrepancy and Generalized Empirical Likelihood to create not only distribution tests that retain per-sample interpretability, but also metrics that include label information. We find that such tests predict the degree of mode dr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25209;&#37327;&#22823;&#23567;&#36873;&#25321;&#65292;&#31283;&#23450;&#20102;&#39118;&#38505;&#34892;&#20026;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;</title><link>http://arxiv.org/abs/2306.08432</link><description>&lt;p&gt;
&#25209;&#27425;&#20351;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;&#35268;&#33539;&#39118;&#38505;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Batches Stabilize the Minimum Norm Risk in High Dimensional Overparameterized Linear Regression. (arXiv:2306.08432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25209;&#37327;&#22823;&#23567;&#36873;&#25321;&#65292;&#31283;&#23450;&#20102;&#39118;&#38505;&#34892;&#20026;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#36890;&#24120;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#26377;&#29992;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#36890;&#36807;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#29305;&#24449;&#30340;&#26368;&#23567;&#35268;&#33539;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#25209;&#37327;&#20998;&#21306;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#24314;&#35758;&#26368;&#23567;&#35268;&#33539;&#20272;&#35745;&#37327;&#30340;&#33258;&#28982;&#23567;&#25209;&#37327;&#29256;&#26412;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#34920;&#26126;&#20854;&#19982;&#22122;&#22768;&#27700;&#24179;&#20197;&#21450;&#36807;&#24230;&#21442;&#25968;&#21270;&#27604;&#20363;&#25104;&#21453;&#27604;&#65292;&#23545;&#20110;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#30340;&#36873;&#25321;&#12290;&#19982;&#26368;&#23567;&#35268;&#33539;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#31283;&#23450;&#30340;&#39118;&#38505;&#34892;&#20026;&#65292;&#20854;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#27604;&#20363;&#19978;&#21333;&#35843;&#36882;&#22686;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25209;&#22788;&#29702;&#25152;&#25552;&#20379;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#37325;&#21472;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning algorithms that divide the data into batches are prevalent in many machine-learning applications, typically offering useful trade-offs between computational efficiency and performance. In this paper, we examine the benefits of batch-partitioning through the lens of a minimum-norm overparameterized linear regression model with isotropic Gaussian features. We suggest a natural small-batch version of the minimum-norm estimator, and derive an upper bound on its quadratic risk, showing it is inversely proportional to the noise level as well as to the overparameterization ratio, for the optimal choice of batch size. In contrast to minimum-norm, our estimator admits a stable risk behavior that is monotonically increasing in the overparameterization ratio, eliminating both the blowup at the interpolation point and the double-descent phenomenon. Interestingly, we observe that this implicit regularization offered by the batch partition is partially explained by feature overlap between t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18462</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#35821;&#35328;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks against Language Models via Neighbourhood Comparison. (arXiv:2305.18462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#37051;&#22495;&#27604;&#36739;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#26469;&#25552;&#39640;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#36825;&#20123;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#23384;&#22312;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#25915;&#20987;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#30340;&#27010;&#29575;&#20998;&#37197;&#32473;&#35757;&#32451;&#26679;&#26412;&#32780;&#38750;&#38750;&#35757;&#32451;&#28857;&#12290;&#28982;&#32780;&#65292;&#23545;&#27169;&#22411;&#20998;&#25968;&#30340;&#31616;&#21333;&#38408;&#20540;&#35774;&#23450;&#24448;&#24448;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#26679;&#26412;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21442;&#32771;&#27169;&#22411;&#30340;&#25915;&#20987;&#21487;&#20197;&#23558;&#27169;&#22411;&#20998;&#25968;&#19982;&#22312;&#31867;&#20284;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21442;&#32771;&#27169;&#22411;&#33719;&#24471;&#30340;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;MIAs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35757;&#32451;&#21442;&#32771;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#30340;&#20570;&#27861;&#26159;&#20551;&#23450;&#25932;&#26041;&#30693;&#36947;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20551;&#23450;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#37051;&#22495;&#26679;&#26412;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#22312;&#26356;&#29616;&#23454;&#30340;&#25104;&#21592;&#25512;&#26029;&#22330;&#26223;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#20960;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#23545;&#35805;&#29983;&#25104;&#65292;&#24182;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic sce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#37325;&#24314;&#22270;&#20687;&#65292;&#32780;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#31561;&#20851;&#38190;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.08265</link><description>&lt;p&gt;
&#26080;&#38656;&#27531;&#24046;&#35745;&#31639;&#30340;&#36710;&#36742;&#26816;&#27979;&#19982;&#20998;&#31867;&#65306;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#27880;&#20837;&#21152;&#36895;HEVC&#22270;&#20687;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Detection and Classification without Residual Calculation: Accelerating HEVC Image Decoding with Random Perturbation Injection. (arXiv:2305.08265v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#30417;&#25511;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25200;&#21160;&#37325;&#24314;&#22270;&#20687;&#65292;&#32780;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#24314;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#31561;&#20851;&#38190;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20998;&#26512;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#20132;&#36890;&#30417;&#25511;&#26041;&#38754;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#39640;&#25928;&#35270;&#39057;&#25968;&#25454;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35270;&#39057;&#35299;&#30721;&#25216;&#26415;&#35745;&#31639;&#23494;&#38598;&#19988;&#26102;&#38388;&#32791;&#36153;&#22823;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#22312;&#21387;&#32553;&#22495;&#20013;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25200;&#21160;&#30340;&#21387;&#32553;&#22495;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#20132;&#36890;&#30417;&#25511;&#24212;&#29992;&#65292;&#22312;HEVC&#27604;&#29305;&#27969;&#20013;&#37325;&#24314;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#24314;&#35758;&#20351;&#29992;&#38543;&#26426;&#25200;&#21160;&#20195;&#26367;&#27531;&#24046;&#25968;&#20540;&#65292;&#21019;&#36896;&#20986;&#21407;&#22987;&#22270;&#20687;&#30340;&#31934;&#31616;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#19982;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37325;&#28857;&#20851;&#27880;&#36710;&#36742;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#36890;&#36807;&#19981;&#20351;&#29992;&#27531;&#24046;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#22270;&#20687;&#37325;&#26500;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of video analytics, particularly traffic surveillance, there is a growing need for efficient and effective methods for processing and understanding video data. Traditional full video decoding techniques can be computationally intensive and time-consuming, leading researchers to explore alternative approaches in the compressed domain. This study introduces a novel random perturbation-based compressed domain method for reconstructing images from High Efficiency Video Coding (HEVC) bitstreams, specifically designed for traffic surveillance applications. To the best of our knowledge, our method is the first to propose substituting random perturbations for residual values, creating a condensed representation of the original image while retaining information relevant to video understanding tasks, particularly focusing on vehicle detection and classification as key use cases.  By not using residual data, our proposed method significantly reduces the data needed in the image recon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32852;&#21512;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#26696;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07448</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Deterministic Policy Gradient for End-to-End Communication Systems without Prior Channel Knowledge. (arXiv:2305.07448v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#26080;&#20808;&#39564;&#20449;&#36947;&#30693;&#35782;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32852;&#21512;&#35757;&#32451;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#26696;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#20102;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#22522;&#20110;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#20849;&#21516;&#20248;&#21270;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#21457;&#36865;&#22120;&#21644;&#25509;&#25910;&#22120;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;E2E&#23398;&#20064;&#26550;&#26500;&#38656;&#35201;&#20808;&#21069;&#30340;&#21487;&#24494;&#20998;&#20449;&#36947;&#27169;&#22411;&#26469;&#20849;&#21516;&#35757;&#32451;&#21457;&#23556;&#25509;&#25910;&#22120;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#20960;&#20046;&#19981;&#21487;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#25509;&#25910;&#22120;DNN&#30340;&#25439;&#22833;&#20540;&#20316;&#20026;&#22870;&#21169;&#26469;&#35757;&#32451;&#21457;&#23556;&#22120;DNN&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#21457;&#23556;&#22120;&#21644;&#25509;&#25910;&#22120;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#30340;&#20449;&#36947;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;DDPG&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23454;&#29616;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End (E2E) learning-based concept has been recently introduced to jointly optimize both the transmitter and the receiver in wireless communication systems. Unfortunately, this E2E learning architecture requires a prior differentiable channel model to jointly train the deep neural networks (DNNs) at the transceivers, which is hardly obtained in practice. This paper aims to solve this issue by developing a deep deterministic policy gradient (DDPG)-based framework. In particular, the proposed solution uses the loss value of the receiver DNN as the reward to train the transmitter DNN. The simulation results then show that our proposed solution can jointly train the transmitter and the receiver without requiring the prior channel model. In addition, we demonstrate that the proposed DDPG-based solution can achieve better detection performance compared to the state-of-the-art solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#19981;&#26399;&#26395;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;</title><link>http://arxiv.org/abs/2305.07176</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automatic Radiology Report Generation by Learning with Increasingly Hard Negatives. (arXiv:2305.07176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#19981;&#26029;&#22256;&#38590;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#23454;&#29616;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#36991;&#20813;&#20135;&#29983;&#19981;&#26399;&#26395;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21307;&#23398;&#22270;&#20687;&#25110;&#25253;&#21578;&#36890;&#24120;&#30001;&#20110;&#35299;&#21078;&#32467;&#26500;&#30340;&#30456;&#21516;&#20869;&#23481;&#32780;&#30456;&#20284;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20010;&#20307;&#22270;&#20687;&#30340;&#29420;&#29305;&#24615;&#65292;&#23481;&#26131;&#20135;&#29983;&#19981;&#26399;&#26395;&#30340;&#36890;&#29992;&#25110;&#19981;&#21305;&#37197;&#30340;&#25253;&#21578;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21306;&#20998;&#26368;&#25509;&#36817;&#30340;&#36127;&#26679;&#26412;&#65292;&#21363;&#22256;&#38590;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#21028;&#21035;&#22270;&#20687;&#21644;&#25253;&#21578;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#33719;&#24471;&#26356;&#20855;&#21028;&#21035;&#33021;&#21147;&#30340;&#29305;&#24449;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#36880;&#28176;&#25552;&#39640;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20026;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#21019;&#24314;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#19981;&#26029;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#35270;&#20026;&#36741;&#21161;&#21464;&#37327;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36807;&#31243;&#21046;&#23450;&#20026;&#19968;&#20010;&#26368;&#23567;-&#26368;&#22823;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report generation is challenging as medical images or reports are usually similar to each other due to the common content of anatomy. This makes a model hard to capture the uniqueness of individual images and is prone to producing undesired generic or mismatched reports. This situation calls for learning more discriminative features that could capture even fine-grained mismatches between images and reports. To achieve this, this paper proposes a novel framework to learn discriminative image and report features by distinguishing them from their closest peers, i.e., hard negatives. Especially, to attain more discriminative features, we gradually raise the difficulty of such a learning task by creating increasingly hard negative reports for each image in the feature space during training, respectively. By treating the increasingly hard negatives as auxiliary variables, we formulate this process as a min-max alternating optimisation problem. At each iteration, condition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06329</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65306;&#21151;&#33021;&#21644;&#34920;&#31034;&#24615;&#27979;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Network Models: A Survey of Functional and Representational Measures. (arXiv:2305.06329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#19988;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20197;&#20102;&#35299;&#21644;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#35266;&#28857;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#26159;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20004;&#20010;&#20114;&#34917;&#30340;&#35266;&#28857;&#65292;&#21363;(i) &#34920;&#31034;&#24615;&#30456;&#20284;&#65292;&#32771;&#34385;&#20013;&#38388;&#31070;&#32463;&#23618;&#30340;&#28608;&#27963;&#24046;&#24322;&#65292;&#21644;(ii) &#21151;&#33021;&#30456;&#20284;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24615;&#27979;&#37327;&#30340;&#23478;&#26063;&#12290;&#38500;&#20102;&#25552;&#20379;&#29616;&#26377;&#27979;&#37327;&#30340;&#35814;&#32454;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#21487;&#20197;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#20026;&#25105;&#20204;&#30340;&#31038;&#21306;&#21442;&#19982;&#26356;&#22810;&#26377;&#29992;&#30340;&#24037;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity of neural networks has become an issue of great importance and research interest to understand and utilize differences of neural networks. While there are several perspectives on how neural networks can be similar, we specifically focus on two complementing perspectives, i.e., (i) representational similarity, which considers how activations of intermediate neural layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In this survey, we provide a comprehensive overview of these two families of similarity measures for neural network models. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties and relationships of these measures, and point to open research problems. Further, we provide practical recommendations that can guide researchers as well as practitioners in applying the measures. We hope our work lays a foundation for our community to engage in more s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#65292;&#21457;&#29616;&#25104;&#20687;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05927</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33181;&#20851;&#33410;X&#32447;&#29031;&#29255;&#39044;&#27979;&#39628;&#32929;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Predicting Progression of Patellofemoral Osteoarthritis Based on Lateral Knee Radiographs, Demographic Data and Symptomatic Assessments. (arXiv:2305.05927v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#33181;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#36827;&#23637;&#65292;&#21457;&#29616;&#25104;&#20687;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;&#39628;&#32929;&#20851;&#33410;&#39592;&#20851;&#33410;&#28814;&#65288;PFOA&#65289;&#22312;&#19971;&#24180;&#20869;&#25918;&#23556;&#24615;&#36827;&#23637;&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#26469;&#33258;MOST&#30740;&#31350;&#22522;&#32447;&#30340;&#20027;&#20307;&#65288;1832&#20010;&#20027;&#20307;&#65292;3276&#20010;&#33181;&#30422;&#65289;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#30340;&#26631;&#24535;&#26816;&#27979;&#24037;&#20855;&#65288;BoneFinder&#65289;&#22312;&#33181;&#20851;&#33410;X&#32447;&#30340;&#20391;&#38754;&#35782;&#21035;PF&#20851;&#33410;&#24863;&#20852;&#36259;&#21306;&#22495;&#12290;&#22312;5&#20493;&#20132;&#21449;&#39564;&#35777;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#25104;&#20687;&#25968;&#25454;&#24320;&#21457;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;DL&#26041;&#27861;&#26469;&#39044;&#27979;PFOA&#36827;&#23637;&#12290;&#24320;&#21457;&#20102;&#19968;&#32452;&#22522;&#20110;&#24050;&#30693;&#39118;&#38505;&#22240;&#32032;&#30340;&#22522;&#32447;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#39118;&#38505;&#22240;&#32032;&#21253;&#25324;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;BMI&#21644;WOMAC&#35780;&#20998;&#65292;&#20197;&#21450;&#33003;&#39592;-&#32929;&#39592;&#20851;&#33410;&#30340;&#25918;&#23556;&#24615;&#39592;&#20851;&#33410;&#28814;&#20998;&#26399;&#65288;KL&#20998;&#25968;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25104;&#20687;&#21644;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#22312;&#20010;&#20307;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#27169;&#22411;&#30340;&#24615;&#33021;&#33719;&#24471;&#20102;&#26368;&#20339;&#32508;&#21512;&#24615;&#33021;&#65292;AUC-ROC&#20026;0.83&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;DL&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#39044;&#27979;PFOA&#36827;&#23637;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#38500;&#20102;&#20020;&#24202;&#25968;&#25454;&#22806;&#36824;&#38656;&#35201;&#32435;&#20837;&#25104;&#20687;&#25968;&#25454;&#20197;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel framework that utilizes deep learning (DL) and attention mechanisms to predict the radiographic progression of patellofemoral osteoarthritis (PFOA) over a period of seven years. This study included subjects (1832 subjects, 3276 knees) from the baseline of the MOST study. PF joint regions-of-interest were identified using an automated landmark detection tool (BoneFinder) on lateral knee X-rays. An end-to-end DL method was developed for predicting PFOA progression based on imaging data in a 5-fold cross-validation setting. A set of baselines based on known risk factors were developed and analyzed using gradient boosting machine (GBM). Risk factors included age, sex, BMI and WOMAC score, and the radiographic osteoarthritis stage of the tibiofemoral joint (KL score). Finally, we trained an ensemble model using both imaging and clinical data. Among the individual models, the performance of our deep convolutional neural network attention model achieved the b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03960</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#27969;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#35268;&#21017;&#20043;&#22806;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;(arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text. (arXiv:2305.03960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;PET&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#21160;&#21019;&#24314;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#32321;&#29712;&#24037;&#20316;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#21516;&#19968;&#27969;&#31243;&#23454;&#20307;&#37325;&#22797;&#25552;&#21450;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#33258;&#21160;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#65292;&#21487;&#36991;&#20813;&#25163;&#21160;&#21019;&#24314;&#24418;&#24335;&#21270;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#20174;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#27969;&#31243;&#23454;&#20307;&#65288;&#22914;&#21442;&#19982;&#32773;&#12289;&#27963;&#21160;&#12289;&#23545;&#35937;&#31561;&#65289;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#24102;&#26377;&#25991;&#26412;&#27969;&#31243;&#25551;&#36848;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;(PET)&#24050;&#32463;&#20986;&#29256;&#65292;&#20854;&#20276;&#38543;&#30528;&#19968;&#31181;&#22522;&#26412;&#30340;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#65292;PET&#32570;&#20047;&#26377;&#20851;&#20004;&#20010;&#25552;&#21450;&#26159;&#21542;&#25351;&#20195;&#20102;&#30456;&#21516;&#25110;&#19981;&#21516;&#30340;&#27969;&#31243;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#26159;&#21542;&#22312;&#30446;&#26631;&#27169;&#22411;&#20013;&#21019;&#24314;&#19968;&#20010;&#25110;&#20004;&#20010;&#24314;&#27169;&#20803;&#32032;&#30340;&#37325;&#35201;&#20915;&#31574;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#20363;&#22914;&#65292;&#20004;&#20010;&#25968;&#25454;&#22788;&#29702;&#30340;&#25552;&#21450;&#26159;&#21542;&#24847;&#21619;&#30528;&#22788;&#29702;&#19981;&#21516;&#25110;&#30456;&#21516;&#30340;&#25968;&#25454;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#27969;&#31243;&#23454;&#20307;&#30340;&#25552;&#21450;&#26469;&#25193;&#23637;PET&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#32447;&#25216;&#26415;&#27969;&#31243;&#25552;&#21462;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Automated generation of business process models from natural language text is an emerging methodology for avoiding the manual creation of formal business process models. For this purpose, process entities like actors, activities, objects etc., and relations among them are extracted from textual process descriptions. A high-quality annotated corpus of textual process descriptions (PET) has been published accompanied with a basic process extraction approach. In its current state, however, PET lacks information about whether two mentions refer to the same or different process entities, which corresponds to the crucial decision of whether to create one or two modeling elements in the target model. Consequently, it is ambiguous whether, for instance, two mentions of data processing mean processing of different, or the same data. In this paper, we extend the PET dataset by clustering mentions of process entities and by proposing a new baseline technique for process extraction equipped with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#39044;&#27979;&#26612;&#27833;&#26426;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20026;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13799</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#26612;&#27833;&#26426;&#27668;&#20307;&#27969;&#21160;&#21160;&#21147;&#23398;&#21644;&#26410;&#30693;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks for predicting gas flow dynamics and unknown parameters in diesel engines. (arXiv:2304.13799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#39044;&#27979;&#26612;&#27833;&#26426;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#20026;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#26612;&#27833;&#26426;&#20581;&#24247;&#30417;&#27979;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#21457;&#21160;&#26426;&#21160;&#21147;&#23398;&#65292;&#35782;&#21035;&#8220;&#24179;&#22343;&#20540;&#8221;&#27169;&#22411;&#20013;&#30340;&#26410;&#30693;&#21442;&#25968;&#65292;&#24182;&#39044;&#27979;&#32500;&#25252;&#38656;&#27714;&#12290;PINN&#27169;&#22411;&#24212;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#20960;&#20309;&#28065;&#36718;&#22686;&#21387;&#22120;&#21644;&#24223;&#27668;&#20877;&#24490;&#29615;&#30340;&#26612;&#27833;&#26426;&#65292;&#20351;&#29992;&#36873;&#23450;&#29366;&#24577;&#21464;&#37327;&#30340;&#27979;&#37327;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#19979;&#65292;PINN&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#22320;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#21644;&#21160;&#24577;&#65292;&#33258;&#36866;&#24212;&#26435;&#37325;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21487;&#21152;&#36895;&#25910;&#25947;&#12290;&#36825;&#20123;&#27169;&#25311;&#30340;&#36755;&#20837;&#25968;&#25454;&#26469;&#33258;&#23454;&#38469;&#21457;&#21160;&#26426;&#36816;&#34892;&#26465;&#20214;&#65292;&#32780;&#36755;&#20986;&#25968;&#25454;&#26159;&#27169;&#25311;&#25968;&#25454;&#65292;&#20351;&#36825;&#25104;&#20026;PINN&#39044;&#27979;&#30495;&#23454;&#21160;&#24577;&#31995;&#32479;&#33021;&#21147;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#12290;&#26612;&#27833;&#26426;&#30340;&#24179;&#22343;&#20540;&#27169;&#22411;&#21253;&#25324;&#32463;&#39564;&#20844;&#24335;&#26469;&#34920;&#31034;&#26576;&#20123;&#29366;&#24577;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;PINN&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#22320;&#39044;&#27979;&#21160;&#24577;&#24182;&#35782;&#21035;&#26410;&#30693;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a physics-informed neural network (PINN) approach for monitoring the health of diesel engines. The aim is to evaluate the engine dynamics, identify unknown parameters in a "mean value" model, and anticipate maintenance requirements. The PINN model is applied to diesel engines with a variable-geometry turbocharger and exhaust gas recirculation, using measurement data of selected state variables. The results demonstrate the ability of the PINN model to predict simultaneously both unknown parameters and dynamics accurately with both clean and noisy data, and the importance of the self-adaptive weight in the loss function for faster convergence. The input data for these simulations are derived from actual engine running conditions, while the outputs are simulated data, making this a practical case study of PINN's ability to predict real-world dynamical systems. The mean value model of the diesel engine incorporates empirical formulae to represent certain states, but the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.13710</link><description>&lt;p&gt;
&#24102;&#31181;&#26893;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#65306;&#19968;&#31181;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hopfield model with planted patterns: a teacher-student self-supervised learning model. (arXiv:2304.13710v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;Hopfield&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#65292;&#34429;&#28982;&#19968;&#20123;&#26465;&#20214;&#23545;&#20110;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#36825;&#31181;&#23398;&#20064;&#27169;&#24335;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Hopfield&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#35760;&#24518;&#23384;&#20648;&#21644;&#26816;&#32034;&#30340;&#20856;&#22411;&#27169;&#22411;&#65292;&#20294;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#32467;&#26500;&#21270;&#27169;&#24335;&#30340;Hopfield&#27169;&#22411;&#30340;&#36866;&#24403;&#25512;&#24191;&#26469;&#26500;&#24314;Boltzmann&#26426;&#30340;&#24072;&#29983;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#33258;&#26059;&#21464;&#37327;&#26159;&#26426;&#22120;&#26435;&#37325;&#65292;&#27169;&#24335;&#23545;&#24212;&#20110;&#35757;&#32451;&#38598;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#30456;&#22270;&#26469;&#20998;&#26512;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#20123;&#30456;&#22270;&#26159;&#36890;&#36807;&#35757;&#32451;&#38598;&#22823;&#23567;&#12289;&#25968;&#25454;&#38598;&#22122;&#22768;&#21644;&#25512;&#26029;&#28201;&#24230;&#65288;&#21363;&#26435;&#37325;&#27491;&#21017;&#21270;&#65289;&#26469;&#26500;&#24314;&#30340;&#12290;&#20351;&#29992;&#23567;&#32780;&#23500;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#26426;&#22120;&#21487;&#20197;&#36890;&#36807;&#35760;&#24518;&#26469;&#23398;&#20064;&#12290;&#20351;&#29992;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#31034;&#20363;&#25968;&#20197;&#36229;&#36807;&#20020;&#30028;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#65292;&#31995;&#32479;&#30340;&#23384;&#20648;&#38480;&#21046;&#25104;&#20026;&#20135;&#29983;&#19968;&#31181;&#23398;&#20064;&#27169;&#24335;&#30340;&#26426;&#20250;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#31995;&#32479;&#21487;&#20197;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Hopfield networks are known as paradigmatic models for memory storage and retrieval, modern artificial intelligence systems mainly stand on the machine learning paradigm. We show that it is possible to formulate a teacher-student self-supervised learning problem with Boltzmann machines in terms of a suitable generalization of the Hopfield model with structured patterns, where the spin variables are the machine weights and patterns correspond to the training set's examples. We analyze the learning performance by studying the phase diagram in terms of the training set size, the dataset noise and the inference temperature (i.e. the weight regularization). With a small but informative dataset the machine can learn by memorization. With a noisy dataset, an extensive number of examples above a critical threshold is needed. In this regime the memory storage limits of the system becomes an opportunity for the occurrence of a learning regime in which the system can generalize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;</title><link>http://arxiv.org/abs/2304.10717</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;Navier-Stokes&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Network Combined with Characteristic-Based Split for Solving Navier-Stokes Equations. (arXiv:2304.10717v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#20998;&#35010;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27714;&#35299;&#26102;&#38388;&#20381;&#36182;&#30340;Navier-Stokes&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#23558;&#36755;&#20986;&#21442;&#25968;&#21644;&#30456;&#24212;&#30340;&#25439;&#22833;&#20540;&#20998;&#31163;&#65292;&#20351;&#36755;&#20986;&#21442;&#25968;&#38388;&#30340;&#26435;&#37325;&#19981;&#34987;&#32771;&#34385;&#12290;&#19981;&#26159;&#25152;&#26377;&#30340;&#20559;&#23548;&#25968;&#37117;&#21442;&#19982;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#20854;&#20313;&#39033;&#20250;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#19982;&#20256;&#32479;&#30340;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#36895;&#12290;&#26412;&#26041;&#27861;&#23558;&#26631;&#31614;&#25968;&#25454;&#12289;&#29289;&#29702;&#32422;&#26463;&#21644;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#20808;&#39564;&#20449;&#24687;&#65292;&#23558;N-S&#26041;&#31243;&#30340;&#27531;&#24046;&#35270;&#20026;&#21518;&#39564;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#39537;&#21160;&#21644;&#26080;&#25968;&#25454;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;&#23427;&#33021;&#22815;&#27714;&#35299;&#21487;&#21387;&#32553;&#30340;&#27973;&#27700;&#26041;&#31243;&#21644;&#19981;&#21487;&#21387;&#32553;&#30340;N-S&#26041;&#31243;&#12290;&#30001;&#20110;&#36793;&#30028;&#26465;&#20214;&#24050;&#30693;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#33719;&#24471;&#27969;&#22330;&#20449;&#24687;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, physics-informed neural network (PINN) based on characteristic-based split (CBS) is proposed, which can be used to solve the time-dependent Navier-Stokes equations (N-S equations). In this method, The output parameters and corresponding losses are separated, so the weights between output parameters are not considered. Not all partial derivatives participate in gradient backpropagation, and the remaining terms will be reused.Therefore, compared with traditional PINN, this method is a rapid version. Here, labeled data, physical constraints and network outputs are regarded as priori information, and the residuals of the N-S equations are regarded as posteriori information. So this method can deal with both data-driven and data-free problems. As a result, it can solve the special form of compressible N-S equations -- -Shallow-Water equations, and incompressible N-S equations. As boundary conditions are known, this method only needs the flow field information at a certain tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.06833</link><description>&lt;p&gt;
&#35780;&#20272;-&#20248;&#21270;&#26041;&#27861;&#19982;&#38598;&#25104;&#35780;&#20272;&#20248;&#21270;&#27861;&#65306;&#22522;&#20110;&#38543;&#26426;&#20248;&#21183;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#38500;&#20102;&#38656;&#35201;&#20248;&#21270;&#20219;&#21153;&#65292;&#36824;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;&#26368;&#20339;&#32463;&#39564;&#30446;&#26631;&#24615;&#33021;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#38598;&#25104;&#20272;&#35745;&#21644;&#20248;&#21270;&#36807;&#31243;&#12290;&#24403;&#27169;&#22411;&#34987;&#38169;&#35823;&#22320;&#25351;&#23450;&#26102;&#65292;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#26174;&#31034;&#20986;&#20248;&#20110;&#31616;&#21333;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#25490;&#24207;&#22312;&#24378;&#28872;&#30340;&#24847;&#20041;&#19979;&#34987;&#39072;&#20498;&#12290;&#22312;&#21463;&#38480;&#26465;&#20214;&#21644;&#24403;&#19978;&#19979;&#25991;&#29305;&#24449;&#21487;&#29992;&#26102;&#65292;&#31867;&#20284;&#30340;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2304.05365</link><description>&lt;p&gt;
&#25105;&#20204;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#21527;&#65311;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20010;&#24615;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Did we personalize? Assessing personalization by an online reinforcement learning algorithm using resampling. (arXiv:2304.05365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#22797;&#37319;&#26679;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#22312;&#32447; RL &#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#31243;&#24230;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20010;&#24615;&#21270;&#27835;&#30103;&#24207;&#21015;&#20197;&#25903;&#25345;&#29992;&#25143;&#37319;&#21462;&#26356;&#20581;&#24247;&#30340;&#34892;&#20026;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#31181;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;&#28041;&#21450;&#21040;&#22522;&#20110;&#29992;&#25143;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#27963;&#21160;&#27700;&#24179;&#12289;&#20301;&#32622;&#31561;&#65289;&#22312;&#20309;&#26102;&#27835;&#30103;&#20197;&#21450;&#22914;&#20309;&#27835;&#30103;&#30340;&#20915;&#23450;&#12290;&#22312;&#32447;RL&#31639;&#27861;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#21382;&#21490;&#21453;&#39304;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20010;&#24615;&#21270;&#36825;&#20123;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#35201;&#20915;&#23450;&#26159;&#21542;&#24212;&#22312;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#20248;&#21270;&#8221;&#24178;&#39044;&#20013;&#21253;&#21547;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25968;&#25454;&#35777;&#25454;&#65292;&#34920;&#26126;RL&#31639;&#27861;&#23454;&#38469;&#19978;&#27491;&#22312;&#23558;&#27835;&#30103;&#20010;&#24615;&#21270;&#36866;&#24212;&#20854;&#29992;&#25143;&#12290;&#30001;&#20110;RL&#31639;&#27861;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#23545;&#20854;&#22312;&#26576;&#20123;&#29366;&#24577;&#19979;&#30340;&#23398;&#20064;&#24182;&#20351;&#29992;&#27492;&#23398;&#20064;&#26469;&#25552;&#20379;&#29305;&#23450;&#27835;&#30103;&#30340;&#33021;&#21147;&#20135;&#29983;&#35823;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#23450;&#20041;&#30340;&#20010;&#24615;&#21270;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#37325;&#22797;&#37319;&#26679;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#22312;&#32447;RL&#31639;&#27861;&#23454;&#29616;&#30340;&#20010;&#24615;&#21270;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#23383;&#20581;&#24247;&#30340;&#20010;&#24615;&#21270;&#24178;&#39044;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in using reinforcement learning (RL) to personalize sequences of treatments in digital health to support users in adopting healthier behaviors. Such sequential decision-making problems involve decisions about when to treat and how to treat based on the user's context (e.g., prior activity level, location, etc.). Online RL is a promising data-driven approach for this problem as it learns based on each user's historical responses and uses that knowledge to personalize these decisions. However, to decide whether the RL algorithm should be included in an ``optimized'' intervention for real-world deployment, we must assess the data evidence indicating that the RL algorithm is actually personalizing the treatments to its users. Due to the stochasticity in the RL algorithm, one may get a false impression that it is learning in certain states and using this learning to provide specific treatments. We use a working definition of personalization and introduce a resamp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.03442</link><description>&lt;p&gt;
&#29983;&#25104;&#20195;&#29702;: &#20154;&#31867;&#34892;&#20026;&#30340;&#20132;&#20114;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#21487;&#36171;&#33021;&#20110;&#20174;&#27785;&#28024;&#24335;&#29615;&#22659;&#21040;&#20154;&#38469;&#20132;&#27969;&#25490;&#32451;&#31354;&#38388;&#21040;&#21407;&#22411;&#24037;&#20855;&#30340;&#20132;&#20114;&#24335;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#20195;&#29702;&#8212;&#8212;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#30340;&#35745;&#31639;&#26426;&#36719;&#20214;&#20195;&#29702;&#12290;&#29983;&#25104;&#20195;&#29702;&#20250;&#36215;&#24202;&#65292;&#20570;&#26089;&#39184;&#65292;&#21435;&#24037;&#20316;&#65307;&#33402;&#26415;&#23478;&#30011;&#30011;&#65292;&#20316;&#23478;&#20889;&#20316;&#65307;&#20182;&#20204;&#24418;&#25104;&#35266;&#28857;&#65292;&#20114;&#30456;&#27880;&#24847;&#65292;&#24182;&#24320;&#22987;&#20132;&#35848;&#65307;&#20182;&#20204;&#22238;&#24518;&#36807;&#21435;&#30340;&#26085;&#23376;&#24182;&#35745;&#21010;&#26410;&#26469;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#20195;&#29702;&#33021;&#22815;&#23454;&#29616;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23384;&#20648;&#20195;&#29702;&#30340;&#32463;&#21382;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32508;&#21512;&#36825;&#20123;&#35760;&#24518;&#21040;&#26356;&#39640;&#23618;&#27425;&#30340;&#21453;&#24605;&#65292;&#20197;&#21450;&#21160;&#24577;&#26816;&#32034;&#36825;&#20123;&#35760;&#24518;&#20197;&#35268;&#21010;&#34892;&#20026;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#29983;&#25104;&#20195;&#29702;&#20197;&#22635;&#20805;&#21463;&#12298;&#27169;&#25311;&#20154;&#29983;&#12299;&#21551;&#21457;&#30340;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#19982;25&#20010;&#20195;&#29702;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#31243;&#24207;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.00989</link><description>&lt;p&gt;
&#27867;&#22411;&#28304;&#20195;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Execution of Generic Source Code. (arXiv:2304.00989v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#31243;&#24207;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#21542;&#20351;&#29992;&#26681;&#25454;&#28304;&#20195;&#30721;&#32452;&#21512;&#32780;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#36880;&#35821;&#21477;&#25191;&#34892;Python&#31243;&#24207;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#35299;&#37322;&#65288;Neural Interpretation&#65292;NI&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25353;&#29031;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#28304;&#20195;&#30721;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20801;&#35768;&#32570;&#22833;&#23450;&#20041;&#12290;NI&#20445;&#30041;&#28304;&#20195;&#30721;&#32467;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;&#21464;&#37327;&#37117;&#26377;&#19968;&#20010;&#21521;&#37327;&#32534;&#30721;&#65292;&#27599;&#20010;&#20989;&#25968;&#25191;&#34892;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;NI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#20855;&#26377;&#32534;&#35793;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#32452;&#35013;&#30001;&#28304;&#20195;&#30721;&#8220;&#32534;&#31243;&#8221;&#30340;&#31070;&#32463;&#23618;&#12290;NI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#28789;&#27963;&#30340;&#20195;&#30721;&#29702;&#35299;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#26080;&#20855;&#20307;&#36755;&#20837;&#30340;&#30333;&#30418;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Python program be executed statement-by-statement by neural networks composed according to the source code? We formulate the Neuro-Symbolic Execution Problem and introduce Neural Interpretation (NI), the first neural model for the execution of generic source code that allows missing definitions. NI preserves source code structure, where every variable has a vector encoding, and every function executes a neural network. NI is a novel neural model of computers with a compiler architecture that can assemble neural layers "programmed" by source code. NI is the first neural model capable of executing Py150 dataset programs, including library functions without concrete inputs, and it can be trained with flexible code understanding objectives. We demonstrate white-box execution without concrete inputs for variable misuse localization and repair.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAHALO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24110;&#21161;&#22788;&#29702;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17156</link><description>&lt;p&gt;
MAHALO: &#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. (arXiv:2303.17156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAHALO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35266;&#27979;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24110;&#21161;&#22788;&#29702;&#25968;&#25454;&#38598;&#36136;&#37327;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;&#31163;&#32447;&#35266;&#27979;&#23398;&#20064;&#65288;PLfO&#65289;&#30340;&#26032;&#30340;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#33539;&#20363;&#12290;&#31163;&#32447;PLfO&#26088;&#22312;&#20351;&#29992;&#36136;&#37327;&#19981;&#20339;&#30340;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#31574;&#30053;&#65306;1&#65289;&#20165;&#26377;&#19968;&#37096;&#20998;&#36712;&#36857;&#34987;&#26631;&#35760;&#20026;&#22870;&#21169;&#65292;2&#65289;&#26631;&#35760;&#36712;&#36857;&#21487;&#33021;&#19981;&#21253;&#21547;&#21160;&#20316;&#65292;3&#65289;&#26631;&#35760;&#36712;&#36857;&#21487;&#33021;&#36136;&#37327;&#19981;&#39640;&#65292;4&#65289;&#24635;&#20307;&#25968;&#25454;&#21487;&#33021;&#19981;&#20855;&#22791;&#20840;&#38754;&#24615;&#12290;&#36825;&#20123;&#32570;&#38519;&#22312;&#30495;&#23454;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#24456;&#24120;&#35265;&#65292;&#22240;&#27492;&#31163;&#32447;PLfO&#21253;&#25324;&#35768;&#22810;&#29616;&#26377;&#30340;&#31163;&#32447;&#23398;&#20064;&#35774;&#32622;&#65292;&#21253;&#25324;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;ILfO&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#20551;&#35774;&#36866;&#24212;&#23398;&#20064;&#30340;&#31163;&#32447;PLfO&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;MAHALO&#65289;&#12290; MAHALO&#22522;&#20110;&#31163;&#32447;RL&#20013;&#30340;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#65292;&#20351;&#29992;&#32771;&#34385;&#30001;&#20110;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24615;&#33021;&#19979;&#30028;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26377;&#20851;&#35266;&#23519;&#21644;&#21160;&#20316;&#27169;&#24577;&#30340;&#26377;&#38480;&#20551;&#35774;&#36827;&#34892;&#23545;&#25239;&#21464;&#25442;&#26469;&#23454;&#29616;&#36825;&#19968;&#24819;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAHALO&#22312;&#21508;&#31181;&#31163;&#32447;PLfO&#20219;&#21153;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new paradigm for sequential decision making, called offline Policy Learning from Observation (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the overall data may not have full coverage. Such imperfection is common in real-world learning scenarios, so offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), ILfO, and reinforcement learning (RL). In this work, we present a generic approach, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for offline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient converge. We implement this idea by adversarially tr
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#24120;&#24120;&#26080;&#27861;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Sparse Low-Dimensional Decision&#27169;&#22411;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#65292;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#26356;&#23481;&#26131;&#34987;&#20154;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#19982;&#20854;&#20182;&#23494;&#38598;&#39640;&#32500;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13166</link><description>&lt;p&gt;
&#20351;&#29992;&#23569;&#37327;&#29305;&#24449;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Take 5: Interpretable Image Classification with a Handful of Features. (arXiv:2303.13166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13166
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#26080;&#27861;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Sparse Low-Dimensional Decision&#27169;&#22411;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#36827;&#34892;&#20915;&#31574;&#65292;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#26356;&#23481;&#26131;&#34987;&#20154;&#29702;&#35299;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#19982;&#20854;&#20182;&#23494;&#38598;&#39640;&#32500;&#27169;&#22411;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#25968;&#21315;&#20010;&#22823;&#22810;&#19981;&#21487;&#29702;&#35299;&#30340;&#29305;&#24449;&#26469;&#35782;&#21035;&#21333;&#20010;&#31867;&#21035;&#65292;&#36825;&#26159;&#20219;&#20309;&#20154;&#37117;&#26080;&#27861;&#29702;&#35299;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31232;&#30095;&#20302;&#32500;&#20915;&#31574;&#23618;&#65292;&#33021;&#22815;&#37327;&#21270;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#32454;&#39063;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21482;&#26377;&#24403;&#29305;&#24449;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#21482;&#26377;&#26497;&#23569;&#37327;&#30340;&#29305;&#24449;&#29992;&#20110;&#21333;&#20010;&#20915;&#31574;&#26102;&#65292;&#20154;&#25165;&#33021;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#26368;&#21518;&#19968;&#23618;&#24517;&#39035;&#26159;&#31232;&#30095;&#19988;&#32500;&#25968;&#36739;&#20302;&#30340;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#31232;&#30095;&#20302;&#32500;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;Sparse Low-Dimensional Decision&#65288;SLDD&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#30456;&#27604;&#23494;&#38598;&#39640;&#32500;&#20915;&#31574;&#23618;&#65292;SLDD&#27169;&#22411;&#22312;&#26412;&#22320;&#21644;&#20840;&#23616;&#19978;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#33021;&#22815;&#20445;&#25345;&#31454;&#20105;&#24615;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12414</link><description>&lt;p&gt;
&#24310;&#36831;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;DFL&#22312;&#27599;&#20010;&#20840;&#23616;&#32858;&#21512;&#38388;&#38548;&#26399;&#38388;&#23545;&#35774;&#22791;&#25968;&#25454;&#38598;&#25191;&#34892;&#22810;&#20010;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#26412;&#22320;&#23376;&#32593;&#32476;&#20013;&#38388;&#26029;&#22320;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#12290;&#20113;&#26381;&#21153;&#22120;&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#21512;&#24182;&#22120;&#23558;&#26412;&#22320;&#27169;&#22411;&#19982;&#20840;&#23616;&#37096;&#32626;&#27169;&#22411;&#21516;&#27493;&#12290;DFL&#30340;&#25910;&#25947;&#34892;&#20026;&#22312;&#24191;&#20041;&#25968;&#25454;&#24322;&#36136;&#24615;&#24230;&#37327;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#24471;&#20986;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#20197;&#23454;&#29616;O(1/k)&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861;&#26469;&#23454;&#29616;DFL&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11562</link><description>&lt;p&gt;
&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#20351;&#29992;&#26082;&#33021;&#25311;&#21512;&#21448;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24378;&#20581;&#25439;&#22833;&#20989;&#25968;&#26159;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#38745;&#24577;&#26435;&#34913;&#19982;DNN&#23398;&#20064;&#26631;&#31614;&#22122;&#22768;&#30340;&#21160;&#24577;&#24615;&#30456;&#30683;&#30462;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;(DAL)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;DNN&#20542;&#21521;&#20110;&#20808;&#23398;&#20064;&#19968;&#33324;&#21270;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#36807;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;DAL&#26368;&#21021;&#22686;&#24378;&#20102;&#25311;&#21512;&#33021;&#21147;&#65292;&#28982;&#21518;&#36880;&#28176;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#25105;&#20204;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#36825;&#20123;&#26679;&#20363;&#26356;&#23481;&#26131;&#26631;&#35760;&#20026;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#24182;&#24341;&#20837;&#33258;&#20030;&#39033;&#26469;&#36827;&#19968;&#27493;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08613</link><description>&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;&#20449;&#24687;&#33719;&#21462;&#65306;&#27491;&#30830;&#35745;&#20998;&#35268;&#21017;&#36935;&#21040;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#28608;&#21169;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22996;&#25176;&#26041;&#21644;&#20195;&#29702;&#26041;&#20043;&#38388;&#30340; Stackelberg &#21338;&#24328;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#23459;&#24067;&#20102;&#19968;&#26465;&#24471;&#20998;&#35268;&#21017;&#26469;&#25351;&#23450;&#20184;&#27454;&#65292;&#28982;&#21518;&#20195;&#29702;&#26041;&#36873;&#25321;&#26368;&#22823;&#21270;&#20854;&#33258;&#36523;&#21033;&#28070;&#21644;&#25253;&#21578;&#20449;&#24687;&#30340;&#21162;&#21147;&#27700;&#24179;&#12290;&#25105;&#20204;&#20174;&#22996;&#25176;&#26041;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#21363;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20132;&#20114;&#26469;&#35774;&#35745;&#26368;&#20248;&#35745;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861; (Auer et al., 2002) &#37327;&#36523;&#23450;&#21046;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312; T &#27425;&#36845;&#20195;&#21518;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615; $T^{2/3}$-&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#23545;&#22996;&#25176;&#26041;&#26368;&#20248;&#21033;&#28070;&#36827;&#34892;&#31934;&#32454;&#20272;&#35745;&#30340;&#36807;&#31243;&#20197;&#21450;&#20445;&#23432;&#32416;&#27491;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#26041;&#30340;&#34892;&#21160;&#24471;&#21040;&#26377;&#25928;&#28608;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#26159;&#28176;&#36827;&#26368;&#23567;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#35745;&#31639;&#27010;&#29575;&#30340;&#19978;&#38480;&#25110;&#19979;&#38480;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.05396</link><description>&lt;p&gt;
&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#21644;&#20195;&#29702;&#21464;&#37327;&#38480;&#21046;&#21463;&#30410;&#21644;&#20260;&#23475;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies. (arXiv:2303.05396v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#35745;&#31639;&#27010;&#29575;&#30340;&#19978;&#38480;&#25110;&#19979;&#38480;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two methods for bounding the probabilities of benefit and harm under unmeasured confounding, one is to compute the upper or lower bound of the probability through sensitivity parameters, and the other is to derive tighter bounds using a measured nondifferential proxy of the unmeasured confounder.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#20219;&#19968;&#27010;&#29575;&#30340;&#65288;&#19978;&#38480;&#25110;&#19979;&#38480;&#65289;&#65292;&#20316;&#20026;&#35266;&#23519;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#20010;&#30452;&#35266;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#20989;&#25968;&#65292;&#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#21576;&#29616;&#32473;&#20998;&#26512;&#24072;&#20316;&#20026;2-D&#22270;&#20197;&#21327;&#21161;&#20854;&#20915;&#31574;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#26434;&#22240;&#32032;&#30340;&#27979;&#37327;&#38750;&#24046;&#24322;&#20195;&#29702;&#21464;&#37327;&#65288;&#21363;&#30452;&#25509;&#25928;&#24212;&#65289;&#12290;&#20351;&#29992;&#27492;&#20195;&#29702;&#21464;&#37327;&#65292;&#21487;&#20197;&#20174;&#20165;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#23548;&#20986;&#27604;&#29616;&#26377;&#30028;&#38480;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two methods for bounding the probabilities of benefit and harm under unmeasured confounding. The first method computes the (upper or lower) bound of either probability as a function of the observed data distribution and two intuitive sensitivity parameters which, then, can be presented to the analyst as a 2-D plot to assist her in decision making. The second method assumes the existence of a measured nondifferential proxy (i.e., direct effect) of the unmeasured confounder. Using this proxy, tighter bounds than the existing ones can be derived from just the observed data distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30005;&#27969;&#21453;&#39304;&#26469;&#20811;&#26381;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#30005;&#27969;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21452;&#36275;&#34892;&#36208;&#12290;</title><link>http://arxiv.org/abs/2303.03724</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#27969;&#21453;&#39304;&#23398;&#20064;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#21452;&#36275;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Learning Bipedal Walking for Humanoids with Current Feedback. (arXiv:2303.03724v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30005;&#27969;&#21453;&#39304;&#26469;&#20811;&#26381;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#30005;&#27969;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#21452;&#36275;&#34892;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25216;&#26415;&#21644;&#22312;&#20223;&#30495;&#20013;&#36827;&#34892;&#35757;&#32451;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#25511;&#21046;&#22120;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#20855;&#26377;&#30452;&#39537;&#25191;&#34892;&#22120;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#21644;&#20855;&#26377;&#20302;&#40831;&#36718;&#20256;&#21160;&#31995;&#32479;&#30340;&#36731;&#22411;&#21452;&#36275;&#26426;&#22120;&#20154;&#12290;&#23545;&#20110;&#30495;&#23454;&#22823;&#23567;&#30340;&#20154;&#24418;&#26426;&#22120;&#20154;&#65292;&#30001;&#20110;&#20855;&#26377;&#36739;&#22823;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#24046;&#36317;&#65292;&#24212;&#29992;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#20811;&#26381;&#20154;&#24418;&#26426;&#22120;&#20154;&#20013;&#30001;&#20110;&#22312;&#25191;&#34892;&#22120;&#32423;&#21035;&#19978;&#30340;&#25197;&#30697;&#36319;&#36394;&#19981;&#20934;&#30830;&#32780;&#20135;&#29983;&#30340;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#24046;&#36317;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36890;&#36807;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#20154;&#24037;&#38477;&#32423;&#30340;&#36139;&#24369;&#25197;&#30697;&#36319;&#36394;&#26465;&#20214;&#19979;&#21033;&#29992;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25191;&#34892;&#22120;&#30005;&#27969;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#30495;&#23454;&#30340;HRP-5P&#20154;&#24418;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#65292;&#23454;&#29616;&#21452;&#36275;&#34892;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep reinforcement learning (RL) based techniques combined with training in simulation have offered a new approach to developing robust controllers for legged robots. However, the application of such approaches to real hardware has largely been limited to quadrupedal robots with direct-drive actuators and light-weight bipedal robots with low gear-ratio transmission systems. Application to real, life-sized humanoid robots has been less common arguably due to a large sim2real gap. In this paper, we present an approach for effectively overcoming the sim2real gap issue for humanoid robots arising from inaccurate torque-tracking at the actuator level. Our key idea is to utilize the current feedback from the actuators on the real robot, after training the policy in a simulation environment artificially degraded with poor torque-tracking. Our approach successfully trains a unified, end-to-end policy in simulation that can be deployed on a real HRP-5P humanoid robot to achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26399;&#26395;&#19968;&#33268;&#24615;&#65288;EC&#65289;&#30340;&#26032;&#22411;&#26657;&#20934;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#36827;&#34892;&#21518;&#35757;&#32451;&#37325;&#26032;&#32553;&#25918;&#65292;&#20351;&#24179;&#22343;&#39564;&#35777;&#32622;&#20449;&#24230;&#19982;&#24179;&#22343;&#27491;&#30830;&#26631;&#31614;&#27604;&#20363;&#30456;&#19968;&#33268;&#65292;&#22312;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31867;&#20284;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02644</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#30340;&#26399;&#26395;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expectation consistency for calibration of neural networks. (arXiv:2303.02644v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26399;&#26395;&#19968;&#33268;&#24615;&#65288;EC&#65289;&#30340;&#26032;&#22411;&#26657;&#20934;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#36827;&#34892;&#21518;&#35757;&#32451;&#37325;&#26032;&#32553;&#25918;&#65292;&#20351;&#24179;&#22343;&#39564;&#35777;&#32622;&#20449;&#24230;&#19982;&#24179;&#22343;&#27491;&#30830;&#26631;&#31614;&#27604;&#20363;&#30456;&#19968;&#33268;&#65292;&#22312;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31867;&#20284;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#30340;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#30528;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#24050;&#32463;&#26377;&#25253;&#36947;&#25351;&#20986;&#23427;&#20204;&#22312;&#39044;&#27979;&#32622;&#20449;&#24230;&#26041;&#38754;&#24448;&#24448;&#23384;&#22312;&#36807;&#24230;&#20048;&#35266;&#30340;&#38382;&#39064;&#12290;&#23547;&#25214;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#26041;&#27861;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#26356;&#22909;&#22320;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#21162;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#26399;&#26395;&#19968;&#33268;&#24615;&#65288;EC&#65289;&#30340;&#26032;&#22411;&#26657;&#20934;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#36827;&#34892;&#21518;&#35757;&#32451;&#37325;&#26032;&#32553;&#25918;&#65292;&#24378;&#21046;&#35201;&#27714;&#24179;&#22343;&#39564;&#35777;&#32622;&#20449;&#24230;&#19982;&#24179;&#22343;&#27491;&#30830;&#26631;&#31614;&#27604;&#20363;&#30456;&#19968;&#33268;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;EC&#26041;&#27861;&#22312;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#30456;&#20284;&#30340;&#26657;&#20934;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#31867;&#20284;&#30340;&#39564;&#35777;&#26679;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;EC&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#21407;&#29702;&#65288;&#21363;Nishimori&#24658;&#31561;&#24335;&#65289;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#28176;&#36817;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their incredible performance, it is well reported that deep neural networks tend to be overoptimistic about their prediction confidence. Finding effective and efficient calibration methods for neural networks is therefore an important endeavour towards better uncertainty quantification in deep learning. In this manuscript, we introduce a novel calibration technique named expectation consistency (EC), consisting of a post-training rescaling of the last layer weights by enforcing that the average validation confidence coincides with the average proportion of correct labels. First, we show that the EC method achieves similar calibration performance to temperature scaling (TS) across different neural network architectures and data sets, all while requiring similar validation samples and computational resources. However, we argue that EC provides a principled method grounded on a Bayesian optimality principle known as the Nishimori identity. Next, we provide an asymptotic characteri
&lt;/p&gt;</description></item><item><title>NovPhy&#26159;&#19968;&#20010;&#20026;&#20102;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2303.01711</link><description>&lt;p&gt;
NovPhy&#65306;&#19968;&#20010;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#30340;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
NovPhy: A Testbed for Physical Reasoning in Open-world Environments. (arXiv:2303.01711v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01711
&lt;/p&gt;
&lt;p&gt;
NovPhy&#26159;&#19968;&#20010;&#20026;&#20102;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#23558;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#34701;&#20837;&#36825;&#20123;AI&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20294;&#20165;&#20165;&#20855;&#22791;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#36275;&#20197;&#22312;&#30495;&#23454;&#29289;&#29702;&#29615;&#22659;&#20013;&#36816;&#34892;&#65311;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38754;&#23545;&#20043;&#21069;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#26032;&#39062;&#24773;&#20917;&#12290;&#20316;&#20026;&#20154;&#31867;&#65292;&#25105;&#20204;&#33021;&#22815;&#25104;&#21151;&#22320;&#36866;&#24212;&#36825;&#20123;&#24773;&#20917;&#12290;&#21516;&#26679;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#38656;&#35201;&#20855;&#22791;&#22312;&#38754;&#23545;&#26032;&#39062;&#24773;&#20917;&#26102;&#27491;&#24120;&#36816;&#20316;&#30340;&#33021;&#21147;&#65292;&#25165;&#33021;&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#29289;&#29702;&#29615;&#22659;&#20013;&#26377;&#25928;&#36816;&#34892;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#26679;&#30340;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;NovPhy&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;&#27979;&#35797;&#24179;&#21488;&#21253;&#25324;&#38656;&#35201;&#26234;&#33021;&#20307;&#26816;&#27979;&#21644;&#36866;&#24212;&#29289;&#29702;&#22330;&#26223;&#20013;&#30340;&#26032;&#39062;&#24773;&#20917;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#21019;&#24314;&#27979;&#35797;&#24179;&#21488;&#20013;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20843;&#31181;&#20195;&#34920;&#19981;&#21516;&#26032;&#39062;&#24773;&#20917;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new testbed, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The testbed consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the testbed, we develop eight novelties representing a diverse novelt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.01254</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26641;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;(PETs)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#8212;&#8212;&#20840;&#21516;&#24577;&#21152;&#23494;(FHE)&#65292;&#23427;&#20801;&#35768;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FHE&#24212;&#29992;&#20110;&#22522;&#20110;&#26641;&#22411;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24471;&#21040;&#20102;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#65292;&#24182;&#24050;&#23454;&#29616;&#22312;Concrete-ML&#24211;&#20013;&#65292;&#35813;&#24211;&#22312;https://github.com/zama-ai/concrete-ml. &#24320;&#28304;&#12290;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;FHE&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.14353</link><description>&lt;p&gt;
&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A semantic backdoor attack against Graph Convolutional Networks. (arXiv:2302.14353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#22270;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GCNs&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;&#65292;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;GCNs&#20013;&#65292;&#20351;&#24471;&#25915;&#20987;&#27169;&#22411;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22914;&#26524;&#25915;&#20987;&#32773;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#28608;&#27963;&#20102;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#23558;&#34987;&#24694;&#24847;&#22320;&#20462;&#25913;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#26631;&#31614;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GCNs&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65288;SBAG&#65289;&#26469;&#25581;&#31034;GCNs&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;SBAG&#20351;&#29992;&#26679;&#26412;&#20013;&#30340;&#26576;&#31181;&#33410;&#28857;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;&#21040;GCNs&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) have been very effective in addressing the issue of various graph-structured related tasks, such as node classification and graph classification. However, recent research has shown that GCNs are vulnerable to a new type of threat called the backdoor attack, where the adversary can inject hidden backdoor into the GCNs so that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed to the attacker-specified target label if the hidden backdoor is activated by the attacker-defined trigger. In this paper, we investigate whether such semantic backdoor attacks are possible for GCNs and propose a Semantic Backdoor Attack against GCNs(SBAG) under the context of graph classification to reveal the existence of this security vulnerability in GCNs. The SBAG uses a certain type of node in the samples as a backdoor trigger and injects hidden backdoor into GCNs models through poisoning training data. The backdoor will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#23398;&#20064;&#36890;&#36807;&#26102;&#38388;&#65288;SLTT&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#24573;&#30053;&#20102;&#19981;&#37325;&#35201;&#30340;&#36335;&#24452;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#24320;&#38144;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.14311</link><description>&lt;p&gt;
&#38754;&#21521;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks. (arXiv:2302.14311v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#23398;&#20064;&#36890;&#36807;&#26102;&#38388;&#65288;SLTT&#65289;&#26041;&#27861;&#20197;&#25552;&#39640;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#24573;&#30053;&#20102;&#19981;&#37325;&#35201;&#30340;&#36335;&#24452;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#24320;&#38144;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#33021;&#28304;&#39640;&#25928;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35757;&#32451;&#19981;&#21487;&#24494;&#20998;&#30340;SNN&#27169;&#22411;&#65292;&#36890;&#36807;&#20195;&#29702;&#26799;&#24230;&#65288;SG&#65289;&#30340;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36739;&#22823;&#30340;&#20869;&#23384;&#24320;&#38144;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;&#23398;&#20064;&#36890;&#36807;&#26102;&#38388;&#65288;SLTT&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;BPTT&#30456;&#27604;&#22823;&#22823;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;SNN&#30340;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#26102;&#38388;&#22495;&#23545;&#26368;&#32456;&#35745;&#31639;&#30340;&#26799;&#24230;&#30340;&#36129;&#29486;&#24456;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#24573;&#30053;&#35745;&#31639;&#22270;&#20013;&#30340;&#19981;&#37325;&#35201;&#36335;&#24452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#26631;&#37327;&#20056;&#27861;&#30340;&#25968;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;&#29420;&#31435;&#20110;&#24635;&#26102;&#38388;&#27493;&#38271;&#30340;&#23567;&#20869;&#23384;&#21344;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;SLTT&#30340;&#19968;&#31181;&#21464;&#20307;SLTT-K&#65292;&#35813;&#26041;&#27861;&#21482;&#20801;&#35768;&#21453;&#21521;&#20256;&#25773;&#22312;&#29305;&#23450;&#26102;&#38388;&#27493;&#38271;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are promising energy-efficient models for neuromorphic computing. For training the non-differentiable SNN models, the backpropagation through time (BPTT) with surrogate gradients (SG) method has achieved high performance. However, this method suffers from considerable memory cost and training time during training. In this paper, we propose the Spatial Learning Through Time (SLTT) method that can achieve high performance while greatly improving training efficiency compared with BPTT. First, we show that the backpropagation of SNNs through the temporal domain contributes just a little to the final calculated gradients. Thus, we propose to ignore the unimportant routes in the computational graph during backpropagation. The proposed method reduces the number of scalar multiplications and achieves a small memory occupation that is independent of the total time steps. Furthermore, we propose a variant of SLTT, called SLTT-K, that allows backpropagation only at 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13417</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#22122;&#22768;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with structured noise improves classification and generalization. (arXiv:2302.13417v3 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13417
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21270;&#22122;&#22768;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22312;&#23398;&#20064;&#20013;&#30340;&#31215;&#26497;&#20316;&#29992;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#20013;&#19968;&#20010;&#24050;&#32463;&#34987;&#30830;&#35748;&#30340;&#27010;&#24565;&#65292;&#36825;&#34920;&#26126;&#29978;&#33267;&#29983;&#29289;&#31995;&#32479;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#30340;&#26426;&#21046;&#26469;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Gardner&#21644;&#21512;&#20316;&#32773;&#25552;&#20986;&#30340;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#26159;&#22312;&#24490;&#29615;&#32593;&#32476;&#20013;&#27880;&#20837;&#22122;&#22768;&#30340;&#20856;&#22411;&#31034;&#20363;&#65292;&#24490;&#29615;&#32593;&#32476;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#31070;&#32463;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#28155;&#21152;&#32467;&#26500;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#65292;&#20351;&#24471;&#21487;&#20197;&#25509;&#36817;&#23436;&#32654;&#20998;&#31867;&#21644;&#26368;&#22823;&#21560;&#24341;&#22495;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25152;&#35859;&#30340;&#36203;&#24067;&#29983;&#35268;&#21017;&#22312;&#22122;&#22768;&#36798;&#21040;&#26368;&#22823;&#19988;&#25968;&#25454;&#26159;&#32593;&#32476;&#21160;&#21147;&#23398;&#30340;&#22266;&#23450;&#28857;&#26102;&#19982;&#22122;&#22768;&#35757;&#32451;&#31639;&#27861;&#19968;&#33268;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#20339;&#22122;&#22768;&#25968;&#25454;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#26469;&#36229;&#36234;&#22122;&#22768;&#35757;&#32451;&#21644;&#36203;&#24067;&#29983;&#35268;&#21017;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beneficial role of noise in learning is nowadays a consolidated concept in the field of artificial neural networks, suggesting that even biological systems might take advantage of similar mechanisms to maximize their performance. The training-with-noise algorithm proposed by Gardner and collaborators is an emblematic example of a noise injection procedure in recurrent networks, which are usually employed to model real neural systems. We show how adding structure into noisy training data can substantially improve the algorithm performance, allowing to approach perfect classification and maximal basins of attraction. We also prove that the so-called Hebbian unlearning rule coincides with the training-with-noise algorithm when noise is maximal and data are fixed points of the network dynamics. A sampling scheme for optimal noisy data is eventually proposed and implemented to outperform both the training-with-noise and the Hebbian unlearning procedures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#65292;&#19988;&#19981;&#33021;&#24212;&#29992;&#20110;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#20197;&#21450;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25439;&#22833;&#36739;&#22823;&#12290;&#20026;&#27492;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASSET&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2302.11408</link><description>&lt;p&gt;
ASSET&#65306;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. (arXiv:2302.11408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#65292;&#19988;&#19981;&#33021;&#24212;&#29992;&#20110;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#20197;&#21450;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25439;&#22833;&#36739;&#22823;&#12290;&#20026;&#27492;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASSET&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#26159;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;&#26222;&#21450;&#24212;&#29992;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#36739;&#23569;&#12290;&#25104;&#21151;&#30340;&#21518;&#38376;&#25915;&#20987;&#20063;&#22312;&#36825;&#20123;&#26032;&#30340;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#36866;&#29992;&#24615;&#32570;&#20047;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#35780;&#20272;56&#31181;&#25915;&#20987;&#35774;&#32622;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#22312;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#19979;&#20840;&#37096;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;SSL&#21644;TL&#26102;&#65292;&#23427;&#20204;&#35201;&#20040;&#21464;&#24471;&#19981;&#36866;&#29992;&#65292;&#35201;&#20040;&#36973;&#21463;&#36739;&#22823;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Active Separation via Offset (ASSET)&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#20043;&#38388;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#23427;&#20204;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07801</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#21462;&#35777;: &#23545;&#20250;&#21592;&#38544;&#31169;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy. (arXiv:2302.07801v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#25104;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22270;&#20687;&#22788;&#29702;&#24212;&#29992;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#20294;&#20154;&#20204;&#20063;&#25285;&#24551;&#20854;&#28508;&#22312;&#30340;&#28389;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#38544;&#31169;&#20405;&#29359;&#21644;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#65292;&#32771;&#34385;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#23427;&#20204;&#30340;&#26576;&#20123;&#29420;&#29305;&#29305;&#24615;&#20026;&#25915;&#20987;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#12290;&#36890;&#36807;&#23545;&#25915;&#20987;&#21521;&#37327;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#27599;&#31181;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#25915;&#20987;&#22330;&#26223;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26131;&#24471;&#30340;&#25968;&#37327;&#24182;&#20855;&#26377;&#26497;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#23436;&#32654;&#30340;&#25915;&#20987;&#24615;&#33021;&#65288;&gt;0.9 AUCROC&#65289;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (&gt;0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#32467;&#26524;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.07181</link><description>&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#20219;&#21153;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum algorithms applied to satellite mission planning for Earth observation. (arXiv:2302.07181v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#32467;&#26524;&#20248;&#20110;&#20256;&#32479;&#31639;&#27861;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#25104;&#20687;&#21355;&#26143;&#26159;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#33021;&#22815;&#20840;&#29699;&#36861;&#36394;&#24037;&#19994;&#27963;&#21160;&#12290;&#24212;&#29992;&#26696;&#20363;&#28085;&#30422;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#22825;&#27668;&#39044;&#25253;&#21040;&#25968;&#23383;&#22320;&#22270;&#12289;&#30899;&#36275;&#36857;&#36861;&#36394;&#21644;&#26893;&#34987;&#30417;&#27979;&#31561;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#21355;&#26143;&#38590;&#20197;&#21046;&#36896;&#12289;&#32500;&#25252;&#25104;&#26412;&#39640;&#19988;&#21457;&#23556;&#36827;&#20837;&#36712;&#36947;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#21355;&#26143;&#24517;&#39035;&#34987;&#26377;&#25928;&#22320;&#21033;&#29992;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#21363;&#21355;&#26143;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#22823;&#35268;&#27169;&#19978;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#65292;&#22914;&#36138;&#23146;&#24378;&#21270;&#23398;&#20064;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#24120;&#21487;&#20197;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#37327;&#23376;&#31639;&#27861;&#26469;&#35299;&#20915;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#32467;&#26524;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#23454;&#29616;&#30340;&#32463;&#20856;&#31639;&#27861;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22312;&#21253;&#21547;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#23436;&#25104;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Earth imaging satellites are a crucial part of our everyday lives that enable global tracking of industrial activities. Use cases span many applications, from weather forecasting to digital maps, carbon footprint tracking, and vegetation monitoring. However, there are limitations; satellites are difficult to manufacture, expensive to maintain, and tricky to launch into orbit. Therefore, satellites must be employed efficiently. This poses a challenge known as the satellite mission planning problem, which could be computationally prohibitive to solve on large scales. However, close-to-optimal algorithms, such as greedy reinforcement learning and optimization algorithms, can often provide satisfactory resolutions. This paper introduces a set of quantum algorithms to solve the mission planning problem and demonstrate an advantage over the classical algorithms implemented thus far. The problem is formulated as maximizing the number of high-priority tasks completed on real datasets containin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#12289;&#19981;&#23436;&#25972;&#12289;&#25130;&#23614;&#21644;&#20445;&#23494;&#30340;&#29983;&#23384;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02807</link><description>&lt;p&gt;
&#32852;&#37030;&#29983;&#23384;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Federated Survival Forests. (arXiv:2302.02807v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#24067;&#24335;&#12289;&#19981;&#23436;&#25972;&#12289;&#25130;&#23614;&#21644;&#20445;&#23494;&#30340;&#29983;&#23384;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#32479;&#35745;&#23398;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#20851;&#27880;&#20110;&#23545;&#20154;&#32676;&#20013;&#29305;&#23450;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#24314;&#27169;&#12290;&#29983;&#23384;&#20998;&#26512;&#22312;&#21307;&#30103;&#12289;&#24037;&#31243;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#28041;&#21450;&#21040;&#20998;&#24067;&#24335;&#12289;&#19981;&#23436;&#25972;&#12289;&#25130;&#23614;&#21644;&#20445;&#23494;&#30340;&#29983;&#23384;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#29983;&#23384;&#20998;&#26512;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#32452;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#32852;&#37030;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#32852;&#37030;&#29983;&#23384;&#20998;&#26512;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#25104;&#21151;&#30340;&#29983;&#23384;&#27169;&#22411;&#20043;&#19968;&#8212;&#8212;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#30340;&#32852;&#37030;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a subfield of statistics concerned with modeling the occurrence time of a particular event of interest for a population. Survival analysis found widespread applications in healthcare, engineering, and social sciences. However, real-world applications involve survival datasets that are distributed, incomplete, censored, and confidential. In this context, federated learning can tremendously improve the performance of survival analysis applications. Federated learning provides a set of privacy-preserving techniques to jointly train machine learning models on multiple datasets without compromising user privacy, leading to a better generalization performance. However, despite the widespread development of federated learning in recent AI research, few studies focus on federated survival analysis. In this work, we present a novel federated algorithm for survival analysis based on one of the most successful survival models, the random survival forest. We call the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31579;&#36873;&#20998;&#31867;&#22120;&#30340;&#32452;&#20869;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20351;&#29992;&#26657;&#20934;&#30340;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#23545;&#24863;&#20852;&#36259;&#30340;&#20154;&#21475;&#32676;&#20307;&#20869;&#30340;&#21512;&#26684;&#25104;&#21592;&#23384;&#22312;&#19981;&#20844;&#24179;&#23545;&#24453;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#21518;&#22788;&#29702;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#20462;&#25913;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#32452;&#20869;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#31579;&#36873;&#20998;&#31867;&#22120;&#30340;&#32452;&#20869;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Within-Group Fairness of Screening Classifiers. (arXiv:2302.00025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31579;&#36873;&#20998;&#31867;&#22120;&#30340;&#32452;&#20869;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#20351;&#29992;&#26657;&#20934;&#30340;&#20998;&#31867;&#22120;&#21487;&#33021;&#23384;&#22312;&#23545;&#24863;&#20852;&#36259;&#30340;&#20154;&#21475;&#32676;&#20307;&#20869;&#30340;&#21512;&#26684;&#25104;&#21592;&#23384;&#22312;&#19981;&#20844;&#24179;&#23545;&#24453;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#21518;&#22788;&#29702;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#20462;&#25913;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#32452;&#20869;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31579;&#36873;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21508;&#31181;&#36873;&#25321;&#36807;&#31243;&#20013;&#30340;&#20505;&#36873;&#20154;&#35782;&#21035;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#26657;&#20934;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#38408;&#20540;&#20915;&#31574;&#35268;&#21017;&#35782;&#21035;&#20986;&#26399;&#26395;&#25968;&#30446;&#30340;&#21512;&#26684;&#20505;&#36873;&#20154;&#30340;&#26368;&#23567;&#38598;&#21512;&#12290;&#36825;&#25903;&#25345;&#23558;&#26657;&#20934;&#20316;&#20026;&#31579;&#36873;&#20998;&#31867;&#22120;&#30340;&#21807;&#19968;&#35201;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20351;&#29992;&#26657;&#20934;&#30340;&#20998;&#31867;&#22120;&#30340;&#31579;&#36873;&#31574;&#30053;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#40092;&#20026;&#20154;&#30693;&#30340;&#32452;&#20869;&#19981;&#20844;&#24179;&#31867;&#22411;&#8212;&#8212;&#23427;&#20204;&#21487;&#33021;&#19981;&#20844;&#24179;&#22320;&#23545;&#24453;&#24863;&#20852;&#36259;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#21512;&#26684;&#25104;&#21592;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#22914;&#26524;&#20998;&#31867;&#22120;&#28385;&#36275;&#32452;&#20869;&#21333;&#35843;&#24615;&#65292;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#21487;&#20197;&#36991;&#20813;&#65292;&#32452;&#20869;&#21333;&#35843;&#24615;&#26159;&#27599;&#20010;&#32676;&#20307;&#20869;&#30340;&#19968;&#31181;&#33258;&#28982;&#21333;&#35843;&#24615;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#39640;&#25928;&#21518;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#32473;&#23450;&#30340;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#20854;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screening classifiers are increasingly used to identify qualified candidates in a variety of selection processes. In this context, it has been recently shown that, if a classifier is calibrated, one can identify the smallest set of candidates which contains, in expectation, a desired number of qualified candidates using a threshold decision rule. This lends support to focusing on calibration as the only requirement for screening classifiers. In this paper, we argue that screening policies that use calibrated classifiers may suffer from an understudied type of within-group unfairness -- they may unfairly treat qualified members within demographic groups of interest. Further, we argue that this type of unfairness can be avoided if classifiers satisfy within-group monotonicity, a natural monotonicity property within each of the groups. Then, we introduce an efficient post-processing algorithm based on dynamic programming to minimally modify a given calibrated classifier so that its probab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;&#22235;&#26143;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#38382;&#39064;&#19981;&#24635;&#26159;&#28041;&#21450;&#21040;&#23884;&#22871;&#30340;&#19977;&#20803;&#32452;&#65292;&#23545;&#20110;3+1&#30340;&#22235;&#26143;&#32452;&#21512;&#32780;&#35328;&#65292;&#37319;&#29992;&#22235;&#26143;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#8220;&#23884;&#22871;&#8221;&#19977;&#20803;&#32452;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.09930</link><description>&lt;p&gt;
&#22235;&#26143;&#31995;&#32479;&#24182;&#38750;&#24635;&#26159;&#23884;&#22871;&#30340;&#19977;&#20803;&#32452;&#65306;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quadruple-star systems are not always nested triples: a machine learning approach to dynamical stability. (arXiv:2301.09930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21457;&#29616;&#22235;&#26143;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#38382;&#39064;&#19981;&#24635;&#26159;&#28041;&#21450;&#21040;&#23884;&#22871;&#30340;&#19977;&#20803;&#32452;&#65292;&#23545;&#20110;3+1&#30340;&#22235;&#26143;&#32452;&#21512;&#32780;&#35328;&#65292;&#37319;&#29992;&#22235;&#26143;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#8220;&#23884;&#22871;&#8221;&#19977;&#20803;&#32452;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#23545;&#20110;&#22235;&#26143;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#31283;&#23450;&#24615;&#38382;&#39064;&#36890;&#24120;&#34987;&#35270;&#20026;&#28041;&#21450;&#21040;&#26500;&#25104;&#22235;&#26143;&#30340;&#20004;&#20010;&#8220;&#23884;&#22871;&#8221;&#19977;&#20803;&#32452;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#26032;&#39062;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#26681;&#25454;&#20854;&#31283;&#23450;&#24615;&#65288;&#25110;&#38271;&#26399;&#26377;&#30028;&#24615;&#65289;&#30452;&#25509;&#23545;2+2&#21644;3+1&#30340;&#22235;&#26143;&#32452;&#21512;&#36827;&#34892;&#20998;&#31867;&#12290;&#20998;&#31867;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20998;&#21035;&#30001;$5\times10^5$&#20010;&#22235;&#26143;&#32452;&#21512;&#36827;&#34892;&#27169;&#25311;&#65292;&#27169;&#25311;&#37319;&#29992;&#20102;&#39640;&#31934;&#24230;&#30340;&#30452;&#25509;$N$-body&#20195;&#30721;MSTAR&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#21442;&#25968;&#31354;&#38388;&#30740;&#31350;&#65292;&#23558;&#22235;&#26143;&#32452;&#21512;&#21644;&#19977;&#20803;&#32452;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#22235;&#26143;MLP&#27169;&#22411;&#27604;&#8220;&#23884;&#22871;&#8221;&#19977;&#20803;&#32452;MLP&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#23545;&#20110;3+1&#30340;&#22235;&#26143;&#32452;&#21512;&#23588;&#20026;&#26174;&#33879;&#12290;2+2 MLP&#27169;&#22411;&#21644;3+1 MLP&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;94%&#21644;93%&#65292;&#32780;&#8220;&#23884;&#22871;&#8221;&#19977;&#20803;&#32452;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;88%&#21644;66%&#12290;&#36825;&#23545;&#20110;&#22235;&#26143;&#31995;&#32479;&#26377;&#30528;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical stability of quadruple-star systems has traditionally been treated as a problem involving two `nested' triples which constitute a quadruple. In this novel study, we employed a machine learning algorithm, the multi-layer perceptron (MLP), to directly classify 2+2 and 3+1 quadruples based on their stability (or long-term boundedness). The training data sets for the classification, comprised of $5\times10^5$ quadruples each, were integrated using the highly accurate direct $N$-body code MSTAR. We also carried out a limited parameter space study of zero-inclination systems to directly compare quadruples to triples. We found that both our quadruple MLP models perform better than a `nested' triple MLP approach, which is especially significant for 3+1 quadruples. The classification accuracies for the 2+2 MLP and 3+1 MLP models are 94% and 93% respectively, while the scores for the `nested' triple approach are 88% and 66% respectively. This is a crucial implication for quadruple 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#20998;&#26512;&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#21306;&#38388;&#21487;&#36798;&#24615;&#12290;&#36890;&#36807;&#28151;&#21512;&#21333;&#35843;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#23558;&#38381;&#29615;&#21160;&#21147;&#23398;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#22312;&#20445;&#30041;&#31995;&#32479;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;&#37319;&#29992;&#21333;&#20010;&#36712;&#36857;&#35745;&#31639;&#36229;&#30697;&#24418;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#21306;&#31574;&#30053;&#20248;&#21270;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2301.07912</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#19982;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#21306;&#38388;&#21487;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
Interval Reachability of Nonlinear Dynamical Systems with Neural Network Controllers. (arXiv:2301.07912v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#20998;&#26512;&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#21306;&#38388;&#21487;&#36798;&#24615;&#12290;&#36890;&#36807;&#28151;&#21512;&#21333;&#35843;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#23558;&#38381;&#29615;&#21160;&#21147;&#23398;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31995;&#32479;&#20013;&#65292;&#36825;&#31181;&#23884;&#20837;&#26041;&#27861;&#22312;&#20445;&#30041;&#31995;&#32479;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#26041;&#27861;&#12290;&#37319;&#29992;&#21333;&#20010;&#36712;&#36857;&#35745;&#31639;&#36229;&#30697;&#24418;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#21306;&#31574;&#30053;&#20248;&#21270;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21306;&#38388;&#20998;&#26512;&#30340;&#35745;&#31639;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#20005;&#26684;&#39564;&#35777;&#12290;&#32473;&#23450;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#39564;&#35777;&#31639;&#27861;&#20026;&#20854;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26500;&#24314;&#21253;&#21547;&#20989;&#25968;&#12290;&#21463;&#21040;&#28151;&#21512;&#21333;&#35843;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#21253;&#21547;&#20989;&#25968;&#21644;&#24320;&#29615;&#31995;&#32479;&#30340;&#20998;&#35299;&#20989;&#25968;&#23558;&#38381;&#29615;&#21160;&#21147;&#23398;&#23884;&#20837;&#21040;&#19968;&#20010;&#26356;&#22823;&#30340;&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31070;&#32463;&#25511;&#21046;&#22238;&#36335;&#30340;&#23433;&#20840;&#24615;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#23884;&#20837;&#31995;&#32479;&#30340;&#21333;&#20010;&#36712;&#36857;&#26377;&#25928;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#36229;&#30697;&#24418;&#36807;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#20998;&#21306;&#31574;&#30053;&#21033;&#29992;&#36825;&#31181;&#35745;&#31639;&#20248;&#21183;&#65292;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#21487;&#36798;&#38598;&#20272;&#35745;&#65292;&#21516;&#26102;&#24179;&#34913;&#20102;&#20854;&#36816;&#34892;&#26102;&#38388;&#21644;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a computationally efficient framework, based on interval analysis, for rigorous verification of nonlinear continuous-time dynamical systems with neural network controllers. Given a neural network, we use an existing verification algorithm to construct inclusion functions for its input-output behavior. Inspired by mixed monotone theory, we embed the closed-loop dynamics into a larger system using an inclusion function of the neural network and a decomposition function of the open-loop system. This embedding provides a scalable approach for safety analysis of the neural control loop while preserving the nonlinear structure of the system.  We show that one can efficiently compute hyper-rectangular over-approximations of the reachable sets using a single trajectory of the embedding system. We design an algorithm to leverage this computational advantage through partitioning strategies, improving our reachable set estimates while balancing its runtime with tunable paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04785</link><description>&lt;p&gt;
&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#12289;&#20351;&#29992;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#21450;&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#25915;&#20987;&#20013;&#26469;&#29983;&#25104;&#24378;&#26377;&#21147;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#39057;&#29575;&#21407;&#29702;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#21363;\textit{&#36739;&#20302;&#30340;&#39057;&#29575;&#20808;&#23398;&#20064;}&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20173;&#28982;&#25104;&#31435;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#65292;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
&lt;/p&gt;</description></item><item><title>L-SeqSleepNet&#26159;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#32771;&#34385;&#25972;&#20010;&#30561;&#30496;&#21608;&#26399;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#30561;&#30496;&#20998;&#26399;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.03441</link><description>&lt;p&gt;
L-SeqSleepNet&#65306;&#20840;&#21608;&#26399;&#38271;&#24207;&#21015;&#24314;&#27169;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
L-SeqSleepNet: Whole-cycle Long Sequence Modelling for Automatic Sleep Staging. (arXiv:2301.03441v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03441
&lt;/p&gt;
&lt;p&gt;
L-SeqSleepNet&#26159;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#32771;&#34385;&#25972;&#20010;&#30561;&#30496;&#21608;&#26399;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#30561;&#30496;&#20998;&#26399;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30561;&#30496;&#21576;&#21608;&#26399;&#24615;&#65292;&#22823;&#32422;90&#20998;&#38047;&#19968;&#21608;&#26399;&#65292;&#36825;&#24847;&#21619;&#30528;&#30561;&#30496;&#25968;&#25454;&#20013;&#23384;&#22312;&#38271;&#26399;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#30561;&#30496;&#20998;&#26399;&#27169;&#22411;&#26102;&#65292;&#25506;&#32034;&#36825;&#31181;&#38271;&#26399;&#20381;&#36182;&#24615;&#19968;&#30452;&#26410;&#34987;&#35302;&#21450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#32534;&#30721;&#25972;&#20010;&#30561;&#30496;&#21608;&#26399;&#30340;&#36923;&#36753;&#23545;&#20110;&#25913;&#36827;&#30561;&#30496;&#20998;&#26399;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#23545;&#27492;&#30446;&#30340;&#26469;&#35828;&#26159;&#20302;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;L-SeqSleepNet&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#25972;&#20010;&#21608;&#26399;&#30340;&#30561;&#30496;&#20449;&#24687;&#36827;&#34892;&#30561;&#30496;&#20998;&#26399;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#33041;&#30005;&#22270;&#35774;&#32622;&#19979;&#65292;&#21253;&#25324;&#20256;&#32479;&#22810;&#23548;&#30561;&#30496;&#30417;&#27979;&#65288;PSG&#65289;&#20013;&#30340;&#22836;&#30382;&#33041;&#30005;&#22270;&#12289;&#32819;&#20869;&#33041;&#30005;&#22270;&#21644;&#32819;&#21518;&#33041;&#30005;&#22270;&#65288;cEEGrid&#65289;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#33041;&#30005;&#36890;&#36947;&#36755;&#20837;&#65292;&#20063;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#20351;&#29992;L-SeqSleepNet&#36827;&#34892;&#25972;&#20010;&#21608;&#26399;&#24314;&#27169;&#33021;&#22815;&#25552;&#39640;&#30561;&#30496;&#20998;&#26399;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human sleep is cyclical with a period of approximately 90 minutes, implying long temporal dependency in the sleep data. Yet, exploring this long-term dependency when developing sleep staging models has remained untouched. In this work, we show that while encoding the logic of a whole sleep cycle is crucial to improve sleep staging performance, the sequential modelling approach in existing state-of-the-art deep learning models are inefficient for that purpose. We thus introduce a method for efficient long sequence modelling and propose a new deep learning model, L-SeqSleepNet, which takes into account whole-cycle sleep information for sleep staging. Evaluating L-SeqSleepNet on four distinct databases of various sizes, we demonstrate state-of-the-art performance obtained by the model over three different EEG setups, including scalp EEG in conventional Polysomnography (PSG), in-ear EEG, and around-the-ear EEG (cEEGrid), even with a single EEG channel input. Our analyses also show that L-S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#25913;&#21892;&#20102;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.03345</link><description>&lt;p&gt;
&#28508;&#22312;&#35889;&#35268;&#33539;&#21270;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latent Spectral Regularization for Continual Learning. (arXiv:2301.03345v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#25913;&#21892;&#20102;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#29289;&#26234;&#33021;&#22312;&#19968;&#29983;&#20013;&#38543;&#30528;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#31215;&#32047;&#32780;&#26377;&#26426;&#22320;&#22686;&#38271;&#65292;&#20294;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#23481;&#26131;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#30340;&#19968;&#31181;&#22810;&#21151;&#33021;&#21487;&#38752;&#35299;&#20915;&#26041;&#26696;&#65307;&#28982;&#32780;&#65292;&#31361;&#28982;&#30340;&#36755;&#20837;&#20013;&#26029;&#21644;&#20869;&#23384;&#38480;&#21046;&#20250;&#25913;&#21464;&#23427;&#20204;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#32773;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#37325;&#26032;&#28436;&#31034;&#30340;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#28857;&#36234;&#26469;&#36234;&#28151;&#21512;&#65292;&#24178;&#25200;&#20102;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23427;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#26045;&#21152;&#20102;&#36739;&#24369;&#30340;&#35201;&#27714;&#65292;&#20419;&#36827;&#20102;&#20998;&#21306;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#37325;&#28436;&#30340;CL&#26041;&#27861;&#36731;&#26494;&#32452;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;SOTA&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While biological intelligence grows organically as new knowledge is gathered throughout life, Artificial Neural Networks forget catastrophically whenever they face a changing training data distribution. Rehearsal-based Continual Learning (CL) approaches have been established as a versatile and reliable solution to overcome this limitation; however, sudden input disruptions and memory constraints are known to alter the consistency of their predictions. We study this phenomenon by investigating the geometric characteristics of the learner's latent space and find that replayed data points of different classes increasingly mix up, interfering with classification. Hence, we propose a geometric regularizer that enforces weak requirements on the Laplacian spectrum of the latent space, promoting a partitioning behavior. We show that our proposal, called Continual Spectral Regularizer (CaSpeR), can be easily combined with any rehearsal-based CL approach and improves the performance of SOTA meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01470</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#23454;&#29616;&#33258;&#20027;&#36187;&#36710;&#31995;&#32479;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model Parameter Identification via a Hyperparameter Optimization Scheme for Autonomous Racing Systems. (arXiv:2301.01470v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#21442;&#20248;&#21270;&#26041;&#26696;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#65288;MIHO&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;MIHO&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#65292;&#21442;&#25968;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36710;&#36742;&#22312;&#22330;&#22320;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#21644;&#31283;&#23450;&#36991;&#38556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#65288;MIHO&#65289;&#23454;&#29616;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26469;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#35782;&#21035;&#21160;&#24577;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;MIHO&#36827;&#34892;AV-21&#20840;&#23610;&#23544;&#33258;&#20027;&#36187;&#36710;&#30340;&#27169;&#22411;&#21442;&#25968;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;&#20248;&#21270;&#21518;&#30340;&#21442;&#25968;&#34701;&#20837;&#25105;&#20204;&#24179;&#21488;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MIHO&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20256;&#32479;&#30340;&#21442;&#25968;&#35782;&#21035;&#26041;&#27861;&#24555;13&#20493;&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;MIHO&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22330;&#22320;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#30340;&#31995;&#32479;&#20855;&#26377;&#31283;&#23450;&#30340;&#36991;&#38556;&#24615;&#33021;&#21644;&#39640;&#36895;&#34892;&#39542;&#24615;&#33021;&#65292;&#22312;&#21360;&#31532;&#23433;&#32435;&#27874;&#21033;&#26031;&#36710;&#36895;&#20844;&#22253;&#21644;&#25289;&#26031;&#32500;&#21152;&#26031;&#36710;&#36895;&#20844;&#22253;&#30340;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;217&#20844;&#37324;/&#23567;&#26102;&#30340;&#39640;&#36895;&#34892;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this letter, we propose a model parameter identification method via a hyperparameter optimization scheme (MIHO). Our method adopts an efficient explore-exploit strategy to identify the parameters of dynamic models in a data-driven optimization manner. We utilize MIHO for model parameter identification of the AV-21, a full-scaled autonomous race vehicle. We then incorporate the optimized parameters for the design of model-based planning and control systems of our platform. In experiments, MIHO exhibits more than 13 times faster convergence than traditional parameter identification methods. Furthermore, the parametric models learned via MIHO demonstrate good fitness to the given datasets and show generalization ability in unseen dynamic scenarios. We further conduct extensive field tests to validate our model-based system, demonstrating stable obstacle avoidance and high-speed driving up to 217 km/h at the Indianapolis Motor Speedway and Las Vegas Motor Speedway. The source code for M
&lt;/p&gt;</description></item><item><title>FFNeRV&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27969;&#20449;&#24687;&#34701;&#20837;&#24103;&#32423;&#34920;&#31034;&#20013;&#65292;&#20197;&#21033;&#29992;&#35270;&#39057;&#20013;&#24103;&#20043;&#38388;&#30340;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#24341;&#20837;&#20102;&#23436;&#20840;&#21367;&#31215;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2212.12294</link><description>&lt;p&gt;
FFNeRV&#65306;&#27969;&#23548;&#24341;&#30340;&#36880;&#24103;&#31070;&#32463;&#34920;&#31034;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos. (arXiv:2212.12294v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12294
&lt;/p&gt;
&lt;p&gt;
FFNeRV&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27969;&#20449;&#24687;&#34701;&#20837;&#24103;&#32423;&#34920;&#31034;&#20013;&#65292;&#20197;&#21033;&#29992;&#35270;&#39057;&#20013;&#24103;&#20043;&#38388;&#30340;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#24341;&#20837;&#20102;&#23436;&#20840;&#21367;&#31215;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39046;&#22495;&#65292;&#20063;&#34987;&#31216;&#20026;&#22522;&#20110;&#22352;&#26631;&#25110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181;&#24418;&#24335;&#20449;&#21495;&#30340;&#20986;&#33394;&#34920;&#31034;&#12289;&#29983;&#25104;&#21644;&#25805;&#20316;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35270;&#39057;&#34920;&#31034;&#26469;&#35828;&#65292;&#23558;&#20687;&#32032;&#22352;&#26631;&#26144;&#23556;&#21040;RGB&#39068;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#30456;&#23545;&#36739;&#20302;&#65292;&#25910;&#25947;&#36895;&#24230;&#24930;&#19988;&#25512;&#29702;&#36895;&#24230;&#24930;&#12290;&#36880;&#24103;&#35270;&#39057;&#34920;&#31034;&#26368;&#36817;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#20986;&#29616;&#65292;&#23558;&#26102;&#38388;&#22352;&#26631;&#26144;&#23556;&#21040;&#25972;&#20010;&#24103;&#65292;&#25552;&#39640;&#20102;&#21387;&#32553;&#29575;&#21644;&#32534;&#30721;&#36895;&#24230;&#12290;&#34429;&#28982;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#35270;&#39057;&#21387;&#32553;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FFNeRV&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27969;&#20449;&#24687;&#34701;&#20837;&#24103;&#32423;&#34920;&#31034;&#20013;&#65292;&#20197;&#21033;&#29992;&#35270;&#39057;&#20013;&#24103;&#20043;&#38388;&#30340;&#26102;&#38388;&#20887;&#20313;&#65292;&#21463;&#21040;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#21551;&#21457;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32500;&#26102;&#38388;&#32593;&#26684;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural fields, also known as coordinate-based or implicit neural representations, have shown a remarkable capability of representing, generating, and manipulating various forms of signals. For video representations, however, mapping pixel-wise coordinates to RGB colors has shown relatively low compression performance and slow convergence and inference speed. Frame-wise video representation, which maps a temporal coordinate to its entire frame, has recently emerged as an alternative method to represent videos, improving compression rates and encoding speed. While promising, it has still failed to reach the performance of state-of-the-art video compression algorithms. In this work, we propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to exploit the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#22312;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#26816;&#39564;&#20855;&#26377;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26816;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09201</link><description>&lt;p&gt;
&#20855;&#26377;&#35889;&#27491;&#21017;&#21270;&#30340;&#26680;&#21452;&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Spectral Regularized Kernel Two-Sample Tests. (arXiv:2212.09201v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#23884;&#20837;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#26816;&#39564;&#22312;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#26816;&#39564;&#20855;&#26377;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26816;&#39564;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36817;&#20046;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#19968;&#31181;&#22312;&#38750;&#21442;&#25968;&#26816;&#39564;&#38382;&#39064;&#20013;&#24191;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#23884;&#20837;&#30340;&#27010;&#24565;&#26469;&#22788;&#29702;&#19968;&#33324;&#65288;&#21363;&#38750;&#27431;&#20960;&#37324;&#24471;&#65289;&#22495;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#29702;&#35299;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#26500;&#24314;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#26368;&#20248;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21452;&#26679;&#26412;&#26816;&#39564;&#22312;Hellinger&#36317;&#31163;&#19979;&#30340;&#20998;&#31163;&#36793;&#30028;&#26041;&#38754;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#27491;&#21017;&#21270;&#30340;MMD&#26816;&#39564;&#20462;&#25913;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21327;&#26041;&#24046;&#20449;&#24687;&#65288;MMD&#26816;&#39564;&#26080;&#27861;&#25429;&#33719;&#65289;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#20855;&#26377;&#27604;MMD&#26816;&#39564;&#26356;&#23567;&#30340;&#20998;&#31163;&#36793;&#30028;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#36848;&#26816;&#39564;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26469;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#36866;&#24212;&#26816;&#39564;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, an approach that has gained a lot of popularity to tackle non-parametric testing problems on general (i.e., non-Euclidean) domains is based on the notion of reproducing kernel Hilbert space (RKHS) embedding of probability distributions. The main goal of our work is to understand the optimality of two-sample tests constructed based on this approach. First, we show that the popular MMD (maximum mean discrepancy) two-sample test is not optimal in terms of the separation boundary measured in Hellinger distance. Second, we propose a modification to the MMD test based on spectral regularization by taking into account the covariance information (which is not captured by the MMD test) and prove the proposed test to be minimax optimal with a smaller separation boundary than that achieved by the MMD test. Third, we propose an adaptive version of the above test which involves a data-driven strategy to choose the regularization parameter and show the adaptive test to be almos
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#27169;&#37325;&#21442;&#25968;&#21270;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;RepQ-ViT&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformers&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#21644;&#25512;&#26029;&#20998;&#24320;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#37327;&#21270;&#21644;&#39640;&#25928;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2212.08254</link><description>&lt;p&gt;
RepQ-ViT&#65306;&#22522;&#20110;&#35268;&#27169;&#37325;&#21442;&#25968;&#21270;&#23454;&#29616;&#23545;Vision Transformers&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers. (arXiv:2212.08254v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#27169;&#37325;&#21442;&#25968;&#21270;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;RepQ-ViT&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformers&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#37327;&#21270;&#21644;&#25512;&#26029;&#20998;&#24320;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#37327;&#21270;&#21644;&#39640;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#23454;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#26657;&#20934;&#65292;&#26080;&#38656;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;Vision Transformers (ViTs) &#25552;&#20986;&#20102;&#20960;&#31181;PTQ&#26041;&#26696;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#20986;&#29616;&#38750;&#24120;&#20005;&#37325;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#21270;&#35268;&#27169;&#37325;&#21442;&#25968;&#21270;&#30340;ViTs PTQ&#26694;&#26550;&#8212;&#8212;RepQ-ViT&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;RepQ-ViT&#23558;&#37327;&#21270;&#21644;&#25512;&#26029;&#36807;&#31243;&#20998;&#24320;&#65292;&#21069;&#32773;&#37319;&#29992;&#22797;&#26434;&#30340;&#37327;&#21270;&#22120;&#65292;&#21518;&#32773;&#37319;&#29992;&#20855;&#26377;&#35268;&#27169;&#37325;&#21442;&#25968;&#21270;&#30340;&#31616;&#21270;&#37327;&#21270;&#22120;&#12290;&#36825;&#20445;&#35777;&#20102;&#20934;&#30830;&#37327;&#21270;&#21644;&#39640;&#25928;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20026;&#20102;&#28385;&#36275;&#30446;&#26631;&#30828;&#20214;&#32780;&#29306;&#29298;&#20102;&#37327;&#21270;&#24615;&#33021;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#20010;&#20855;&#26377;&#26497;&#31471;&#20998;&#24067;&#30340;&#32452;&#20214;&#65306;&#20855;&#26377;&#20005;&#37325;&#36890;&#36947;&#38388;&#21464;&#24322;&#30340;LayerNorm&#21518;&#28608;&#27963;&#21644;pos&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Recently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which distinguishes it from existing approaches that sacrifice quantization performance to meet the target hardware. More specifically, we focus on two components with extreme distributions: post-LayerNorm activations with severe inter-channel variation and pos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2212.08049</link><description>&lt;p&gt;
&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;
&lt;/p&gt;
&lt;p&gt;
Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20999;&#29255;&#30340;&#26041;&#24335;&#23450;&#20041;&#20102;&#20999;&#29255;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#31185;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21464;&#24471;&#26497;&#20854;&#27969;&#34892;&#12290;OT&#38382;&#39064;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#28304;&#21644;&#30446;&#26631;&#27979;&#24230;&#30340;&#24635;&#36136;&#37327;&#30456;&#31561;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#26368;&#20248;&#20559;&#36716;&#36816;&#36755;&#65288;OPT&#65289;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#30340;&#26041;&#27861;&#12290;&#19982;OT&#38382;&#39064;&#31867;&#20284;&#65292;OPT&#30340;&#35745;&#31639;&#20381;&#36182;&#20110;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;&#36890;&#24120;&#22312;&#39640;&#32500;&#24230;&#20013;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21464;&#24471;&#35745;&#31639;&#19978;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19968;&#32500;&#38750;&#36127;&#27979;&#24230;&#20043;&#38388;OPT&#38382;&#39064;&#30340;&#26377;&#25928;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#36981;&#24490;&#20999;&#29255;OT&#36317;&#31163;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#21033;&#29992;&#20999;&#29255;&#23450;&#20041;&#20102;&#20999;&#29255;OPT&#36317;&#31163;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20999;&#29255;OPT-based&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#21644;&#31934;&#24230;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;Sliced-OPT&#22312;&#22122;&#22768;&#28857;&#20113;&#37197;&#20934;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
&lt;/p&gt;</description></item><item><title>LLEDA&#26159;&#19968;&#20010;&#32456;&#36523;&#33258;&#21161;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21463;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#65288;CLS&#65289;&#29702;&#35770;&#21551;&#21457;&#65292;&#27169;&#20223;&#28023;&#39532;&#21644;&#26032;&#30382;&#36136;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#28508;&#24847;&#35782;&#22238;&#25918;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#21644;&#28176;&#36827;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2211.09027</link><description>&lt;p&gt;
LLEDA -- &#32456;&#36523;&#33258;&#21161;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
LLEDA -- Lifelong Self-Supervised Domain Adaptation. (arXiv:2211.09027v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09027
&lt;/p&gt;
&lt;p&gt;
LLEDA&#26159;&#19968;&#20010;&#32456;&#36523;&#33258;&#21161;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21463;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#65288;CLS&#65289;&#29702;&#35770;&#21551;&#21457;&#65292;&#27169;&#20223;&#28023;&#39532;&#21644;&#26032;&#30382;&#36136;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#28508;&#24847;&#35782;&#22238;&#25918;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#21644;&#28176;&#36827;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#21160;&#29289;&#20855;&#26377;&#32456;&#36523;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#19981;&#20250;&#22240;&#20026;&#23398;&#20064;&#26032;&#30693;&#35782;&#32780;&#20002;&#22833;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#26032;&#20449;&#24687;&#19982;&#26087;&#30693;&#35782;&#20914;&#31361;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#65288;CLS&#65289;&#29702;&#35770;&#35748;&#20026;&#65292;&#28023;&#39532;&#21644;&#26032;&#30382;&#36136;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20351;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#33021;&#22815;&#36827;&#34892;&#38271;&#26399;&#19988;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20869;&#23384;&#22238;&#25918;&#21161;&#20110;&#38477;&#20302;&#36951;&#24536;&#12290;&#25152;&#25552;&#20986;&#30340;&#32456;&#36523;&#33258;&#21161;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;LLEDA&#65289;&#26694;&#26550;&#20174;CLS&#29702;&#35770;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#27169;&#20223;&#20004;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#20114;&#21160;&#65306;&#21463;&#28023;&#39532;&#21551;&#21457;&#30340;DA&#32593;&#32476;&#36805;&#36895;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#21463;&#26032;&#30382;&#36136;&#21551;&#21457;&#30340;SSL&#32593;&#32476;&#36880;&#28176;&#23398;&#20064;&#26080;&#22495;&#20449;&#24687;&#30340;&#26222;&#36941;&#34920;&#31034;&#12290;LLEDA&#30340;&#28508;&#24847;&#35782;&#22238;&#25918;&#25216;&#26415;&#20419;&#36827;&#20102;&#36825;&#20004;&#20010;&#31995;&#32479;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and animals have the ability to continuously learn new information over their lifetime without losing previously acquired knowledge. However, artificial neural networks struggle with this due to new information conflicting with old knowledge, resulting in catastrophic forgetting. The complementary learning systems (CLS) theory suggests that the interplay between hippocampus and neocortex systems enables long-term and efficient learning in the mammalian brain, with memory replay facilitating the interaction between these two systems to reduce forgetting. The proposed Lifelong Self-Supervised Domain Adaptation (LLEDA) framework draws inspiration from the CLS theory and mimics the interaction between two networks: a DA network inspired by the hippocampus that quickly adjusts to changes in data distribution and an SSL network inspired by the neocortex that gradually learns domain-agnostic general representations. LLEDA's latent replay technique facilitates communication between thes
&lt;/p&gt;</description></item><item><title>CaloFlow&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#37327;&#33021;&#22120;&#22270;&#29255;&#12289;&#30452;&#26041;&#22270;&#21644;&#20998;&#31867;&#22120;&#35777;&#26126;&#20102;&#20854;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.14245</link><description>&lt;p&gt;
CaloFlow&#29992;&#20110;CaloChallenge&#25968;&#25454;&#38598;1&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CaloFlow for CaloChallenge Dataset 1. (arXiv:2210.14245v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14245
&lt;/p&gt;
&lt;p&gt;
CaloFlow&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#37327;&#33021;&#22120;&#22270;&#29255;&#12289;&#30452;&#26041;&#22270;&#21644;&#20998;&#31867;&#22120;&#35777;&#26126;&#20102;&#20854;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CaloFlow&#26159;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#24555;&#36895;&#37327;&#33021;&#22120;&#27169;&#25311;&#30340;&#26032;&#26041;&#27861;&#12290;&#23558;CaloFlow&#24212;&#29992;&#20110;Fast Calorimeter Simulation Challenge 2022&#30340;&#20809;&#23376;&#21644;&#24102;&#30005;&#960;&#20171;&#23376;Geant4 shower&#30340;Dataset 1&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;Geant4&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;&#37327;&#33021;&#22120;shower&#22270;&#20687;&#12289;&#39640;&#32423;&#29305;&#24449;&#30340;&#30452;&#26041;&#22270;&#20197;&#21450;&#20998;&#31867;&#22120;&#23558;CaloFlow&#19982;Geant4&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#26469;&#35777;&#26126;&#26679;&#26412;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
CaloFlow is a new and promising approach to fast calorimeter simulation based on normalizing flows. Applying CaloFlow to the photon and charged pion Geant4 showers of Dataset 1 of the Fast Calorimeter Simulation Challenge 2022, we show how it can produce high-fidelity samples with a sampling time that is several orders of magnitude faster than Geant4. We demonstrate the fidelity of the samples using calorimeter shower images, histograms of high-level features, and aggregate metrics such as a classifier trained to distinguish CaloFlow from Geant4 samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#26469;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#21644;&#24050;&#22788;&#29702;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;&#12290;&#22312;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#65292;&#30456;&#23545;&#20110;DeltaCNN&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;&#39640;&#36798;90%&#12290;</title><link>http://arxiv.org/abs/2210.09887</link><description>&lt;p&gt;
MotionDeltaCNN&#65306;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#31232;&#30095;CNN&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos. (arXiv:2210.09887v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#26469;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#21644;&#24050;&#22788;&#29702;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24103;&#24046;&#24322;&#30340;&#25512;&#26029;&#12290;&#22312;&#31227;&#21160;&#25668;&#20687;&#26426;&#35270;&#39057;&#20013;&#65292;&#30456;&#23545;&#20110;DeltaCNN&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;&#39640;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#36755;&#20837;&#19978;&#36827;&#34892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#20869;&#23384;&#24102;&#23485;&#12290;&#26368;&#36817;&#65292;DeltaCNN&#36890;&#36807;&#20165;&#22788;&#29702;&#19982;&#19978;&#19968;&#24103;&#30456;&#27604;&#26377;&#26174;&#33879;&#26356;&#26032;&#30340;&#20687;&#32032;&#26469;&#20943;&#23569;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;DeltaCNN&#20381;&#36182;&#20110;&#38745;&#24577;&#25668;&#20687;&#26426;&#36755;&#20837;&#12290;&#31227;&#21160;&#25668;&#20687;&#26426;&#32473;&#22914;&#20309;&#39640;&#25928;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#22270;&#20687;&#21306;&#22495;&#19982;&#24050;&#22788;&#29702;&#21306;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26368;&#23567;&#21270;&#26356;&#26032;&#29575; - &#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#24320;&#38144;&#19988;&#26080;&#38656;&#30693;&#36947;&#26410;&#26469;&#24103;&#30340;&#25668;&#20687;&#26426;&#22806;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MotionDeltaCNN&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#31227;&#21160;&#25668;&#20687;&#26426;&#30340;&#31232;&#30095;CNN&#25512;&#26029;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29699;&#38754;&#32531;&#20914;&#21306;&#21644;&#22635;&#20805;&#21367;&#31215;&#65292;&#20197;&#20415;&#26080;&#32541;&#34701;&#21512;&#26032;&#25581;&#31034;&#30340;&#21306;&#22495;&#21644;&#20197;&#21069;&#22788;&#29702;&#30340;&#21306;&#22495; - &#21516;&#26102;&#19981;&#22686;&#21152;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#31227;&#21160;&#25668;&#20687;&#22836;&#35270;&#39057;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#36229;&#36807;DeltaCNN&#22810;&#36798;90%&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network inference on video input is computationally expensive and requires high memory bandwidth. Recently, DeltaCNN managed to reduce the cost by only processing pixels with significant updates over the previous frame. However, DeltaCNN relies on static camera input. Moving cameras add new challenges in how to fuse newly unveiled image regions with already processed regions efficiently to minimize the update rate - without increasing memory overhead and without knowing the camera extrinsics of future frames. In this work, we propose MotionDeltaCNN, a sparse CNN inference framework that supports moving cameras. We introduce spherical buffers and padded convolutions to enable seamless fusion of newly unveiled regions and previously processed regions -- without increasing memory footprint. Our evaluation shows that we outperform DeltaCNN by up to 90% for moving camera videos.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26114;&#36149;&#35780;&#20272;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#20989;&#25968;&#20013;&#25214;&#21040;&#19968;&#32452;&#23616;&#37096;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36825;&#20351;&#24471;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#21487;&#20197;&#24555;&#36895;&#20999;&#25442;&#21040;&#26368;&#20248;&#35299;&#65292;&#24182;&#33719;&#24471;&#26368;&#20339;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.06635</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#20989;&#25968;&#20013;&#25214;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Optimization Framework for Finding Local Optima in Expensive Multi-Modal Functions. (arXiv:2210.06635v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26114;&#36149;&#35780;&#20272;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#20989;&#25968;&#20013;&#25214;&#21040;&#19968;&#32452;&#23616;&#37096;&#21644;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36825;&#20351;&#24471;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#21487;&#20197;&#24555;&#36895;&#20999;&#25442;&#21040;&#26368;&#20248;&#35299;&#65292;&#24182;&#33719;&#24471;&#26368;&#20339;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#20855;&#26377;&#26114;&#36149;&#20989;&#25968;&#35780;&#20272;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#20248;&#21270;&#30340;&#27969;&#34892;&#20840;&#23616;&#20248;&#21270;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#33021;&#22815;&#25214;&#21040;&#21333;&#20010;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#25214;&#21040;&#19968;&#32452;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#35299;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#30001;&#20110;&#21508;&#31181;&#23454;&#38469;&#38480;&#21046;&#65288;&#20363;&#22914;&#36164;&#28304;&#38480;&#21046;&#12289;&#29289;&#29702;&#32422;&#26463;&#31561;&#65289;&#65292;&#23454;&#26045;&#26576;&#20123;&#26368;&#20248;&#35299;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#24050;&#30693;&#22810;&#20010;&#35299;&#65292;&#21487;&#20197;&#24555;&#36895;&#20999;&#25442;&#21040;&#21478;&#19968;&#20010;&#35299;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#35780;&#20272;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#20989;&#25968;&#30340;&#19968;&#32452;&#23616;&#37096;/&#20840;&#23616;&#35299;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#34920;&#31034;&#30446;&#26631;&#20989;&#25968;&#30340;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#35774;&#32622;&#12290;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#25512;&#23548;&#20102;&#32852;&#21512;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular global optimization scheme for sample-efficient optimization in domains with expensive function evaluations. The existing BO techniques are capable of finding a single global optimum solution. However, finding a set of global and local optimum solutions is crucial in a wide range of real-world problems, as implementing some of the optimal solutions might not be feasible due to various practical restrictions (e.g., resource limitation, physical constraints, etc.). In such domains, if multiple solutions are known, the implementation can be quickly switched to another solution, and the best possible system performance can still be obtained. This paper develops a multimodal BO framework to effectively find a set of local/global solutions for expensive-to-evaluate multimodal objective functions. We consider the standard BO setting with Gaussian process regression representing the objective function. We analytically derive the joint distribution of the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;NPUs&#19978;&#30340;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36864;&#20986;&#24863;&#30693;&#25250;&#21344;&#24335;&#26381;&#21153;&#30340;&#27969;&#20307;&#25209;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13443</link><description>&lt;p&gt;
&#27969;&#20307;&#25209;&#22788;&#29702;&#65306;&#36793;&#32536;NPUs&#19978;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#20986;&#24863;&#30693;&#25250;&#21344;&#24335;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fluid Batching: Exit-Aware Preemptive Serving of Early-Exit Neural Networks on Edge NPUs. (arXiv:2209.13443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36793;&#32536;NPUs&#19978;&#30340;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36864;&#20986;&#24863;&#30693;&#25250;&#21344;&#24335;&#26381;&#21153;&#30340;&#27969;&#20307;&#25209;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#25104;&#20026;&#20027;&#24178;&#65292;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#19981;&#26029;&#25193;&#23637;&#12290;&#37492;&#20110;&#26234;&#33021;&#35774;&#22791;&#22312;&#28040;&#36153;&#39046;&#22495;&#30340;&#20016;&#23500;&#21644;&#26080;&#22788;&#19981;&#22312;&#65292;&#27491;&#22312;&#24418;&#25104;&#8220;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#8221;&#65292;&#20854;&#20013;&#24863;&#30693;&#26159;&#24182;&#21457;&#36827;&#34892;&#30340;&#32780;&#19981;&#26159;&#29420;&#31435;&#36827;&#34892;&#30340;&#12290;&#36825;&#23558;&#22312;&#36793;&#32536;&#37096;&#32626;&#20013;&#24515;&#21270;&#30340;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#65288;NPUs&#65289;&#30340;&#35774;&#22791;&#25512;&#29702;&#33539;&#24335;&#36716;&#21521;&#65292;&#26234;&#33021;&#23478;&#23621;&#25110;&#33258;&#21160;&#39550;&#39542;&#31561;&#22810;&#20010;&#35774;&#22791;&#21487;&#20197;&#27969;&#24335;&#20256;&#36755;&#25968;&#25454;&#20197;&#20415;&#20351;&#29992;&#21160;&#24577;&#36895;&#29575;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#25552;&#20379;&#20102;&#36755;&#20837;&#25209;&#22788;&#29702;&#30340;&#22686;&#24378;&#28508;&#21147;&#65292;&#20294;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#21644;&#20307;&#39564;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#36127;&#36733;&#19979;&#12290;&#21516;&#26102;&#65292;&#37096;&#32626;&#21160;&#24577;DNN&#65292;&#21253;&#25324;&#38543;&#26426;&#35745;&#31639;&#22270;&#65288;&#20363;&#22914;&#25552;&#21069;&#36864;&#20986;&#65288;EE&#65289;&#27169;&#22411;&#65289;&#65292;&#22312;&#36825;&#31867;&#31995;&#32479;&#20013;&#24341;&#20837;&#20102;&#19968;&#32500;&#21160;&#24577;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#21069;&#36864;&#20986;&#24863;&#30693;&#35843;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With deep neural networks (DNNs) emerging as the backbone in a multitude of computer vision tasks, their adoption in real-world applications broadens continuously. Given the abundance and omnipresence of smart devices in the consumer landscape, "smart ecosystems'' are being formed where sensing happens concurrently rather than standalone. This is shifting the on-device inference paradigm towards deploying centralised neural processing units (NPUs) at the edge, where multiple devices (e.g. in smart homes or autonomous vehicles) can stream their data for processing with dynamic rates. While this provides enhanced potential for input batching, naive solutions can lead to subpar performance and quality of experience, especially under spiking loads. At the same time, the deployment of dynamic DNNs, comprising stochastic computation graphs (e.g. early-exit (EE) models), introduces a new dimension of dynamic behaviour in such systems. In this work, we propose a novel early-exit-aware scheduli
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.10510</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#20809;&#21488;&#21644;&#21512;&#25104;&#21040;&#23454;&#38469;&#36866;&#24212;&#23398;&#20064;&#23545;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;
&lt;/p&gt;
&lt;p&gt;
Learning to Relight Portrait Images via a Virtual Light Stage and Synthetic-to-Real Adaptation. (arXiv:2209.10510v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32918;&#20687;&#22270;&#20687;&#37325;&#26032;&#29031;&#26126;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#20154;&#30340;&#32918;&#20687;&#22270;&#20687;&#21644;&#30446;&#26631;&#29031;&#26126;&#30340;&#29615;&#22659;&#22270;&#65292;&#32918;&#20687;&#29031;&#26126;&#26088;&#22312;&#37325;&#26032;&#29031;&#26126;&#22270;&#20687;&#20013;&#30340;&#20154;&#29289;&#65292;&#22909;&#20687;&#35813;&#20154;&#20986;&#29616;&#22312;&#20855;&#26377;&#30446;&#26631;&#29031;&#26126;&#30340;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#36755;&#20837;-&#36755;&#20986;&#37197;&#23545;&#30340;&#25968;&#25454;&#38598;&#26469;&#30417;&#30563;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#20809;&#21488;&#25429;&#25417;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#27530;&#25429;&#25417;&#35774;&#22791;&#21644;&#32791;&#26102;&#30340;&#21162;&#21147;&#65292;&#38480;&#21046;&#21482;&#26377;&#23569;&#25968;&#36164;&#28304;&#20016;&#23500;&#30340;&#23454;&#39564;&#23460;&#25165;&#33021;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20809;&#21488;&#30340;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#29031;&#26126;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#35748;&#35782;&#65292;&#21363;&#32918;&#20687;&#22270;&#20687;&#30340;&#25104;&#21151;&#37325;&#26032;&#29031;&#26126;&#21462;&#20915;&#20110;&#20004;&#20010;&#26465;&#20214;&#12290;&#39318;&#20808;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#27169;&#20223;&#22522;&#20110;&#29289;&#29702;&#21407;&#29702;&#30340;&#37325;&#26032;&#29031;&#26126;&#30340;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#36755;&#20986;&#24517;&#39035;&#26159;&#29031;&#29255;&#36136;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a portrait image of a person and an environment map of the target lighting, portrait relighting aims to re-illuminate the person in the image as if the person appeared in an environment with the target lighting. To achieve high-quality results, recent methods rely on deep learning. An effective approach is to supervise the training of deep neural networks with a high-fidelity dataset of desired input-output pairs, captured with a light stage. However, acquiring such data requires an expensive special capture rig and time-consuming efforts, limiting access to only a few resourceful laboratories. To address the limitation, we propose a new approach that can perform on par with the state-of-the-art (SOTA) relighting methods without requiring a light stage. Our approach is based on the realization that a successful relighting of a portrait image depends on two conditions. First, the method needs to mimic the behaviors of physically-based relighting. Second, the output has to be photo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#65292;&#36824;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#27169;&#25311;&#21644;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.04512</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#27531;&#24046;&#65306;&#20302;&#20449;&#22122;&#27604;&#19979;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Residuals in Non-linear Factor Models: Precision Matrix Estimation of Returns with Low Signal-to-Noise Ratio. (arXiv:2209.04512v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#65292;&#36824;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#27169;&#25311;&#21644;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#22240;&#23376;&#27169;&#22411;&#23545;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#22238;&#25253;&#30340;&#31934;&#30830;&#30697;&#38453;&#36827;&#34892;&#19968;&#33268;&#20272;&#35745;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#27861;&#21363;&#20351;&#22312;&#37329;&#34701;&#24066;&#22330;&#20856;&#22411;&#30340;&#20449;&#22122;&#27604;&#20302;&#30340;&#29615;&#22659;&#20013;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#24369;&#22240;&#23376;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#23545;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#36164;&#20135;&#25968;&#37327;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#26399;&#20272;&#35745;&#39118;&#38505;&#24314;&#31435;&#20102;&#32479;&#19968;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#19968;&#33268;&#35823;&#24046;&#21327;&#26041;&#24046;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#21644;&#23454;&#35777;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a consistent estimator and rate of convergence for the precision matrix of asset returns in large portfolios using a non-linear factor model within the deep learning framework. Our estimator remains valid even in low signal-to-noise ratio environments typical for financial markets and is compatible with weak factors. Our theoretical analysis establishes uniform bounds on expected estimation risk based on deep neural networks for an expanding number of assets. Additionally, we provide a new consistent data-dependent estimator of error covariance in deep neural networks. Our models demonstrate superior accuracy in extensive simulations and the empirics.
&lt;/p&gt;</description></item><item><title>FedOBD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#26469;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#27492;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#25152;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05174</link><description>&lt;p&gt;
FedOBD&#65306;&#26426;&#20250;&#20027;&#20041;&#22359;&#20002;&#24323;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning. (arXiv:2208.05174v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05174
&lt;/p&gt;
&lt;p&gt;
FedOBD&#26159;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#26469;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#27492;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#25152;&#20135;&#29983;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#34892;&#19994;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#19979;&#35757;&#32451;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#27169;&#22411;&#21442;&#25968;dropout&#65292;&#20294;&#21333;&#29420;&#25805;&#20316;&#27169;&#22411;&#21442;&#25968;&#19981;&#20165;&#22312;&#22823;&#35268;&#27169;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#26080;&#27861;&#26377;&#25928;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#23545;&#32553;&#25918;&#21644;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26426;&#20250;&#20027;&#20041;&#22359;&#20002;&#24323;&#65288;FedOBD&#65289;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#21019;&#26032;&#28857;&#22312;&#20110;&#23558;&#22823;&#22411;&#27169;&#22411;&#20998;&#35299;&#20026;&#35821;&#20041;&#22359;&#65292;&#20197;&#20415;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#32773;&#21487;&#20197;&#26426;&#20250;&#20027;&#20041;&#22320;&#19978;&#20256;&#37327;&#21270;&#30340;&#22359;&#65292;&#36825;&#20123;&#22359;&#34987;&#35748;&#20026;&#23545;&#20110;&#35757;&#32451;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;FedOBD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural networks possess considerable expressive power. They are well-suited for complex learning tasks in industrial applications. However, large-scale models pose significant challenges for training under the current Federated Learning (FL) paradigm. Existing approaches for efficient FL training often leverage model parameter dropout. However, manipulating individual model parameters is not only inefficient in meaningfully reducing the communication overhead when training large-scale FL models, but may also be detrimental to the scaling efforts and model performance as shown by recent research. To address these issues, we propose the Federated Opportunistic Block Dropout (FedOBD) approach. The key novelty is that it decomposes large-scale models into semantic blocks so that FL participants can opportunistically upload quantized blocks, which are deemed to be significant towards training the model, to the FL server for aggregation. Extensive experiments evaluating FedOBD ag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20445;&#20581;&#39046;&#22495;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00953</link><description>&lt;p&gt;
&#23545;&#33041;&#37096;&#32959;&#30244;MRI&#21644;COVID-19&#33016;&#37096;X-ray&#22270;&#20687;&#30340;&#35270;&#35273;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Visual Interpretable and Explainable Deep Learning Models for Brain Tumor MRI and COVID-19 Chest X-ray Images. (arXiv:2208.00953v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#65292;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#20445;&#20581;&#39046;&#22495;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35299;&#37322;&#27169;&#22411;&#25512;&#29702;&#30340;&#24402;&#22240;&#25216;&#26415;&#21487;&#33021;&#22686;&#21152;&#20020;&#24202;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#20219;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#29992;&#20110;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#36807;&#31243;&#20013;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#20351;&#29992;&#33258;&#36866;&#24212;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#31215;&#20998;&#65292;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#33041;&#37096;&#32959;&#30244;MRI&#21644;COVID-19&#33016;&#37096;X-ray&#25968;&#25454;&#38598;&#25152;&#20570;&#30340;&#39044;&#27979;&#36827;&#34892;&#20102;&#24402;&#22240;&#12290;&#35813;&#25216;&#26415;&#31361;&#20986;&#20102;&#21487;&#33021;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#26292;&#38706;&#20102;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#36755;&#20837;&#19982;&#39044;&#27979;&#20043;&#38388;&#20851;&#32852;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#38416;&#26126;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24402;&#22240;&#26174;&#31034;&#20102;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#36879;&#26126;&#24230;&#30340;&#28508;&#21147;&#65292;&#20026;&#39046;&#22495;&#19987;&#23478;&#25581;&#31034;&#20102;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#12290;&#26412;&#30740;&#31350;&#25512;&#36827;&#20102;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#21152;&#21307;&#23398;&#20445;&#20581;&#39046;&#22495;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning shows promise for medical image analysis but lacks interpretability, hindering adoption in healthcare. Attribution techniques that explain model reasoning may increase trust in deep learning among clinical stakeholders. This paper aimed to evaluate attribution methods for illuminating how deep neural networks analyze medical images. Using adaptive path-based gradient integration, we attributed predictions from brain tumor MRI and COVID-19 chest X-ray datasets made by recent deep convolutional neural network models. The technique highlighted possible biomarkers, exposed model biases, and offered insights into the links between input and prediction. Our analysis demonstrates the method's ability to elucidate model reasoning on these datasets. The resulting attributions show promise for improving deep learning transparency for domain experts by revealing the rationale behind predictions. This study advances model interpretability to increase trust in deep learning among heal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#27493;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#36817;&#20284;&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#20004;&#27493;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#31616;&#21270;&#20026;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36817;&#20284;&#26377;&#25928;&#24615;&#21644;&#39044;&#27979;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.12377</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19968;&#27493;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
A novel Deep Learning approach for one-step Conformal Prediction approximation. (arXiv:2207.12377v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#27493;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;&#36817;&#20284;&#12290;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#20004;&#27493;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#31616;&#21270;&#20026;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36817;&#20284;&#26377;&#25928;&#24615;&#21644;&#39044;&#27979;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#24102;&#26377;&#21487;&#27979;&#32622;&#20449;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#36234;&#26469;&#36234;&#21463;&#20154;&#20204;&#30340;&#27426;&#36814;&#12290;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#26694;&#26550;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#38480;&#21046;&#26465;&#20214;&#19979;&#20445;&#35777;&#26368;&#22823;&#38169;&#35823;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31526;&#21512;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20004;&#27493;CP&#26041;&#27861;&#36817;&#20284;&#20026;&#19968;&#27493;&#12290;&#36890;&#36807;&#35780;&#20272;&#21644;&#24809;&#32602;&#19982;&#20005;&#26684;&#39044;&#26399;&#30340;CP&#36755;&#20986;&#20998;&#24067;&#30340;&#20559;&#24046;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#21644;&#31526;&#21512;&#24615;p&#20540;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#19971;&#20010;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#39640;&#36798;86%&#30340;&#21516;&#26102;&#65292;&#20173;&#20445;&#25345;&#30528;&#21487;&#27604;&#36739;&#30340;&#36817;&#20284;&#26377;&#25928;&#24615;&#21644;&#39044;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning predictions with measurable confidence are increasingly desirable for real-world problems, especially in high-risk settings. The Conformal Prediction (CP) framework is a versatile solution that guarantees a maximum error rate given minimal constraints. In this paper, we propose a novel conformal loss function that approximates the traditionally two-step CP approach in a single step. By evaluating and penalising deviations from the stringent expected CP output distribution, a Deep Learning model may learn the direct relationship between the input data and the conformal p-values. We carry out a comprehensive empirical evaluation to show our novel loss function's competitiveness for seven binary and multi-class prediction tasks on five benchmark datasets. On the same datasets, our approach achieves significant training time reductions up to 86% compared to Aggregated Conformal Prediction (ACP), while maintaining comparable approximate validity and predictive efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;2&#32423;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26080;&#29992;&#24037;&#20316;&#65292;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99%</title><link>http://arxiv.org/abs/2207.11312</link><description>&lt;p&gt;
HybMT: &#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HybMT: Hybrid Meta-Predictor based ML Algorithm for Fast Test Vector Generation. (arXiv:2207.11312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#20803;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#24555;&#36895;&#27979;&#35797;&#21521;&#37327;&#29983;&#25104;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;2&#32423;&#39044;&#27979;&#22120;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#26080;&#29992;&#24037;&#20316;&#65292;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;99%
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#30005;&#36335;&#27979;&#35797;&#26159;&#19968;&#39033;&#39640;&#24230;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#23545;&#20110;&#29616;&#20170;&#30340;&#22797;&#26434;&#35774;&#35745;&#65292;&#36890;&#24120;&#20351;&#29992;&#30830;&#23450;&#24615;&#27979;&#35797;&#29983;&#25104;&#65288;DTG&#65289;&#31639;&#27861;&#29983;&#25104;&#35768;&#22810;&#38590;&#20197;&#26816;&#27979;&#30340;&#25925;&#38556;&#30340;&#27979;&#35797;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#22686;&#21152;&#27979;&#35797;&#35206;&#30422;&#29575;&#24182;&#20943;&#23569;&#24635;&#20307;&#27979;&#35797;&#26102;&#38388;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20943;&#23569;&#20102;&#20256;&#32479;&#30340;&#36335;&#24452;&#23548;&#21521;&#20915;&#31574;&#21046;&#23450;&#65288;PODEM&#65289;&#31639;&#27861;&#20013;&#30340;&#26080;&#29992;&#24037;&#20316;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27979;&#35797;&#36136;&#37327;&#12290;&#23545;&#20110;PODEM&#30340;&#21464;&#20307;&#65292;&#24456;&#22810;&#26102;&#20505;&#38656;&#35201;&#22238;&#28335;&#65292;&#22240;&#20026;&#32487;&#32493;&#25191;&#34892;&#24050;&#32463;&#26080;&#27861;&#21462;&#24471;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22312;&#31639;&#27861;&#25191;&#34892;&#30340;&#19981;&#21516;&#38454;&#27573;&#39044;&#27979;&#26368;&#20339;&#31574;&#30053;&#12290;&#26412;&#25991;&#30340;&#26032;&#36129;&#29486;&#26159;&#19968;&#20010;2&#32423;&#39044;&#27979;&#22120;:&#39030;&#23618;&#26159;&#19968;&#20010;&#20803;&#39044;&#27979;&#22120;&#65292;&#22312;&#36739;&#20302;&#23618;&#20013;&#36873;&#25321;&#20960;&#20010;&#39044;&#27979;&#22120;&#20043;&#19968;&#12290;&#25105;&#20204;&#36873;&#25321;&#32473;&#23450;&#30005;&#36335;&#21644;&#30446;&#26631;&#32593;&#30340;&#26368;&#20339;&#39044;&#27979;&#22120;&#12290;&#21457;&#29616;&#39030;&#23618;&#20803;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24230;&#20026;99\%&#12290;&#36825;&#23548;&#33268;&#20102;
&lt;/p&gt;
&lt;p&gt;
Testing an integrated circuit (IC) is a highly compute-intensive process. For today's complex designs, tests for many hard-to-detect faults are typically generated using deterministic test generation (DTG) algorithms. Machine Learning (ML) is being increasingly used to increase the test coverage and decrease the overall testing time. Such proposals primarily reduce the wasted work in the classic Path Oriented Decision Making (PODEM) algorithm without compromising on the test quality. With variants of PODEM, many times there is a need to backtrack because further progress cannot be made. There is thus a need to predict the best strategy at different points in the execution of the algorithm. The novel contribution of this paper is a 2-level predictor: the top level is a meta predictor that chooses one of several predictors at the lower level. We choose the best predictor given a circuit and a target net. The accuracy of the top-level meta predictor was found to be 99\%. This leads to a s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN-CEST&#65289;&#21152;&#36895;&#21644;&#37327;&#21270;&#20102;&#19977;&#32500;&#21322;&#23454;&#24515;MT/CEST&#25104;&#20687;&#65292;&#24182;&#19988;&#22823;&#22823;&#32553;&#30701;&#20102;&#37319;&#38598;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.11297</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN-CEST&#65289;&#21152;&#36895;&#21644;&#37327;&#21270;&#30340;&#19977;&#32500;&#21322;&#23454;&#24515;MT/CEST&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Accelerated and Quantitative 3D Semisolid MT/CEST Imaging using a Generative Adversarial Network (GAN-CEST). (arXiv:2207.11297v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11297
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN-CEST&#65289;&#21152;&#36895;&#21644;&#37327;&#21270;&#20102;&#19977;&#32500;&#21322;&#23454;&#24515;MT/CEST&#25104;&#20687;&#65292;&#24182;&#19988;&#22823;&#22823;&#32553;&#30701;&#20102;&#37319;&#38598;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22823;&#22823;&#32553;&#30701;&#33719;&#21462;&#37327;&#21270;&#30340;&#19977;&#32500;&#21270;&#23398;&#20132;&#25442;&#39281;&#21644;&#36716;&#31227;&#65288;CEST&#65289;&#21644;&#21322;&#23454;&#24515;&#30913;&#21270;&#36716;&#31227;&#65288;MT&#65289;&#25104;&#20687;&#25152;&#38656;&#30340;&#37319;&#38598;&#26102;&#38388;&#65292;&#20197;&#21450;&#23454;&#29616;&#24555;&#36895;&#30340;&#21270;&#23398;&#20132;&#25442;&#21442;&#25968;&#22270;&#37325;&#24314;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;3&#20010;&#19981;&#21516;&#20301;&#32622;&#12289;3&#20010;&#19981;&#21516;&#25195;&#25551;&#20202;&#22411;&#21495;&#21644;&#32447;&#22280;&#30340;3T&#20020;&#24202;&#25195;&#25551;&#20202;&#65292;&#33719;&#21462;&#20102;L-&#31934;&#27688;&#37240;&#22934;&#39764;&#12289;&#25972;&#20010;&#33041;&#37096;&#21644;&#20581;&#24247;&#24535;&#24895;&#32773;&#12289;&#30284;&#30151;&#24739;&#32773;&#21644;&#24515;&#33039;&#24739;&#32773;&#30340;&#23567;&#33151;&#32908;&#32905;&#30340;&#19977;&#32500;CEST&#21644;MT&#30913;&#20849;&#25391;&#25351;&#32441;&#65288;MRF&#65289;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#35774;&#35745;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30417;&#30563;&#26694;&#26550;&#65288;GAN-CEST&#65289;&#65292;&#23398;&#20064;&#20174;&#20943;&#23569;&#30340;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#21040;&#37327;&#21270;&#20132;&#25442;&#21442;&#25968;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#20445;&#30041;&#24863;&#30693;&#21644;&#23450;&#37327;&#20869;&#23481;&#12290;&#32467;&#26524;&#65306;GAN-CEST&#30340;&#19977;&#32500;&#37319;&#38598;&#26102;&#38388;&#20026;42-52&#31186;&#65292;&#27604;CEST-MRF&#32553;&#30701;&#20102;70%&#12290;&#25972;&#20010;&#33041;&#37096;&#30340;&#23450;&#37327;&#37325;&#24314;&#21482;&#38656;0.8&#31186;&#12290;&#35266;&#23519;&#21040;&#20102;&#26497;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To substantially shorten the acquisition time required for quantitative 3D chemical exchange saturation transfer (CEST) and semisolid magnetization transfer (MT) imaging and allow for rapid chemical exchange parameter map reconstruction. Methods: Three-dimensional CEST and MT magnetic resonance fingerprinting (MRF) datasets of L-arginine phantoms, whole-brains, and calf muscles from healthy volunteers, cancer patients, and cardiac patients were acquired using 3T clinical scanners at 3 different sites, using 3 different scanner models and coils. A generative adversarial network supervised framework (GAN-CEST) was then designed and trained to learn the mapping from a reduced input data space to the quantitative exchange parameter space, while preserving perceptual and quantitative content. Results: The GAN-CEST 3D acquisition time was 42-52 seconds, 70% shorter than CEST-MRF. The quantitative reconstruction of the entire brain took 0.8 seconds. An excellent agreement was observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#26435;&#34913;&#12290;&#22312;&#39640;&#26031;&#25512;&#29702;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20302;&#31209;&#25512;&#26029;&#27169;&#22411;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#32479;&#35745;&#36817;&#20284;&#35823;&#24046;&#65292;&#20294;&#36739;&#20302;&#30340;&#35745;&#31639;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2207.11208</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#26435;&#34913;&#65306;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Statistical and Computational Trade-offs in Variational Inference: A Case Study in Inferential Model Selection. (arXiv:2207.11208v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#26435;&#34913;&#12290;&#22312;&#39640;&#26031;&#25512;&#29702;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20302;&#31209;&#25512;&#26029;&#27169;&#22411;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#32479;&#35745;&#36817;&#20284;&#35823;&#24046;&#65292;&#20294;&#36739;&#20302;&#30340;&#35745;&#31639;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#25104;&#20026;&#20102;&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#30340;&#28909;&#38376;&#26367;&#20195;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#26435;&#34913;&#32479;&#35745;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#26435;&#34913;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#20855;&#26377;&#23545;&#35282;&#21152;&#20302;&#31209;&#31934;&#24230;&#30697;&#38453;&#30340;&#39640;&#26031;&#25512;&#29702;&#27169;&#22411;&#65288;&#25110;&#21464;&#20998;&#36924;&#36817;&#23478;&#26063;&#65289;&#20013;&#30340;&#36825;&#31181;&#26435;&#34913;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#35823;&#24046;&#21644;&#39057;&#29575;&#20027;&#20041;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#35823;&#24046;&#12290;&#20174;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#21464;&#20998;&#21518;&#39564;&#30456;&#23545;&#20110;&#31934;&#30830;&#21518;&#39564;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#65292;&#20302;&#31209;&#25512;&#26029;&#27169;&#22411;&#20135;&#29983;&#20855;&#26377;&#26356;&#39640;&#32479;&#35745;&#36924;&#36817;&#35823;&#24046;&#20294;&#26356;&#20302;&#35745;&#31639;&#35823;&#24046;&#30340;&#21464;&#20998;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference has recently emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) in large-scale Bayesian inference. The core idea is to trade statistical accuracy for computational efficiency. In this work, we study these statistical and computational trade-offs in variational inference via a case study in inferential model selection. Focusing on Gaussian inferential models (or variational approximating families) with diagonal plus low-rank precision matrices, we initiate a theoretical study of the trade-offs in two aspects, Bayesian posterior inference error and frequentist uncertainty quantification error. From the Bayesian posterior inference perspective, we characterize the error of the variational posterior relative to the exact posterior. We prove that, given a fixed computation budget, a lower-rank inferential model produces variational posteriors with a higher statistical approximation error, but a lower computational error; it reduces varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#24212;&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2207.07563</link><description>&lt;p&gt;
QSAN: &#19968;&#31181;&#36817;&#26399;&#21487;&#23454;&#29616;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QSAN: A Near-term Achievable Quantum Self-Attention Network. (arXiv:2207.07563v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#24212;&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SAM&#65289;&#25797;&#38271;&#25429;&#25417;&#29305;&#24449;&#30340;&#20869;&#37096;&#36830;&#25509;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#23545;&#39640;&#32500;&#25968;&#25454;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25506;&#32034;&#20102;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;QSAM&#65289;&#65292;&#21253;&#25324;&#37327;&#23376;&#36923;&#36753;&#30456;&#20284;&#24230;&#65288;QLS&#65289;&#21644;&#37327;&#23376;&#20301;&#33258;&#27880;&#24847;&#21147;&#24471;&#20998;&#30697;&#38453;&#65288;QBSASM&#65289;&#65292;&#20316;&#20026;QSAN&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#22686;&#24378;SAM&#30340;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#12290;QLS&#29992;&#20110;&#38450;&#27490;&#27979;&#37327;&#33719;&#21462;&#20869;&#31215;&#65292;&#20351;&#24471;QSAN&#33021;&#22815;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23436;&#20840;&#23454;&#29616;&#65292;&#32780;QBSASM&#20316;&#20026;QSAN&#28436;&#36827;&#30340;&#32467;&#26524;&#65292;&#20135;&#29983;&#19968;&#20010;&#33021;&#26377;&#25928;&#21453;&#26144;&#36755;&#20986;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#23494;&#24230;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;QSAN&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#65292;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#21387;&#32553;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Mechanism (SAM) is good at capturing the internal connections of features and greatly improves the performance of machine learning models, espeacially requiring efficient characterization and feature extraction of high-dimensional data. A novel Quantum Self-Attention Network (QSAN) is proposed for image classification tasks on near-term quantum devices. First, a Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity (QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM) is explored as the theoretical basis of QSAN to enhance the data representation of SAM. QLS is employed to prevent measurements from obtaining inner products to allow QSAN to be fully implemented on quantum computers, and QBSASM as a result of the evolution of QSAN to produce a density matrix that effectively reflects the attention distribution of the output. Then, the framework for one-step realization and quantum circuits of QSAN are designed for fully considering the compression
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#34920;&#26684;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#21307;&#23398;&#35786;&#26029;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24212;&#23545;&#19978;&#19979;&#28216;&#29305;&#24449;&#38598;&#19981;&#21516;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.15306</link><description>&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Deep Tabular Models. (arXiv:2206.15306v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15306
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#26684;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#21307;&#23398;&#35786;&#26029;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#24212;&#23545;&#19978;&#19979;&#28216;&#29305;&#24449;&#38598;&#19981;&#21516;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#34920;&#26684;&#27169;&#22411;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#22635;&#34917;&#20102;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#21183;&#26159;&#23427;&#20204;&#23398;&#20064;&#20102;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#22312;&#26032;&#39046;&#22495;&#20013;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#29305;&#24615;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#21033;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#19981;&#21487;&#25110;&#32570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19978;&#28216;&#25968;&#25454;&#20026;&#34920;&#26684;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;GBDT&#27169;&#22411;&#26356;&#20855;&#20915;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#34920;&#26684;&#36801;&#31227;&#23398;&#20064;&#30340;&#29616;&#23454;&#21307;&#23398;&#35786;&#26029;&#22522;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#20351;&#29992;&#19978;&#28216;&#25968;&#25454;&#25552;&#39640;&#21508;&#31181;&#34920;&#26684;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#24615;&#33021;&#30340;&#25351;&#21335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#29305;&#24449;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19978;&#28216;&#21644;&#19979;&#28216;&#29305;&#24449;&#38598;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#36825;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#30340;&#34920;&#26684;&#29305;&#23450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they learn reusable features and are easily fine-tuned in new domains. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we demonstrate that upstream data gives tabular neural networks a decisive advantage over widely used GBDT models. We propose a realistic medical diagnosis benchmark for tabular transfer learning, and we present a how-to guide for using upstream data to boost performance with a variety of tabular neural network architectures. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world appli
&lt;/p&gt;</description></item><item><title>FairGrad&#26159;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#26435;&#37325;&#26469;&#23454;&#29616;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.10923</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#65306;FairGrad
&lt;/p&gt;
&lt;p&gt;
FairGrad: Fairness Aware Gradient Descent. (arXiv:2206.10923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10923
&lt;/p&gt;
&lt;p&gt;
FairGrad&#26159;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#26435;&#37325;&#26469;&#23454;&#29616;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20998;&#31867;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19981;&#20250;&#19981;&#20844;&#24179;&#27495;&#35270;&#20154;&#32676;&#23376;&#38598;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#25110;&#32773;&#28041;&#21450;&#38590;&#20197;&#23454;&#29616;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairGrad&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#26469;&#24378;&#21270;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#21462;&#20915;&#20110;&#26159;&#21542;&#20855;&#26377;&#20248;&#21183;&#12290;FairGrad&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#24320;&#38144;&#26368;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FairGrad&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#20869;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;FairGrad&#21487;&#20197;&#22312;https://pypi.org/project/fairgrad&#19978;&#20316;&#20026;&#19968;&#20010;PyPI&#21253;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.  FairGrad is available as a PyPI package at https://pypi.org/project/fairgrad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#22522;&#20934;- CARLANE&#65292;&#21253;&#25324;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#26377;&#27880;&#37322;&#30340;&#22270;&#29255;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.08083</link><description>&lt;p&gt;
CARLANE: &#20174;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#30340;&#36710;&#36947;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains. (arXiv:2206.08083v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#22522;&#20934;- CARLANE&#65292;&#21253;&#25324;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#26377;&#27880;&#37322;&#30340;&#22270;&#29255;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#36890;&#36807;&#23558;&#27169;&#22411;&#20174;&#26377;&#26631;&#31614;&#30340;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#20943;&#36731;&#22495;&#28418;&#31227;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#36710;&#36947;&#26816;&#27979;&#19978;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20123;&#26041;&#21521;&#19978;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CARLANE&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;3&#21521;&#27169;&#25311;&#21040;&#30495;&#23454;&#22495;&#36866;&#24212;&#22522;&#20934;&#12290;CARLANE&#21253;&#25324;&#21333;&#30446;&#26631;&#25968;&#25454;&#38598;MoLane&#21644;TuLane&#65292;&#20197;&#21450;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;MuLane&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#20849;&#21253;&#21547;16.3&#19975;&#24352;&#29420;&#29305;&#30340;&#22270;&#29255;&#65292;&#20854;&#20013;&#26377;11.8&#19975;&#24352;&#26377;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#25253;&#21578;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21407;&#22411;&#20132;&#21449;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35780;&#20272;&#20013;&#30340;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07813</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29983;&#21629;&#23433;&#20840;&#29615;&#22659;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#38169;&#35823;&#34892;&#20026;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#37325;&#22823;&#38169;&#35823;&#65292;&#22240;&#27492;&#23427;&#20204;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#30340;&#25925;&#38556;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#27979;&#35797;DRL&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20195;&#29702;&#29983;&#25104;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#30340;&#29366;&#24577;&#24207;&#21015;&#21464;&#21270;&#65292;&#20197;&#25506;&#32034;&#29615;&#22659;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;DRL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#20445;&#25345;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#27979;&#35797;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#19979;&#30340;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.03324</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient decentralized multi-agent learning in asymmetric bipartite queueing systems. (arXiv:2206.03324v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#19979;&#30340;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26381;&#21153;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;N&#20010;&#20195;&#29702;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#20174;K&#20010;&#26381;&#21153;&#22120;&#35831;&#27714;&#26381;&#21153;&#65292;&#21363;&#19981;&#36827;&#34892;&#36890;&#20449;&#22320;&#36816;&#34892;&#30456;&#21516;&#30340;&#31639;&#27861;&#12290;&#20808;&#21069;&#30340;&#20998;&#25955;&#24335;&#31639;&#27861;&#20165;&#23616;&#38480;&#20110;&#23545;&#31216;&#31995;&#32479;&#65292;&#20854;&#24615;&#33021;&#38543;&#30528;&#26381;&#21153;&#22120;&#25968;&#37327;&#25351;&#25968;&#32423;&#38477;&#20302;&#65292;&#38656;&#35201;&#36890;&#36807;&#20849;&#20139;&#38543;&#26426;&#24615;&#21644;&#21807;&#19968;&#30340;&#20195;&#29702;&#36523;&#20221;&#36827;&#34892;&#36890;&#20449;&#65292;&#19988;&#35745;&#31639;&#37327;&#24040;&#22823;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24403;&#27599;&#20010;&#20195;&#29702;&#20998;&#25955;&#22320;&#36816;&#34892;&#26102;&#65292;&#22312;&#26222;&#36890;&#30340;&#38750;&#23545;&#31216;&#20108;&#20998;&#25490;&#38431;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20855;&#22791;&#38468;&#21152;&#30340;&#40065;&#26834;&#24615;&#36136;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20026;&#35813;&#38382;&#39064;&#30340;&#38598;&#20013;&#24335;&#24773;&#20917;&#25552;&#20379;&#20102;&#39318;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#26377;&#25928;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study decentralized multi-agent learning in bipartite queueing systems, a standard model for service systems. In particular, N agents request service from K servers in a fully decentralized way, i.e, by running the same algorithm without communication. Previous decentralized algorithms are restricted to symmetric systems, have performance that is degrading exponentially in the number of servers, require communication through shared randomness and unique agent identities, and are computationally demanding. In contrast, we provide a simple learning algorithm that, when run decentrally by each agent, leads the queueing system to have efficient performance in general asymmetric bipartite queueing systems while also having additional robustness properties. Along the way, we provide the first provably efficient UCB-based algorithm for the centralized case of the problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Hessian-based&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#27867;&#21270;&#30028;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#24494;&#35843;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20851;&#31639;&#27861;&#21644;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.02659</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;Hessian&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Hessian-based&#20998;&#26512;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#65292;&#32473;&#20986;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#27867;&#21270;&#30028;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#24494;&#35843;&#38754;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20851;&#31639;&#27861;&#21644;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30740;&#31350;&#24494;&#35843;&#30340;&#27867;&#21270;&#29305;&#24615;&#65292;&#20197;&#29702;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#36739;&#23567;&#25110;&#35757;&#32451;&#26631;&#31614;&#22122;&#22768;&#26102;&#32463;&#24120;&#35266;&#23519;&#21040;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#20381;&#36182;&#20110;&#19982;&#24494;&#35843;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#65288;&#21363;&#39044;&#35757;&#32451;&#32593;&#32476;&#65289;&#36317;&#31163;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#22122;&#22768;&#31283;&#23450;&#24615;&#31561;&#27010;&#24565;&#12290;&#26412;&#25991;&#36890;&#36807;PAC-Bayesian&#20998;&#26512;&#30830;&#23450;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#19982;&#24494;&#35843;&#27169;&#22411;&#30340;&#35266;&#23519;&#21040;&#30340;&#27867;&#21270;&#24046;&#36317;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#20174;&#29702;&#35770;&#19978;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;Hessian&#36317;&#31163;&#30340;&#24494;&#35843;&#27169;&#22411;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#36824;&#23545;&#24494;&#35843;&#23545;&#25239;&#26631;&#31614;&#22122;&#22768;&#36827;&#34892;&#20102;&#25193;&#23637;&#30740;&#31350;&#65292;&#36807;&#25311;&#21512;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65307;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24182;&#22312;&#31867;&#26465;&#20214;&#29420;&#31435;&#20551;&#35774;&#19979;&#32473;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25345;&#32493;&#21516;&#35843;&#20998;&#26512;&#22797;&#26434;&#36807;&#28193;&#32593;&#32476;&#65292;&#21457;&#29616;&#31895;&#31890;&#21270;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#25552;&#39640;&#21160;&#24577;&#29366;&#24577;&#26816;&#27979;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.02530</link><description>&lt;p&gt;
&#31895;&#31890;&#21270;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#30340;&#25345;&#32493;&#21516;&#35843;
&lt;/p&gt;
&lt;p&gt;
Persistent Homology of Coarse Grained State Space Networks. (arXiv:2206.02530v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25345;&#32493;&#21516;&#35843;&#20998;&#26512;&#22797;&#26434;&#36807;&#28193;&#32593;&#32476;&#65292;&#21457;&#29616;&#31895;&#31890;&#21270;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#25552;&#39640;&#21160;&#24577;&#29366;&#24577;&#26816;&#27979;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23545;&#21160;&#24577;&#29366;&#24577;&#26816;&#27979;&#30340;&#22797;&#26434;&#36807;&#28193;&#32593;&#32476;&#36827;&#34892;&#25299;&#25169;&#20998;&#26512;&#12290;&#36807;&#28193;&#32593;&#32476;&#30001;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26500;&#25104;&#65292;&#24182;&#21033;&#29992;&#22270;&#35770;&#24037;&#20855;&#25581;&#31034;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#24037;&#20855;&#22312;&#24635;&#32467;&#36825;&#31181;&#22270;&#20013;&#30340;&#22797;&#26434;&#25299;&#25169;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#25345;&#32493;&#21516;&#35843;&#26469;&#30740;&#31350;&#36825;&#20123;&#32593;&#32476;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#31895;&#31890;&#21270;&#29366;&#24577;&#31354;&#38388;&#32593;&#32476;&#65288;CGSSN&#65289;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#21160;&#24577;&#29366;&#24577;&#26816;&#27979;&#19982;&#20004;&#31181;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65306;&#20351;&#29992;TDA&#32467;&#21512;&#24207;&#25968;&#20998;&#21306;&#32593;&#32476;&#65288;OPNs&#65289;&#21644;&#23558;&#25345;&#32493;&#21516;&#35843;&#26631;&#20934;&#24212;&#29992;&#20110;&#20449;&#21495;&#30340;&#26102;&#28382;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CGSSN&#25429;&#25417;&#21040;&#24213;&#23618;&#21160;&#24577;&#31995;&#32479;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#34920;&#29616;&#20026;&#21160;&#24577;&#29366;&#24577;&#26816;&#27979;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is dedicated to the topological analysis of complex transitional networks for dynamic state detection. Transitional networks are formed from time series data and they leverage graph theory tools to reveal information about the underlying dynamic system. However, traditional tools can fail to summarize the complex topology present in such graphs. In this work, we leverage persistent homology from topological data analysis to study the structure of these networks. We contrast dynamic state detection from time series using a coarse-grained state-space network (CGSSN) and topological data analysis (TDA) to two state of the art approaches: ordinal partition networks (OPNs) combined with TDA and the standard application of persistent homology to the time-delay embedding of the signal. We show that the CGSSN captures rich information about the dynamic state of the underlying dynamical system as evidenced by a significant improvement in dynamic state detection and noise robustness in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2205.09702</link><description>&lt;p&gt;
&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#20837;&#24182;&#21457;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09702
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24182;&#34892;&#21644;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35774;&#35745;&#20102;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#21518;&#65292;&#30740;&#31350;&#20102;&#21508;&#31181;GNN&#27169;&#22411;&#12289;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#21644;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24378;&#22823;&#30340;&#24037;&#20855;&#20043;&#19968;&#12290;&#23427;&#20204;&#24120;&#24120;&#22312;&#26080;&#32467;&#26500;&#32593;&#32476;&#19978;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#22270;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#37117;&#38750;&#24120;&#22797;&#26434;&#65292;&#23427;&#20204;&#29420;&#29305;&#22320;&#23558;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#29305;&#24615;&#19982;&#23494;&#38598;&#21644;&#35268;&#21017;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20351;&#24471;&#22312;&#29616;&#20195;&#22823;&#35268;&#27169;&#24182;&#34892;&#26550;&#26500;&#19978;&#39640;&#25928;&#25191;&#34892;GNNs&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;GNNs&#20013;&#30340;&#24182;&#34892;&#24615;&#20998;&#31867;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;&#27969;&#27700;&#32447;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20998;&#31867;&#26041;&#27861;&#26469;&#30740;&#31350;&#20247;&#22810;GNN&#27169;&#22411;&#12289;GNN&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12289;&#36719;&#20214;&#26694;&#26550;&#25110;&#30828;&#20214;&#21152;&#36895;&#22120;&#20013;&#30340;&#24182;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#20316;&#28145;&#24230;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#36890;&#20449;&#37327;&#21644;&#21516;&#27493;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30456;&#20851;&#24352;&#37327;&#30340;&#31232;&#30095;&#24615;/&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#22312;&#20855;&#26377;&#26725;&#24809;&#32602;&#30340;&#22238;&#24402;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#24182;&#25552;&#20379;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#32467;&#26524;&#26469;&#21152;&#36895;&#35745;&#31639;&#26102;&#38388;&#12290;&#36890;&#36807;&#22312;B&#26679;&#26465;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09515</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#29992;&#20110;&#36125;&#21494;&#26031;&#26725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Variational Inference for Bayesian Bridge Regression. (arXiv:2205.09515v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#22312;&#20855;&#26377;&#26725;&#24809;&#32602;&#30340;&#22238;&#24402;&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#24182;&#25552;&#20379;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#32467;&#26524;&#26469;&#21152;&#36895;&#35745;&#31639;&#26102;&#38388;&#12290;&#36890;&#36807;&#22312;B&#26679;&#26465;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#22312;&#20855;&#26377;&#26725;&#24809;&#32602;&#30340;&#22238;&#24402;&#27169;&#22411;&#19978;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#12290;&#26725;&#26041;&#27861;&#20351;&#29992;$\ell_{\alpha}$&#33539;&#25968;&#65292;&#20854;&#20013;$\alpha \in (0, +\infty)$&#65292;&#23545;&#22238;&#24402;&#31995;&#25968;&#30340;&#22823;&#20540;&#36827;&#34892;&#24809;&#32602;&#65292;&#21253;&#25324;Lasso ($\alpha = 1$)&#21644;&#23725;&#22238;&#24402;$(\alpha = 2)$&#24809;&#32602;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#20840;&#36125;&#21494;&#26031;&#25512;&#26029;&#26080;&#32541;&#22320;&#25552;&#20379;&#20102;&#25152;&#26377;&#27169;&#22411;&#21442;&#25968;&#30340;&#32852;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#23613;&#31649;&#26725;&#22238;&#24402;&#30340;MCMC&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#36739;&#24930;&#12290;ADVI&#23454;&#29616;&#20801;&#35768;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#65288;&#30001;&#20110;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#31639;&#27861;&#65289;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26102;&#38388;&#19982;MCMC&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;B&#26679;&#26465;&#38750;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#20102;&#26041;&#27861;&#35828;&#26126;&#65292;&#23613;&#31649;&#35813;&#26041;&#27861;&#23545;&#20110;&#20854;&#20182;&#22522;&#30784;&#20989;&#25968;&#30340;&#36873;&#25321;&#20063;&#21487;&#20197;&#26080;&#32541;&#20351;&#29992;&#12290;&#27169;&#25311;&#30740;&#31350;&#35828;&#26126;&#20102;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the implementation of Automatic Differentiation Variational inference (ADVI) for Bayesian inference on regression models with bridge penalization. The bridge approach uses $\ell_{\alpha}$ norm, with $\alpha \in (0, +\infty)$ to define a penalization on large values of the regression coefficients, which includes the Lasso ($\alpha = 1$) and ridge $(\alpha = 2)$ penalizations as special cases. Full Bayesian inference seamlessly provides joint uncertainty estimates for all model parameters. Although MCMC aproaches are available for bridge regression, it can be slow for large dataset, specially in high dimensions. The ADVI implementation allows the use of small batches of data at each iteration (due to stochastic gradient based algorithms), therefore speeding up computational time in comparison with MCMC. We illustrate the approach on non-parametric regression models with B-splines, although the method works seamlessly for other choices of basis functions. A simulation study shows
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#24847;&#31867;&#22411;&#30340;&#21518;&#38376;&#23884;&#20837;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23884;&#20837;&#20989;&#25968;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2205.06900</link><description>&lt;p&gt;
MM-BD: &#20351;&#29992;&#26368;&#22823;&#36793;&#32536;&#32479;&#35745;&#26816;&#27979;&#20219;&#24847;&#31867;&#22411;&#32972;&#26223;&#27169;&#24335;&#30340;&#21518;&#35757;&#32451;&#21518;&#38376;&#25915;&#20987; (arXiv:2205.06900v2 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
MM-BD: Post-Training Detection of Backdoor Attacks with Arbitrary Backdoor Pattern Types Using a Maximum Margin Statistic. (arXiv:2205.06900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#24847;&#31867;&#22411;&#30340;&#21518;&#38376;&#23884;&#20837;&#25915;&#20987;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23884;&#20837;&#20989;&#25968;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#26159;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#19968;&#31181;&#37325;&#35201;&#23545;&#25239;&#23041;&#32961;&#65292;&#24403;&#23884;&#20837;&#21518;&#38376;&#27169;&#24335;&#26102;&#65292;&#26469;&#33258;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#31867;&#30340;&#27979;&#35797;&#26679;&#26412;&#23558;&#34987;(&#35823;)&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#30340;&#30446;&#26631;&#31867;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#21518;&#35757;&#32451;&#21518;&#38376;&#38450;&#24481;&#22330;&#26223;&#65292;&#38450;&#24481;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#26080;&#27861;&#35775;&#38382;&#35757;&#32451;&#38598;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20986;&#26159;&#21542;&#21463;&#21040;&#20102;&#21518;&#38376;&#25915;&#20987;&#12290;&#35768;&#22810;&#21518;&#35757;&#32451;&#26816;&#27979;&#22120;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#20351;&#29992;&#29305;&#23450;&#30340;&#19968;&#31181;&#25110;&#23569;&#25968;&#20960;&#31181;&#21518;&#38376;&#23884;&#20837;&#20989;&#25968;&#30340;&#25915;&#20987; (&#20363;&#22914;&#65292;&#34917;&#19969;&#26367;&#25442;&#25110;&#21152;&#27861;&#25915;&#20987;)&#12290;&#24403;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21518;&#38376;&#23884;&#20837;&#20989;&#25968;&#65288;&#38450;&#24481;&#32773;&#26410;&#30693;&#65289;&#19982;&#38450;&#24481;&#32773;&#20551;&#35774;&#30340;&#21518;&#38376;&#23884;&#20837;&#20989;&#25968;&#19981;&#21516;&#26102;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#20219;&#24847;&#31867;&#22411;&#30340;&#21518;&#38376;&#23884;&#20837;&#25915;&#20987;&#65292;&#24182;&#19988;&#19981;&#23545;&#23884;&#20837;&#20989;&#25968;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks are an important type of adversarial threat against deep neural network classifiers, wherein test samples from one or more source classes will be (mis)classified to the attacker's target class when a backdoor pattern is embedded. In this paper, we focus on the post-training backdoor defense scenario commonly considered in the literature, where the defender aims to detect whether a trained classifier was backdoor-attacked without any access to the training set. Many post-training detectors are designed to detect attacks that use either one or a few specific backdoor embedding functions (e.g., patch-replacement or additive attacks). These detectors may fail when the backdoor embedding function used by the attacker (unknown to the defender) is different from the backdoor embedding function assumed by the defender. In contrast, we propose a post-training defense that detects backdoor attacks with arbitrary types of backdoor embeddings, without making any assumptions about 
&lt;/p&gt;</description></item><item><title>kNN-Embed&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23494;&#38598;&#36817;&#20284;&#26368;&#36817;&#37051;&#26816;&#32034;&#22810;&#26679;&#24615;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#24179;&#28369;&#28151;&#21512;&#30340;&#23398;&#20064;&#39033;&#30446;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#36820;&#22238;&#21453;&#26144;&#29992;&#25143;&#22810;&#20010;&#20852;&#36259;&#30340;&#22810;&#26679;&#20505;&#36873;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2205.06205</link><description>&lt;p&gt;
kNN-Embed:&#22522;&#20110;&#23616;&#37096;&#24179;&#28369;&#30340;&#23884;&#20837;&#28151;&#21512;&#29992;&#20110;&#22810;&#20852;&#36259;&#20505;&#36873;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
kNN-Embed: Locally Smoothed Embedding Mixtures For Multi-interest Candidate Retrieval. (arXiv:2205.06205v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06205
&lt;/p&gt;
&lt;p&gt;
kNN-Embed&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#23494;&#38598;&#36817;&#20284;&#26368;&#36817;&#37051;&#26816;&#32034;&#22810;&#26679;&#24615;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#24179;&#28369;&#28151;&#21512;&#30340;&#23398;&#20064;&#39033;&#30446;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#36820;&#22238;&#21453;&#26144;&#29992;&#25143;&#22810;&#20010;&#20852;&#36259;&#30340;&#22810;&#26679;&#20505;&#36873;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20505;&#36873;&#26816;&#32034;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#31995;&#32479;&#26469;&#26816;&#32034;&#19982;&#29992;&#25143;&#36755;&#20837;&#30456;&#20851;&#30340;&#28508;&#22312;&#21830;&#21697;&#12290;&#36825;&#20123;&#20505;&#36873;&#21830;&#21697;&#38543;&#21518;&#20250;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#25490;&#21517;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#21518;&#32493;&#38454;&#27573;&#36827;&#34892;&#25490;&#21517;&#21644;&#20462;&#21098;&#12290;&#20316;&#20026;&#25512;&#33616;&#28431;&#26007;&#30340;&#39030;&#37096;&#65292;&#26816;&#32034;&#21040;&#19968;&#20010;&#39640;&#22238;&#24518;&#30340;&#20505;&#36873;&#38598;&#21512;&#20197;&#36755;&#20837;&#21040;&#19979;&#28216;&#30340;&#25490;&#21517;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#26469;&#33258;&#21333;&#20010;&#23494;&#38598;&#26597;&#35810;&#23884;&#20837;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANN&#65289;; &#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#20855;&#26377;&#35768;&#22810;&#36817;&#20284;&#37325;&#22797;&#39033;&#30340;&#20302;&#22810;&#26679;&#24615;&#32467;&#26524;&#38598;&#12290;&#30001;&#20110;&#29992;&#25143;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#20852;&#36259;&#65292;&#20505;&#36873;&#26816;&#32034;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#36820;&#22238;&#19968;&#20010;&#21453;&#26144;&#29992;&#25143;&#22810;&#20010;&#20852;&#36259;&#30340;&#22810;&#26679;&#30340;&#20505;&#36873;&#38598;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;kNN-Embed&#65292;&#19968;&#31181;&#25913;&#21892;&#23494;&#38598;ANN&#26816;&#32034;&#22810;&#26679;&#24615;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;kNN-Embed&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#24179;&#28369;&#28151;&#21512;&#30340;&#23398;&#20064;&#39033;&#30446;&#32858;&#31867;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Candidate retrieval is the first stage in recommendation systems, where a light-weight system is used to retrieve potentially relevant items for an input user. These candidate items are then ranked and pruned in later stages of recommender systems using a more complex ranking model. As the top of the recommendation funnel, it is important to retrieve a high-recall candidate set to feed into downstream ranking models. A common approach is to leverage approximate nearest neighbor (ANN) search from a single dense query embedding; however, this approach this can yield a low-diversity result set with many near duplicates. As users often have multiple interests, candidate retrieval should ideally return a diverse set of candidates reflective of the user's multiple interests. To this end, we introduce kNN-Embed, a general approach to improving diversity in dense ANN-based retrieval. kNN-Embed represents each user as a smoothed mixture over learned item clusters that represent distinct "intere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26680;&#26041;&#27861;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#20013;&#20998;&#21035;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#38169;&#35823;&#27010;&#29575;&#21644;&#25511;&#21046;&#38169;&#35823;&#27010;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;MMD&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2203.12777</link><description>&lt;p&gt;
&#26680;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Kernel Robust Hypothesis Testing. (arXiv:2203.12777v2 [eess.SP] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26680;&#26041;&#27861;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#20013;&#20998;&#21035;&#30740;&#31350;&#20102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#38169;&#35823;&#27010;&#29575;&#21644;&#25511;&#21046;&#38169;&#35823;&#27010;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;MMD&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#22312;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#19979;&#65292;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#34987;&#20551;&#35774;&#22312;&#26576;&#20123;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#65292;&#24182;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#27979;&#35797;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#34920;&#29616;&#26368;&#20248;&#12290;&#26412;&#25991;&#23558;&#20351;&#29992;&#26680;&#26041;&#27861;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#36896;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#21363;&#20197;&#26469;&#33258;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#32463;&#39564;&#20998;&#24067;&#20026;&#20013;&#24515;&#65292;&#24182;&#36890;&#36807;&#26680;&#22343;&#20540;&#23884;&#20837;&#30340;&#36317;&#31163;&#26469;&#32422;&#26463;&#65292;&#21363;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#65288;MMD&#65289;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#21644;Neyman-Pearson&#35774;&#32622;&#12290;&#23545;&#20110;&#36125;&#21494;&#26031;&#35774;&#32622;&#65292;&#21363;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24403;&#23383;&#27597;&#34920;&#26159;&#26377;&#38480;&#30340;&#26102;&#65292;&#39318;&#20808;&#24471;&#21040;&#20102;&#26368;&#20339;&#27979;&#35797;&#12290;&#24403;&#23383;&#27597;&#34920;&#26159;&#26080;&#38480;&#30340;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#37327;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#38169;&#35823;&#27010;&#29575;&#12290;&#23545;&#20110;Neyman-Pearson&#35774;&#32622;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#21270;&#31532;&#20108;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#21516;&#26102;&#25511;&#21046;&#22312;&#32473;&#23450;&#27700;&#24179;&#19979;&#30340;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;MMD&#30340;&#27979;&#35797;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#28176;&#36817;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of robust hypothesis testing is studied, where under the null and the alternative hypotheses, the data-generating distributions are assumed to be in some uncertainty sets, and the goal is to design a test that performs well under the worst-case distributions over the uncertainty sets. In this paper, uncertainty sets are constructed in a data-driven manner using kernel method, i.e., they are centered around empirical distributions of training samples from the null and alternative hypotheses, respectively; and are constrained via the distance between kernel mean embeddings of distributions in the reproducing kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian setting and the Neyman-Pearson setting are investigated. For the Bayesian setting where the goal is to minimize the worst-case error probability, an optimal test is firstly obtained when the alphabet is finite. When the alphabet is infinite, a tractable approximation is proposed to quantify the worst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;KINet&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#28857;&#34920;&#31034;&#21644;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#25512;&#24191;&#21040;&#19981;&#21516;&#22330;&#26223;&#20013;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#26410;&#26469;&#30340;&#20851;&#38190;&#28857;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2202.09006</link><description>&lt;p&gt;
KINet&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#30340;&#26080;&#30417;&#30563;&#21069;&#21521;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KINet: Unsupervised Forward Models for Robotic Pushing Manipulation. (arXiv:2202.09006v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;KINet&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#28857;&#34920;&#31034;&#21644;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#25512;&#24191;&#21040;&#19981;&#21516;&#22330;&#26223;&#20013;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#26410;&#26469;&#30340;&#20851;&#38190;&#28857;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#26159;&#21069;&#21521;&#39044;&#27979;&#30340;&#37325;&#35201;&#25277;&#35937;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21069;&#21521;&#27169;&#22411;&#36890;&#36807;&#24191;&#27867;&#30340;&#30417;&#30563;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#65288;&#20363;&#22914;&#23545;&#35937;&#31867;&#21035;&#21644;&#36793;&#30028;&#26694;&#65289;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#20013;&#24456;&#38590;&#33719;&#24471;&#36825;&#26679;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KINet&#65288;&#20851;&#38190;&#28857;&#20132;&#20114;&#32593;&#32476;&#65289;--&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#23558;&#23545;&#35937;&#19982;&#20851;&#38190;&#28857;&#22352;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#21457;&#29616;&#31995;&#32479;&#30340;&#22270;&#34920;&#36798;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#32452;&#20851;&#38190;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#20272;&#35745;&#23398;&#20064;&#19968;&#20010;&#21160;&#20316;&#26465;&#20214;&#30340;&#21069;&#21521;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#20851;&#38190;&#28857;&#29366;&#24577;&#12290;&#36890;&#36807;&#22312;&#20851;&#38190;&#28857;&#31354;&#38388;&#20013;&#23398;&#20064;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#25512;&#24191;&#21040;&#20855;&#26377;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#65292;&#26032;&#39062;&#30340;&#32972;&#26223;&#21644;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#20960;&#20309;&#24418;&#29366;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric representation is an essential abstraction for forward prediction. Most existing forward models learn this representation through extensive supervision (e.g., object class and bounding box) although such ground-truth information is not readily accessible in reality. To address this, we introduce KINet (Keypoint Interaction Network) -- an end-to-end unsupervised framework to reason about object interactions based on a keypoint representation. Using visual observations, our model learns to associate objects with keypoint coordinates and discovers a graph representation of the system as a set of keypoint embeddings and their relations. It then learns an action-conditioned forward model using contrastive estimation to predict future keypoint states. By learning to perform physical reasoning in the keypoint space, our model automatically generalizes to scenarios with a different number of objects, novel backgrounds, and unseen object geometries. Experiments demonstrate the ef
&lt;/p&gt;</description></item><item><title>D4&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30456;&#20132;&#38598;&#21512;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#39057;&#22495;&#20013;&#30340;&#20887;&#20313;&#21644;&#26174;&#33879;&#24615;&#20998;&#21106;&#25216;&#26415;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#23376;&#31354;&#38388;&#32500;&#24230;&#65292;&#20351;&#23545;&#25239;&#24615;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26356;&#38590;&#34987;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2202.05687</link><description>&lt;p&gt;
D4: &#20351;&#29992;&#19981;&#30456;&#20132;&#38598;&#21512;&#20849;&#21516;&#26816;&#27979;&#23545;&#25239;&#24615;&#25193;&#25955;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
D4: Detection of Adversarial Diffusion Deepfakes Using Disjoint Ensembles. (arXiv:2202.05687v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05687
&lt;/p&gt;
&lt;p&gt;
D4&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30456;&#20132;&#38598;&#21512;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#39057;&#22495;&#20013;&#30340;&#20887;&#20313;&#21644;&#26174;&#33879;&#24615;&#20998;&#21106;&#25216;&#26415;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#23376;&#31354;&#38388;&#32500;&#24230;&#65292;&#20351;&#23545;&#25239;&#24615;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26356;&#38590;&#34987;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#23545;&#25239;&#25163;&#27573;&#28155;&#21152;&#20102;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#25200;&#21160;&#20197;&#36867;&#36991;&#26816;&#27979;&#30340;&#23545;&#25163;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disjoint Diffusion Deepfake Detection (D4)&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25913;&#21892;&#40657;&#30418;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26816;&#27979;&#22120;&#12290;D4&#20351;&#29992;&#19968;&#20010;&#22312;&#39057;&#35889;&#19981;&#30456;&#20132;&#23376;&#38598;&#19978;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#21033;&#29992;&#39057;&#22495;&#20013;&#30340;&#20887;&#20313;&#65292;&#24182;&#24212;&#29992;&#19968;&#20010;&#26174;&#33879;&#24615;&#20998;&#21106;&#25216;&#26415;&#23558;&#39057;&#29575;&#20998;&#37327;&#20998;&#37197;&#32473;&#22810;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#36825;&#20123;&#19981;&#30456;&#20132;&#30340;&#38598;&#21512;&#23548;&#33268;&#36755;&#20837;&#23376;&#31354;&#38388;&#32500;&#24230;&#30340;&#38477;&#20302;&#65292;&#20174;&#32780;&#20351;&#40657;&#30418;&#25915;&#20987;&#32773;&#26356;&#38590;&#25214;&#21040;&#23545;&#25239;&#24615;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;D4&#26041;&#27861;&#23545;&#25239;&#22810;&#20010;&#40657;&#30418;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting diffusion-generated deepfake images remains an open problem. Current detection methods fail against an adversary who adds imperceptible adversarial perturbations to the deepfake to evade detection. In this work, we propose Disjoint Diffusion Deepfake Detection (D4), a deepfake detector designed to improve black-box adversarial robustness beyond de facto solutions such as adversarial training. D4 uses an ensemble of models over disjoint subsets of the frequency spectrum to significantly improve adversarial robustness. Our key insight is to leverage a redundancy in the frequency domain and apply a saliency partitioning technique to disjointly distribute frequency components across multiple models. We formally prove that these disjoint ensembles lead to a reduction in the dimensionality of the input subspace where adversarial deepfakes lie, thereby making adversarial deepfakes harder to find for black-box attacks. We then empirically validate the D4 method against several black-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#26426;&#22120;&#36864;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#23553;&#38381;&#24418;&#24335;&#26356;&#26032;&#65292;&#33021;&#22815;&#22238;&#28335;&#24615;&#22320;&#35843;&#25972;&#35757;&#32451;&#25968;&#25454;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#32416;&#27491;&#12290;</title><link>http://arxiv.org/abs/2108.11577</link><description>&lt;p&gt;
&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#26426;&#22120;&#36864;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Features and Labels. (arXiv:2108.11577v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#26426;&#22120;&#36864;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#23553;&#38381;&#24418;&#24335;&#26356;&#26032;&#65292;&#33021;&#22815;&#22238;&#28335;&#24615;&#22320;&#35843;&#25972;&#35757;&#32451;&#25968;&#25454;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#32416;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#20449;&#24687;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#37096;&#20998;&#25764;&#38144;&#35757;&#32451;&#36807;&#31243;&#12290;&#24403;&#25935;&#24863;&#25968;&#25454;&#65288;&#22914;&#20449;&#29992;&#21345;&#21495;&#30721;&#25110;&#23494;&#30721;&#65289;&#24847;&#22806;&#36827;&#20837;&#27169;&#22411;&#24182;&#38656;&#35201;&#20043;&#21518;&#21024;&#38500;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#36864;&#35757;&#32451;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#21024;&#38500;&#20010;&#21035;&#25968;&#25454;&#28857;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#38656;&#35201;&#25764;&#38144;&#36739;&#22823;&#32452;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36864;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#30340;&#23553;&#38381;&#24418;&#24335;&#26356;&#26032;&#23454;&#29616;&#36864;&#35757;&#32451;&#12290;&#23427;&#33021;&#22815;&#23545;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#36827;&#34892;&#22238;&#28335;&#24615;&#35843;&#25972;&#65292;&#20174;&#32780;&#32416;&#27491;&#25968;&#25454;&#27844;&#38706;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#23545;&#20110;&#20855;&#26377;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20855;&#26377;&#29702;&#35770;&#25903;&#25345;&#30340;&#36864;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removing information from a machine learning model is a non-trivial task that requires to partially revert the training process. This task is unavoidable when sensitive data, such as credit card numbers or passwords, accidentally enter the model and need to be removed afterwards. Recently, different concepts for machine unlearning have been proposed to address this problem. While these approaches are effective in removing individual data points, they do not scale to scenarios where larger groups of features and labels need to be reverted. In this paper, we propose the first method for unlearning features and labels. Our approach builds on the concept of influence functions and realizes unlearning through closed-form updates of model parameters. It enables to adapt the influence of training data on a learning model retrospectively, thereby correcting data leaks and privacy issues. For learning models with strongly convex loss functions, our method provides certified unlearning with theo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$C^3$&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#23545;&#35805;&#20013;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#35270;&#39057;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.08914</link><description>&lt;p&gt;
$C^3$: &#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#32452;&#21512;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$C^3$&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#23545;&#35805;&#20013;&#30340;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#35270;&#39057;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#23558;&#35270;&#39057;&#29702;&#35299;&#21644;&#23545;&#35805;&#29702;&#35299;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#19982;&#23545;&#35805;&#21644;&#35270;&#39057;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#26465;&#20214;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#37096;&#20998;&#26159;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#32780;&#38750;&#21457;&#23637;&#22810;&#27169;&#24577;&#25512;&#29702;&#23454;&#29616;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#23545;&#25239;&#23545;&#27604;&#23398;&#20064;&#65288;$C^3$&#65289;&#26041;&#27861;&#65292;&#20197;&#24320;&#21457;&#35270;&#39057;&#23545;&#35805;&#20013;&#20851;&#20110;&#20107;&#23454;&#21644;&#21453;&#20107;&#23454;&#26679;&#26412;&#30340;&#23545;&#27604;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#23545;&#35805;&#20013;&#30340;&#26631;&#35760;&#30340;&#20107;&#23454;/&#21453;&#20107;&#23454;&#37319;&#26679;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#23545;&#35937;&#32423;&#25110;&#21160;&#20316;&#32423;&#21464;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#38598;&#20013;&#20110;&#23545;&#27604;&#38544;&#34255;&#29366;&#24577;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2103.00676</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#24456;&#22810;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20854;&#20013;&#65292;&#32477;&#22823;&#22810;&#25968;&#25915;&#20987;&#36890;&#36807;&#20462;&#25913;&#21333;&#20010;&#25991;&#26723;&#26631;&#35760;&#26469;&#23454;&#29616;&#25104;&#21151;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#12290;&#27599;&#31181;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#37117;&#30001;&#19968;&#32452;&#29305;&#23450;&#30340;&#22522;&#26412;&#32452;&#20214;&#23450;&#20041;&#65292;&#20363;&#22914;&#23545;&#25915;&#20987;&#32773;&#30340;&#32422;&#26463;&#25110;&#29305;&#23450;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#36827;&#34892;&#35843;&#26597;&#65292;&#24182;&#25552;&#21462;&#27599;&#31181;&#25915;&#20987;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19982;&#25915;&#20987;&#26080;&#20851;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#35843;&#30740;&#65292;&#20174;&#32780;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#65292;&#24182;&#26041;&#20415;&#36827;&#34892;&#32452;&#20214;&#27604;&#36739;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#36825;&#19968;&#39046;&#22495;&#65292;&#24182;&#25512;&#21160;&#23545;&#20110;&#20010;&#20307;&#25915;&#20987;&#32452;&#20214;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#22312;&#20999;&#25442;&#27425;&#25968;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21576;&#29616;&#20986;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.03803</link><description>&lt;p&gt;
&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;: &#20999;&#25442;&#39044;&#31639;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lazy OCO: Online Convex Optimization on a Switching Budget. (arXiv:2102.03803v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#22312;&#20999;&#25442;&#27425;&#25968;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21576;&#29616;&#20986;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#29609;&#23478;&#22312;T&#36718;&#20013;&#30340;&#26399;&#26395;&#20999;&#25442;&#20915;&#31574;&#19981;&#36229;&#36807;S&#27425;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#31163;&#25955;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#31867;&#20284;&#38382;&#39064;&#65292;&#26368;&#36817;&#20063;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#23545;&#25163;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#26080;&#30693;&#35774;&#32622;&#20013;&#25552;&#20986;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#19968;&#33324;&#20984;&#25439;&#22833;&#24314;&#31435;&#20102;O(T/S)&#30340;&#36951;&#25022;&#19978;&#30028;&#20197;&#21450;&#24378;&#20984;&#25439;&#22833;&#30340;&#36817;&#20284;O(T/S^2)&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#22312;&#19968;&#33324;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#36951;&#25022;&#20165;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#20056;&#27861;log T&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;log T&#27425;&#20999;&#25442;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34917;&#20805;&#20102;&#19982;&#25105;&#20204;&#32771;&#34385;&#30340;&#19968;&#20123;&#24773;&#20917;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online convex optimization where the player is permitted to switch decisions at most $S$ times in expectation throughout $T$ rounds. Similar problems have been addressed in prior work for the discrete decision set setting, and more recently in the continuous setting but only with an adaptive adversary. In this work, we aim to fill the gap and present computationally efficient algorithms in the more prevalent oblivious setting, establishing a regret bound of $O(T/S)$ for general convex losses and $\widetilde O(T/S^2)$ for strongly convex losses. In addition, for stochastic i.i.d.~losses, we present a simple algorithm that performs $\log T$ switches with only a multiplicative $\log T$ factor overhead in its regret in both the general and strongly convex settings. Finally, we complement our algorithms with lower bounds that match our upper bounds in some of the cases we consider.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21305;&#37197;&#24066;&#22330;&#20013;P2P&#20511;&#36151;&#30340;&#24066;&#22330;&#35774;&#35745;&#21644;&#25277;&#35937;&#65292;&#36890;&#36807;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#24066;&#22330;&#20004;&#36793;&#30340;&#20195;&#29702;&#20154;&#25928;&#29992;&#21644;&#23545;&#20511;&#27454;&#20154;&#20844;&#24179;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22522;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#25216;&#26415;&#65292;&#36151;&#27454;&#20154;&#21487;&#20197;&#26681;&#25454;&#31454;&#20105;&#30340;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#65292;&#24433;&#21709;&#20854;&#25237;&#36164;&#22238;&#25253;&#12290;&#27169;&#25311;&#23454;&#39564;&#26174;&#31034;&#36151;&#27454;&#20154;&#30340;&#36951;&#25022;&#21462;&#20915;&#20110;&#21021;&#22987;&#20559;&#22909;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2011.04400</link><description>&lt;p&gt;
&#21305;&#37197;&#24066;&#22330;&#20013;&#30340;&#36172;&#24466;&#65306;&#28857;&#23376;&#21644;&#23545;&#31561;&#20511;&#36151;&#30340;&#25552;&#35758;
&lt;/p&gt;
&lt;p&gt;
Bandits in Matching Markets: Ideas and Proposals for Peer Lending. (arXiv:2011.04400v5 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.04400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21305;&#37197;&#24066;&#22330;&#20013;P2P&#20511;&#36151;&#30340;&#24066;&#22330;&#35774;&#35745;&#21644;&#25277;&#35937;&#65292;&#36890;&#36807;&#23558;&#20854;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#24066;&#22330;&#20004;&#36793;&#30340;&#20195;&#29702;&#20154;&#25928;&#29992;&#21644;&#23545;&#20511;&#27454;&#20154;&#20844;&#24179;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#22522;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#25216;&#26415;&#65292;&#36151;&#27454;&#20154;&#21487;&#20197;&#26681;&#25454;&#31454;&#20105;&#30340;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#65292;&#24433;&#21709;&#20854;&#25237;&#36164;&#22238;&#25253;&#12290;&#27169;&#25311;&#23454;&#39564;&#26174;&#31034;&#36151;&#27454;&#20154;&#30340;&#36951;&#25022;&#21462;&#20915;&#20110;&#21021;&#22987;&#20559;&#22909;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#21040;&#21305;&#37197;&#24066;&#22330;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#26368;&#26032;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35797;&#22270;&#21046;&#23450;&#21644;&#25277;&#35937;P2P&#20511;&#36151;&#24066;&#22330;&#35774;&#35745;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#33539;&#24335;&#65292;&#20026;&#23545;&#31561;&#20511;&#36151;&#20174;&#21305;&#37197;&#24066;&#22330;&#30340;&#35282;&#24230;&#36827;&#34892;&#26500;&#24819;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#26102;&#23562;&#37325;&#20511;&#27454;&#20154;&#21644;&#36151;&#27454;&#20154;&#20559;&#22909;&#26102;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19987;&#38376;&#30340;&#24066;&#22330;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#32771;&#34385;&#24066;&#22330;&#20004;&#36793;&#30340;&#20195;&#29702;&#20154;&#30340;&#19981;&#21516;&#25928;&#29992;&#65292;&#21516;&#26102;&#29702;&#35299;&#23545;&#20511;&#27454;&#20154;&#30340;&#20844;&#24179;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#25216;&#26415;&#65292;&#20801;&#35768;&#36151;&#27454;&#20154;&#26681;&#25454;&#31454;&#20105;&#30340;&#19981;&#30830;&#23450;&#24615;&#21160;&#24577;&#35843;&#25972;&#36873;&#25321;&#65292;&#36825;&#20063;&#20250;&#24433;&#21709;&#20182;&#20204;&#30340;&#25237;&#36164;&#22238;&#25253;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26368;&#20339;&#20511;&#27454;&#20154;-&#36151;&#27454;&#20154;&#21305;&#37197;&#30340;&#36951;&#25022;&#21160;&#24577;&#65292;&#24182;&#21457;&#29616;&#36151;&#27454;&#20154;&#30340;&#36951;&#25022;&#21462;&#20915;&#20110;&#21021;&#22987;&#20559;&#22909;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent applications of sequential decision making in matching markets, in this paper we attempt at formulating and abstracting market designs for P2P lending. We describe a paradigm to set the stage for how peer to peer investments can be conceived from a matching market perspective, especially when both borrower and lender preferences are respected. We model these specialized markets as an optimization problem and consider different utilities for agents on both sides of the market while also understanding the impact of equitable allocations to borrowers. We devise a technique based on sequential decision making that allow the lenders to adjust their choices based on the dynamics of uncertainty from competition over time and that also impacts the rewards in return for their investments. Using simulated experiments we show the dynamics of the regret based on the optimal borrower-lender matching and find that the lender regret depends on the initial preferences set by the le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2010.08657</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.08657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#38754;&#23545;&#19968;&#31995;&#21015;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#23398;&#20064;&#26032;&#31867;&#21035;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#31867;&#21035;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24120;&#24120;&#20250;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#25928;&#30340;&#26041;&#27861;&#21033;&#29992;&#23384;&#20648;&#22312;&#19968;&#20010;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#21516;&#26102;&#25193;&#23637;&#26368;&#32456;&#20998;&#31867;&#22120;&#33410;&#28857;&#20197;&#23481;&#32435;&#26032;&#30340;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#22266;&#23450;&#20998;&#31867;&#22120;&#26367;&#20195;&#20102;&#25193;&#23637;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#38454;&#27573;&#24320;&#22987;&#23601;&#21463;&#21040;&#20998;&#31867;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#19982;&#26631;&#20934;&#25193;&#23637;&#20998;&#31867;&#22120;&#30456;&#21453;&#65292;&#36825;&#26679;&#20570;&#26377;&#20197;&#19979;&#22909;&#22788;&#65306;(a)&#26410;&#26469;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#30340;&#19968;&#24320;&#22987;&#23601;&#33021;&#30475;&#21040;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#30340;&#27491;&#26679;&#26412;&#65307;(b)&#33021;&#22815;&#23398;&#20064;&#19981;&#38543;&#30528;&#26032;&#31867;&#21035;&#30340;&#21152;&#20837;&#32780;&#25913;&#21464;&#20854;&#20960;&#20309;&#37197;&#32622;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#33041;&#30005;&#22270;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#40654;&#26364;&#27969;&#24418;&#21644;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#23398;&#20064;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24046;&#20998;&#29109;&#21644;&#23545;&#25968;&#21151;&#29575;&#35889;&#23494;&#24230;&#29305;&#24449;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#12290;&#36890;&#36807;&#26377;&#25928;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2008.08633</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#21644;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#30340;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space. (arXiv:2008.08633v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#33041;&#30005;&#22270;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#40654;&#26364;&#27969;&#24418;&#21644;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#23398;&#20064;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24046;&#20998;&#29109;&#21644;&#23545;&#25968;&#21151;&#29575;&#35889;&#23494;&#24230;&#29305;&#24449;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#12290;&#36890;&#36807;&#26377;&#25928;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#33041;&#30005;&#22270;(EEG)&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20026;&#20102;&#23398;&#20064;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20174;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#31354;&#38388;&#21327;&#26041;&#24046;&#30697;&#38453;(Riemannian manifold)&#20013;&#33719;&#21462;&#40654;&#26364;&#24179;&#22343;&#20540;&#21644;&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20999;&#31354;&#38388;&#23398;&#20064;&#23558;&#31354;&#38388;&#20449;&#24687;&#25237;&#24433;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#20840;&#36830;&#25509;&#23618;&#26469;&#23398;&#20064;&#31354;&#38388;&#20449;&#24687;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;EEG&#20449;&#21495;&#20013;&#25552;&#21462;&#24046;&#20998;&#29109;&#21644;&#23545;&#25968;&#21151;&#29575;&#35889;&#23494;&#24230;&#29305;&#24449;&#26469;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#29992;&#24102;&#26377;&#36719;&#27880;&#24847;&#26426;&#21046;&#30340;&#28145;&#23618;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#12290;&#20026;&#20102;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23398;&#20064;&#23558;&#27880;&#24847;&#26435;&#37325;&#24212;&#29992;&#20110;&#29305;&#23450;&#23884;&#20837;&#29305;&#24449;&#20197;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#27969;&#34892;&#30340;&#19982;EEG&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24773;&#32490;&#35782;&#21035;&#12289;&#35686;&#35273;&#24615;&#35782;&#21035;&#21644;&#30561;&#30496;&#21628;&#21560;&#26242;&#20572;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel deep neural architecture for learning electroencephalogram (EEG). To learn the spatial information, our model first obtains the Riemannian mean and distance from spatial covariance matrices (SCMs) on a Riemannian manifold. We then project the spatial information onto a Euclidean space via tangent space learning. Following, two fully connected layers are used to learn the spatial information embeddings. Moreover, our proposed method learns the temporal information via differential entropy and logarithm power spectrum density features extracted from EEG signals in a Euclidean space using a deep long short-term memory network with a soft attention mechanism. To combine the spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets across three popular EEG-related tasks, notably emotion recognition, vigilance es
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;COVID-19&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20026;COVID-19&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#19988;&#19968;&#33268;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2007.10785</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;COVID-19&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Automated Detection and Forecasting of COVID-19 using Deep Learning Techniques: A Review. (arXiv:2007.10785v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.10785
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;COVID-19&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20026;COVID-19&#30340;&#21307;&#23398;&#35786;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#19988;&#19968;&#33268;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#30149;&#27602;&#65288;&#25110;COVID-19&#65289;&#26159;&#19968;&#31181;&#21361;&#38505;&#30340;&#30142;&#30149;&#65292;&#36890;&#36807;&#30452;&#25509;&#24433;&#21709;&#32954;&#37096;&#21361;&#21450;&#20840;&#29699;&#35768;&#22810;&#20154;&#30340;&#20581;&#24247;&#12290;COVID-19&#26159;&#19968;&#31181;&#20013;&#31561;&#22823;&#23567;&#12289;&#26377;&#21253;&#34987;&#30340;&#30149;&#27602;&#65292;&#20855;&#26377;&#21333;&#38142;RNA&#65292;&#21516;&#26102;&#20063;&#25317;&#26377;&#26368;&#22823;&#30340;RNA&#22522;&#22240;&#32452;&#20043;&#19968;&#65292;&#22823;&#32422;120&#32435;&#31859;&#12290;X&#23556;&#32447;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25104;&#20687;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#20174;&#36825;&#20123;&#21307;&#23398;&#22270;&#20687;&#20013;&#35782;&#21035;COVID-19&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#26469;&#33719;&#24471;&#19968;&#33268;&#30340;&#39640;&#24615;&#33021;&#12290;&#22312;AI&#26041;&#27861;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#26368;&#36817;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19982;ML&#19981;&#21516;&#65292;DL&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#23436;&#25104;&#29305;&#24449;&#25552;&#21462;&#12289;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#30340;&#25152;&#26377;&#38454;&#27573;&#12290;&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#20851;&#20110;&#20351;&#29992;DL&#25216;&#26415;&#36827;&#34892;COVID-19&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronavirus, or COVID-19, is a hazardous disease that has endangered the health of many people around the world by directly affecting the lungs. COVID-19 is a medium-sized, coated virus with a single-stranded RNA, and also has one of the largest RNA genomes and is approximately 120 nm. The X-Ray and computed tomography (CT) imaging modalities are widely used to obtain a fast and accurate medical diagnosis. Identifying COVID-19 from these medical images is extremely challenging as it is time-consuming and prone to human errors. Hence, artificial intelligence (AI) methodologies can be used to obtain consistent high performance. Among the AI methods, deep learning (DL) networks have gained popularity recently compared to conventional machine learning (ML). Unlike ML, all stages of feature extraction, feature selection, and classification are accomplished automatically in DL models. In this paper, a complete survey of studies on the application of DL techniques for COVID-19 diagnostic and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20248;&#21270;&#25551;&#36848;&#20026;&#19968;&#20010;&#36807;&#31243;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#23545;&#22797;&#26434;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#24314;&#27169;&#65292;&#24182;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#12290;</title><link>http://arxiv.org/abs/1909.05207</link><description>&lt;p&gt;
&#22312;&#32447;&#20984;&#20248;&#21270;&#23548;&#35770;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Convex Optimization. (arXiv:1909.05207v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1909.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20248;&#21270;&#25551;&#36848;&#20026;&#19968;&#20010;&#36807;&#31243;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#23545;&#22797;&#26434;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#24314;&#27169;&#65292;&#24182;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20248;&#21270;&#25551;&#36848;&#20026;&#19968;&#20010;&#36807;&#31243;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#22826;&#22797;&#26434;&#65292;&#26080;&#27861;&#24314;&#31435;&#20840;&#38754;&#30340;&#29702;&#35770;&#27169;&#22411;&#24182;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#21644;&#25968;&#23398;&#20248;&#21270;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#19968;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#35266;&#23519;&#21040;&#38382;&#39064;&#30340;&#26356;&#22810;&#26041;&#38754;&#26102;&#36890;&#36807;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#26159;&#24517;&#35201;&#19988;&#26377;&#30410;&#30340;&#12290;&#23558;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#36807;&#31243;&#30340;&#35266;&#28857;&#22312;&#21508;&#20010;&#39046;&#22495;&#21464;&#24471;&#31361;&#20986;&#65292;&#24182;&#22312;&#24314;&#27169;&#21644;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#19968;&#37096;&#20998;&#30340;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#28040;&#36153;&#32773;&#22312;&#22810;&#20010;&#20135;&#21697;&#31867;&#21035;&#20013;&#30340;&#20559;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#26102;&#21464;&#20135;&#21697;&#23646;&#24615;&#21644;&#20135;&#21697;&#32570;&#36135;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#25913;&#36827;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/1906.02635</link><description>&lt;p&gt;
&#22312;&#22810;&#20010;&#20135;&#21697;&#31867;&#21035;&#20013;&#36827;&#34892;&#28040;&#36153;&#32773;&#36873;&#25321;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Inference for Consumer Choice Across Many Product Categories. (arXiv:1906.02635v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#28040;&#36153;&#32773;&#22312;&#22810;&#20010;&#20135;&#21697;&#31867;&#21035;&#20013;&#30340;&#20559;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#26102;&#21464;&#20135;&#21697;&#23646;&#24615;&#21644;&#20135;&#21697;&#32570;&#36135;&#30340;&#24773;&#20917;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#25913;&#36827;&#20043;&#22788;&#22312;&#20110;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#31163;&#25955;&#36873;&#25321;&#20013;&#28040;&#36153;&#32773;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#28040;&#36153;&#32773;&#22312;&#19968;&#20010;&#31867;&#21035;&#20013;&#36873;&#25321;&#33267;&#22810;&#19968;&#20010;&#20135;&#21697;&#65292;&#20294;&#22312;&#22810;&#20010;&#31867;&#21035;&#20013;&#24182;&#34892;&#36873;&#25321;&#12290;&#28040;&#36153;&#32773;&#30340;&#25928;&#29992;&#23545;&#19981;&#21516;&#31867;&#21035;&#26159;&#21487;&#21152;&#24615;&#30340;&#12290;&#22905;&#23545;&#20135;&#21697;&#23646;&#24615;&#30340;&#20559;&#22909;&#20197;&#21450;&#22905;&#23545;&#20215;&#26684;&#30340;&#25935;&#24863;&#24230;&#22312;&#19981;&#21516;&#20135;&#21697;&#38388;&#26377;&#25152;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#22312;&#20135;&#21697;&#38388;&#26159;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#20851;&#20110;&#27010;&#29575;&#27169;&#22411;&#20013;&#30697;&#38453;&#20998;&#35299;&#30340;&#25216;&#26415;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#32771;&#34385;&#26102;&#21464;&#20135;&#21697;&#23646;&#24615;&#21644;&#20135;&#21697;&#32570;&#36135;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#21547;&#26377;&#20215;&#26684;&#21464;&#21160;&#25110;&#32570;&#36135;&#20135;&#21697;&#30340;&#30041;&#23384;&#25968;&#25454;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#32771;&#34385;&#27599;&#20010;&#31867;&#21035;&#30340;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#27169;&#22411;&#30340;&#25913;&#36827;&#20043;&#19968;&#26159;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#20559;&#22909;&#30340;&#24322;&#36136;&#24615;&#65288;&#36890;&#36807;&#36328;&#31867;&#21035;&#27719;&#24635;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in parallel. The consumer's utility is additive in the different categories. Her preferences about product attributes as well as her price sensitivity vary across products and are in general correlated across products. We build on techniques from the machine learning literature on probabilistic models of matrix factorization, extending the methods to account for time-varying product attributes and products going out of stock. We evaluate the performance of the model using held-out data from weeks with price changes or out of stock products. We show that our model improves over traditional modeling approaches that consider each category in isolation. One source of the improvement is the ability of the model to accurately estimate heterogeneity in preferences (by pooling information across categories); 
&lt;/p&gt;</description></item></channel></rss>