<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;FLORAS&#26041;&#27861;&#65292;&#21487;&#28040;&#38500;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;FLORAS&#21487;&#20197;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#24046;&#20998;&#38544;&#31169;&#31561;&#32423;&#65292;&#24182;&#19988;&#36890;&#36807;&#25512;&#23548;&#25910;&#25947;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#30340;&#24179;&#31283;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.08280</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;&#24046;&#20998;&#38544;&#31169;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Wireless Federated Learning Using Orthogonal Sequences. (arXiv:2306.08280v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;FLORAS&#26041;&#27861;&#65292;&#21487;&#28040;&#38500;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;FLORAS&#21487;&#20197;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#24046;&#20998;&#38544;&#31169;&#31561;&#32423;&#65292;&#24182;&#19988;&#36890;&#36807;&#25512;&#23548;&#25910;&#25947;&#30028;&#38480;&#65292;&#23454;&#29616;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#30340;&#24179;&#31283;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#19978;&#34892;&#31354;&#20013;&#35745;&#31639;&#26041;&#27861;FLORAS&#65292;&#29992;&#20110;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#12290;FLORAS&#20174;&#36890;&#20449;&#35774;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#21033;&#29992;&#27491;&#20132;&#24207;&#21015;&#30340;&#24615;&#36136;&#28040;&#38500;&#20102;&#21457;&#36865;&#31471;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSIT&#65289;&#35201;&#27714;&#12290;&#20174;&#38544;&#31169;&#20445;&#25252;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;FLORAS&#21487;&#20197;&#25552;&#20379;&#39033;&#30446;&#32423;&#21644;&#23458;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#31995;&#32479;&#21442;&#25968;&#65292;FLORAS&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;DP&#31561;&#32423;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;FL&#25910;&#25947;&#30028;&#38480;&#65292;&#32467;&#21512;&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#20197;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#24046;&#20998;&#38544;&#31169;&#32423;&#21035;&#20043;&#38388;&#23454;&#29616;&#24179;&#31283;&#30340;&#26435;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;FLORAS&#30456;&#23545;&#20110;&#22522;&#20934;AirComp&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#25351;&#23548;&#19981;&#21516;&#26435;&#34913;&#26465;&#20214;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;FL&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel privacy-preserving uplink over-the-air computation (AirComp) method, termed FLORAS, for single-input single-output (SISO) wireless federated learning (FL) systems. From the communication design perspective, FLORAS eliminates the requirement of channel state information at the transmitters (CSIT) by leveraging the properties of orthogonal sequences. From the privacy perspective, we prove that FLORAS can offer both item-level and client-level differential privacy (DP) guarantees. Moreover, by adjusting the system parameters, FLORAS can flexibly achieve different DP levels at no additional cost. A novel FL convergence bound is derived which, combined with the privacy guarantees, allows for a smooth tradeoff between convergence rate and differential privacy levels. Numerical results demonstrate the advantages of FLORAS compared with the baseline AirComp method, and validate that our analytical results can guide the design of privacy-preserving FL with different tradeoff 
&lt;/p&gt;</description></item><item><title>FRIGATE&#26159;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;GNN&#36827;&#34892;&#20844;&#36335;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08277</link><description>&lt;p&gt;
FRIGATE:&#20844;&#36335;&#32593;&#32476;&#30340;&#33410;&#32422;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks. (arXiv:2306.08277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08277
&lt;/p&gt;
&lt;p&gt;
FRIGATE&#26159;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;GNN&#36827;&#34892;&#20844;&#36335;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#20844;&#36335;&#32593;&#32476;&#19978;&#30340;&#26102;&#31354;&#36807;&#31243;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22312;&#21457;&#23637;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#19977;&#20010;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#20844;&#36335;&#32593;&#32476;&#30340;&#20551;&#35774;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#23450;&#27599;&#20010;&#33410;&#28857;&#37117;&#37197;&#22791;&#20256;&#24863;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#28982;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#25152;&#26377;&#20301;&#32622;&#65288;&#33410;&#28857;&#65289;&#21487;&#33021;&#27809;&#26377;&#37197;&#22791;&#20256;&#24863;&#22120;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20551;&#23450;&#25152;&#26377;&#23433;&#35013;&#30340;&#20256;&#24863;&#22120;&#37117;&#20855;&#26377;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#12290;&#28982;&#32780;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#36890;&#20449;&#20013;&#30340;&#25968;&#25454;&#20002;&#22833;&#31561;&#21407;&#22240;&#65292;&#36825;&#20063;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#21518;&#65292;&#20551;&#35774;&#38745;&#24577;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#32593;&#32476;&#20869;&#36830;&#36890;&#24615;&#20250;&#30001;&#20110;&#36947;&#36335;&#23553;&#38381;&#12289;&#26032;&#36335;&#20462;&#24314;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FRIGATE&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#32570;&#28857;&#12290;FRIGATE&#30001;&#26102;&#31354;GNN&#39537;&#21160;&#65292;&#23558;&#20301;&#32622;&#12289;&#25299;&#25169;&#21644;&#26102;&#38388;&#20449;&#24687;&#38598;&#25104;&#21040;&#33410;&#28857;&#34920;&#31034;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;FRIGATE&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20844;&#36335;&#32593;&#32476;&#19978;&#26102;&#31354;&#36807;&#31243;&#30340;&#28436;&#21464;&#65292;&#32780;&#26080;&#38656;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;FRIGATE&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#20197;&#21450;&#36793;&#26041;&#21521;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08274</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#22312;&#26377;&#21521;&#25110;&#26080;&#21521;&#22270;&#20013;&#65292;&#20026;&#20160;&#20040;&#35201;&#36873;&#25321;&#21738;&#20010;&#65311;&#23454;&#35777;&#30740;&#31350;&#21644;&#31616;&#21333;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Using Either Aggregated Features or Adjacency Lists in Directed or Undirected Graph? Empirical Study and Simple Classification Method. (arXiv:2306.08274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#20197;&#21450;&#36793;&#26041;&#21521;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20998;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#26368;&#28909;&#38376;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#20851;&#27880;&#33410;&#28857;&#34920;&#31034;&#65288;&#32858;&#21512;&#29305;&#24449; vs. &#37051;&#25509;&#34920;&#65289;&#21644;&#36755;&#20837;&#22270;&#30340;&#36793;&#26041;&#21521;&#65288;&#26377;&#21521; vs. &#26080;&#21521;&#65289;&#36873;&#25321;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#19981;&#21516;&#33410;&#28857;&#34920;&#31034;&#21644;&#36793;&#26041;&#21521;&#30340;&#21508;&#31181;GNNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21333;&#19968;&#30340;&#32452;&#21512;&#31283;&#23450;&#22320;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;A2DUG&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification is one of the hottest tasks in graph analysis. In this paper, we focus on the choices of node representations (aggregated features vs. adjacency lists) and the edge direction of an input graph (directed vs. undirected), which have a large influence on classification results. We address the first empirical study to benchmark the performance of various GNNs that use either combination of node representations and edge directions. Our experiments demonstrate that no single combination stably achieves state-of-the-art results across datasets, which indicates that we need to select appropriate combinations depending on the characteristics of datasets. In response, we propose a simple yet holistic classification method A2DUG which leverages all combinations of node representation variants in directed and undirected graphs. We demonstrate that A2DUG stably performs well on various datasets. Surprisingly, it largely outperforms the current state-of-the-art methods in several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#65292;&#29992;&#20110;&#36861;&#36394;&#22826;&#38451;&#30340;&#27963;&#21160;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#26041;&#38754;&#20934;&#30830;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.08270</link><description>&lt;p&gt;
&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series Transformation, Entropy Measures and Machine Learning Approaches. (arXiv:2306.08270v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#65292;&#29992;&#20110;&#36861;&#36394;&#22826;&#38451;&#30340;&#27963;&#21160;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#26041;&#38754;&#20934;&#30830;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#30340;&#24615;&#36136;&#38750;&#24120;&#22797;&#26434;&#65292;&#20854;&#35266;&#27979;&#22270;&#20687;&#29305;&#24449;&#26159;&#20102;&#35299;&#22826;&#38451;&#27963;&#21160;&#12289;&#31354;&#38388;&#21644;&#22320;&#29699;&#22825;&#27668;&#26465;&#20214;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26469;&#28304;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#20351;&#29992;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#32479;&#35745;&#21644;&#29109;&#24230;&#37327;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36861;&#36394;&#22826;&#38451;&#27963;&#21160;&#12290;&#35813;&#25216;&#26415;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#35745;&#21644;&#29109;&#24230;&#37327;&#25110;&#30452;&#25509;&#20998;&#31867;&#31561;&#26041;&#27861;&#20174;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20026;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#21644;&#8220;&#38750;&#39118;&#26292;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36861;&#36394;&#22826;&#38451;&#27963;&#21160;&#30340;&#28508;&#22312;&#20934;&#30830;&#24615;&#20026;&#32422;.
&lt;/p&gt;
&lt;p&gt;
The sun is highly complex in nature and its observatory imagery features is one of the most important sources of information about the sun activity, space and Earth's weather conditions. The NASA, solar Dynamics Observatory captures approximately 70,000 images of the sun activity in a day and the continuous visual inspection of this solar observatory images is challenging. In this study, we developed a technique of tracking the sun's activity using 2D circular kernel time series transformation, statistical and entropy measures, with machine learning approaches. The technique involves transforming the solar observatory image section into 1-Dimensional time series (1-DTS) while the statistical and entropy measures (Approach 1) and direct classification (Approach 2) is used to capture the extraction features from the 1-DTS for machine learning classification into 'solar storm' and 'no storm'. We found that the potential accuracy of the model in tracking the activity of the sun is approxim
&lt;/p&gt;</description></item><item><title>LargeST&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.08259</link><description>&lt;p&gt;
LargeST: &#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#20132;&#36890;&#39044;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting. (arXiv:2306.08259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08259
&lt;/p&gt;
&lt;p&gt;
LargeST&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#26234;&#24935;&#22478;&#24066;&#39033;&#30446;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#27169;&#24335;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#38598;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LargeST&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;8600&#20010;&#20256;&#24863;&#22120;&#12289;&#35206;&#30422;5&#24180;&#26102;&#38388;&#21644;&#21253;&#25324;&#32454;&#33268;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;LargeST&#36827;&#34892;&#28145;&#20837;&#25968;&#25454;&#20998;&#26512;&#24182;&#28436;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#32780;&#35328;&#26159;&#19968;&#20010;&#26356;&#20026;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08256</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#30315;&#30187;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Seizure Prediction with Generative Diffusion Model. (arXiv:2306.08256v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30315;&#30187;&#39044;&#27979;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#37325;&#28857;&#22312;&#20110;&#21306;&#20998;&#21457;&#20316;&#21069;&#29366;&#24577;&#19982;&#21457;&#20316;&#21518;&#29366;&#24577;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#30315;&#30187;&#39044;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21457;&#20316;&#21069;&#19982;&#21457;&#20316;&#21518;&#29366;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25968;&#25454;&#25193;&#22686;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#30452;&#35266;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#36890;&#36807;&#37325;&#21472;&#25110;&#37325;&#26032;&#32452;&#21512;&#25968;&#25454;&#26469;&#29983;&#25104;&#26679;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;&#36716;&#25442;&#26080;&#27861;&#23436;&#20840;&#25506;&#32034;&#29305;&#24449;&#31354;&#38388;&#24182;&#25552;&#20379;&#26032;&#20449;&#24687;&#65292;&#25152;&#20197;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#21463;&#21040;&#21407;&#22987;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#30315;&#30187;&#33041;&#30005;&#22270;&#34920;&#31034;&#22312;&#19981;&#21516;&#21457;&#20316;&#20043;&#38388;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#26679;&#26412;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#20197;&#22312;&#26032;&#30340;&#30315;&#30187;&#21457;&#20316;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#12290;&#26041;&#27861;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#27169;&#22411;&#26469;&#23545;&#21407;&#22987;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#20197;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#65306;DiffEEG&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;SVM&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.08243</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#38381;&#30151;&#24178;&#39044;&#20998;&#26512;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MMASD&#12290;
&lt;/p&gt;
&lt;p&gt;
MMASD: A Multimodal Dataset for Autism Intervention Analysis. (arXiv:2306.08243v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08243
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MMASD&#30340;&#33258;&#38381;&#30151;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#27835;&#30103;&#24178;&#39044;&#12290;&#23427;&#21253;&#25324;&#20174;32&#21517;&#33258;&#38381;&#30151;&#24739;&#20799;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#21457;&#32946;&#24615;&#30142;&#30149;&#65292;&#20854;&#29305;&#24449;&#26159;&#37325;&#22823;&#30340;&#31038;&#20132;&#27807;&#36890;&#38556;&#30861;&#21644;&#22256;&#38590;&#30340;&#30693;&#35273;&#21644;&#34920;&#36798;&#27807;&#36890;&#25552;&#31034;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20419;&#36827;&#33258;&#38381;&#30151;&#30740;&#31350;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#29305;&#23450;&#20998;&#26512;&#65292;&#24182;&#22312;&#33258;&#38381;&#30151;&#31038;&#21306;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#38480;&#21046;&#20102;&#30001;&#20110;&#25968;&#25454;&#20849;&#20139;&#22797;&#26434;&#24615;&#32780;&#36328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#24320;&#28304;&#25968;&#25454;&#38598;MMASD&#20316;&#20026;&#22810;&#27169;&#24335;ASD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#33258;&#24739;&#26377;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#28216;&#25103;&#27835;&#30103;&#24178;&#39044;&#12290;MMASD&#21253;&#25324;32&#21517;&#24739;&#26377;ASD&#30340;&#20799;&#31461;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#20174;100&#22810;&#23567;&#26102;&#30340;&#24178;&#39044;&#24405;&#38899;&#20013;&#20998;&#27573;&#30340;1,315&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#20026;&#20419;&#36827;&#20844;&#20849;&#35775;&#38382;&#65292;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#22235;&#31181;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#30340;&#25968;&#25454;&#65306;&#65288;1&#65289;&#20809;&#27969;&#65292;&#65288;2&#65289;2D&#39592;&#26550;&#65292;&#65288;3&#65289;3D&#39592;&#26550;&#21644;&#65288;4&#65289;&#20020;&#24202;&#21307;&#29983;ASD&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism spectrum disorder (ASD) is a developmental disorder characterized by significant social communication impairments and difficulties perceiving and presenting communication cues. Machine learning techniques have been broadly adopted to facilitate autism studies and assessments. However, computational models are primarily concentrated on specific analysis and validated on private datasets in the autism community, which limits comparisons across models due to privacy-preserving data sharing complications. This work presents a novel privacy-preserving open-source dataset, MMASD as a MultiModal ASD benchmark dataset, collected from play therapy interventions of children with Autism. MMASD includes data from 32 children with ASD, and 1,315 data samples segmented from over 100 hours of intervention recordings. To promote public access, each data sample consists of four privacy-preserving modalities of data: (1) optical flow, (2) 2D skeleton, (3) 3D skeleton, and (4) clinician ASD evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35838;&#31243;&#23376;&#30446;&#26631;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064; (CSIRL)&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26694;&#26550;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#23376;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#27169;&#20223;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#20840;&#23616;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08232</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35838;&#31243;&#23376;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35838;&#31243;&#23376;&#30446;&#26631;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064; (CSIRL)&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26694;&#26550;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#23376;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#27169;&#20223;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#20840;&#23616;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#37325;&#26500;&#22870;&#21169;&#20989;&#25968;&#20197;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#24050;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;IRL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#20840;&#23616;&#22870;&#21169;&#20989;&#25968;&#20197;&#26368;&#23567;&#21270;&#27169;&#20223;&#32773;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#36712;&#36857;&#24046;&#24322;&#65292;&#20294;&#36825;&#20123;&#20840;&#23616;&#35774;&#35745;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#22122;&#22768;&#21644;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#23548;&#33268;&#19981;&#36866;&#24403;&#30340;&#22870;&#21169;&#20998;&#37197;&#65292;&#20174;&#32780;&#38477;&#20302;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22270;&#20687;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#32467;&#21512;&#20004;&#31181;&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;SVAE&#39318;&#27425;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.08230</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31163;&#25955;&#34920;&#31034;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26080;&#20559;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#22270;&#20687;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30340;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#32467;&#21512;&#20004;&#31181;&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;SVAE&#39318;&#27425;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22270;&#24418;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32452;&#21512;&#65292;&#25105;&#20204;&#23398;&#20064;&#20855;&#26377;&#20004;&#31181;&#26694;&#26550;&#20248;&#21183;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290; &#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#20174;&#22270;&#24418;&#27169;&#22411;&#32487;&#25215;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#28145;&#24230;&#23398;&#20064;&#20013;&#32487;&#25215;&#20102;&#36866;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#28789;&#27963;&#20284;&#28982;&#65292;&#20294;&#26159;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#20248;&#21270;&#25361;&#25112;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;SVAE&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#35777;&#26126;&#20102;SVAE&#22312;&#21547;&#26377;&#32570;&#22833;&#25968;&#25454;&#19988;&#21253;&#21547;&#31163;&#25955;&#28508;&#21464;&#37327;&#26102;&#22788;&#29702;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20869;&#23384;&#39640;&#25928;&#38544;&#24335;&#24494;&#20998;&#26041;&#26696;&#20351;&#24471;SVAE&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#24555;&#22320;&#23398;&#20064;&#20934;&#30830;&#30340;&#22270;&#24418;&#27169;&#22411;&#21442;&#25968;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#35745;&#31639;&#33258;&#28982;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#23548;&#20986;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#21457;&#29616;&#30340;&#20559;&#24046;&#12290;&#36825;&#20123;&#20248;&#21270;&#21019;&#26032;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#23558;SVAE&#19982;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#36807;&#28193;&#31574;&#30053;&#8212;&#8212;&#36807;&#28193;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;&#27493;&#24577;&#30340;&#22797;&#26434;&#24615;&#20998;&#20026;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#30340;&#19987;&#29992;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#34920;&#24449;&#25299;&#23637;&#26426;&#22120;&#20154;&#30340;&#22810;&#26679;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25216;&#33021;&#24211;&#20013;&#20219;&#24847;&#31574;&#30053;&#23545;&#20043;&#38388;&#31283;&#20581;&#36807;&#28193;&#12290;</title><link>http://arxiv.org/abs/2306.08224</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#34920;&#24449;&#36890;&#36807;&#31574;&#30053;&#36807;&#28193;&#25299;&#23637;&#25935;&#25463;&#26426;&#22120;&#20154;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Expanding Versatility of Agile Locomotion through Policy Transitions Using Latent State Representation. (arXiv:2306.08224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#36807;&#28193;&#31574;&#30053;&#8212;&#8212;&#36807;&#28193;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;&#27493;&#24577;&#30340;&#22797;&#26434;&#24615;&#20998;&#20026;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#30340;&#19987;&#29992;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#34920;&#24449;&#25299;&#23637;&#26426;&#22120;&#20154;&#30340;&#22810;&#26679;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#25216;&#33021;&#24211;&#20013;&#20219;&#24847;&#31574;&#30053;&#23545;&#20043;&#38388;&#31283;&#20581;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#36807;&#28193;&#31574;&#30053;&#8212;&#8212;&#36807;&#28193;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27493;&#24577;&#30340;&#22797;&#26434;&#24615;&#20998;&#20026;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#30340;&#19987;&#29992;&#36816;&#21160;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#31574;&#30053;&#19982;&#31283;&#20581;&#30340;&#36807;&#28193;&#26041;&#24335;&#32479;&#19968;&#20026;&#21333;&#20010;&#36830;&#36143;&#30340;&#20803;&#25511;&#21046;&#22120;&#65292;&#24182;&#26816;&#26597;&#28508;&#22312;&#29366;&#24577;&#34920;&#24449;&#65292;&#25299;&#23637;&#26426;&#22120;&#20154;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#36845;&#20195;&#22320;&#25193;&#23637;&#20854;&#25216;&#33021;&#24211;&#21644;&#22312;&#24211;&#20013;&#20219;&#24847;&#31574;&#30053;&#23545;&#20043;&#38388;&#31283;&#20581;&#22320;&#36807;&#28193;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#28155;&#21152;&#26032;&#25216;&#33021;&#19981;&#20250;&#24341;&#20837;&#20219;&#20309;&#25913;&#21464;&#20043;&#21069;&#23398;&#20064;&#25216;&#33021;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19981;&#21040;&#19968;&#23567;&#26102;&#21363;&#21487;&#23436;&#25104;&#21333;&#20010;&#36816;&#21160;&#31574;&#30053;&#30340;&#22521;&#35757;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24456;&#26377;&#25928;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#36716;&#25442;&#23545;&#20013;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;19%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the transition-net, a robust transition strategy that expands the versatility of robot locomotion in the real-world setting. To this end, we start by distributing the complexity of different gaits into dedicated locomotion policies applicable to real-world robots. Next, we expand the versatility of the robot by unifying the policies with robust transitions into a single coherent meta-controller by examining the latent state representations. Our approach enables the robot to iteratively expand its skill repertoire and robustly transition between any policy pair in a library. In our framework, adding new skills does not introduce any process that alters the previously learned skills. Moreover, training of a locomotion policy takes less than an hour with a single consumer GPU. Our approach is effective in the real-world and achieves a 19% higher average success rate for the most challenging transition pairs in our experiments compared to existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#24418;&#24335;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#26469;&#25214;&#21040;&#26368;&#20248;&#33410;&#28857;&#23884;&#20837;&#65292;&#20174;&#32780;&#32531;&#35299;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#22122;&#22768;&#27979;&#37327;&#25361;&#25112;&#23545;&#22270;&#25968;&#25454;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.08210</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#22122;&#22768;&#22270;&#19978;&#30340;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Robust Learning on Noisy Graphs. (arXiv:2306.08210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#24418;&#24335;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#26469;&#25214;&#21040;&#26368;&#20248;&#33410;&#28857;&#23884;&#20837;&#65292;&#20174;&#32780;&#32531;&#35299;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#22122;&#22768;&#27979;&#37327;&#25361;&#25112;&#23545;&#22270;&#25968;&#25454;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#22270;&#35745;&#31639;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#65292;&#25299;&#25169;&#25110;&#33410;&#28857;&#20449;&#24687;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#27979;&#37327;&#25361;&#25112;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#25928;&#26524;&#12290;&#35266;&#27979;&#20013;&#30340;&#36825;&#20123;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#30772;&#22351;&#22270;&#25968;&#25454;&#20013;&#30340;&#20851;&#38190;&#27169;&#24335;&#65292;&#26368;&#32456;&#23548;&#33268;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#33391;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22270;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;&#26469;&#23884;&#20837;&#33410;&#28857;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#24418;&#24335;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#26469;&#25214;&#21040;&#26368;&#20248;&#33410;&#28857;&#23884;&#20837;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#23398;&#20064;&#36807;&#31243;&#23548;&#33268;&#20102;&#25913;&#36827;&#30340;&#33410;&#28857;&#34920;&#31034;&#21644;&#26356;&#24378;&#22766;&#30340;&#22270;&#39044;&#27979;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#22270;&#25968;&#25454;&#20013;&#22122;&#22768;&#27979;&#37327;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have shown impressive capabilities in solving various graph learning tasks, particularly excelling in node classification. However, their effectiveness can be hindered by the challenges arising from the widespread existence of noisy measurements associated with the topological or nodal information present in real-world graphs. These inaccuracies in observations can corrupt the crucial patterns within the graph data, ultimately resulting in undesirable performance in practical applications. To address these issues, this paper proposes a novel uncertainty-aware graph learning framework motivated by distributionally robust optimization. Specifically, we use a graph neural network-based encoder to embed the node features and find the optimal node embeddings by minimizing the worst-case risk through a minimax formulation. Such an uncertainty-aware learning process leads to improved node representations and a more robust graph predictive model that effectively mitigates
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21452;&#37325;&#31574;&#30053;&#35299;&#20915;ARC&#20219;&#21153;&#65292;&#36890;&#36807;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;&#21644;&#24341;&#20837;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#26469;&#25552;&#39640;AI&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#25968;&#25454;&#21644;&#27169;&#22411;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#20026;&#26410;&#26469;AGI&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.08204</link><description>&lt;p&gt;
&#35299;&#35868;ARC&#65306;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer. (arXiv:2306.08204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21452;&#37325;&#31574;&#30053;&#35299;&#20915;ARC&#20219;&#21153;&#65292;&#36890;&#36807;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;&#21644;&#24341;&#20837;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#26469;&#25552;&#39640;AI&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#25968;&#25454;&#21644;&#27169;&#22411;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#20026;&#26410;&#26469;AGI&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#27861;&#26469;&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#25991;&#26412;&#38598;&#65288;ARC&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;Transformer&#22312;&#27169;&#20223;&#23398;&#20064;&#33539;&#24335;&#19979;&#24314;&#27169;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#65292;&#24182;&#24341;&#20837;&#20102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;Push and Pull&#32858;&#31867;&#26041;&#27861;&#12290;&#36825;&#31181;&#21452;&#37325;&#31574;&#30053;&#22686;&#24378;&#20102;AI&#30340;ARC&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24182;&#20026;AGI&#36827;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#39640;&#32423;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#12289;&#24378;&#22823;&#30340;&#22521;&#35757;&#25968;&#25454;&#38598;&#21644;&#31934;&#32454;&#30340;&#27169;&#22411;&#32467;&#26500;&#38656;&#27714;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;&#20915;&#31574;Transformer&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#24182;&#25512;&#21160;&#26410;&#26469;AGI&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of artificial general intelligence (AGI), we tackle Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged approach. We employ the Decision Transformer in an imitation learning paradigm to model human problem-solving, and introduce an object detection algorithm, the Push and Pull clustering method. This dual strategy enhances AI's ARC problem-solving skills and provides insights for AGI progression. Yet, our work reveals the need for advanced data collection tools, robust training datasets, and refined model structures. This study highlights potential improvements for Decision Transformers and propels future AGI research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08201</link><description>&lt;p&gt;
&#25351;&#25968;&#26063;&#22122;&#22768;&#19979;&#30340;&#22270;&#25289;&#26222;&#25289;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Laplacian Learning with Exponential Family Noise. (arXiv:2306.08201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08201
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#38754;&#20020;&#30340;&#24120;&#35265;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#24213;&#23618;&#22270;&#24448;&#24448;&#26159;&#26410;&#30693;&#30340;&#12290;&#34429;&#28982;&#20026;&#36830;&#32493;&#30340;&#22270;&#20449;&#21495;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22270;&#25512;&#26029;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914;&#31163;&#25955;&#35745;&#25968;&#65289;&#65292;&#25512;&#26029;&#20854;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#26694;&#26550;&#25512;&#24191;&#21040;&#25351;&#25968;&#26063;&#22122;&#22768;&#20998;&#24067;&#65292;&#20197;&#24314;&#27169;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#31639;&#27861;&#65292;&#20174;&#22122;&#22768;&#20449;&#21495;&#20013;&#20272;&#35745;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#26410;&#35266;&#27979;&#30340;&#24179;&#28369;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#22312;&#22122;&#22768;&#27169;&#22411;&#19981;&#21305;&#37197;&#24773;&#20917;&#19979;&#20248;&#20110;&#31454;&#20105;&#25289;&#26222;&#25289;&#26031;&#20272;&#35745;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common challenge in applying graph machine learning methods is that the underlying graph of a system is often unknown. Although different graph inference methods have been proposed for continuous graph signals, inferring the graph structure underlying other types of data, such as discrete counts, is under-explored. In this paper, we generalize a graph signal processing (GSP) framework for learning a graph from smooth graph signals to the exponential family noise distribution to model various data types. We propose an alternating algorithm that estimates the graph Laplacian as well as the unobserved smooth representation from the noisy signals. We demonstrate in synthetic and real-world data that our new algorithm outperforms competing Laplacian estimation methods under noise model mismatch.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;POP&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#19968;&#32452;&#20219;&#21153;&#25351;&#23450;&#30340;&#25552;&#31034;&#21644;&#19968;&#32452;&#20840;&#23616;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#38598;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08200</link><description>&lt;p&gt;
POP: Prompt Of Prompts for Continual Learning
&lt;/p&gt;
&lt;p&gt;
POP: Prompt Of Prompts for Continual Learning. (arXiv:2306.08200v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;POP&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#19968;&#32452;&#20219;&#21153;&#25351;&#23450;&#30340;&#25552;&#31034;&#21644;&#19968;&#32452;&#20840;&#23616;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#38598;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23427;&#26088;&#22312;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;CL&#26041;&#27861;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#31354;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25317;&#26377;&#20581;&#22766;&#29305;&#24449;&#34920;&#31034;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;CL&#38382;&#39064;&#30340;&#26377;&#36259;&#22522;&#30784;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36824;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#22823;&#37096;&#20998;&#30340;&#34920;&#31034;&#30340;&#19968;&#33324;&#24615;&#22522;&#26412;&#19981;&#21463;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#23398;&#20064;&#26082;&#20026;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#21448;&#20026;&#20840;&#23616;&#25552;&#31034;&#30340;&#27169;&#22411;&#65292;&#21363;&#25429;&#25417;&#36328;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Of Prompts&#65288;POP&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#19968;&#32452;&#20219;&#21153;&#25351;&#23450;&#30340;&#25552;&#31034;&#21644;&#19968;&#32452;&#20840;&#23616;&#25552;&#31034;&#65288;&#21363;POP&#65289;&#26469;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#20197;&#38598;&#25104;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has attracted increasing attention in the recent past. It aims to mimic the human ability to learn new concepts without catastrophic forgetting. While existing CL methods accomplish this to some extent, they are still prone to semantic drift of the learned feature space. Foundation models, which are endowed with a robust feature representation, learned from very large datasets, provide an interesting substrate for the solution of the CL problem. Recent work has also shown that they can be adapted to specific tasks by prompt tuning techniques that leave the generality of the representation mostly unscathed. An open question is, however, how to learn both prompts that are task specific and prompts that are global, i.e. capture cross-task information. In this work, we propose the Prompt Of Prompts (POP) model, which addresses this goal by progressively learning a group of task-specified prompts and a group of global prompts, denoted as POP, to integrate information
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; CGNN&#65292;&#23427;&#21033;&#29992;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23545;&#33410;&#28857;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#36807;&#28388;&#20986;&#22122;&#22768;&#33410;&#28857;&#21644;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08194</link><description>&lt;p&gt;
&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs under Label Noise. (arXiv:2306.08194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; CGNN&#65292;&#23427;&#21033;&#29992;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23545;&#33410;&#28857;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#36807;&#28388;&#20986;&#22122;&#22768;&#33410;&#28857;&#21644;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#20132;&#20998;&#26512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#33410;&#28857;&#30340;&#26631;&#31614;&#20449;&#24687;&#26159;&#20934;&#30830;&#30340;&#65292;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#22270;&#19978;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNN&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#36827;&#22686;&#24378;&#33410;&#28857;&#30340;&#20004;&#20010;&#35270;&#35282;&#20855;&#26377;&#19968;&#33268;&#30340;&#34920;&#31034;&#12290;&#30001;&#20110;&#36825;&#20010;&#27491;&#21017;&#21270;&#39033;&#19981;&#33021;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#23427;&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#22270;&#19978;&#26816;&#27979;&#22122;&#22768;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#27979;&#37327;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#37051;&#23621;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#35782;&#21035;&#22122;&#22768;&#33410;&#28857;&#12290;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CGNN&#21487;&#20197;&#26377;&#25928;&#22320;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#23545;&#33410;&#28857;&#20998;&#31867;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#27169;&#22411;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24402;&#32435;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#20219;&#21153;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08192</link><description>&lt;p&gt;
&#38024;&#23545;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#24402;&#32435;&#32447;&#24615;&#25506;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Linear Probing for Few-shot Node Classification. (arXiv:2306.08192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24402;&#32435;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#29616;&#26377;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#20219;&#21153;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#24050;&#25104;&#20026;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#26377;&#25928;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#36716;&#23548;&#35774;&#32622;&#19979;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#36716;&#23548;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#26356;&#24191;&#27867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#24402;&#32435;&#35774;&#32622;&#12290;&#36825;&#20010;&#30095;&#24573;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#24418;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#26694;&#26550;&#22312;&#24402;&#32435;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24402;&#32435;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#29305;&#21035;&#35774;&#35745;&#30340;&#31616;&#21333;&#20294;&#26377;&#31454;&#20105;&#21147;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#20803;&#23398;&#20064;&#33539;&#24335;&#22312;&#22270;&#24418;&#39046;&#22495;&#30340;&#24037;&#20316;&#26041;&#24335;&#25552;&#20379;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has emerged as a powerful training strategy for few-shot node classification, demonstrating its effectiveness in the transductive setting. However, the existing literature predominantly focuses on transductive few-shot node classification, neglecting the widely studied inductive setting in the broader few-shot learning community. This oversight limits our comprehensive understanding of the performance of meta-learning based methods on graph data. In this work, we conduct an empirical study to highlight the limitations of current frameworks in the inductive few-shot node classification setting. Additionally, we propose a simple yet competitive baseline approach specifically tailored for inductive few-shot node classification tasks. We hope our work can provide a new path forward to better understand how the meta-learning paradigm works in the graph domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23567;&#21488;&#38454;&#20449;&#21495;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.08191</link><description>&lt;p&gt;
&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Large-scale Spatial Problems with Convolutional Neural Networks. (arXiv:2306.08191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23567;&#21488;&#38454;&#20449;&#21495;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#24471;&#21040;&#20102;&#19981;&#26029;&#22686;&#24378;&#30340;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#20419;&#36827;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#25668;&#20837;&#25968;&#25454;&#37327;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#27491;&#22312;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#65292;&#22240;&#27492;&#65292;&#36716;&#21521;&#25552;&#39640;&#25928;&#29575;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#25552;&#39640;&#22823;&#35268;&#27169;&#31354;&#38388;&#38382;&#39064;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21487;&#20197;&#22312;&#23567;&#21488;&#38454;&#20449;&#21495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#35780;&#20272;&#20219;&#24847;&#22823;&#23567;&#20449;&#21495;&#26102;&#20960;&#20046;&#19981;&#20250;&#21457;&#29983;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#25552;&#20379;&#20102;&#25152;&#24471;&#21040;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;CNN&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#65292;&#36825;&#26159;&#36801;&#31227;&#23398;&#20064;&#20013;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#30340;&#19968;&#20010;&#23646;&#24615;&#12290;&#22312;&#22522;&#20110;&#31227;&#21160;&#22522;&#30784;&#26550;&#26500;&#30340;&#38656;&#27714;&#65288;MID&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#32467;&#26524;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;MID&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#30334;&#20010;&#20195;&#29702;&#65292;&#36825;&#22312;&#27492;&#21069;&#30340;&#24037;&#20316;&#20013;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning research has been accelerated by increasingly powerful hardware, which facilitated rapid growth in the model complexity and the amount of data ingested. This is becoming unsustainable and therefore refocusing on efficiency is necessary. In this paper, we employ transfer learning to improve training efficiency for large-scale spatial problems. We propose that a convolutional neural network (CNN) can be trained on small windows of signals, but evaluated on arbitrarily large signals with little to no performance degradation, and provide a theoretical bound on the resulting generalization error. Our proof leverages shift-equivariance of CNNs, a property that is underexploited in transfer learning. The theoretical results are experimentally supported in the context of mobile infrastructure on demand (MID). The proposed approach is able to tackle MID at large scales with hundreds of agents, which was computationally intractable prior to this work.
&lt;/p&gt;</description></item><item><title>Adobe&#30340;20,000&#22810;&#31181;&#23383;&#20307;&#24211;&#26159;&#20010;&#36873;&#25321;&#24656;&#24807;&#30151;&#24739;&#32773;&#30340;&#22121;&#26790;&#65292;&#26412;&#25991;&#36890;&#36807;&#21019;&#36896;&#19968;&#20010;&#22522;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#31995;&#32479;&#26469;&#33258;&#21160;&#32473;&#29992;&#25143;&#25552;&#20379;&#21512;&#36866;&#30340;&#23383;&#20307;&#25512;&#33616;&#65292;&#30446;&#21069;&#24050;&#34987;&#25968;&#30334;&#19975; Adobe Express &#29992;&#25143;&#20351;&#29992;&#65292;&#28857;&#20987;&#29575;&#39640;&#36798; 25% &#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2306.08188</link><description>&lt;p&gt;
&#22522;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#19978;&#19979;&#25991;&#23383;&#20307;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Contextual Font Recommendations based on User Intent. (arXiv:2306.08188v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08188
&lt;/p&gt;
&lt;p&gt;
Adobe&#30340;20,000&#22810;&#31181;&#23383;&#20307;&#24211;&#26159;&#20010;&#36873;&#25321;&#24656;&#24807;&#30151;&#24739;&#32773;&#30340;&#22121;&#26790;&#65292;&#26412;&#25991;&#36890;&#36807;&#21019;&#36896;&#19968;&#20010;&#22522;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#31995;&#32479;&#26469;&#33258;&#21160;&#32473;&#29992;&#25143;&#25552;&#20379;&#21512;&#36866;&#30340;&#23383;&#20307;&#25512;&#33616;&#65292;&#30446;&#21069;&#24050;&#34987;&#25968;&#30334;&#19975; Adobe Express &#29992;&#25143;&#20351;&#29992;&#65292;&#28857;&#20987;&#29575;&#39640;&#36798; 25% &#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adobe Fonts &#25317;&#26377;&#36229;&#36807; 20,000 &#31181;&#29420;&#29305;&#30340;&#23383;&#20307;&#24211;&#65292;&#29992;&#20110;&#21019;&#36896;&#22270;&#24418;&#12289;&#28023;&#25253;&#12289;&#22797;&#21512;&#29289;&#31561;&#12290;&#30001;&#20110;&#23383;&#20307;&#24211;&#30340;&#29305;&#27530;&#24615;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#23383;&#20307;&#26159;&#19968;&#39033;&#38656;&#35201;&#22823;&#37327;&#32463;&#39564;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#23545;&#20110;&#22823;&#22810;&#25968; Adobe &#20135;&#21697;&#29992;&#25143;&#65292;&#29305;&#21035;&#26159; Adobe Express &#30340;&#26222;&#36890;&#29992;&#25143;&#65292;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#36873;&#25321;&#40664;&#35748;&#23383;&#20307;&#32780;&#19981;&#26159;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#21487;&#29992;&#23383;&#20307;&#12290;&#26412;&#30740;&#31350;&#21019;&#36896;&#20102;&#19968;&#20010;&#22522;&#20110;&#29992;&#25143;&#24847;&#22270;&#30340;&#31995;&#32479;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19978;&#19979;&#25991;&#23383;&#20307;&#25512;&#33616;&#65292;&#20197;&#21327;&#21161;&#20182;&#20204;&#30340;&#21019;&#20316;&#20043;&#26053;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25509;&#21463;&#22810;&#35821;&#35328;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#25512;&#33616;&#36866;&#24403;&#30340;&#23383;&#20307;&#12290;&#26681;&#25454;&#29992;&#25143;&#30340;&#36164;&#26684;&#65292;&#20813;&#36153;&#21644;&#20184;&#36153;&#23383;&#20307;&#30340;&#28151;&#21512;&#27604;&#20363;&#23558;&#20250;&#20570;&#20986;&#30456;&#24212;&#30340;&#35843;&#25972;&#12290;&#35813;&#21151;&#33021;&#30446;&#21069;&#24050;&#34987;&#25968;&#30334;&#19975; Adobe Express &#29992;&#25143;&#20351;&#29992;&#65292;&#28857;&#20987;&#29575;&#39640;&#36798; 25% &#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adobe Fonts has a rich library of over 20,000 unique fonts that Adobe users utilize for creating graphics, posters, composites etc. Due to the nature of the large library, knowing what font to select can be a daunting task that requires a lot of experience. For most users in Adobe products, especially casual users of Adobe Express, this often means choosing the default font instead of utilizing the rich and diverse fonts available. In this work, we create an intent-driven system to provide contextual font recommendations to users to aid in their creative journey. Our system takes in multilingual text input and recommends suitable fonts based on the user's intent. Based on user entitlements, the mix of free and paid fonts is adjusted. The feature is currently used by millions of Adobe Express users with a CTR of &gt;25%.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08175</link><description>&lt;p&gt;
DCTX-Conformer&#65306;&#38024;&#23545;&#20302;&#24310;&#36831;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;Conformer&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#12290;&#35832;&#22914;&#21452;&#27169;&#24335;&#21644;&#21160;&#24577;&#20998;&#22359;&#35757;&#32451;&#31561;&#25216;&#26415;&#26377;&#21161;&#20110;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#23436;&#25972;&#21644;&#26377;&#38480;&#30340;&#36807;&#21435;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#24335;&#35782;&#21035;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#32479;&#19968;ASR&#31995;&#32479;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;&#21160;&#24577;&#19978;&#19979;&#25991;Conformer&#65288;DCTX-Conformer&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;&#38750;&#37325;&#21472;&#30340;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19968;&#22359;&#30340;&#24038;&#19978;&#19979;&#25991;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#30001;&#20110;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#30446;&#21069;&#26368;&#20248;&#35299;&#25552;&#21319;&#20102;25.0%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32780;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;CLIP&#27169;&#22411;&#65288;Dp-CLIP&#65289;&#65292;&#26088;&#22312;&#20445;&#25252;&#22810;&#27169;&#24577;AI&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#19982;&#26631;&#20934;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08173</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#20013;&#20445;&#25252;&#25968;&#25454;&#65306;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#29992;&#20110;CLIP&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Safeguarding Data in Multimodal AI: A Differentially Private Approach to CLIP Training. (arXiv:2306.08173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;CLIP&#27169;&#22411;&#65288;Dp-CLIP&#65289;&#65292;&#26088;&#22312;&#20445;&#25252;&#22810;&#27169;&#24577;AI&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#19982;&#26631;&#20934;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;CLIP&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32852;&#21512;&#35757;&#32451;&#24443;&#24213;&#25913;&#21464;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#20294;&#20854;&#21487;&#33021;&#26080;&#24847;&#20013;&#25259;&#38706;&#25935;&#24863;&#20449;&#24687;&#30340;&#28508;&#21147;&#38656;&#35201;&#38598;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#25913;&#36827;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Dp-CLIP&#22312;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#22810;&#26679;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#25345;&#20102;&#19982;&#26631;&#20934;&#30340;&#38750;&#31169;&#26377;CLIP&#27169;&#22411;&#21516;&#31561;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32447;&#24615;&#34920;&#31034;&#35774;&#32622;&#19979;&#20998;&#26512;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26799;&#24230;&#34987;&#21098;&#36753;&#26102;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks. While CLIP has revolutionized multimodal learning through joint training on images and text, its potential to unintentionally disclose sensitive information necessitates the integration of privacy-preserving mechanisms. We introduce a differentially private adaptation of the Contrastive Language-Image Pretraining (CLIP) model that effectively addresses privacy concerns while retaining accuracy. Our proposed method, Dp-CLIP, is rigorously evaluated on benchmark datasets encompassing diverse vision-and-language tasks such as image classification and visual question answering. We demonstrate that our approach retains performance on par with the standard non-private CLIP model. Furthermore, we analyze our proposed algorithm under linear representation settings. We derive the convergence rate of our algorithm and show a trade-off between utility and privacy when gradients are clipped pe
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#20173;&#28982;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#26159;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#19981;&#26159;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#12290;</title><link>http://arxiv.org/abs/2306.08167</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#24615;&#33021;&#20026;&#20160;&#20040;&#20250;&#19979;&#38477;&#65311;&#23545;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#30340;&#20154;&#24037;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Where Does My Model Underperform? A Human Evaluation of Slice Discovery Algorithms. (arXiv:2306.08167v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08167
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#20173;&#28982;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#20294;&#26159;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#19981;&#26159;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#20041;&#36830;&#36143;&#30340;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#8220;&#29255;&#27573;&#8221;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#39640;&#24179;&#22343;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#20173;&#28982;&#20250;&#20986;&#29616;&#36825;&#31181;&#38382;&#39064;&#12290;&#36825;&#31181;&#34892;&#20026;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#20559;&#35265;&#22312;&#37096;&#32626;&#20013;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#30830;&#23450;&#36825;&#20123;&#24615;&#33021;&#19979;&#38477;&#30340;&#29255;&#27573;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#19994;&#32773;&#32570;&#20047;&#35775;&#38382;&#32676;&#32452;&#27880;&#37322;&#20197;&#23450;&#20041;&#20854;&#25968;&#25454;&#30340;&#36830;&#36143;&#23376;&#38598;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#39537;&#21160;&#65292;ML&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#26032;&#30340;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#30340;&#36830;&#36143;&#21644;&#39640;&#35823;&#24046;&#23376;&#38598;&#20998;&#32452;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#26159;&#21542;&#24110;&#21161;&#20154;&#31867;&#27491;&#30830;&#24418;&#25104;&#20182;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#20551;&#35774;&#36824;&#24456;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21463;&#25511;&#29992;&#25143;&#30740;&#31350;&#65288;N = 15&#65289;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#36755;&#20986;&#30340;40&#20010;&#29255;&#27573;&#65292;&#24182;&#35201;&#27714;&#20182;&#20204;&#24418;&#25104;&#26377;&#20851;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#21709;&#24212;&#21464;&#37327;&#26159;&#21442;&#19982;&#32773;&#27491;&#30830;&#25353;&#38169;&#35823;&#29575;&#23545;&#29255;&#27573;&#36827;&#34892;&#25490;&#21517;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#65288;1&#65289;&#20004;&#31181;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#37117;&#19981;&#20250;&#35753;&#21442;&#19982;&#32773;&#22312;&#20551;&#35774;&#19978;&#34920;&#29616;&#20986;&#31995;&#32479;&#24615;&#30340;&#20248;&#21183;&#65307;&#65288;2&#65289;&#21363;&#20351;&#22312;&#21516;&#19968;&#31181;&#29255;&#27573;&#21457;&#29616;&#31639;&#27861;&#20013;&#65292;&#21442;&#19982;&#32773;&#22312;&#27491;&#30830;&#23545;&#29255;&#27573;&#36827;&#34892;&#25490;&#24207;&#30340;&#33021;&#21147;&#19978;&#20063;&#23384;&#22312;&#26174;&#30528;&#21464;&#24322;&#65307;&#65288;3&#65289;&#23545;&#35937;&#31867;&#21035;&#30340;&#38169;&#35823;&#29575;&#27604;&#23545;&#35937;&#22823;&#23567;&#25110;&#20301;&#32622;&#31561;&#38544;&#21547;&#35821;&#20041;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#29255;&#27573;&#38590;&#24230;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#29983;&#25104;&#30340;&#29255;&#27573;&#24182;&#38750;&#30830;&#23450;&#20154;&#24037;&#20174;&#19994;&#32773;&#38382;&#39064;&#24615;&#29255;&#27573;&#30340;&#38134;&#24377;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#24517;&#39035;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models that achieve high average accuracy can still underperform on semantically coherent subsets (i.e. "slices") of data. This behavior can have significant societal consequences for the safety or bias of the model in deployment, but identifying these underperforming slices can be difficult in practice, especially in domains where practitioners lack access to group annotations to define coherent subsets of their data. Motivated by these challenges, ML researchers have developed new slice discovery algorithms that aim to group together coherent and high-error subsets of data. However, there has been little evaluation focused on whether these tools help humans form correct hypotheses about where (for which groups) their model underperforms. We conduct a controlled user study (N = 15) where we show 40 slices output by two state-of-the-art slice discovery algorithms to users, and ask them to form hypotheses about where an object detection model underperforms. Our res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#38142;&#25509;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#23454;&#29616;&#20102;&#26032;&#22411;&#36830;&#25509;&#21058;&#30340;&#21046;&#22791;&#12290;</title><link>http://arxiv.org/abs/2306.08166</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#24555;&#36895;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28857;&#20113;&#23545;&#40784;&#30340;&#36830;&#25509;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment. (arXiv:2306.08166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#38142;&#25509;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#23454;&#29616;&#20102;&#26032;&#22411;&#36830;&#25509;&#21058;&#30340;&#21046;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Proteolysis-Targeting Chimeras(PROTACs)&#26159;&#19968;&#31867;&#26032;&#22411;&#23567;&#20998;&#23376;&#65292;&#35774;&#35745;&#25104;&#26725;&#26753;&#65292;&#23558;E3&#36830;&#25509;&#37238;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#34507;&#30333;&#36136;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#20419;&#36827;&#20854;&#21518;&#32493;&#38477;&#35299;&#12290; PROTACs&#30001;&#20004;&#20010;&#34507;&#30333;&#36136;&#32467;&#21512;&#30340;&#8220;&#27963;&#24615;&#8221;&#21306;&#22495;&#32452;&#25104;&#65292;&#30001;&#8220;&#36830;&#25509;&#8221;&#21306;&#22495;&#36830;&#25509;&#12290; &#36830;&#25509;&#21306;&#22495;&#30340;&#35774;&#35745;&#30001;&#20854;&#30456;&#20114;&#20316;&#29992;&#32473;&#20986;&#20960;&#20309;&#21644;&#21270;&#23398;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#38656;&#35201;&#26368;&#22823;&#21270;&#33647;&#29289;&#26679;&#26412;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ShapeLinker&#65292;&#19968;&#31181;&#29992;&#20110;&#20840;&#26032;&#35774;&#35745;&#38142;&#25509;&#29289;&#30340;&#26041;&#27861;&#12290; &#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#22238;&#24402;SMILES&#29983;&#25104;&#22120;&#19978;&#36827;&#34892;&#29255;&#27573;&#36830;&#25509;&#12290; &#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#30456;&#20851;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28857;&#20113;&#23545;&#40784;&#24471;&#20998;&#26469;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290; &#36825;&#31181;&#26032;&#26041;&#27861;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;&#30456;&#20851;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#24182;&#22312;&#29983;&#25104;&#20551;&#23450;&#30446;&#26631;&#36830;&#25509;&#21058;&#26500;&#36896;&#30340;&#26032;&#22411;&#36830;&#25509;&#21058;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small molecules which are designed to act as a bridge between an E3 ligase and a disease-relevant protein, thereby promoting its subsequent degradation. PROTACs are composed of two protein binding "active" domains, linked by a "linker" domain. The design of the linker domain is challenging due to geometric and chemical constraints given by its interactions, and the need to maximize drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method for de novo design of linkers. It performs fragment-linking using reinforcement learning on an autoregressive SMILES generator. The method optimizes for a composite score combining relevant physicochemical properties and a novel, attention-based point cloud alignment score. This new method successfully generates linkers that satisfy both relevant 2D and 3D requirements, and achieves state-of-the-art results in producing novel linkers assuming a target linker confor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.08162</link><description>&lt;p&gt;
INT2.1&#65306;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#23454;&#29616;&#31934;&#32454;&#21487;&#35843;&#30340;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26497;&#20854;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65288;EMEF&#65289;&#65292;&#24182;&#26681;&#25454;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#38169;&#35823;&#20462;&#27491;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37327;&#21270;&#36807;&#31243;&#20013;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20869;&#23384;&#35201;&#27714;&#38477;&#20302;&#22810;&#36798;5.6&#20493;&#65292;&#20174;&#32780;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#32416;&#38169;&#65288;LREC&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22686;&#21152;&#30340;LoRA&#23618;&#26469;&#25913;&#21892;&#37327;&#21270;&#27169;&#22411;&#19982;&#20854;&#28014;&#28857;&#25968;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32416;&#38169;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;INT2&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#36798;&#21040;&#36825;&#31181;&#24615;&#33021;&#30340;INT2&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.08157</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#22240;&#26524;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#37329;&#34701;&#21644;&#25237;&#36164;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#29420;&#29305;&#30340;&#21306;&#22359;&#38142;&#30456;&#20851;&#29305;&#24615;&#65292;&#22914;&#38544;&#31169;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#19981;&#21487;&#36861;&#36394;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#21463;&#27426;&#36814;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#27874;&#21160;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21152;&#23494;&#36135;&#24065;&#20173;&#28982;&#26159;&#19968;&#31181;&#39640;&#39118;&#38505;&#25237;&#36164;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;DBN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20803;&#35774;&#32622;&#19979;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#65292;&#20197;&#39044;&#27979;&#20116;&#31181;&#27969;&#34892;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#26684;&#36816;&#21160;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;&#30340;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.08153</link><description>&lt;p&gt;
&#65288;&#25193;&#22823;&#65289;&#24102;&#29366;&#30697;&#38453;&#20998;&#35299;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#38544;&#31169;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Amplified) Banded Matrix Factorization: A unified approach to private training. (arXiv:2306.08153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;&#30340;&#30697;&#38453;&#20998;&#35299;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#19979;&#30340;&#30697;&#38453;&#20998;&#35299;&#65288;MF&#65289;&#26426;&#21046;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#31169;-&#25928;&#29992;-&#35745;&#31639;&#25240;&#34935;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20294;&#26159;&#22312;&#20998;&#25955;&#21644;&#32852;&#21512;&#35774;&#32622;&#20013;&#65292;&#20173;&#23384;&#22312;MF&#19981;&#26131;&#36866;&#29992;&#30340;&#23454;&#20363;&#65292;&#25110;&#32773;&#20854;&#20182;&#31639;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#25240;&#34935;&#65288;&#36890;&#24120;&#38543;&#30528; &#949; &#21464;&#23567;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24102;&#29366;&#30697;&#38453;&#26500;&#24314;MF&#26426;&#21046;&#65292;&#22312;&#25152;&#26377;&#38544;&#31169;&#39044;&#31639;&#20013;&#23558;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#32435;&#20837;&#20998;&#25955;&#21644;&#32852;&#21512;&#35757;&#32451;&#35774;&#32622;&#20013;&#12290;&#20851;&#38190;&#25216;&#26415;&#26159;&#24102;&#29366;&#30697;&#38453;&#30340;&#26500;&#36896;&#12290;&#23545;&#20110;&#36328;&#35774;&#22791;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#35774;&#22791;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#25918;&#26494;&#30340;&#35774;&#22791;&#21442;&#19982;&#27169;&#24335;&#65292;&#19982;&#23454;&#38469;&#30340;FL&#22522;&#30784;&#35774;&#26045;&#30456;&#23481;&#65288;&#22914;&#20135;&#21697;&#37096;&#32626;&#25152;&#31034;&#65289;&#12290;&#22312;&#38598;&#20013;&#24335;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#24102;&#29366;&#30697;&#38453;&#20855;&#26377;&#19982; ubiquitous DP-SGD algorithm &#30456;&#21516;&#30340;&#38544;&#31169;&#25918;&#22823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix factorization (MF) mechanisms for differential privacy (DP) have substantially improved the state-of-the-art in privacy-utility-computation tradeoffs for ML applications in a variety of scenarios, but in both the centralized and federated settings there remain instances where either MF cannot be easily applied, or other algorithms provide better tradeoffs (typically, as $\epsilon$ becomes small).  In this work, we show how MF can subsume prior state-of-the-art algorithms in both federated and centralized training settings, across all privacy budgets. The key technique throughout is the construction of MF mechanisms with banded matrices. For cross-device federated learning (FL), this enables multiple-participations with a relaxed device participation schema compatible with practical FL infrastructure (as demonstrated by a production deployment). In the centralized setting, we prove that banded matrices enjoy the same privacy amplification results as for the ubiquitous DP-SGD algo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21151;&#29575;&#21378;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20648;&#33021;&#19982;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#32452;&#21512;&#26368;&#22823;&#21270;&#33021;&#28304;&#24066;&#22330;&#30340;&#25910;&#20837;&#65292;&#26368;&#23567;&#21270;&#20648;&#33021;&#25439;&#32791;&#25104;&#26412;&#20197;&#21450;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21066;&#20943;&#12290;&#36890;&#36807;&#37096;&#20214;&#32423;&#27169;&#25311;&#22120;&#22788;&#29702;&#20102;&#20648;&#33021;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#32806;&#21512;&#12289;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#38750;&#32447;&#24615;&#23384;&#20648;&#27169;&#22411;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;RL&#26041;&#27861;&#22788;&#29702;&#22797;&#26434;&#20648;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#21160;&#20316;&#19982;&#31995;&#32479;&#32422;&#26463;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.08147</link><description>&lt;p&gt;
&#22810;&#24066;&#22330;&#33021;&#28304;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;
&lt;/p&gt;
&lt;p&gt;
Multi-market Energy Optimization with Renewables via Reinforcement Learning. (arXiv:2306.08147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21151;&#29575;&#21378;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20648;&#33021;&#19982;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#32452;&#21512;&#26368;&#22823;&#21270;&#33021;&#28304;&#24066;&#22330;&#30340;&#25910;&#20837;&#65292;&#26368;&#23567;&#21270;&#20648;&#33021;&#25439;&#32791;&#25104;&#26412;&#20197;&#21450;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21066;&#20943;&#12290;&#36890;&#36807;&#37096;&#20214;&#32423;&#27169;&#25311;&#22120;&#22788;&#29702;&#20102;&#20648;&#33021;&#36807;&#31243;&#20013;&#30340;&#26102;&#38388;&#32806;&#21512;&#12289;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#38750;&#32447;&#24615;&#23384;&#20648;&#27169;&#22411;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;RL&#26041;&#27861;&#22788;&#29702;&#22797;&#26434;&#20648;&#33021;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#21160;&#20316;&#19982;&#31995;&#32479;&#32422;&#26463;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#30340;&#21151;&#29575;&#21378;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20648;&#33021;&#19982;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#32452;&#21512;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#33021;&#28304;&#24066;&#22330;&#30340;&#25910;&#20837;&#12289;&#26368;&#23567;&#21270;&#20648;&#33021;&#25439;&#32791;&#25104;&#26412;&#20197;&#21450;&#21487;&#20877;&#29983;&#33021;&#28304;&#25968;&#37327;&#30340;&#21066;&#20943;&#12290;&#35813;&#26694;&#26550;&#22788;&#29702;&#20102;&#20648;&#33021;&#35774;&#22791;&#30340;&#26102;&#38388;&#32806;&#21512;&#22797;&#26434;&#24615;&#12289;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#37327;&#21644;&#33021;&#28304;&#20215;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#38750;&#32447;&#24615;&#20648;&#33021;&#27169;&#22411;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#23558;&#38382;&#39064;&#35270;&#20026;&#20998;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#37096;&#20214;&#32423;&#27169;&#25311;&#22120;&#26469;&#22788;&#29702;&#20648;&#33021;&#12290;&#23427;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#22797;&#26434;&#30340;&#20648;&#33021;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#38656;&#35201;&#20984;&#24615;&#21644;&#21487;&#24494;&#20998;&#30340;&#37096;&#20214;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#30340;&#37325;&#35201;&#29305;&#28857;&#26159;&#30830;&#20445;&#31574;&#30053;&#21160;&#20316;&#23562;&#37325;&#31995;&#32479;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#38750;&#27861;&#21160;&#20316;&#25237;&#24433;&#21040;&#23433;&#20840;&#29366;&#24577;&#21160;&#20316;&#38598;&#19978;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#36341;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a deep reinforcement learning (RL) framework for optimizing the operations of power plants pairing renewable energy with storage. The objective is to maximize revenue from energy markets while minimizing storage degradation costs and renewable curtailment. The framework handles complexities such as time coupling by storage devices, uncertainty in renewable generation and energy prices, and non-linear storage models. The study treats the problem as a hierarchical Markov Decision Process (MDP) and uses component-level simulators for storage. It utilizes RL to incorporate complex storage models, overcoming restrictions of optimization-based methods that require convex and differentiable component models. A significant aspect of this approach is ensuring policy actions respect system constraints, achieved via a novel method of projecting potentially infeasible actions onto a safe state-action set. The paper demonstrates the efficacy of this approach through extensive 
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#19988;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#20063;&#26174;&#31034;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08128</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#28145;&#24230;&#39640;&#20809;&#35889;&#20462;&#22797;&#31639;&#27861;&#32771;&#34385;&#31232;&#30095;&#24615;&#21644;&#20302;&#31209;&#24615;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Deep Hyperspectral Inpainting with the Sparsity and Low-Rank Considerations. (arXiv:2306.08128v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#19988;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#20063;&#26174;&#31034;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#36890;&#24120;&#30001;&#25968;&#30334;&#20010;&#31364;&#19988;&#36830;&#32493;&#30340;&#20809;&#35889;&#27874;&#27573;&#32452;&#25104;&#65292;&#27599;&#20010;&#27874;&#27573;&#21253;&#21547;&#20102;&#34987;&#25104;&#20687;&#22330;&#26223;&#30340;&#29289;&#36136;&#32452;&#25104;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#22122;&#22768;&#12289;&#22833;&#30495;&#25110;&#25968;&#25454;&#20002;&#22833;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20854;&#36136;&#37327;&#21644;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;&#65306;&#20302;&#31209;&#21644;&#31232;&#30095;&#32422;&#26463;&#8220;&#25554;&#20837;-&#25773;&#25918;&#8221; (LRS-PnP) &#20197;&#21450;&#20854;&#25193;&#23637;&#31639;&#27861; LRS-PnP-DIP&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#20173;&#26080;&#38656;&#22806;&#37096;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#26576;&#20123;&#28201;&#21644;&#20551;&#35774;&#19979;&#36827;&#34892;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#20445;&#35777;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#26377;&#21161;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#20135;&#29983;&#22806;&#35266;&#21644;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20462;&#22797;&#32467;&#26524;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral images are typically composed of hundreds of narrow and contiguous spectral bands, each containing information about the material composition of the imaged scene. However, these images can be affected by various sources of noise, distortions, or data losses, which can significantly degrade their quality and usefulness. To address these problems, we introduce two novel self-supervised Hyperspectral Images (HSI) inpainting algorithms: Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP), and its extension LRS-PnP-DIP, which features the strong learning capability, but is still free of external training data. We conduct the stability analysis under some mild assumptions which guarantees the algorithm to converge. It is specifically very helpful for the practical applications. Extensive experiments demonstrate that the proposed solution is able to produce visually and qualitatively superior inpainting results, achieving state-of-the-art performance. The code for reproduci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#35757;&#32451;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#21487;&#34987;&#35777;&#26126;&#20026;&#21487;&#21387;&#32553;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.08125</link><description>&lt;p&gt;
&#24102;&#26377;&#27785;&#37325;&#23614;&#37096;SGD&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#35757;&#32451;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#21487;&#34987;&#35777;&#26126;&#20026;&#21487;&#21387;&#32553;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#21644;&#21387;&#32553;&#19982;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30740;&#31350;&#23545;&#35937;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21487;&#20197;&#24433;&#21709;&#23398;&#20064;&#21442;&#25968;&#21521;&#37327;&#30340;&#21387;&#32553;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#35757;&#32451;&#21160;&#24577;&#23545;&#21387;&#32553;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#19981;&#21487;&#39564;&#35777;&#30340;&#20551;&#35774;&#65292;&#30001;&#20110;&#38544;&#21547;&#24615;&#36136;&#65292;&#24471;&#20986;&#30340;&#29702;&#35770;&#24182;&#27809;&#26377;&#25552;&#20379;&#23454;&#29992;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#31639;&#27861;&#30340;&#36755;&#20986;&#33021;&#22815;&#34987;&#35777;&#26126;&#26159;&#21487;&#21387;&#32553;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20351;&#29992;SGD&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#27880;&#20837;&#38468;&#21152;&#30340;&#27785;&#37325;&#23614;&#37096;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08122</link><description>&lt;p&gt;
&#20174;&#21477;&#23376;&#21040;&#25991;&#26723;&#23618;&#38754;&#30340;AI&#29983;&#25104;&#25220;&#34989;&#26816;&#27979;&#65306;&#36229;&#36234;&#40657;&#21283;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#20381;&#36182;&#23548;&#33268;&#20102;&#25220;&#34989;&#29616;&#35937;&#30340;&#22686;&#21152;&#12290;&#29616;&#26377;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#26377;&#38480;&#65292;&#24448;&#24448;&#20135;&#29983;&#35823;&#25253;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#25552;&#20379;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#21487;&#37327;&#21270;&#25351;&#26631;&#65292;&#26041;&#20415;&#20154;&#31867;&#35780;&#20272;&#32773;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#32473;&#23450;&#38382;&#39064;&#30340;&#22810;&#20010;&#37322;&#20041;&#29256;&#26412;&#65292;&#23558;&#23427;&#20204;&#36755;&#20837;LLM&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#23398;&#29983;&#22238;&#31572;&#20013;&#30340;&#21477;&#23376;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20154;&#31867;&#21644;AI&#25991;&#26412;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;94&#65285;&#65292;&#20026;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#25220;&#34989;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#20943;&#23569;&#20102;&#26032;&#27169;&#22411;&#35757;&#32451;&#25110;&#37325;&#26032;&#37197;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#40657;&#21283;&#23376;&#26041;&#27861;&#26356;&#36879;&#26126;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.08121</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;ID&#36827;&#34892;&#26356;&#22909;&#30340;&#27867;&#21270;&#65306;&#25512;&#33616;&#25490;&#21517;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Better Generalization with Semantic IDs: A case study in Ranking for Recommendations. (arXiv:2306.08121v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#20041;ID&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#36825;&#20123;ID&#26159;&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#28040;&#38500;ID&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35821;&#20041;ID&#33021;&#26356;&#22909;&#22320;&#25552;&#39640;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#22909;&#30340;&#29289;&#21697;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36890;&#24120;&#65292;&#19968;&#39033;&#21830;&#21697;&#20250;&#34987;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38543;&#26426;&#29983;&#25104;&#30340;ID&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#36890;&#36807;&#23398;&#20064;&#19982;&#38543;&#26426;ID&#20540;&#30456;&#23545;&#24212;&#30340;&#23884;&#20837;&#26469;&#34920;&#31034;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#22312;&#29289;&#21697;&#25968;&#37327;&#22823;&#19988;&#29289;&#21697;&#26381;&#20174;&#24130;&#24459;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#8212;&#8212;&#36825;&#26159;&#30495;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#30340;&#20856;&#22411;&#29305;&#24449;&#8212;&#8212;&#20250;&#26377;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#36825;&#20250;&#23548;&#33268;&#29289;&#21697;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#27169;&#22411;&#26080;&#27861;&#23545;&#23614;&#37096;&#21644;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#29289;&#21697;&#36827;&#34892;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;&#23436;&#20840;&#28040;&#38500;&#36825;&#20123;ID&#29305;&#24449;&#21450;&#20854;&#23398;&#20064;&#30340;&#23884;&#20837;&#20197;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#20250;&#20005;&#37325;&#38477;&#20302;&#25512;&#33616;&#36136;&#37327;&#12290;&#22522;&#20110;&#20869;&#23481;&#30340;&#29289;&#21697;&#23884;&#20837;&#26356;&#20026;&#21487;&#38752;&#65292;&#20294;&#23545;&#20110;&#29992;&#25143;&#36807;&#21435;&#30340;&#29289;&#21697;&#20132;&#20114;&#24207;&#21015;&#26469;&#35828;&#65292;&#23427;&#20204;&#25104;&#26412;&#39640;&#19988;&#20351;&#29992;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#20041;ID&#26469;&#34920;&#31034;&#31163;&#25955;&#30340;&#29289;&#21697;&#65292;&#36825;&#20123;ID&#26159;&#36890;&#36807;&#20351;&#29992;RQ-VAE&#20174;&#20869;&#23481;&#23884;&#20837;&#20013;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#25429;&#25417;&#27010;&#24565;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training good representations for items is critical in recommender models. Typically, an item is assigned a unique randomly generated ID, and is commonly represented by learning an embedding corresponding to the value of the random ID. Although widely used, this approach have limitations when the number of items are large and items are power-law distributed -- typical characteristics of real-world recommendation systems. This leads to the item cold-start problem, where the model is unable to make reliable inferences for tail and previously unseen items. Removing these ID features and their learned embeddings altogether to combat cold-start issue severely degrades the recommendation quality. Content-based item embeddings are more reliable, but they are expensive to store and use, particularly for users' past item interaction sequence. In this paper, we use Semantic IDs, a compact discrete item representations learned from content embeddings using RQ-VAE that captures hierarchy of concep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#23494;&#30721;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08116</link><description>&lt;p&gt;
CipherSniffer: &#20998;&#31867;&#23494;&#30721;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
CipherSniffer: Classifying Cipher Types. (arXiv:2306.08116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#35299;&#20915;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#23494;&#30721;&#25968;&#25454;&#38598;&#65292;&#26368;&#32456;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#30721;&#26159;&#21152;&#23494;&#36890;&#20449;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#26377;&#24456;&#22810;&#19981;&#21516;&#30340;&#23494;&#30721;&#31867;&#22411;&#65292;&#36825;&#20351;&#24471;&#20351;&#29992;&#26292;&#21147;&#30772;&#35299;&#26469;&#35299;&#23494;&#23494;&#30721;&#30340;&#35745;&#31639;&#36153;&#29992;&#26114;&#36149;&#12290;&#26412;&#25991;&#23558;&#35299;&#23494;&#20219;&#21153;&#20316;&#20026;&#20998;&#31867;&#38382;&#39064;&#26469;&#26694;&#26550;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#32622;&#25442;&#12289;&#26367;&#25442;&#12289;&#25991;&#26412;&#21453;&#36716;&#12289;&#21333;&#35789;&#21453;&#36716;&#12289;&#21477;&#23376;&#31227;&#20301;&#21644;&#26410;&#21152;&#23494;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;Tokenizer-Model&#32452;&#21512;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ciphers are a powerful tool for encrypting communication. There are many different cipher types, which makes it computationally expensive to solve a cipher using brute force. In this paper, we frame the decryption task as a classification problem. We first create a dataset of transpositions, substitutions, text reversals, word reversals, sentence shifts, and unencrypted text. Then, we evaluate the performance of various tokenizer-model combinations on this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;Nesterov&#21160;&#37327;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2306.08109</link><description>&lt;p&gt;
&#38024;&#23545;&#37096;&#20998;&#24378;&#20984;&#24615;&#65292;&#22312;Nesterov&#21160;&#37327;&#27861;&#19979;&#21152;&#36895;&#25910;&#25947;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity. (arXiv:2306.08109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;Nesterov&#21160;&#37327;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#20998;&#26512;&#32858;&#28966;&#20110;&#34920;&#24449;&#25439;&#22833;&#20989;&#25968;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;Polyak-Lojaciewicz&#65288;PL&#65289;&#26465;&#20214;&#21644;&#21463;&#38480;&#24378;&#20984;&#24615;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#26799;&#24230;&#19979;&#38477;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20294;&#26159;Nesterov&#21160;&#37327;&#27861;&#26159;&#21542;&#22312;&#31867;&#20284;&#30340;&#26465;&#20214;&#21644;&#20551;&#35774;&#19979;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21482;&#26377;&#37096;&#20998;&#21442;&#25968;&#28385;&#36275;&#24378;&#20984;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;Nesterov&#21160;&#37327;&#27861;&#22312;&#36825;&#31181;&#30446;&#26631;&#20989;&#25968;&#19979;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#23454;&#29616;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#28145;&#24230;ReLU&#32593;&#32476;&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#35777;&#26126;&#38750;&#24179;&#20961;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#29575;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art analyses on the convergence of gradient descent for training neural networks focus on characterizing properties of the loss landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity. While gradient descent converges linearly under such conditions, it remains an open question whether Nesterov's momentum enjoys accelerated convergence under similar settings and assumptions. In this work, we consider a new class of objective functions, where only a subset of the parameters satisfies strong convexity, and show Nesterov's momentum achieves acceleration in theory for this objective class. We provide two realizations of the problem class, one of which is deep ReLU networks, which --to the best of our knowledge--constitutes this work the first that proves accelerated convergence rate for non-trivial neural network architectures.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#37329;&#25345;&#20179;&#30340;&#32593;&#32476;&#20998;&#26512;&#26469;&#35745;&#31639;&#32929;&#31080;&#30340;&#25317;&#25380;&#20998;&#25968;&#65292;&#26500;&#24314;&#20986;&#30340;&#22810;&#22836;&#22836;&#23544;&#21644;&#31354;&#22836;&#22836;&#23544;&#30340;&#23545;&#20914;&#32452;&#21512;&#20855;&#26377;&#36127;&#30456;&#20851;&#21644;&#27491;&#24066;&#22330;&#22238;&#25253;&#19979;&#20984;&#24615;&#30340;&#29305;&#24615;&#65292;&#33021;&#25552;&#20379;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26399;&#26435;&#31574;&#30053;&#25110;&#22797;&#26434;&#30340;&#25968;&#20540;&#20248;&#21270;&#30340;&#23545;&#20914;&#25237;&#36164;&#32452;&#21512;&#65292;&#29992;&#20110;&#23545;&#20914;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#65292;&#21253;&#25324;&#23614;&#37096;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08105</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#24066;&#22330;&#39118;&#38505;&#23545;&#20914;&#19982;&#25317;&#25380;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Model-Free Market Risk Hedging Using Crowding Networks. (arXiv:2306.08105v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#37329;&#25345;&#20179;&#30340;&#32593;&#32476;&#20998;&#26512;&#26469;&#35745;&#31639;&#32929;&#31080;&#30340;&#25317;&#25380;&#20998;&#25968;&#65292;&#26500;&#24314;&#20986;&#30340;&#22810;&#22836;&#22836;&#23544;&#21644;&#31354;&#22836;&#22836;&#23544;&#30340;&#23545;&#20914;&#32452;&#21512;&#20855;&#26377;&#36127;&#30456;&#20851;&#21644;&#27491;&#24066;&#22330;&#22238;&#25253;&#19979;&#20984;&#24615;&#30340;&#29305;&#24615;&#65292;&#33021;&#25552;&#20379;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26399;&#26435;&#31574;&#30053;&#25110;&#22797;&#26434;&#30340;&#25968;&#20540;&#20248;&#21270;&#30340;&#23545;&#20914;&#25237;&#36164;&#32452;&#21512;&#65292;&#29992;&#20110;&#23545;&#20914;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#65292;&#21253;&#25324;&#23614;&#37096;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#25380;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35774;&#35745;&#25237;&#36164;&#32452;&#21512;&#31574;&#30053;&#20013;&#26368;&#37325;&#35201;&#30340;&#39118;&#38505;&#22240;&#32032;&#20043;&#19968;&#12290;&#26412;&#25991;&#20351;&#29992;&#22522;&#37329;&#25345;&#20179;&#30340;&#32593;&#32476;&#20998;&#26512;&#26469;&#35745;&#31639;&#32929;&#31080;&#30340;&#25317;&#25380;&#20998;&#25968;&#65292;&#26500;&#24314;&#20986;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#25968;&#20540;&#20248;&#21270;&#30340;&#25104;&#26412;&#19981;&#21464;&#30340;&#22810;&#22836;&#22836;&#23544;&#21644;&#31354;&#22836;&#22836;&#23544;&#30340;&#23545;&#20914;&#32452;&#21512;&#65292;&#20197;&#27169;&#22411;&#33258;&#30001;&#30340;&#26041;&#24335;&#20026;&#24066;&#22330;&#39118;&#38505;&#25552;&#20379;&#20445;&#25252;&#26426;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#36127;&#30456;&#20851;&#21644;&#27491;&#24066;&#22330;&#22238;&#25253;&#19979;&#30340;&#20984;&#24615;&#29305;&#24615;&#65292;&#33021;&#22815;&#22312;&#23567;&#24133;&#21644;&#22823;&#24133;&#24066;&#22330;&#20215;&#26684;&#27874;&#21160;&#26102;&#25552;&#20379;&#20445;&#25252;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#22810;&#22836;&#22836;&#23544;&#21644;&#31354;&#22836;&#22836;&#23544;&#23545;&#20914;&#32452;&#21512;&#28155;&#21152;&#21040;&#22522;&#30784;&#32452;&#21512;&#20013;&#65292;&#20363;&#22914;&#20256;&#32479;&#30340;60/40&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26399;&#26435;&#31574;&#30053;&#25110;&#22797;&#26434;&#30340;&#25968;&#20540;&#20248;&#21270;&#30340;&#23545;&#20914;&#32452;&#21512;&#65292;&#26469;&#23545;&#20914;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#65292;&#21253;&#25324;&#23614;&#37096;&#39118;&#38505;&#12290;&#27492;&#31867;&#23545;&#20914;&#30340;&#24635;&#25104;&#26412;&#30456;&#24403;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowding is widely regarded as one of the most important risk factors in designing portfolio strategies. In this paper, we analyze stock crowding using network analysis of fund holdings, which is used to compute crowding scores for stocks. These scores are used to construct costless long-short portfolios, computed in a distribution-free (model-free) way and without using any numerical optimization, with desirable properties of hedge portfolios. More specifically, these long-short portfolios provide protection for both small and large market price fluctuations, due to their negative correlation with the market and positive convexity as a function of market returns. By adding our long-short portfolio to a baseline portfolio such as a traditional 60/40 portfolio, our method provides an alternative way to hedge portfolio risk including tail risk, which does not require costly option-based strategies or complex numerical optimization. The total cost of such hedging amounts to the total cost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#24182;&#25104;&#21151;&#25512;&#24191;&#21040;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.08102</link><description>&lt;p&gt;
&#38754;&#21521;&#22495;&#24863;&#30693;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Few-Shot Learning for Optical Coherence Tomography Noise Reduction. (arXiv:2306.08102v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#24182;&#25104;&#21151;&#25512;&#24191;&#21040;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25955;&#26001;&#22122;&#22768;&#19968;&#30452;&#26159;&#21307;&#23398;&#25104;&#20687;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#38477;&#22122;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#26377;&#30417;&#30563;&#30340;&#23398;&#20064;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#22122;&#38899;&#38477;&#20302;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#21482;&#38656;&#35201;&#21333;&#24352;&#22270;&#20687;&#25110;&#37096;&#20998;&#22270;&#20687;&#20197;&#21450;&#30456;&#24212;&#30340;&#21435;&#26001;&#22320;&#38754;&#23454;&#20917;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#19981;&#21516;&#25104;&#20687;&#31995;&#32479;&#30340;&#22495;&#36716;&#31227;&#38382;&#39064;&#36827;&#34892;&#20102;&#21046;&#23450;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#24212;&#19981;&#21516;&#25104;&#20687;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speckle noise has long been an extensively studied problem in medical imaging. In recent years, there have been significant advances in leveraging deep learning methods for noise reduction. Nevertheless, adaptation of supervised learning models to unseen domains remains a challenging problem. Specifically, deep neural networks (DNNs) trained for computational imaging tasks are vulnerable to changes in the acquisition system's physical parameters, such as: sampling space, resolution, and contrast. Even within the same acquisition system, performance degrades across datasets of different biological tissues. In this work, we propose a few-shot supervised learning framework for optical coherence tomography (OCT) noise reduction, that offers a dramatic increase in training speed and requires only a single image, or part of an image, and a corresponding speckle suppressed ground truth, for training. Furthermore, we formulate the domain shift problem for OCT diverse imaging systems, and prove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;</title><link>http://arxiv.org/abs/2306.08094</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807; ChatGPT &#23454;&#29616;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65311;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#22810;&#65292;&#21516;&#26102;&#20063;&#20984;&#26174;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22797;&#26434;&#30340;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#22686;&#21152;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#23383;&#32534;&#30721;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08086</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safe Use of Neural Networks. (arXiv:2306.08086v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#26041;&#27861;&#65292;&#20351;&#29992;&#25968;&#23383;&#32534;&#30721;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#20869;&#37096;&#25968;&#23383;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20005;&#37325;&#24433;&#21709;&#20915;&#31574;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#25968;&#23383;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#36807;&#31243;&#20013;&#30340;&#31639;&#26415;&#38169;&#35823;&#20197;&#30830;&#20445;&#20854;&#23433;&#20840;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#24471;&#21040;&#20102;&#26377;&#25928;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks in modern communication systems can be susceptible to internal numerical errors that can drastically effect decision results. Such structures are composed of many sections each of which generally contain weighting operations and activation function evaluations. The safe use comes from methods employing number based codes that can detect arithmetic errors in the network's processing steps. Each set of operations generates parity values dictated by a code in two ways. One set of parities is obtained from a section's outputs while a second comparable set is developed directly from the original inputs. The parity values protecting the activation functions involve a Taylor series approximation to the activation functions. We focus on using long numerically based convolutional codes because of the large size of data sets. The codes are based on Discrete Fourier Transform kernels and there are many design options available. Mathematical program simulations show our error-detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292; &#28982;&#21518;&#23454;&#29616;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.08076</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#21644;&#29305;&#24449;&#22806;&#25512;&#22312;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization. (arXiv:2306.08076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292; &#28982;&#21518;&#23454;&#29616;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#28041;&#21450;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#23398;&#20064;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#32447;&#24615;&#22806;&#25512;&#35774;&#35745;&#65292;&#36890;&#36807;&#22806;&#25512;&#32467;&#26500;&#21644;&#29305;&#24449;&#31354;&#38388;&#26469;&#29983;&#25104;&#36229;&#20986;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#22270;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#30446;&#26631;&#36716;&#31227;&#36807;&#31243;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#22270;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#19988;&#25345;&#32493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization deals with the prevalent learning scenario where test distribution shifts from training distribution. With rising application demands and inherent complexity, graph OOD problems call for specialized solutions. While data-centric methods exhibit performance enhancements on many generic machine learning tasks, there is a notable absence of data augmentation methods tailored for graph OOD generalization. In this work, we propose to achieve graph OOD generalization with the novel design of non-Euclidean-space linear extrapolation. The proposed augmentation strategy extrapolates both structure and feature spaces to generate OOD graph data. Our design tailors OOD samples for specific shifts without corrupting underlying causal mechanisms. Theoretical analysis and empirical results evidence the effectiveness of our method in solving target shifts, showing substantial and constant improvements across various graph OOD tasks.
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>CVGP&#26159;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#35774;&#35745;&#21152;&#24555;&#31526;&#21495;&#34920;&#36798;&#24335;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.08057</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#36827;&#34892;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression via Control Variable Genetic Programming. (arXiv:2306.08057v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08057
&lt;/p&gt;
&lt;p&gt;
CVGP&#26159;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#35774;&#35745;&#21152;&#24555;&#31526;&#21495;&#34920;&#36798;&#24335;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#31526;&#21495;&#34920;&#36798;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#23398;&#20064;&#31616;&#21333;&#30340;&#34920;&#36798;&#24335;&#12290;&#22238;&#24402;&#28041;&#21450;&#35768;&#22810;&#33258;&#21464;&#37327;&#30340;&#34920;&#36798;&#24335;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#21463;&#31185;&#23398;&#30028;&#24191;&#27867;&#20351;&#29992;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#21464;&#37327;&#30340;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#65288;CVGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#33258;&#21464;&#37327;&#30340;&#31526;&#21495;&#22238;&#24402;&#12290;CVGP&#36890;&#36807;&#23450;&#21046;&#23454;&#39564;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#21152;&#24555;&#20102;&#31526;&#21495;&#34920;&#36798;&#24335;&#30340;&#21457;&#29616;&#36807;&#31243;&#12290;&#23427;&#39318;&#20808;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#25311;&#21512;&#28041;&#21450;&#23569;&#37327;&#33258;&#21464;&#37327;&#30340;&#31616;&#21333;&#34920;&#36798;&#24335;&#65292;&#22312;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#20013;&#65292;&#20854;&#20013;&#20854;&#20182;&#21464;&#37327;&#34987;&#20445;&#25345;&#20026;&#24120;&#37327;&#12290;&#28982;&#21518;&#36890;&#36807;&#22686;&#21152;&#26032;&#30340;&#33258;&#21464;&#37327;&#25193;&#23637;&#20197;&#21069;&#23398;&#20064;&#21040;&#30340;&#34920;&#36798;&#24335;&#65292;&#20351;&#29992;&#26032;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#36825;&#20123;&#21464;&#37327;&#34987;&#20801;&#35768;&#21464;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CVGP&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CVGP&#33021;&#22815;&#23398;&#20064;&#28041;&#21450;&#35768;&#22810;&#33258;&#21464;&#37327;&#30340;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;CARBS&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#26469;&#35299;&#20915;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#35843;&#25972;&#20013;&#30340;&#8220;&#40657;&#39764;&#27861;&#8221;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#35745;&#31639;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#23588;&#20854;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.08055</link><description>&lt;p&gt;
&#22312;&#20280;&#32553;&#24615;&#35757;&#32451;&#20013;&#20248;&#21270;&#36229;&#21442;&#25968;&#65306;&#35745;&#31639;&#25928;&#29575;&#35757;&#32451;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;CARBS&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#26469;&#35299;&#20915;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#35843;&#25972;&#20013;&#30340;&#8220;&#40657;&#39764;&#27861;&#8221;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#35745;&#31639;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#30456;&#21516;&#30340;&#35745;&#31639;&#37327;&#33719;&#24471;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#31995;&#32479;&#35843;&#25972;&#36824;&#19981;&#26222;&#36941;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26356;&#26159;&#22914;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#35780;&#20272;&#26114;&#36149;&#65292;&#36229;&#21442;&#25968;&#36739;&#22810;&#65292;&#38656;&#35201;&#36827;&#34892;&#38590;&#20197;&#25226;&#25569;&#30340;&#25240;&#20013;&#12289;&#39044;&#31639;&#21644;&#25628;&#32034;&#36793;&#30028;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25552;&#20986;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#31283;&#20581;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693; Pareto &#21306;&#22495;&#36125;&#21494;&#26031;&#25628;&#32034;&#65288;CARBS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#12290;CARBS &#22312;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#22240;&#27492;&#21363;&#20351;&#22312;&#27169;&#22411;&#32553;&#25918;&#30340;&#21516;&#26102;&#20063;&#21487;&#20197;&#35843;&#25972;&#27169;&#22411;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#8220;&#40657;&#39764;&#27861;&#8221;&#35843;&#25972;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#31616;&#21333;&#30340;&#22522;&#32447;&#65288;ProcGen &#35770;&#25991;&#20013;&#25552;&#20379;&#30340; PPO &#26041;&#27861;&#65289;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25972;&#20010; ProcGen &#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35780;&#20272;&#26102;&#22797;&#21046;&#20102; Bertinetto &#31561;&#20154;&#30340;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#65292;&#20294;&#29305;&#21035;&#36866;&#21512;&#35745;&#31639;&#21463;&#38480;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#35774;&#35745;&#24072;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#25110;&#38480;&#21046;&#22521;&#35757;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the "black magic" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08044</link><description>&lt;p&gt;
&#21098;&#26525;&#26041;&#24335;&#25552;&#39640;&#21487;&#38752;&#31574;&#30053;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#37325;&#30151;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#20915;&#31574;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#26377;&#26395;&#21046;&#23450;&#31934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20027;&#35201;&#22522;&#20110;&#27515;&#20129;&#29575;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#31163;&#32447;&#20272;&#35745;&#30340;&#31283;&#23450;&#24615;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#23558;&#30456;&#20851;&#20294;&#22024;&#26434;&#30340;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65288;&#20363;&#22914;&#24739;&#32773;&#29983;&#23384;&#29575;&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#26681;&#25454;&#25152;&#26377;&#21487;&#29992;&#22870;&#21169;&#23545;&#21160;&#20316;&#38598;&#36827;&#34892;&#21098;&#26525;&#65292;&#28982;&#21518;&#22522;&#20110;&#31232;&#30095;&#20027;&#35201;&#22870;&#21169;&#65292;&#20351;&#29992;&#21463;&#38480;&#21160;&#20316;&#38598;&#36827;&#34892;&#26368;&#32456;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#35299;&#31163;&#20934;&#30830;&#21644;&#36817;&#20284;&#22870;&#21169;&#26469;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#30340;&#28508;&#22312;&#25197;&#26354;&#65292;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08041</link><description>&lt;p&gt;
&#20851;&#20110;&#20266;&#36896;&#32435;&#20160;&#22343;&#34913;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#26356;&#25913;&#25968;&#25454;&#38598;&#20197;&#23433;&#35013;&#65288;&#28508;&#22312;&#34394;&#20551;&#30340;&#65289;&#21807;&#19968;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#32435;&#20160;&#22343;&#34913;&#28857;(Nash equilibrium)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#21363;&#30001;&#20854;Q&#20989;&#25968;&#35268;&#23450;&#30340;&#28216;&#25103;&#30340;&#38598;&#21512;&#65292;&#20854;&#20855;&#26377;&#21807;&#19968;&#30340;&#32852;&#21512;&#31574;&#30053;&#20316;&#20026;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23545;&#20110;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#25968;&#25454;&#27745;&#26579;&#20351;&#25152;&#26377;&#21512;&#29702;&#30340;&#28216;&#25103;&#37117;&#22312;&#20854;&#20013;&#26102;&#65292;&#25915;&#20987;&#25165;&#25104;&#21151;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23558;&#24120;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#22810;&#38754;&#20307;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#23545;&#20110;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#36870;&#32435;&#20160;&#38598;&#20197;&#21450;&#30001;&#25968;&#25454;&#24341;&#36215;&#30340;&#21512;&#29702;&#28216;&#25103;&#38598;&#37117;&#26159;Q&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#22810;&#38754;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#30340;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#24517;&#35201;&#30340;&#27493;&#39588;&#25581;&#31034;&#20102;&#31163;&#32447;MARL&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32467;&#26500;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.08021</link><description>&lt;p&gt;
&#21487;&#21464;&#36890;&#36947;&#32500;&#24230;&#30340;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#26465;&#20214;&#19979;&#35774;&#35745;&#34920;&#29616;&#33391;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25214;&#21040;&#26368;&#20248;&#30340;&#36890;&#36947;&#32500;&#24230;&#65288;&#21363;DNN&#23618;&#20013;&#30340;&#36807;&#28388;&#22120;&#25968;&#37327;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24037;&#20316;&#26088;&#22312;&#33258;&#21160;&#21270;DNN&#27169;&#22411;&#23454;&#29616;&#30340;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36890;&#36947;&#32500;&#24230;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#29616;&#39640;&#25928;&#19988;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#37197;&#22791;&#26377;&#25928;&#30340;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;architecture search&#38454;&#27573;GPU-hours&#25552;&#39640;&#20102;1.3-1.7&#20493;&#65292;&#20869;&#23384;&#35201;&#27714;&#25552;&#39640;&#20102;1.5-1.7&#20493;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#21508;&#31181;DNN&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\times$ in GPU-hours and $1.5-1.7\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08014</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#19968;&#37096;&#20998;&#65306;&#35748;&#35782;&#30446;&#26631;&#21644;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#31995;&#32479;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#27867;&#20989;&#32780;&#33258;&#32452;&#32455;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#12289;&#31283;&#23450;&#32467;&#26500;&#65288;&#26234;&#33021;&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#19968;&#20010;&#25512;&#35770;&#65292;&#23427;&#26126;&#30830;&#20102;&#33021;&#22815;&#20026;&#26410;&#26469;&#36827;&#34892;&#35268;&#21010;&#65288;&#20195;&#29702;&#65289;&#30340;&#31995;&#32479;&#26159;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#21253;&#21547;&#20449;&#24687;&#23547;&#27714;&#32452;&#20214;&#30340;&#29305;&#23450;&#33258;&#30001;&#33021;&#27867;&#20989;&#26469;&#36816;&#20316;&#30340;&#12290;&#26412;&#25991;&#26159;&#19968;&#20010;&#31995;&#21015;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#25105;&#20204;&#22312;&#33258;&#30001;&#24418;&#24335;&#22240;&#23376;&#22270;&#19978;&#25512;&#23548;&#20102;AIF&#30340;&#21512;&#25104;&#29256;&#26412;&#12290;&#26412;&#25991;&#37325;&#28857;&#25512;&#23548;&#20102;AIF&#25152;&#20351;&#29992;&#30340;&#33258;&#30001;&#33021;&#27867;&#20989;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#36896;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#24182;&#19982;&#26377;&#20851;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#25509;&#21475;&#30340;AIF&#29256;&#26412;&#12290;&#32467;&#26524;&#28040;&#24687;&#26159;&#22312;&#25105;&#20204;&#30340;&#20276;&#20387;&#35770;&#25991;&#20013;&#24471;&#20986;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#22240;&#23376;&#22270;&#24418;&#24335;&#20013;&#23384;&#22312;&#19968;&#20010;&#32570;&#21475;&#12290;&#34429;&#28982;&#22240;&#23376;&#22270;&#34920;&#36798;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22312;&#25351;&#23450;&#31995;&#32479;&#30446;&#26631;&#26041;&#38754;&#32570;&#20047;&#19968;&#20010;&#22270;&#24418;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#25551;&#36848;&#27861;&#30340;&#26032;&#25193;&#23637;&#65292;&#31216;&#20026;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#65292;&#23427;&#20351;&#31995;&#32479;&#30446;&#26631;&#24471;&#21040;&#26126;&#30830;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#26041;&#26696;&#65292;&#20854;&#20013;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#21644;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08011</link><description>&lt;p&gt;
&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22522;&#20110;&#38544;&#34109;&#21518;&#38376;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. (arXiv:2306.08011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#26041;&#26696;&#65292;&#20854;&#20013;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#21644;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#38754;&#20020;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20010;&#38382;&#39064;&#24448;&#24448;&#34987;&#30740;&#31350;FL&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30740;&#31350;&#25152;&#24573;&#35270;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#30340;FL&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#31169;&#23494;&#25512;&#29702;&#21152;&#24378;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#65288;PI-SBA&#65289;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#30340;&#26426;&#21046;&#21644;&#22522;&#20110;&#28304;&#29305;&#23450;&#30340;&#21518;&#38376;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#26696;&#36824;&#33021;&#26377;&#25928;&#24212;&#23545;&#24694;&#24847;&#23458;&#25143;&#36890;&#36807;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#31363;&#21462;&#31169;&#26377;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.08009</link><description>&lt;p&gt;
DHBE: &#36890;&#36807;&#21463;&#38480;&#23545;&#25239;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#20851;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#26469;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. (arXiv:2306.08009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#32039;&#24613;&#23041;&#32961;&#12290;&#20026;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#65292;&#35768;&#22810;&#24037;&#20316;&#37117;&#24314;&#31435;&#20102;&#19968;&#20010;&#20998;&#38454;&#27573;&#30340;&#27969;&#31243;&#26469;&#20174;&#21463;&#23475;DNN&#20013;&#31227;&#38500;&#21518;&#38376;&#65306;&#26816;&#26597;&#12289;&#23450;&#20301;&#21644;&#25830;&#38500;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#24178;&#20928;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#27969;&#31243;&#26159;&#33030;&#24369;&#30340;&#65292;&#32780;&#19988;&#19981;&#33021;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#25830;&#38500;&#21518;&#38376;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#26080;&#20851;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DHBE&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#25830;&#38500;&#21518;&#38376;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#32988;&#36807;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#22312;&#21160;&#24577;&#38556;&#30861;&#29289;&#22330;&#26223;&#19979;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#21306;&#38388;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#33021;&#36739;&#22909;&#22320;&#23436;&#25104;&#36991;&#38556;&#20219;&#21153;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08008</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#21306;&#38388;&#38480;&#21046;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36991;&#38556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dynamic Interval Restrictions on Action Spaces in Deep Reinforcement Learning for Obstacle Avoidance. (arXiv:2306.08008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#22312;&#21160;&#24577;&#38556;&#30861;&#29289;&#22330;&#26223;&#19979;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#21306;&#38388;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#33021;&#36739;&#22909;&#22320;&#23436;&#25104;&#36991;&#38556;&#20219;&#21153;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#22312;&#21516;&#19968;&#32452;&#21160;&#20316;&#19978;&#25191;&#34892;&#65292;&#20294;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#19979;&#37117;&#19981;&#22815;&#20805;&#20998;&#65292;&#22240;&#20026;&#27599;&#20010;&#27493;&#39588;&#21487;&#29992;&#30340;&#21160;&#20316;&#32452;&#19981;&#21516;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#21160;&#24577;&#38556;&#30861;&#29289;&#22330;&#26223;&#20013;&#20986;&#29616;&#30340;&#21306;&#38388;&#38480;&#21046;&#38382;&#39064;&#12290;&#24403;&#38656;&#35201;&#36991;&#20813;&#23548;&#33268;&#30896;&#25758;&#30340;&#21160;&#20316;&#26102;&#65292;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#23558;&#34987;&#21010;&#20998;&#20026;&#21464;&#37327;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#21306;&#38388;&#25968;&#37327;&#20570;&#20986;&#20102;&#36739;&#24378;&#20551;&#35774;&#65292;&#20165;&#38480;&#20110;&#20984;&#23376;&#38598;&#65292;&#24182;&#19988;&#21487;&#29992;&#30340;&#21160;&#20316;&#26159;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#23398;&#20064;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#21442;&#25968;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;ConstraintNet&#65292;&#33021;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#21306;&#38388;&#32780;&#19981;&#20250;&#21463;&#29615;&#22659;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#36991;&#38556;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#25991;&#29486;&#20013;&#30340;&#24809;&#32602;&#12289;&#25237;&#24433;&#12289;&#26367;&#25442;&#20197;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#23631;&#34109;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms typically act on the same set of actions. However, this is not sufficient for a wide range of real-world applications where different subsets are available at each step. In this thesis, we consider the problem of interval restrictions as they occur in pathfinding with dynamic obstacles. When actions that lead to collisions are avoided, the continuous action space is split into variable parts. Recent research learns with strong assumptions on the number of intervals, is limited to convex subsets, and the available actions are learned from the observations. Therefore, we propose two approaches that are independent of the state of the environment by extending parameterized reinforcement learning and ConstraintNet to handle an arbitrary number of intervals. We demonstrate their performance in an obstacle avoidance task and compare the methods to penalties, projection, replacement, as well as discrete and continuous masking from the literature. The res
&lt;/p&gt;</description></item><item><title>&#26641;&#31361;&#26426;&#21046;&#20026;AI&#39046;&#22495;&#25552;&#20379;&#20102;&#21551;&#21457;&#24615;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20449;&#29992;&#20998;&#37197;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#39640;&#33021;&#32791;&#31561;&#38382;&#39064;&#65292;&#20026;&#26500;&#24314;&#26356;&#24378;&#22823;&#12289;&#26356;&#33410;&#33021;&#30340;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.08007</link><description>&lt;p&gt;
&#21033;&#29992;&#26641;&#31361;&#30340;&#29305;&#24615;&#25512;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#21551;&#21457;&#24335;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Leveraging dendritic properties to advance machine learning and neuro-inspired computing. (arXiv:2306.08007v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08007
&lt;/p&gt;
&lt;p&gt;
&#26641;&#31361;&#26426;&#21046;&#20026;AI&#39046;&#22495;&#25552;&#20379;&#20102;&#21551;&#21457;&#24615;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#20449;&#29992;&#20998;&#37197;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#39640;&#33021;&#32791;&#31561;&#38382;&#39064;&#65292;&#20026;&#26500;&#24314;&#26356;&#24378;&#22823;&#12289;&#26356;&#33410;&#33021;&#30340;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#26159;&#19968;&#20010;&#38750;&#24120;&#33021;&#24178;&#21644;&#39640;&#25928;&#30340;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#30340;&#33021;&#37327;&#22788;&#29702;&#21644;&#23384;&#20648;&#22823;&#37327;&#22024;&#26434;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20449;&#24687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#24040;&#22823;&#30340;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20173;&#28982;&#24456;&#38590;&#22312;&#29983;&#29289;&#20195;&#29702;&#22120;&#20214;&#36731;&#26494;&#23436;&#25104;&#30340;&#20219;&#21153;&#20013;&#31454;&#20105;&#12290;&#22240;&#27492;&#65292;&#33041;&#21551;&#21457;&#24335;&#24037;&#31243;&#24050;&#32463;&#25104;&#20026;&#35774;&#35745;&#21487;&#25345;&#32493;&#30340;&#65292;&#26032;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#26641;&#31361;&#26426;&#21046;&#22914;&#20309;&#28608;&#21457;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#23618;&#32593;&#32476;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#39640;&#33021;&#32791;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#29616;&#26377;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#26641;&#31361;&#30740;&#31350;&#22914;&#20309;&#20026;&#26500;&#24314;&#26356;&#24378;&#22823;&#12289;&#26356;&#33410;&#33021;&#30340;&#20154;&#24037;&#23398;&#20064;&#31995;&#32479;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain is a remarkably capable and efficient system. It can process and store huge amounts of noisy and unstructured information using minimal energy. In contrast, current artificial intelligence (AI) systems require vast resources for training while still struggling to compete in tasks that are trivial for biological agents. Thus, brain-inspired engineering has emerged as a promising new avenue for designing sustainable, next-generation AI systems. Here, we describe how dendritic mechanisms of biological neurons have inspired innovative solutions for significant AI problems, including credit assignment in multilayer networks, catastrophic forgetting, and high energy consumption. These findings provide exciting alternatives to existing architectures, showing how dendritic research can pave the way for building more powerful and energy-efficient artificial learning systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32454;&#24494;&#30340;&#20809;&#20239;&#25925;&#38556;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08004</link><description>&lt;p&gt;
&#26816;&#27979;&#19982;&#20998;&#31867;&#26088;&#22312;&#39044;&#38450;&#20809;&#20239;&#31995;&#32479;&#25925;&#38556;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection and classification of faults aimed at preventive maintenance of PV systems. (arXiv:2306.08004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32454;&#24494;&#30340;&#20809;&#20239;&#25925;&#38556;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#31995;&#32479;&#30340;&#35786;&#26029;&#26088;&#22312;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#35782;&#21035;&#25925;&#38556;&#12290;&#35786;&#26029;&#36825;&#20123;&#25925;&#38556;&#23545;&#20110;&#20445;&#35777;&#33021;&#28304;&#29983;&#20135;&#21644;&#24310;&#38271;&#20809;&#20239;&#21457;&#30005;&#21378;&#30340;&#20351;&#29992;&#23551;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#20026;&#27492;&#25552;&#20986;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#24456;&#23569;&#26377;&#20851;&#27880;&#32454;&#24494;&#25925;&#38556;&#30340;&#26816;&#27979;&#21644;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#36873;&#25321;&#36807;&#31243;&#12290;&#32454;&#24494;&#25925;&#38556;&#26159;&#19968;&#31181;&#20854;&#29305;&#24449;&#31614;&#21517;&#19982;&#20581;&#24247;&#38754;&#26495;&#30340;&#29305;&#24449;&#31614;&#21517;&#38590;&#20197;&#21306;&#21035;&#30340;&#25925;&#38556;&#12290;&#20316;&#20026;&#26816;&#27979;&#32454;&#24494;&#25925;&#38556;&#30340;&#36129;&#29486;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#36873;&#25321;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis in PV systems aims to detect, locate and identify faults. Diagnosing these faults is vital to guarantee energy production and extend the useful life of PV power plants. In the literature, multiple machine learning approaches have been proposed for this purpose. However, few of these works have paid special attention to the detection of fine faults and the specialized process of extraction and selection of features for their classification. A fine fault is one whose characteristic signature is difficult to distinguish to that of a healthy panel. As a contribution to the detection of fine faults (especially of the snail trail type), this article proposes an innovative approach based on the Random Forest (RF) algorithm. This approach uses a complex feature extraction and selection method that improves the computational time of fault classification while maintaining high accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#20809;&#20239;&#30005;&#21378;&#20013;&#26816;&#27979;&#21644;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#65292;&#37319;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08003</link><description>&lt;p&gt;
DTW k-means&#32858;&#31867;&#29992;&#20110;&#20809;&#20239;&#27169;&#22359;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DTW k-means clustering for fault detection in photovoltaic modules. (arXiv:2306.08003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#20809;&#20239;&#30005;&#21378;&#20013;&#26816;&#27979;&#21644;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#65292;&#37319;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#33021;&#28304;&#22312;&#20840;&#29699;&#30340;&#24212;&#29992;&#22686;&#21152;&#34920;&#26126;&#65292;&#20809;&#20239;&#30005;&#21378;&#30340;&#23551;&#21629;&#21644;&#32500;&#25252;&#30452;&#25509;&#20381;&#36182;&#20110;&#24555;&#36895;&#26816;&#27979;&#20809;&#20239;&#30005;&#21378;&#30340;&#20005;&#37325;&#25925;&#38556;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26816;&#27979;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#20102;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#25925;&#38556;&#30340;&#20855;&#20307;&#34892;&#20026;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#21487;&#20197;&#34987;&#24402;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#21162;&#21147;&#65288;&#27599;&#31181;&#25216;&#26415;&#20013;&#26126;&#30830;&#35782;&#21035;&#30340;&#25925;&#38556;&#31867;&#22411;&#65289;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;PV&#30005;&#27744;&#25110;&#19968;&#20010;PV&#27169;&#22359;&#20013;&#39564;&#35777;&#12290;&#36825;&#22312;&#32771;&#34385;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#30340;&#22823;&#35268;&#27169;PV&#30005;&#21378;&#20013;&#24456;&#38590;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#19968;&#20123;&#22522;&#20110;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#30693;&#21517;&#26041;&#27861;&#23581;&#35797;&#26816;&#27979;&#24322;&#24120;&#65292;&#20294;&#19981;&#33021;&#31934;&#30830;&#22320;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#12290;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;&#20581;&#24247;&#38754;&#26495;&#26377;&#25928;&#22320;&#20998;&#32452;&#24182;&#23558;&#20854;&#19982;&#25925;&#38556;&#38754;&#26495;&#20998;&#24320;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;PV&#30005;&#21378;&#30340;&#19968;&#33324;&#25925;&#38556;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;PV&#30005;&#21378;&#20013;&#27599;&#20010;&#38754;&#26495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;k-means&#32858;&#31867;&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21363;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#65292;&#20197;&#20811;&#26381;PV&#38754;&#26495;&#23545;&#19981;&#21516;&#29615;&#22659;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#21709;&#24212;&#30340;&#21464;&#24322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#26816;&#27979;PV&#30005;&#21378;&#20013;&#30340;&#25925;&#38556;&#24182;&#35782;&#21035;&#20854;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increase in the use of photovoltaic (PV) energy in the world has shown that the useful life and maintenance of a PV plant directly depend on theability to quickly detect severe faults on a PV plant. To solve this problem of detection, data based approaches have been proposed in the literature.However, these previous solutions consider only specific behavior of one or few faults. Most of these approaches can be qualified as supervised, requiring an enormous labelling effort (fault types clearly identified in each technology). In addition, most of them are validated in PV cells or one PV module. That is hardly applicable in large-scale PV plants considering their complexity. Alternatively, some unsupervised well-known approaches based on data try to detect anomalies but are not able to identify precisely the type of fault. The most performant of these methods do manage to efficiently group healthy panels and separate them from faulty panels. In that way, this article presents an unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23545;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#22788;&#29702;&#65292;&#24182;&#19988;&#38416;&#36848;&#22914;&#20309;&#23558;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#31561;&#36807;&#31243;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.08001</link><description>&lt;p&gt;
&#20027;&#21160;&#26597;&#35810;&#30340;&#39532;&#23572;&#21487;&#22827;&#24418;&#24335;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Markovian Formalism for Active Querying. (arXiv:2306.08001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23545;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#22788;&#29702;&#65292;&#24182;&#19988;&#38416;&#36848;&#22914;&#20309;&#23558;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#31561;&#36807;&#31243;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#26368;&#36817;&#36827;&#23637;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21464;&#21270;&#24456;&#22823;&#65292;&#32570;&#20047;&#25972;&#20307;&#24615;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#65292;&#29992;&#20110;&#23545;&#20027;&#21160;&#23398;&#20064;&#39046;&#22495;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#26469;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#30340;&#32452;&#32455;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#20855;&#20307;&#27010;&#36848;&#20102;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#20197;&#21450;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#38754;&#22914;&#20309;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#65292;&#24182;&#25351;&#23548;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#38754;&#22914;&#20309;&#36866;&#24212;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning algorithms have been an integral part of recent advances in artificial intelligence. However, the research in the field is widely varying and lacks an overall organizing leans. We outline a Markovian formalism for the field of active learning and survey the literature to demonstrate the organizing capability of our proposed formalism. Our formalism takes a partially observable Markovian system approach to the active learning process as a whole. We specifically outline how querying, dataset augmentation, reward updates, and other aspects of active learning can be viewed as a transition between meta-states in a Markovian system, and give direction into how other aspects of active learning can fit into our formalism.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#25552;&#39640;CLIP-like&#27169;&#22411;&#23545;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#22612;&#23545;&#20110;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#25552;&#31034;&#20102;&#26410;&#26469;&#21487;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.08000</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-Shot Detection of Low Prevalence Chest Pathologies using Domain Pre-trained Language Models. (arXiv:2306.08000v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#25552;&#39640;CLIP-like&#27169;&#22411;&#23545;&#20302;&#24739;&#30149;&#29575;&#33016;&#37096;&#30149;&#30151;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#22612;&#23545;&#20110;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#25552;&#31034;&#20102;&#26410;&#26469;&#21487;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#25104;&#23545;&#30340;&#22270;&#20687;&#35782;&#21035;&#26631;&#31614;&#25968;&#25454;&#26367;&#20195;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#28040;&#38500;&#20102;&#23545;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#20687;CLIP-based CheXzero&#36825;&#26679;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#36825;&#20123;&#22312;&#33016;&#37096;X&#23556;&#32447;&#35299;&#37322;&#39046;&#22495;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#20351;&#29992;CX-BERT&#12289;BlueBERT&#21644;ClinicalBERT&#31561;&#39046;&#22495;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#26367;&#25442;BERT&#26435;&#37325;&#26469;&#22686;&#21152;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#31867;&#20284;&#20110;CLIP&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#20195;&#20215;&#26159;&#25171;&#30772;&#21407;&#22987;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#29305;&#23450;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#27169;&#22411;&#22312;&#26816;&#27979;&#20302;&#24739;&#30149;&#29575;&#30149;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26367;&#25442;&#21407;&#22987;CLIP-BERT&#26435;&#37325;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#24120;&#35265;&#30149;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#25991;&#26412;&#22612;&#22312;&#20302;&#24739;&#30149;&#29575;&#30142;&#30149;&#30340;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#28608;&#21457;&#20102;&#26410;&#26469;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in zero-shot learning have enabled the use of paired image-text data to replace structured labels, replacing the need for expert annotated datasets. Models such as CLIP-based CheXzero utilize these advancements in the domain of chest X-ray interpretation. We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment. We evaluate the performance of zero-shot classification models with domain-specific pre-training for detecting low-prevalence pathologies. Even though replacing the weights of the original CLIP-BERT degrades model performance on commonly found pathologies, we show that pre-trained text towers perform exceptionally better on low-prevalence diseases. This motivates future ensemble models with a combination of differently trained language models for maxima
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#20998;&#26512;&#20114;&#32852;&#32593;&#38450;&#28779;&#22681;&#26085;&#24535;&#25991;&#20214;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#24182;&#20102;&#35299;&#24694;&#24847;&#25805;&#20316;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24433;&#21709;&#20114;&#32852;&#32593;&#12290;</title><link>http://arxiv.org/abs/2306.07997</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#38450;&#28779;&#22681;&#26085;&#24535;&#25991;&#20214;&#30340;&#22810;&#31867;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Approach on Multiclass Classification of Internet Firewall Log Files. (arXiv:2306.07997v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07997
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31181;&#20998;&#31867;&#31639;&#27861;&#65292;&#20998;&#26512;&#20114;&#32852;&#32593;&#38450;&#28779;&#22681;&#26085;&#24535;&#25991;&#20214;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#24182;&#20102;&#35299;&#24694;&#24847;&#25805;&#20316;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24433;&#21709;&#20114;&#32852;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38450;&#28779;&#22681;&#26159;&#36890;&#36807;&#31579;&#36873;&#25152;&#26377;&#36827;&#20837;&#65288;&#20197;&#21450;&#20598;&#23572;&#20986;&#29616;&#30340;&#65289;&#25968;&#25454;&#21253;&#26469;&#20445;&#25252;&#36890;&#20449;&#32593;&#32476;&#23433;&#20840;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#36807;&#28388;&#26159;&#36890;&#36807;&#23558;&#20256;&#20837;&#30340;&#25968;&#25454;&#21253;&#19982;&#19968;&#32452;&#26088;&#22312;&#38450;&#27490;&#24694;&#24847;&#20195;&#30721;&#36827;&#20837;&#32593;&#32476;&#30340;&#35268;&#21017;&#36827;&#34892;&#27604;&#36739;&#26469;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#35843;&#33410;&#36827;&#20837;&#21644;&#31163;&#24320;&#32593;&#32476;&#30340;&#25968;&#25454;&#21253;&#27969;&#65292;&#20114;&#32852;&#32593;&#38450;&#28779;&#22681;&#36319;&#36394;&#25152;&#26377;&#27963;&#21160;&#12290;&#26085;&#24535;&#25991;&#20214;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#24110;&#21161;&#25925;&#38556;&#25490;&#38500;&#21644;&#35786;&#26029;&#65292;&#21516;&#26102;&#20063;&#19982;&#31995;&#32479;&#23457;&#35745;&#21644;&#21462;&#35777;&#30456;&#20851;&#12290;&#38450;&#28779;&#22681;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#38450;&#27490;&#21457;&#36865;&#24694;&#24847;&#25968;&#25454;&#21253;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#38450;&#24481;&#32593;&#32476;&#25915;&#20987;&#24182;&#20102;&#35299;&#24694;&#24847;&#25805;&#20316;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24433;&#21709;&#20114;&#32852;&#32593;&#65292;&#26377;&#24517;&#35201;&#26816;&#26597;&#26085;&#24535;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#38450;&#28779;&#22681;&#20915;&#23450;&#26159;&#21542;'&#20801;&#35768;'&#12289;'&#25298;&#32477;'&#12289;'&#20002;&#24323;'&#25110;'&#37325;&#32622;-&#21452;&#26041;'&#20256;&#20837;&#21644;&#20256;&#20986;&#30340;&#25968;&#25454;&#21253;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#21508;&#31181;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#22810;&#31867;&#20998;&#31867;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20998;&#26512;&#20114;&#32852;&#32593;&#38450;&#28779;&#22681;&#26085;&#24535;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Firewalls are critical components in securing communication networks by screening all incoming (and occasionally exiting) data packets. Filtering is carried out by comparing incoming data packets to a set of rules designed to prevent malicious code from entering the network. To regulate the flow of data packets entering and leaving a network, an Internet firewall keeps a track of all activity. While the primary function of log files is to aid in troubleshooting and diagnostics, the information they contain is also very relevant to system audits and forensics. Firewalls primary function is to prevent malicious data packets from being sent. In order to better defend against cyberattacks and understand when and how malicious actions are influencing the internet, it is necessary to examine log files. As a result, the firewall decides whether to 'allow,' 'deny,' 'drop,' or 'reset-both' the incoming and outgoing packets. In this research, we apply various categorization algorithms to make se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#31070;&#32463;&#32593;&#32476;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.07995</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Semantic-Based Neural Network Repair. (arXiv:2306.07995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#31070;&#32463;&#32593;&#32476;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#36890;&#36807;&#22312; TensorFlow &#21644; PyTorch &#31561;&#26694;&#26550;&#20013;&#36827;&#34892;&#32534;&#31243;&#26500;&#24314;&#65288;&#21644;&#35757;&#32451;&#65289;&#30340;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#24212;&#29992;&#20016;&#23500;&#30340;&#39044;&#23450;&#20041;&#23618;&#25163;&#21160;&#32534;&#20889;&#31070;&#32463;&#32593;&#32476;&#25110;&#36890;&#36807; AutoML &#33258;&#21160;&#29983;&#25104;&#32593;&#32476;&#12290;&#30001;&#20110;&#24517;&#39035;&#28385;&#36275;&#20351;&#29992;&#36825;&#20123;&#23618;&#30340;&#38750;&#24179;&#20961;&#32422;&#26463;&#26465;&#20214;&#65292;&#25152;&#20197;&#20351;&#29992;&#19981;&#21516;&#23618;&#26469;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#38169;&#35823;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#20986;&#23545;&#32593;&#32476;&#36827;&#34892;&#26368;&#23567;&#20462;&#25913;&#20197;&#20351;&#20854;&#21464;&#20026;&#26377;&#25928;&#30340;&#20462;&#25913;&#12290;&#20462;&#25913;&#19968;&#23618;&#21487;&#33021;&#20250;&#23545;&#38543;&#21518;&#30340;&#23618;&#20135;&#29983;&#32423;&#32852;&#25928;&#24212;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#24517;&#39035;&#36882;&#24402;&#22320;&#25628;&#32034;&#20197;&#35782;&#21035;&#8220;&#20840;&#23616;&#8221;&#26368;&#23567;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a "globally" minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2306.07994</link><description>&lt;p&gt;
MSSRNet: &#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#39034;&#24207;&#39118;&#26684;&#34920;&#31034;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#26088;&#22312;&#23558;&#25991;&#26412;&#37325;&#20889;&#20026;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20027;&#35201;&#20869;&#23481;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26469;&#35843;&#33410;&#25991;&#26412;&#39118;&#26684;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20256;&#36798;&#27599;&#20010;&#21333;&#29420;&#20196;&#29260;&#30340;&#39118;&#26684;&#24378;&#24230;&#12290;&#20107;&#23454;&#19978;&#65292;&#25991;&#26412;&#30340;&#27599;&#20010;&#20196;&#29260;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#39118;&#26684;&#24378;&#24230;&#65292;&#24182;&#23545;&#25972;&#20307;&#39118;&#26684;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#25991;&#26412;&#20013;&#30340;&#27599;&#20010;&#20196;&#29260;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20801;&#35768;&#23545;&#39118;&#26684;&#24378;&#24230;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22521;&#35757;&#31283;&#23450;&#24615;&#24182;&#20943;&#36731;&#39640;&#32500;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07992</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23500;&#21547;&#22270;&#29255;&#31561;&#35270;&#35273;&#25968;&#25454;&#19982;&#29289;&#21697;&#20851;&#32852;&#24230;&#22686;&#21152;&#65292;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65288;VARS&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;VARS&#26131;&#21463;&#21040;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21521;&#19982;&#36825;&#20123;&#29289;&#21697;&#20851;&#32852;&#30340;&#24178;&#20928;&#22270;&#20687;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#23545;VARS&#30340;&#25915;&#20987;&#20026;&#24191;&#27867;&#20351;&#29992;VARS&#30340;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#24102;&#26469;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22914;&#20309;&#20445;&#25252;VARS&#20813;&#21463;&#27492;&#31867;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;VARS&#35270;&#35273;&#25915;&#20987;&#30340;&#23433;&#20840;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;VARS&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;(1)&#36890;&#36807;&#22522;&#20110;&#20840;&#23616;&#35270;&#35273;&#20256;&#36755;&#30340;&#22270;&#20687;&#37325;&#26500;&#26469;&#38450;&#24481;&#20197;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;(2)&#20351;&#29992;&#22312;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#23545;VARS&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36328;&#20307;&#31995;&#32467;&#26500;IoT&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.07989</link><description>&lt;p&gt;
&#36328;&#20307;&#31995;&#32467;&#26500;&#29289;&#32852;&#32593;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#35843;&#26597;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Cross-Architectural IoT Malware Threat Hunting. (arXiv:2306.07989v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36328;&#20307;&#31995;&#32467;&#26500;IoT&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38750;Windows&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#30340;&#22686;&#21152;&#24050;&#32463;&#25104;&#20026;&#32593;&#32476;&#23433;&#20840;&#30028;&#30340;&#28966;&#28857;&#12290;&#22260;&#32469;Hunting Windows PE-Based Malwares&#30340;&#30740;&#31350;&#24037;&#20316;&#27491;&#22312;&#25104;&#29087;&#65292;&#32780;&#38024;&#23545;Linux&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#30340;&#21457;&#23637;&#30456;&#23545;&#36739;&#23569;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#34701;&#20837;&#20154;&#31867;&#29983;&#27963;&#30340;&#26234;&#33021;&#35774;&#22791;&#24050;&#32463;&#25104;&#20026;&#40657;&#23458;&#36827;&#34892;&#24694;&#24847;&#27963;&#21160;&#30340;&#19968;&#26465;&#36890;&#36947;&#12290;IoT&#35774;&#22791;&#37319;&#29992;&#21508;&#31181;&#22522;&#20110;Unix&#30340;&#26550;&#26500;&#65292;&#36981;&#24490;ELF&#65288;&#21487;&#25191;&#34892;&#21644;&#21487;&#38142;&#25509;&#26684;&#24335;&#65289;&#20316;&#20026;&#23427;&#20204;&#30340;&#26631;&#20934;&#20108;&#36827;&#21046;&#25991;&#20214;&#35268;&#33539;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#36328;&#20307;&#31995;&#32467;&#26500;IoT&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#20998;&#31867;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#22312;&#29616;&#20195;&#20998;&#31867;&#27861;&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#35843;&#26597;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20851;&#20110;&#36328;&#20307;&#31995;&#32467;&#26500;IoT&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;&#35843;&#26597;&#20013;&#30340;&#23454;&#38469;&#25361;&#25112;&#30340;&#26356;&#22810;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the increase in non-Windows malware threats had turned the focus of the cybersecurity community. Research works on hunting Windows PE-based malwares are maturing, whereas the developments on Linux malware threat hunting are relatively scarce. With the advent of the Internet of Things (IoT) era, smart devices that are getting integrated into human life have become a hackers highway for their malicious activities. The IoT devices employ various Unix-based architectures that follow ELF (Executable and Linkable Format) as their standard binary file specification. This study aims at providing a comprehensive survey on the latest developments in cross-architectural IoT malware detection and classification approaches. Aided by a modern taxonomy, we discuss the feature representations, feature extraction techniques, and machine learning models employed in the surveyed works. We further provide more insights on the practical challenges involved in cross-architectural IoT malwar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#23884;&#20837;&#27604;&#29305;&#24065;&#22320;&#22336;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#38142;&#29615;&#36712;&#36947;&#8221;&#65292;&#29992;&#20110;&#25506;&#27979;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#30005;&#23376;&#29359;&#32618;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.07974</link><description>&lt;p&gt;
&#38142;&#29615;&#36712;&#36947;&#65306;&#27604;&#29305;&#24065;&#21306;&#22359;&#38142;&#30340;&#25299;&#25169;&#22320;&#22336;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Chainlet Orbits: Topological Address Embedding for the Bitcoin Blockchain. (arXiv:2306.07974v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#23884;&#20837;&#27604;&#29305;&#24065;&#22320;&#22336;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8220;&#38142;&#29615;&#36712;&#36947;&#8221;&#65292;&#29992;&#20110;&#25506;&#27979;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#30005;&#23376;&#29359;&#32618;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#29305;&#24065;&#31561;&#21152;&#23494;&#36135;&#24065;&#30340;&#20852;&#36215;&#20351;&#24471;&#20855;&#26377;&#19968;&#23450;&#21311;&#21517;&#24615;&#30340;&#20132;&#26131;&#21464;&#24471;&#26222;&#36941;&#65292;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#21508;&#31181;&#38750;&#27861;&#27963;&#21160;&#30340;&#28608;&#22686;&#65292;&#21253;&#25324;&#21202;&#32034;&#36719;&#20214;&#25903;&#20184;&#21644;&#26263;&#32593;&#24066;&#22330;&#19978;&#30340;&#20132;&#26131;&#31561;&#12290;&#36825;&#20123;&#38750;&#27861;&#27963;&#21160;&#36890;&#24120;&#20351;&#29992;&#27604;&#29305;&#24065;&#20316;&#20026;&#39318;&#36873;&#25903;&#20184;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#38142;&#29615;&#36712;&#36947;&#8221;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#20132;&#26131;&#20013;&#27604;&#29305;&#24065;&#22320;&#22336;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#36827;&#34892;&#22320;&#22336;&#23884;&#20837;&#12290; &#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#21019;&#26032;&#30340;&#22320;&#22336;&#23884;&#20837;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27604;&#29305;&#24065;&#32593;&#32476;&#20013;&#30340;&#30005;&#23376;&#29359;&#32618;&#65292;&#30528;&#37325;&#20851;&#27880;&#20174;&#38750;&#27861;&#34892;&#20026;&#20013;&#20135;&#29983;&#30340;&#29305;&#23450;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#33410;&#28857;&#20998;&#31867;&#23454;&#39564;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of cryptocurrencies like Bitcoin, which enable transactions with a degree of pseudonymity, has led to a surge in various illicit activities, including ransomware payments and transactions on darknet markets. These illegal activities often utilize Bitcoin as the preferred payment method. However, current tools for detecting illicit behavior either rely on a few heuristics and laborious data collection processes or employ computationally inefficient graph neural network (GNN) models that are challenging to interpret.  To overcome the computational and interpretability limitations of existing techniques, we introduce an effective solution called Chainlet Orbits. This approach embeds Bitcoin addresses by leveraging their topological characteristics in transactions. By employing our innovative address embedding, we investigate e-crime in Bitcoin networks by focusing on distinctive substructures that arise from illicit behavior.  The results of our node classification experiments de
&lt;/p&gt;</description></item><item><title>PrivaScissors&#26159;&#19968;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#38388;&#32467;&#26524;&#21644;&#35774;&#22791;&#25968;&#25454;&#12289;&#39044;&#27979;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#36793;&#32536;-&#20113;&#21327;&#20316;&#25512;&#29702;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.07973</link><description>&lt;p&gt;
&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#21327;&#20316;&#25512;&#29702;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PrivaScissors: Enhance the Privacy of Collaborative Inference through the Lens of Mutual Information. (arXiv:2306.07973v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07973
&lt;/p&gt;
&lt;p&gt;
PrivaScissors&#26159;&#19968;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#29992;&#20110;&#20943;&#23569;&#27169;&#22411;&#20013;&#38388;&#32467;&#26524;&#21644;&#35774;&#22791;&#25968;&#25454;&#12289;&#39044;&#27979;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#36793;&#32536;-&#20113;&#21327;&#20316;&#25512;&#29702;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;-&#20113;&#21327;&#20316;&#25512;&#29702;&#20026;&#29289;&#32852;&#32593;&#35774;&#22791;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#20445;&#25252;&#20102;&#21407;&#22987;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21327;&#20316;&#25512;&#29702;&#20173;&#28982;&#20250;&#26292;&#38706;&#35774;&#22791;&#30340;&#25968;&#25454;&#21644;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#22686;&#24378;&#21327;&#20316;&#25512;&#29702;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;PrivaScissors&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#26088;&#22312;&#20943;&#23569;&#27169;&#22411;&#20013;&#38388;&#32467;&#26524;&#21644;&#35774;&#22791;&#25968;&#25454;&#12289;&#39044;&#27979;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PrivaScissors&#22312;&#19981;&#21516;&#25915;&#20987;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-cloud collaborative inference empowers resource-limited IoT devices to support deep learning applications without disclosing their raw data to the cloud server, thus preserving privacy. Nevertheless, prior research has shown that collaborative inference still results in the exposure of data and predictions from edge devices. To enhance the privacy of collaborative inference, we introduce a defense strategy called PrivaScissors, which is designed to reduce the mutual information between a model's intermediate outcomes and the device's data and predictions. We evaluate PrivaScissors's performance on several datasets in the context of diverse attacks and offer a theoretical robustness guarantee.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#38142;DeFi&#27450;&#35784;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;23&#20010;DeFi&#21327;&#35758;&#30340;&#20132;&#26131;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.07972</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#38142;DeFi&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Machine Learning for Multichain DeFi Fraud Detection. (arXiv:2306.07972v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#38142;DeFi&#27450;&#35784;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;23&#20010;DeFi&#21327;&#35758;&#30340;&#20132;&#26131;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2008&#24180;&#27604;&#29305;&#24065;&#25512;&#20986;&#20197;&#26469;&#65292;&#38543;&#30528;&#26080;&#38656;&#35768;&#21487;&#30340;&#21306;&#22359;&#38142;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#24847;&#35782;&#21040;&#23427;&#20204;&#26368;&#36866;&#21512;&#30340;&#29992;&#20363;&#19982;&#20351;&#37329;&#34701;&#31995;&#32479;&#21450;&#20854;&#20248;&#21183;&#21487;&#20379;&#27599;&#20010;&#20154;&#26080;&#32541;&#20351;&#29992;&#32780;&#26080;&#38656;&#20381;&#36182;&#20219;&#20309;&#21463;&#20449;&#20219;&#30340;&#20013;&#20171;&#26377;&#20851;&#12290;&#36328;&#38142;&#26234;&#33021;&#21512;&#32422;&#25552;&#20379;&#20102;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#19982;&#20511;&#36151;&#27744;&#12289;&#33258;&#21160;&#24066;&#22330;&#21046;&#36896;&#21830;&#65288;AMM&#65289;&#20132;&#26131;&#25152;&#12289;&#31283;&#23450;&#24065;&#12289;&#34893;&#29983;&#21697;&#31561;&#36827;&#34892;&#20132;&#20114;&#65292;&#20854;&#32047;&#35745;&#38145;&#23450;&#20215;&#20540;&#24050;&#36229;&#36807;1600&#20159;&#32654;&#20803;&#12290;&#23613;&#31649;DeFi&#24102;&#26469;&#20102;&#39640;&#39069;&#22238;&#25253;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#39118;&#38505;&#12290;&#22810;&#24180;&#26469;&#21457;&#29983;&#20102;&#35768;&#22810;&#37329;&#34701;&#32618;&#26696;&#65292;&#20351;&#24471;&#24694;&#24847;&#27963;&#21160;&#30340;&#26089;&#26399;&#26816;&#27979;&#25104;&#20026;&#20102;&#39640;&#24230;&#20248;&#20808;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#35813;&#25552;&#35758;&#26694;&#26550;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#38142;&#20013;&#25552;&#21462;&#19968;&#32452;&#21151;&#33021;&#65292;&#21253;&#25324;&#26368;&#22823;&#30340;&#20197;&#22826;&#22346;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25910;&#38598;&#30340;&#26368;&#24191;&#27867;DeFi&#21327;&#35758;&#30340;&#20132;&#26131;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65288;23&#20010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the inception of permissionless blockchains with Bitcoin in 2008, it became apparent that their most well-suited use case is related to making the financial system and its advantages available to everyone seamlessly without depending on any trusted intermediaries. Smart contracts across chains provide an ecosystem of decentralized finance (DeFi), where users can interact with lending pools, Automated Market Maker (AMM) exchanges, stablecoins, derivatives, etc. with a cumulative locked value which had exceeded 160B USD. While DeFi comes with high rewards, it also carries plenty of risks. Many financial crimes have occurred over the years making the early detection of malicious activity an issue of high priority. The proposed framework introduces an effective method for extracting a set of features from different chains, including the largest one, Ethereum and it is evaluated over an extensive dataset we gathered with the transactions of the most widely used DeFi protocols (23 in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;PPG&#20449;&#21495;&#31383;&#21475;&#20869;&#30340;&#21487;&#33021;&#24515;&#29575;&#20540;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#32467;&#21512;&#32479;&#35745;&#20998;&#24067;&#30417;&#27979;&#24515;&#29575;&#21464;&#21270;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#65292;&#24182;&#33719;&#24471;&#28085;&#30422;&#24515;&#29575;&#20540;&#33539;&#22260;&#30340;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.07730</link><description>&lt;p&gt;
BeliefPPG: &#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#20174;PPG&#20449;&#21495;&#20013;&#33719;&#24471;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#24515;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
BeliefPPG: Uncertainty-aware Heart Rate Estimation from PPG signals via Belief Propagation. (arXiv:2306.07730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07730
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20351;&#29992;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;PPG&#20449;&#21495;&#31383;&#21475;&#20869;&#30340;&#21487;&#33021;&#24515;&#29575;&#20540;&#20998;&#24067;&#65292;&#28982;&#21518;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#32467;&#21512;&#32479;&#35745;&#20998;&#24067;&#30417;&#27979;&#24515;&#29575;&#21464;&#21270;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#65292;&#24182;&#33719;&#24471;&#28085;&#30422;&#24515;&#29575;&#20540;&#33539;&#22260;&#30340;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#23558;&#24515;&#29575;&#30340;&#28436;&#21464;&#34920;&#31034;&#20026;&#38544;&#34255;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20026;&#32473;&#23450;&#30340;PPG&#20449;&#21495;&#31383;&#21475;&#23548;&#20986;&#21487;&#33021;&#24515;&#29575;&#20540;&#30340;&#20998;&#24067;&#12290;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#65292;&#22312;&#26102;&#38388;&#19978;&#19979;&#25991;&#20013;&#32467;&#21512;&#24515;&#29575;&#21464;&#21270;&#30340;&#32479;&#35745;&#20998;&#24067;&#20197;&#20248;&#21270;&#36825;&#20123;&#20272;&#35745;&#12290;&#20174;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#37327;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#28085;&#30422;&#20102;&#21487;&#33021;&#24515;&#29575;&#20540;&#30340;&#36825;&#20010;&#33539;&#22260;&#65292;&#36825;&#21487;&#20197;&#25429;&#33719;&#22266;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#24847;&#20041;&#19988;&#33391;&#22909;&#26657;&#20934;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel learning-based method that achieves state-of-the-art performance on several heart rate estimation benchmarks extracted from photoplethysmography signals (PPG). We consider the evolution of the heart rate in the context of a discrete-time stochastic process that we represent as a hidden Markov model. We derive a distribution over possible heart rate values for a given PPG signal window through a trained neural network. Using belief propagation, we incorporate the statistical distribution of heart rate changes to refine these estimates in a temporal context. From this, we obtain a quantized probability distribution over the range of possible heart rate values that captures a meaningful and well-calibrated estimate of the inherent predictive uncertainty. We show the robustness of our method on eight public datasets with three different cross-validation experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.07479</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;TikTok&#21644;YouTube&#36825;&#26679;&#30340;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#24179;&#21488;&#30340;&#20915;&#31574;&#31639;&#27861;&#22609;&#36896;&#20102;&#20869;&#23481;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#65292;&#21253;&#25324;&#29983;&#20135;&#32773;&#22312;&#20869;&#23481;&#36136;&#37327;&#19978;&#25237;&#20837;&#22810;&#23569;&#21162;&#21147;&#12290;&#35768;&#22810;&#24179;&#21488;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#20250;&#20135;&#29983;&#36328;&#26102;&#38388;&#30340;&#28608;&#21169;&#65292;&#22240;&#20026;&#20170;&#22825;&#29983;&#20135;&#30340;&#20869;&#23481;&#20250;&#24433;&#21709;&#26410;&#26469;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20135;&#29983;&#30340;&#28608;&#21169;&#65292;&#20998;&#26512;&#20102;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#29983;&#20135;&#30340;&#20869;&#23481;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20687;Hedge&#21644;EXP3&#36825;&#26679;&#30340;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#22320;&#65292;&#20869;&#23481;&#36136;&#37327;&#22312;&#23398;&#20064;&#29575;&#26041;&#38754;&#26377;&#19978;&#38480;&#65292;&#24182;&#19988;&#38543;&#30528;&#20856;&#22411;&#23398;&#20064;&#29575;&#36827;&#23637;&#32780;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#24809;&#32602;&#21019;&#24314;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#29983;&#20135;&#32773;&#8212;&#8212;&#27491;&#30830;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#31574;&#30053;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07419</link><description>&lt;p&gt;
DeepTransition&#65306;&#21487;&#34892;&#24615;&#23548;&#33268;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#29289;&#22312;&#25913;&#21464;&#36816;&#21160;&#36895;&#24230;&#26102;&#33021;&#22815;&#26080;&#32541;&#22320;&#36716;&#25442;&#27493;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21487;&#34892;&#24615;&#65288;&#21363;&#36991;&#20813;&#36300;&#20498;&#65289;&#20195;&#34920;&#27493;&#24577;&#36716;&#25442;&#30340;&#19968;&#20010;&#37325;&#35201;&#26631;&#20934;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;&#12290;&#19968;&#33268;&#20110;&#22235;&#36275;&#21160;&#29289;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#65292;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#22320;&#24418;&#65288;&#21363;&#31359;&#36234;&#36830;&#32493;&#38388;&#38548;&#65289;&#23545;&#24378;&#21046;&#27493;&#24577;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#36275;-&#36454;&#27493;&#24577;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07308</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21551;&#21457;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#30417;&#30563;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Hyperspectral Inpainting with the Optimisation inspired Deep Neural Network Prior. (arXiv:2306.07308v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20462;&#22797;&#31639;&#27861;LRS-PnP-DIP&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#31934;&#30830;&#39044;&#27979;&#32570;&#22833;&#20687;&#32032;&#21644;&#24102;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#25110;&#36229;&#36807;&#20102;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#22270;&#20687;&#20855;&#26377;&#25104;&#30334;&#19978;&#21315;&#20010;&#31364;&#24102;&#35889;&#27573;&#65292;&#20256;&#36882;&#20102;&#22823;&#37327;&#30340;&#31354;&#38388;&#21644;&#35889;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20202;&#22120;&#35823;&#24046;&#21644;&#22823;&#27668;&#21464;&#21270;&#65292;&#23454;&#36341;&#20013;&#24471;&#21040;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#24120;&#24120;&#34987;&#22122;&#22768;&#21644;&#22351;&#28857;&#27745;&#26579;&#65292;&#23548;&#33268;&#32570;&#22833;&#20449;&#24687;&#21487;&#33021;&#20005;&#37325;&#30772;&#22351;&#21518;&#32493;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#21462;&#26679;&#28857;&#20462;&#22797;&#31639;&#27861;&#65292;&#31216;&#20026;&#20302;&#31209;&#31232;&#30095;&#32422;&#26463;&#25554;&#20837;&#25773;&#25918;&#31639;&#27861;&#65288;LRS-PnP&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22270;&#20687;&#30340;&#25152;&#26377;&#20809;&#35889;&#24102;&#37117;&#20002;&#22833;&#65292;LRS-PnP&#20063;&#33021;&#22815;&#39044;&#27979;&#32570;&#22833;&#30340;&#20687;&#32032;&#21644;&#24102;&#12290;&#23558;LRS-PnP&#19982;Deep Image Prior&#65288;DIP&#65289;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#31216;&#20026;LRS-PnP-DIP&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LRS-PnP-DIP&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#20462;&#22797;&#24615;&#33021;&#25110;&#32988;&#36807;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Image (HSI)s cover hundreds or thousands of narrow spectral bands, conveying a wealth of spatial and spectral information. However, due to the instrumental errors and the atmospheric changes, the HSI obtained in practice are often contaminated by noise and dead pixels(lines), resulting in missing information that may severely compromise the subsequent applications. We introduce here a novel HSI missing pixel prediction algorithm, called Low Rank and Sparsity Constraint Plug-and-Play (LRS-PnP). It is shown that LRS-PnP is able to predict missing pixels and bands even when all spectral bands of the image are missing. The proposed LRS-PnP algorithm is further extended to a self-supervised model by combining the LRS-PnP with the Deep Image Prior (DIP), called LRS-PnP-DIP. In a series of experiments with real data, It is shown that the LRS-PnP-DIP either achieves state-of-the-art inpainting performance compared to other learning-based methods, or outperforms them.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07261</link><description>&lt;p&gt;
&#21462;&#28040;&#19971;&#24180;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19971;&#24180;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#35823;&#24046;&#29575;&#30456;&#31561;&#12290;&#36825;&#39033;&#24037;&#20316;&#21551;&#21160;&#20102;&#25968;&#30334;&#31687;&#35770;&#25991;&#65292;&#22768;&#31216;&#33021;&#22815;&#25913;&#36827;&#21518;&#22788;&#29702;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#25968;&#21315;&#20010;&#27169;&#22411;&#35780;&#20272;&#30340;&#23454;&#35777;&#35780;&#20272;&#26469;&#35780;&#20272;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#35770;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#22256;&#25200;&#20102;&#20197;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#19968;&#20010;&#19982;&#20351;&#29992;&#19981;&#21516;&#30340;&#26080;&#32422;&#26463;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#26377;&#20851;&#12290;&#21478;&#19968;&#20010;&#28041;&#21450;&#23454;&#29616;&#19981;&#21516;&#30340;&#32422;&#26463;&#25918;&#26494;&#27700;&#24179;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21462;&#28040;&#22788;&#29702;&#65292;&#22823;&#33268;&#23545;&#24212;&#20110;&#21518;&#22788;&#29702;&#30340;&#21453;&#28436;&#12290;&#21462;&#28040;&#22788;&#29702;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#20351;&#29992;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#21644;&#25918;&#26494;&#32423;&#21035;&#30340;&#26041;&#27861;&#12290;&#35299;&#35835;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;absmax&#30340;&#20998;&#22359;&#37327;&#21270;&#65292;&#25512;&#20986;&#35813;&#26041;&#27861;&#19981;&#26159;&#20449;&#24687;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;&#20316;&#32773;&#20248;&#21270;L1&#37325;&#26500;&#35823;&#24046;&#65292;&#25552;&#20986;&#25913;&#36827;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#22359;&#37327;&#21270;&#12290;&#22312;&#23567;&#22359;&#37327;&#21270;&#37324;&#65292;&#20004;&#31181;&#26041;&#27861;&#24615;&#33021;&#31867;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.06965</link><description>&lt;p&gt;
NF4&#19981;&#26159;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#65288;&#24182;&#19988;&#36825;&#26159;&#22909;&#20107;&#65289;
&lt;/p&gt;
&lt;p&gt;
NF4 Isn't Information Theoretically Optimal (and that's Good). (arXiv:2306.06965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06965
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;absmax&#30340;&#20998;&#22359;&#37327;&#21270;&#65292;&#25512;&#20986;&#35813;&#26041;&#27861;&#19981;&#26159;&#20449;&#24687;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;&#20316;&#32773;&#20248;&#21270;L1&#37325;&#26500;&#35823;&#24046;&#65292;&#25552;&#20986;&#25913;&#36827;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#22359;&#37327;&#21270;&#12290;&#22312;&#23567;&#22359;&#37327;&#21270;&#37324;&#65292;&#20004;&#31181;&#26041;&#27861;&#24615;&#33021;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#19968;&#20123;&#20851;&#20110;&#22522;&#20110;absmax&#30340;&#20998;&#22359;&#37327;&#21270;&#30340;&#31616;&#21333;&#35745;&#31639;&#21644;&#23454;&#39564;&#65292;&#35813;&#26041;&#27861;&#34987;Dettmers&#31561;&#20154;&#22312;2023&#24180;&#25552;&#20986;&#65292;&#20182;&#20204;&#25552;&#20986;&#30340;NF4&#25968;&#25454;&#31867;&#22411;&#34987;&#35748;&#20026;&#26159;&#29992;&#20110;&#34920;&#31034;&#27491;&#24577;&#20998;&#24067;&#26435;&#37325;&#26102;&#30340;&#20449;&#24687;&#29702;&#35770;&#26368;&#20248;&#12290;&#25105;&#23637;&#31034;&#20102;&#36825;&#19981;&#23436;&#20840;&#26159;&#24773;&#20917;&#65292;&#22240;&#20026;&#35201;&#37327;&#21270;&#30340;&#20540;&#30340;&#20998;&#24067;&#21462;&#20915;&#20110;&#22359;&#22823;&#23567;&#12290;&#25105;&#35797;&#22270;&#24212;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#22522;&#20110;&#26368;&#23567;&#21270;&#26399;&#26395;L1&#37325;&#26500;&#35823;&#24046;&#32780;&#19981;&#26159;&#20998;&#20301;&#25968;&#26041;&#27861;&#25512;&#23548;&#20986;&#19968;&#20010;&#25913;&#36827;&#30340;&#32534;&#30721;&#12290;&#36825;&#23548;&#33268;&#22312;&#26356;&#22823;&#30340;&#37327;&#21270;&#22359;&#22823;&#23567;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#22312;&#36739;&#23567;&#30340;&#22359;&#22823;&#23567;&#26102;&#20004;&#20010;&#32534;&#30721;&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This note shares some simple calculations and experiments related to absmax-based blockwise quantization, as used in Dettmers et al., 2023. Their proposed NF4 data type is said to be information theoretically optimal for representing normally distributed weights. I show that this can't quite be the case, as the distribution of the values to be quantized depends on the block-size. I attempt to apply these insights to derive an improved code based on minimizing the expected L1 reconstruction error, rather than the quantile based method. This leads to improved performance for larger quantization block sizes, while both codes perform similarly at smaller block sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#25104;&#21151;&#23558;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASTGNN&#65289;&#26412;&#22320;&#21270;&#33267;&#26497;&#33268;&#65292;&#26080;&#38656;&#31354;&#38388;&#22270;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06930</link><description>&lt;p&gt;
&#23616;&#37096;&#33258;&#36866;&#24212;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Localised Adaptive Spatial-Temporal Graph Neural Network. (arXiv:2306.06930v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#25104;&#21151;&#23558;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASTGNN&#65289;&#26412;&#22320;&#21270;&#33267;&#26497;&#33268;&#65292;&#26080;&#38656;&#31354;&#38388;&#22270;&#21363;&#21487;&#36798;&#21040;&#21516;&#26679;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#25277;&#35937;&#21644;&#27169;&#25311;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#20197;&#21450;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#26412;&#22320;&#21270;&#31354;&#38388;-&#26102;&#38388;&#22270;&#27169;&#22411;&#65311;&#25105;&#20204;&#23558;&#33539;&#22260;&#38480;&#23450;&#22312;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ASTGNN&#65289;&#19978;&#65292;&#36825;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26412;&#22320;&#21270;&#26041;&#27861;&#28041;&#21450;&#23558;&#31354;&#38388;&#22270;&#37051;&#25509;&#30697;&#38453;&#31232;&#30095;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#22270;&#31232;&#30095;&#21270;&#65288;AGS&#65289;&#65292;&#19968;&#31181;&#25104;&#21151;&#23454;&#29616;ASTGNN&#26412;&#22320;&#21270;&#30340;&#22270;&#31232;&#30095;&#21270;&#31639;&#27861;&#65288;&#23436;&#20840;&#26412;&#22320;&#21270;&#65289;&#12290;&#25105;&#20204;&#23558;AGS&#24212;&#29992;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;ASTGNN&#26550;&#26500;&#21644;9&#20010;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;ASTGNN&#20013;&#30340;&#31354;&#38388;&#22270;&#21487;&#20197;&#31232;&#30095;&#21270;&#36229;&#36807;99.5&#65285;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#27979;&#35797;&#20934;&#30830;&#24615;&#30340;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;ASTGNN&#23436;&#20840;&#26412;&#22320;&#21270;&#65292;&#21464;&#24471;&#26080;&#22270;&#19988;&#32431;&#31929;&#30340;&#26102;&#38388;&#65292;&#22823;&#22810;&#25968;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#20063;&#27809;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal graph models are prevailing for abstracting and modelling spatial and temporal dependencies. In this work, we ask the following question: whether and to what extent can we localise spatial-temporal graph models? We limit our scope to adaptive spatial-temporal graph neural networks (ASTGNNs), the state-of-the-art model architecture. Our approach to localisation involves sparsifying the spatial graph adjacency matrices. To this end, we propose Adaptive Graph Sparsification (AGS), a graph sparsification algorithm which successfully enables the localisation of ASTGNNs to an extreme extent (fully localisation). We apply AGS to two distinct ASTGNN architectures and nine spatial-temporal datasets. Intriguingly, we observe that spatial graphs in ASTGNNs can be sparsified by over 99.5\% without any decline in test accuracy. Furthermore, even when ASTGNNs are fully localised, becoming graph-less and purely temporal, we record no drop in accuracy for the majority of tested datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#12290;&#38024;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#31867;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35777;&#26126;&#65292;&#24182;&#25351;&#20986;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06924</link><description>&lt;p&gt;
TASRA: &#19968;&#20221;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI. (arXiv:2306.06924v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#12290;&#38024;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#31867;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35777;&#26126;&#65292;&#24182;&#25351;&#20986;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#20316;&#21697;&#24050;&#32463;&#30830;&#35748;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#32473;&#20154;&#31867;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#21644;&#28781;&#32477;&#32423;&#30340;&#39118;&#38505;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#23581;&#35797;&#36807;&#36827;&#34892;&#20840;&#38754;&#24402;&#32435;&#36825;&#20123;&#39118;&#38505;&#12290;&#35768;&#22810;&#20840;&#38754;&#24615;&#30340;&#20998;&#31867;&#27861;&#37117;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#26377;&#20123;&#26159;&#26377;&#29992;&#30340;&#8212;&#8212;&#23588;&#20854;&#26159;&#22914;&#26524;&#23427;&#20204;&#25581;&#31034;&#20102;&#26032;&#30340;&#39118;&#38505;&#25110;&#23433;&#20840;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#65306;&#21738;&#20123;&#34892;&#20026;&#23548;&#33268;&#20102;&#39118;&#38505;&#65292;&#34892;&#21160;&#32773;&#26159;&#21542;&#32479;&#19968;&#65292;&#20182;&#20204;&#26159;&#21542;&#26159;&#33988;&#24847;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25925;&#20107;&#26469;&#35828;&#26126;&#21508;&#31181;&#39118;&#38505;&#31867;&#22411;&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#65292;&#21253;&#25324;&#26469;&#33258;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24847;&#22806;&#30456;&#20114;&#20316;&#29992;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;&#26469;&#33258;&#33988;&#24847;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ViT&#35270;&#35273;&#36716;&#25442;&#22120;&#22788;&#29702;&#33016;&#37096;X-ray&#22270;&#20687;&#26469;&#33258;&#21160;&#35786;&#26029;COVID-19&#30340;&#21019;&#26032;&#26041;&#27861;&#65292; &#22312;&#20108;&#20803;&#21644;&#19977;&#20803;&#20998;&#31867;&#34920;&#29616;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06914</link><description>&lt;p&gt;
&#22522;&#20110;ViT&#35270;&#35273;&#36716;&#25442;&#22120;&#20998;&#26512;&#33016;&#37096;X-ray&#22270;&#20687;&#20197;&#22686;&#24378;COVID-19&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Enhancing COVID-19 Diagnosis through Vision Transformer-Based Analysis of Chest X-ray Images. (arXiv:2306.06914v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ViT&#35270;&#35273;&#36716;&#25442;&#22120;&#22788;&#29702;&#33016;&#37096;X-ray&#22270;&#20687;&#26469;&#33258;&#21160;&#35786;&#26029;COVID-19&#30340;&#21019;&#26032;&#26041;&#27861;&#65292; &#22312;&#20108;&#20803;&#21644;&#19977;&#20803;&#20998;&#31867;&#34920;&#29616;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20896;&#32954;&#28814;&#30340;&#29190;&#21457;&#24341;&#36215;&#20102;&#20840;&#29699;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#21361;&#26426;&#65292;&#38656;&#35201;&#36890;&#36807;&#19981;&#21516;&#30340;&#35786;&#26029;&#26041;&#24335;&#23545;&#20010;&#20307;&#36827;&#34892;&#35786;&#26029;&#12290;&#25918;&#23556;&#23398;&#24433;&#20687;&#23398;&#65292;&#29305;&#21035;&#26159;X-ray&#25104;&#20687;&#65292;&#24050;&#34987;&#20844;&#35748;&#20026;&#26159;&#26816;&#27979;&#21644;&#25551;&#36848;COVID-19&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;X-ray&#22270;&#20687;&#20013;&#20851;&#20110;&#30149;&#27602;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25552;&#39640;&#35786;&#26029;&#31934;&#24230;&#30340;&#26041;&#27861;&#23398;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#35786;&#26029; COVID-19&#65292;&#36890;&#36807;&#31934;&#35843;&#39044;&#35757;&#32451;&#30340; Vision Transformer (ViT) &#27169;&#22411;&#26469;&#22788;&#29702;&#21407;&#22987;&#33016;&#37096; X-ray &#22270;&#20687;&#12290;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#20108;&#20803;&#20998;&#31867;&#34920;&#29616;&#65288;&#21306;&#20998;COVID-19&#21644;&#27491;&#24120;&#24773;&#20917;&#65289;&#21644;&#19977;&#20803;&#20998;&#31867;&#34920;&#29616;&#65288;&#21306;&#20998;COVID-19&#12289;&#30149;&#27602;&#24615;&#32954;&#28814;&#21644;&#27491;&#24120;&#24773;&#20917;&#65289;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of 2019 Coronavirus (COVID-19) has engendered a momentous global health crisis, necessitating the identification of the ailment in individuals through diverse diagnostic modalities. Radiological imaging, particularly the deployment of X-ray imaging, has been recognized as a pivotal instrument in the detection and characterization of COVID-19. Recent investigations have unveiled invaluable insights pertaining to the virus within X-ray images, instigating the exploration of methodologies aimed at augmenting diagnostic accuracy through the utilization of artificial intelligence (AI) techniques. The current research endeavor posits an innovative framework for the automated diagnosis of COVID-19, harnessing raw chest X-ray images, specifically by means of fine-tuning pre-trained Vision Transformer (ViT) models. The developed models were appraised in terms of their binary classification performance, discerning COVID-19 from Normal cases, as well as their ternary classification per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06674</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#20248;&#21270;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#20256;&#32479;&#27714;&#35299;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#37327;&#36739;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#35268;&#27169;&#36739;&#22823;&#12289;&#26102;&#38388;&#25935;&#24863;&#30340;&#38382;&#39064;&#19978;&#26356;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24555;&#36895;&#26368;&#20248;&#35299;&#36924;&#36817;&#22120;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22823;&#20852;&#36259;&#65292;&#20294;&#26159;&#23558;&#32422;&#26463;&#26465;&#20214;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DeepLDE&#30340;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22312;&#19981;&#20351;&#29992;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23547;&#25214;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23558;&#31561;&#24335;&#32422;&#26463;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#23545;&#19981;&#31561;&#24335;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeepLDE&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#20165;&#38752;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#31561;&#24335;&#32422;&#26463;&#65292;&#38656;&#35201;&#31561;&#24335;&#23884;&#20837;&#30340;&#24110;&#21161;&#12290;&#22312;&#20984;&#12289;&#38750;&#20984;&#21644;&#20132;&#27969;&#26368;&#20248;&#28526;&#27969;&#65288;AC-OPF&#65289;&#38382;&#39064;&#30340;&#27169;&#25311;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepLDE&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19988;&#22987;&#32456;&#20445;&#35777;&#21487;&#34892;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#24555;&#36895;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#24182;&#22312;&#20854;&#20013;&#34701;&#20837;&#20998;&#24067;&#22806;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#37325;&#24314;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#37325;&#24314;&#36895;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#25928;&#26524;&#65292;&#26159;&#20998;&#26512;&#27963;&#20307;&#29983;&#29289;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.06408</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#30340;&#24555;&#36895;&#20809;&#22330;&#19977;&#32500;&#26174;&#24494;&#38236;&#21450;&#20854;&#20998;&#24067;&#22806;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fast light-field 3D microscopy with out-of-distribution detection and adaptation through Conditional Normalizing Flows. (arXiv:2306.06408v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#24555;&#36895;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#24182;&#22312;&#20854;&#20013;&#34701;&#20837;&#20998;&#24067;&#22806;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#37325;&#24314;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#37325;&#24314;&#36895;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#25928;&#26524;&#65292;&#26159;&#20998;&#26512;&#27963;&#20307;&#29983;&#29289;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#19977;&#32500;&#33639;&#20809;&#26174;&#24494;&#38236;&#23545;&#20110;&#30417;&#27979;&#31070;&#32463;&#27963;&#21160;&#31561;&#29983;&#29289;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26694;&#26550;&#30340;&#24555;&#36895;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#24182;&#23558;&#20998;&#24067;&#22806;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#26426;&#21046;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#30830;&#20445;&#37325;&#24314;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#37325;&#24314;&#36895;&#24230;&#21644;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#25928;&#26524;&#65292;&#26159;&#20998;&#26512;&#27963;&#20307;&#29983;&#29289;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time 3D fluorescence microscopy is crucial for the spatiotemporal analysis of live organisms, such as neural activity monitoring. The eXtended field-of-view light field microscope (XLFM), also known as Fourier light field microscope, is a straightforward, single snapshot solution to achieve this. The XLFM acquires spatial-angular information in a single camera exposure. In a subsequent step, a 3D volume can be algorithmically reconstructed, making it exceptionally well-suited for real-time 3D acquisition and potential analysis. Unfortunately, traditional reconstruction methods (like deconvolution) require lengthy processing times (0.0220 Hz), hampering the speed advantages of the XLFM. Neural network architectures can overcome the speed constraints at the expense of lacking certainty metrics, which renders them untrustworthy for the biomedical realm. This work proposes a novel architecture to perform fast 3D reconstructions of live immobilized zebrafish neural activity based on a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06098</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#21487;&#20197;&#20934;&#30830;&#22320;&#21387;&#32553;&#39044;&#22788;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Can Accurately Compress Preconditioners. (arXiv:2306.06098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#23545;&#26799;&#24230;&#20449;&#24687;&#36827;&#34892;&#21387;&#32553;&#65288;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#65289;&#65292;&#23558;&#39044;&#22788;&#29702;&#22120;&#30340;&#23384;&#20648;&#25104;&#26412;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#35268;&#27169;&#30340;&#20108;&#38454;&#20449;&#24687;&#26159;&#25913;&#36827;&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#24615;&#33021;&#30340;&#20027;&#35201;&#36884;&#24452;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31934;&#30830;&#20840;&#30697;&#38453;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22914;&#20840;&#30697;&#38453;Adagrad&#65288;GGT&#65289;&#25110;&#26080;&#30697;&#38453;&#36817;&#20284;&#26354;&#29575;&#65288;M-FAC&#65289;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#65292;&#20063;&#20250;&#36935;&#21040;&#24040;&#22823;&#30340;&#23384;&#20648;&#25104;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#23384;&#20648;&#26799;&#24230;&#30340;&#28369;&#21160;&#31383;&#21475;&#65292;&#20854;&#23384;&#20648;&#38656;&#27714;&#22312;&#27169;&#22411;&#32500;&#24230;&#20013;&#26159;&#25104;&#20493;&#22686;&#21152;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#38169;&#35823;&#21453;&#39304;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#23558;&#39044;&#22788;&#29702;&#22120;&#21387;&#32553;&#22810;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#25910;&#25947;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23558;&#26799;&#24230;&#20449;&#24687;&#39304;&#20837;&#39044;&#22788;&#29702;&#22120;&#20043;&#21069;&#36890;&#36807;&#31232;&#30095;&#21270;&#25110;&#20302;&#31209;&#21387;&#32553;&#21387;&#32553;&#26799;&#24230;&#20449;&#24687;&#65292;&#23558;&#21387;&#32553;&#35823;&#24046;&#21453;&#39304;&#21040;&#26410;&#26469;&#30340;&#36845;&#20195;&#20013;&#12290;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging second-order information at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to medium-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via an efficient and simple-to-implement error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Extensive experiments on deep neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.06024</link><description>&lt;p&gt;
&#21487;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Self-Interpretable Time Series Prediction with Counterfactual Explanations. (arXiv:2306.06024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;CounTS&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#20026;&#21487;&#35299;&#37322;&#24615;&#24314;&#27169;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20110;&#20687;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24605;&#36335;&#65292;&#26088;&#22312;&#24320;&#21457;&#20986;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#34987;&#31216;&#20026;Counterfactual Time Series&#65288;CounTS&#65289;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29983;&#25104;&#21453;&#20107;&#23454;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26102;&#38388;&#24207;&#21015;&#32465;&#26550;&#12289;&#34892;&#21160;&#21644;&#39044;&#27979;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05739</link><description>&lt;p&gt;
&#36291;&#36801;&#20110;&#26641;&#31354;&#38388;&#65306;&#36830;&#32493;&#30340;&#26641;&#24418;&#31995;&#32479;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;
&lt;/p&gt;
&lt;p&gt;
Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#29992;&#20110;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#26041;&#27861;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#25928;&#26524;&#65292;&#21487;&#29992;&#20110;&#21152;&#36895;&#29983;&#21629;&#31185;&#23398;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#36827;&#21270;&#31995;&#32479;&#23398;&#29616;&#22312;&#26159;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#65292;&#21487;&#20197;&#38416;&#26126;&#29983;&#21629;&#26089;&#26399;&#25903;&#31995;&#21644;&#20256;&#26579;&#30149;&#30340;&#36215;&#28304;&#21644;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#33021;&#30340;&#26641;&#30340;&#24191;&#38420;&#31354;&#38388;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#31995;&#32479;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#20102;&#26641;&#24418;&#31995;&#32479;&#25506;&#32034;&#21644;&#25512;&#26029;&#65292;&#20351;&#26799;&#24230;&#35745;&#31639;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#36830;&#32493;&#30340;&#25918;&#26494;&#26041;&#24335;&#20801;&#35768;&#22312;&#26377;&#26681;&#21644;&#26080;&#26681;&#26641;&#20013;&#36328;&#36234;&#26641;&#31354;&#38388;&#65292;&#19988;&#19981;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#30340;&#26080;&#26681;&#26641;&#25512;&#26029;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#20013;&#20934;&#30830;&#22320;&#25512;&#26029;&#20986;&#26641;&#21644;&#26641;&#26681;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#20063;&#24456;&#26377;&#25928;&#65292;&#25105;&#20204;&#22312;&#39052;&#21475;&#21160;&#29289;&#30340;&#31995;&#32479;&#21457;&#32946;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#20107;&#23454;&#19978;&#65292;&#20165;&#20855;&#26377;&#36229;&#25351;&#25968;&#20449;&#21495;&#30340;&#23569;&#25968;&#22522;&#22240;&#36890;&#24120;&#36275;&#20197;&#20998;&#36776;&#33034;&#26894;&#21160;&#29289;&#30340;&#20027;&#35201;&#35889;&#31995;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#21152;&#36895;&#21457;&#29616;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26032;&#36827;&#21270;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is now fundamental in life sciences, providing insights into the earliest branches of life and the origins and spread of epidemics. However, finding suitable phylogenies from the vast space of possible trees remains challenging. To address this problem, for the first time, we perform both tree exploration and inference in a continuous space where the computation of gradients is possible. This continuous relaxation allows for major leaps across tree space in both rooted and unrooted trees, and is less susceptible to convergence to local minima. Our approach outperforms the current best methods for inference on unrooted trees and, in simulation, accurately infers the tree and root in ultrametric cases. The approach is effective in cases of empirical data with negligible amounts of data, which we demonstrate on the phylogeny of jawed vertebrates. Indeed, only a few genes with an ultrametric signal were generally sufficient for resolving the major lineages of vertebrate. With
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05649</link><description>&lt;p&gt;
&#20351;&#29992;CVXPY&#35268;&#23450;&#21644;&#35299;&#20915;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Specifying and Solving Robust Empirical Risk Minimization Problems Using CVXPY. (arXiv:2306.05649v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#21442;&#25968;&#34987;&#36873;&#20026;&#20351;&#24471;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#32473;&#23450;&#30340;&#20984;&#19981;&#30830;&#23450;&#24615;&#38598;&#20869;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#32463;&#39564;&#25439;&#22833;&#12290;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34920;&#36798;&#20026;&#35299;&#26512;&#24418;&#24335;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#21270;&#20351;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#65292;&#36825;&#23558;&#19968;&#20010;min-max&#38382;&#39064;&#36716;&#25442;&#20026;&#19968;&#20010;min-min&#38382;&#39064;&#12290;&#23545;&#20598;&#21270;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24456;&#28902;&#29712;&#20063;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#36825;&#20010;&#23545;&#20598;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20174;&#19968;&#20010;&#19968;&#33324;&#30340;&#20984;&#25439;&#22833;&#31867;&#20013;&#25429;&#25417;&#35768;&#22810;&#26631;&#20934;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#25351;&#23450;&#20219;&#20309;&#21487;&#20197;&#29992;&#32426;&#24459;&#21270;&#20984;&#35268;&#21010;&#65288;DCP&#65289;&#32422;&#26463;&#34920;&#31034;&#30340;&#22797;&#26434;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider robust empirical risk minimization (ERM), where model parameters are chosen to minimize the worst-case empirical loss when each data point varies over a given convex uncertainty set. In some simple cases, such problems can be expressed in an analytical form. In general the problem can be made tractable via dualization, which turns a min-max problem into a min-min problem. Dualization requires expertise and is tedious and error-prone. We demonstrate how CVXPY can be used to automate this dualization procedure in a user-friendly manner. Our framework allows practitioners to specify and solve robust ERM problems with a general class of convex losses, capturing many standard regression and classification problems. Users can easily specify any complex uncertainty set that is representable via disciplined convex programming (DCP) constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04859</link><description>&lt;p&gt;
&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#19982;ML&#22686;&#24378;&#22411;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Island-based Random Dynamic Voltage Scaling vs ML-Enhanced Power Side-Channel Attacks. (arXiv:2306.04859v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#23707;&#23679;&#30340;&#38543;&#26426;&#21160;&#24577;&#30005;&#21387;&#35843;&#33410;&#65288;iRDVS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#38450;&#33539;&#21151;&#29575;&#20391;&#20449;&#36947;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#29420;&#31435;&#30005;&#21387;&#23707;&#30340;&#25968;&#37327;&#23545;&#20449;&#22122;&#27604;&#21644;&#36712;&#36857;&#38169;&#20301;&#30340;&#24433;&#21709;&#12290;&#20316;&#20026;&#25105;&#20204;&#23545;&#38169;&#20301;&#30340;&#20998;&#26512;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#25915;&#20987;&#65292;&#23545;&#20110;&#20855;&#26377;&#19977;&#20010;&#25110;&#26356;&#23569;&#29420;&#31435;&#30005;&#21387;&#30340;&#31995;&#32479;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#22235;&#20010;&#30005;&#21387;&#23707;&#30340;iRDVS&#22312;200k&#21152;&#23494;&#36319;&#36394;&#19979;&#26080;&#27861;&#34987;&#30772;&#35299;&#65292;&#35828;&#26126;iRDVS&#21487;&#20197;&#26377;&#25928;&#12290;&#25105;&#20204;&#26368;&#21518;&#36890;&#36807;&#25551;&#36848;&#19968;&#20010;12&#32435;&#31859;FinFet&#24037;&#33402;&#19979;&#30340;iRDVS&#27979;&#35797;&#33455;&#29255;&#26469;&#32467;&#26463;&#35762;&#35805;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#21464;&#20307;&#30340;AES-256&#21152;&#36895;&#22120;&#65292;&#25152;&#26377;&#36825;&#20123;&#21152;&#36895;&#22120;&#22343;&#28304;&#33258;&#21516;&#19968;RTL&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#21516;&#27493;&#26680;&#24515;&#65292;&#19968;&#20010;&#27809;&#26377;&#20445;&#25252;&#30340;&#24322;&#27493;&#26680;&#24515;&#65292;&#20197;&#21450;&#19968;&#20010;&#20351;&#29992;&#24322;&#27493;&#36923;&#36753;&#37319;&#29992;iRDVS&#25216;&#26415;&#30340;&#26680;&#24515;&#12290;&#33455;&#29255;&#30340;&#23454;&#39564;&#23460;&#27979;&#37327;&#34920;&#26126;&#65292;&#20004;&#20010;&#26410;&#21463;&#20445;&#25252;&#30340;&#21464;&#20307;&#37117;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe and analyze an island-based random dynamic voltage scaling (iRDVS) approach to thwart power side-channel attacks. We first analyze the impact of the number of independent voltage islands on the resulting signal-to-noise ratio and trace misalignment. As part of our analysis of misalignment, we propose a novel unsupervised machine learning (ML) based attack that is effective on systems with three or fewer independent voltages. Our results show that iRDVS with four voltage islands, however, cannot be broken with 200k encryption traces, suggesting that iRDVS can be effective. We finish the talk by describing an iRDVS test chip in a 12nm FinFet process that incorporates three variants of an AES-256 accelerator, all originating from the same RTL. This included a synchronous core, an asynchronous core with no protection, and a core employing the iRDVS technique using asynchronous logic. Lab measurements from the chips indicated that both unprotected variants failed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#21644;&#23548;&#33322;&#65292;&#21516;&#26102;&#32467;&#21512;PID&#25511;&#21046;&#21644;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.03951</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-Based Control of CrazyFlie 2.X Quadrotor. (arXiv:2306.03951v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#21644;&#23548;&#33322;&#65292;&#21516;&#26102;&#32467;&#21512;PID&#25511;&#21046;&#21644;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#26356;&#21152;&#31934;&#30830;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#26088;&#22312;&#25506;&#32034;&#32463;&#20856;&#25511;&#21046;&#31639;&#27861;&#65288;&#22914;PID&#65289;&#21644;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#23454;&#29992;&#30340;&#25511;&#21046;&#26426;&#21046;&#26469;&#25511;&#21046;CrazyFlie 2.X&#22235;&#36724;&#39134;&#34892;&#22120;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#25191;&#34892;PID&#35843;&#25972;&#65292;&#27425;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;&#31532;&#19968;&#20010;&#20219;&#21153;&#30340;&#32463;&#39564;&#65292;&#36890;&#36807;&#19982;&#28783;&#22612;&#23450;&#20301;&#31995;&#32479;&#38598;&#25104;&#23454;&#29616;&#23548;&#33322;&#25511;&#21046;&#12290;&#22312;&#23548;&#33322;&#26041;&#38754;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#26377;&#38480;&#36816;&#21160;&#22522;&#20803;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#31163;&#25955;&#23548;&#33322;&#38382;&#39064;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36830;&#32493;&#23548;&#33322;&#12290; RL&#35757;&#32451;&#30340;&#27169;&#25311;&#23558;&#22312;gym-pybullet-drones&#19978;&#36827;&#34892;&#65292;&#35813;&#24179;&#21488;&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#28304;gym&#29615;&#22659;&#65292;&#24182;&#20351;&#29992;stable-baselines3&#25552;&#20379;RL&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of the project is to explore synergies between classical control algorithms such as PID and contemporary reinforcement learning algorithms to come up with a pragmatic control mechanism to control the CrazyFlie 2.X quadrotor. The primary objective would be performing PID tuning using reinforcement learning strategies. The secondary objective is to leverage the learnings from the first task to implement control for navigation by integrating with the lighthouse positioning system. Two approaches are considered for navigation, a discrete navigation problem using Deep Q-Learning with finite predefined motion primitives, and deep reinforcement learning for a continuous navigation approach. Simulations for RL training will be performed on gym-pybullet-drones, an open-source gym-based environment for reinforcement learning, and the RL implementations are provided by stable-baselines3
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#35843;&#26597;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.02781</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey of Generative AI Applications. (arXiv:2306.02781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#35843;&#26597;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#32467;&#26500;&#21644;&#23545;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#35813;&#35843;&#26597;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#35206;&#30422;&#20102;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#28216;&#25103;&#21644;&#33041;&#20449;&#24687;&#31561;&#21333;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.01248</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;LLMs&#22312;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#20934;&#22791;&#24773;&#20917;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#19968;&#30452;&#26159;&#37319;&#29992;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#29983;&#25104;&#26356;&#33258;&#28982;&#21644;&#36830;&#36143;&#25688;&#35201;&#33021;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#24182;&#20855;&#26377;&#25991;&#26412;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#38382;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#24050;&#20934;&#22791;&#22909;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#21028;&#20915;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#25277;&#35937;&#24615;&#25688;&#35201;&#27169;&#22411;&#21644;&#36890;&#29992;&#39046;&#22495;&#30340;LLMs&#24212;&#29992;&#20110;&#21360;&#24230;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#65292;&#24182;&#26816;&#26597;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#25688;&#35201;&#36136;&#37327;&#30340;&#26631;&#20934;&#24230;&#37327;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#34394;&#26500;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2305.19259</link><description>&lt;p&gt;
Shuffle SGD&#24635;&#26159;&#27604;SGD&#26356;&#22909;&#65306;&#23545;&#20855;&#26377;&#20219;&#24847;&#25968;&#25454;&#39034;&#24207;&#30340;SGD&#36827;&#34892;&#25913;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#21644;&#21333;&#27425;&#27927;&#29260;&#65288;SS&#65289;&#26159;&#36890;&#36807;&#24490;&#29615;&#36941;&#21382;&#35757;&#32451;&#25968;&#25454;&#30340;&#38543;&#26426;&#25110;&#21333;&#20010;&#25490;&#21015;&#30340;&#24120;&#35265;&#36873;&#25321;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#29616;&#26377;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#30340;&#35757;&#32451;&#22330;&#26223;&#20013;&#65292;&#24403;&#26102;&#20195;&#30340;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#38598;&#22823;&#23567;&#26102;&#65292;RR&#21487;&#33021;&#34920;&#29616;&#19981;&#22914;SGD&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#38543;&#26426;/&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#30340;&#22909;&#22788;&#65292;&#24182;&#20026;&#20854;&#38750;&#20984;&#25910;&#25947;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17323</link><description>&lt;p&gt;
&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24378;&#20984;&#20294;&#28508;&#22312;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20248;&#21270;&#30340;&#65288;&#38543;&#26426;&#65289;&#27425;&#26799;&#24230;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31561;&#20215;&#23545;&#20598;&#25551;&#36848;&#65288;&#31867;&#20284;&#20110;&#23545;&#20598;&#24179;&#22343;&#65289;&#26469;&#25551;&#36848;&#32463;&#20856;&#30340;&#27425;&#26799;&#24230;&#27861;&#65292;&#36817;&#31471;&#27425;&#26799;&#24230;&#27861;&#21644;&#20999;&#25442;&#27425;&#26799;&#24230;&#27861;&#12290;&#36825;&#20123;&#31561;&#20215;&#24615;&#33021;&#22815;&#20197; $O(1/T)$ &#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#20998;&#21035;&#36824;&#25552;&#20379;&#20102;&#32463;&#20856;&#21407;&#22987;&#38388;&#38553;&#21644;&#21069;&#20154;&#26410;&#26366;&#20998;&#26512;&#30340;&#23545;&#20598;&#38388;&#38553;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#36825;&#20123;&#32463;&#20856;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#36817;&#20046;&#25152;&#26377;&#30340;&#27493;&#38271;&#36873;&#25321;&#21644;&#19968;&#31995;&#21015;&#30340;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#23545;&#20110;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27425;&#26799;&#24230;&#27861;&#30340;&#26089;&#26399;&#36845;&#20195;&#21487;&#33021;&#20250;&#20986;&#29616;&#25351;&#25968;&#32423;&#30340;&#21457;&#25955;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#22788;&#29702;&#36807;&#36825;&#31181;&#38382;&#39064;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#19981;&#33391;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20173;&#28982;&#30830;&#20445;&#21644; bounds &#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#65292;&#20854;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#39044;&#35757;&#32451;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#20687;&#32032;-&#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.16133</link><description>&lt;p&gt;
OVO: &#24320;&#25918;&#35789;&#27719;&#21344;&#25454;
&lt;/p&gt;
&lt;p&gt;
OVO: Open-Vocabulary Occupancy. (arXiv:2305.16133v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#65292;&#20854;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#39044;&#35757;&#32451;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#20687;&#32032;-&#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#26088;&#22312;&#25512;&#26029;&#20986;&#33258;&#20027;&#20195;&#29702;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#23494;&#38598;&#20960;&#20309;&#21644;&#35821;&#20041;&#12290;&#29616;&#26377;&#30340;&#21344;&#25454;&#39044;&#27979;&#26041;&#27861;&#20960;&#20046;&#23436;&#20840;&#26159;&#22522;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#20307;&#31215;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#39640;&#36136;&#37327;&#65292;&#20294;&#26159;&#29983;&#25104;&#36825;&#20123;&#19977;&#32500;&#27880;&#37322;&#26159;&#36153;&#21147;&#19988;&#25104;&#26412;&#39640;&#26114;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20165;&#21487;&#20197;&#35757;&#32451;&#23569;&#37327;&#29305;&#23450;&#29289;&#20307;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#65292;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#65306;&#65288;1&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#65288;2&#65289;&#20687;&#32032; - &#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#31616;&#21333;&#12289;&#32039;&#20945;&#19988;&#20860;&#23481;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312; NYUv2 &#21644; SemanticKIT &#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#20986;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic occupancy prediction aims to infer dense geometry and semantics of surroundings for an autonomous agent to operate safely in the 3D environment. Existing occupancy prediction methods are almost entirely trained on human-annotated volumetric data. Although of high quality, the generation of such 3D annotations is laborious and costly, restricting them to a few specific object categories in the training dataset. To address this limitation, this paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows semantic occupancy prediction of arbitrary classes but without the need for 3D annotations during training. Keys to our approach are (1) knowledge distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D occupancy network, and (2) pixel-voxel filtering for high-quality training data generation. The resulting framework is simple, compact, and compatible with most state-of-the-art semantic occupancy prediction models. On NYUv2 and SemanticKIT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#21270;&#21512;&#29289;&#36129;&#29486;&#23545;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#36827;&#34892;&#36229;&#20998;&#36776;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14180</link><description>&lt;p&gt;
&#21033;&#29992;&#21270;&#21512;&#29289;&#20114;&#36830;&#30340;&#22810;&#31181;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Multi-BVOC Super-Resolution Exploiting Compounds Inter-Connection. (arXiv:2305.14180v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#31181;&#21270;&#21512;&#29289;&#36129;&#29486;&#23545;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#36827;&#34892;&#36229;&#20998;&#36776;&#30340;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38470;&#22320;&#29983;&#24577;&#31995;&#32479;&#25490;&#25918;&#21040;&#22320;&#29699;&#22823;&#27668;&#20013;&#30340;&#29983;&#29289;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;BVOC&#65289;&#26159;&#22823;&#27668;&#21270;&#23398;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#30001;&#20110;&#27979;&#37327;&#30340;&#31232;&#32570;&#24615;&#65292;&#21487;&#38752;&#30340;BVOC&#25490;&#25918;&#22320;&#22270;&#21487;&#20197;&#25552;&#20379;&#26356;&#23494;&#38598;&#30340;&#25968;&#25454;&#65292;&#20197;&#20379;&#22823;&#27668;&#21270;&#23398;&#12289;&#27668;&#20505;&#21644;&#31354;&#27668;&#36136;&#37327;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#21516;&#21270;&#21512;&#29289;&#36129;&#29486;&#21516;&#26102;&#36229;&#20998;&#36776;&#31895;&#31961;BVOC&#25490;&#25918;&#22320;&#22270;&#30340;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20934;&#30830;&#22320;&#35843;&#26597;&#20960;&#31181;BVOC&#29289;&#31181;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21457;&#29616;&#30340;&#30456;&#20284;&#24615;&#24314;&#31435;&#20102;&#19968;&#20010;Multi-Image Super-Resolution&#65288;MISR&#65289;&#31995;&#32479;&#65292;&#20854;&#20013;&#19982;&#19981;&#21516;&#21270;&#21512;&#29289;&#30456;&#20851;&#32852;&#30340;&#22810;&#20010;&#25490;&#25918;&#22320;&#22270;&#34987;&#38598;&#25104;&#20197;&#25552;&#39640;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#37197;&#32622;&#30340;&#29289;&#31181;&#21644;&#21512;&#24182;BVOC&#25968;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;BVOC&#20851;&#31995;&#32435;&#20837;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#39640;SR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biogenic Volatile Organic Compounds (BVOCs) emitted from the terrestrial ecosystem into the Earth's atmosphere are an important component of atmospheric chemistry. Due to the scarcity of measurement, a reliable enhancement of BVOCs emission maps can aid in providing denser data for atmospheric chemical, climate, and air quality models. In this work, we propose a strategy to super-resolve coarse BVOC emission maps by simultaneously exploiting the contributions of different compounds. To this purpose, we first accurately investigate the spatial inter-connections between several BVOC species. Then, we exploit the found similarities to build a Multi-Image Super-Resolution (MISR) system, in which a number of emission maps associated with diverse compounds are aggregated to boost Super-Resolution (SR) performance. We compare different configurations regarding the species and the number of joined BVOCs. Our experimental results show that incorporating BVOCs' relationship into the process can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.14065</link><description>&lt;p&gt;
&#19981;&#35201;&#35757;&#32451;&#23427;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS-GNN&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32487;&#25215;&#20102;&#20256;&#32479;NAS&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20248;&#21270;&#38590;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;NAS&#26041;&#27861;&#24573;&#35270;&#20102;GNN&#30340;&#29420;&#29305;&#24615;&#65292;&#21363;GNN&#20855;&#26377;&#26080;&#38656;&#35757;&#32451;&#23601;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#28857;&#12290;&#37319;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#30446;&#26631;&#23547;&#25214;&#26368;&#20248;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#24182;&#24471;&#20986;&#19968;&#31181;&#26032;&#30340;NAS-GNN&#26041;&#27861;&#65292;&#21363;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;NAC&#22312;GNN&#19978;&#23454;&#29616;&#20102;&#26080;&#26356;&#26032;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#39640;&#25928;&#35745;&#31639;&#12290;&#22312;&#22810;&#20010;GNN&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13453</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#25512;&#24191;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23460;&#20869;&#23450;&#20301;&#22240;&#20854;&#22312;&#26234;&#33021;&#23478;&#23621;&#12289;&#24037;&#19994;&#33258;&#21160;&#21270;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20381;&#36182;&#20854;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#26080;&#32447;&#21442;&#25968;&#65288;&#22914;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21644;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#65289;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#26080;&#32447;&#35774;&#22791;&#30340;&#20301;&#32622;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36731;&#26494;&#37096;&#32626;&#21040;&#26032;&#29615;&#22659;&#25110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23450;&#20301;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#22810;&#20803;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13060</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#36139;&#27665;&#31391;&#23621;&#27665;&#30001;&#20110;&#36139;&#27665;&#31391;&#20869;&#19981;&#36275;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#32780;&#36973;&#21463;&#22478;&#24066;&#26381;&#21153;&#26080;&#27861;&#35775;&#38382;&#30340;&#22256;&#22659;&#65292;&#32780;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;&#23545;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#37325;&#32452;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#19981;&#33021;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#36139;&#27665;&#31391;&#65292;&#35201;&#20040;&#22312;&#21487;&#36798;&#24615;&#21644;&#24314;&#35774;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#27425;&#20248;&#30340;&#36947;&#36335;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#33719;&#36139;&#27665;&#31391;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36873;&#25321;&#35745;&#21010;&#36947;&#36335;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#25509;&#36139;&#27665;&#31391;&#22320;&#28857;&#30340;&#36947;&#36335;&#35268;&#21010;&#65292;&#20197;&#26368;&#23567;&#30340;&#24314;&#35774;&#25104;&#26412;&#12290;&#23545;&#19981;&#21516;&#22269;&#23478;&#30340;&#30495;&#23454;&#36139;&#27665;&#31391;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07848</link><description>&lt;p&gt;
Meta-Polyp&#65306;&#39640;&#25928;&#24687;&#32905;&#20998;&#21106;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24687;&#32905;&#20998;&#21106;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#19988;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;CNN&#12289;Vision Transformer&#21644;Transformer&#25216;&#26415;&#24320;&#21457;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;2022&#24180;&#65292;Meta-Former&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#20934;&#32447;&#34987;&#24341;&#20837;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#35299;&#20915;&#20102;Vision Transformer&#21644;CNN&#23478;&#26063;&#39592;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Former&#19982;UNet&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#19982;&#32423;&#32852;&#32452;&#21512;&#65292;&#20197;&#22686;&#24378;&#32441;&#29702;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Convformer&#22359;&#65292;&#22522;&#20110;Meta-Former&#30340;&#24605;&#24819;&#65292;&#20197;&#21152;&#24378;&#23616;&#37096;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#36825;&#20123;&#22359;&#23558;&#20840;&#23616;&#20449;&#24687;&#65288;&#20363;&#22914;&#24687;&#32905;&#30340;&#25972;&#20307;&#24418;&#29366;&#65289;&#19982;&#23616;&#37096;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#36827;&#34892;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#36827;&#34892;&#20102;&#31995;&#32479;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#24863;&#30693;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#36825;&#31181;&#20851;&#38190;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25552;&#21462;&#36866;&#21512;&#20110;&#32473;&#23450;&#25351;&#20196;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.06500</link><description>&lt;p&gt;
InstructBLIP: &#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. (arXiv:2305.06500v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#36827;&#34892;&#20102;&#31995;&#32479;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;&#25351;&#20196;&#24863;&#30693;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#36825;&#31181;&#20851;&#38190;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25552;&#21462;&#36866;&#21512;&#20110;&#32473;&#23450;&#25351;&#20196;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39537;&#21160;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27969;&#31243;&#30340;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20986;&#29616;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#35821;&#35328;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#24314;&#31435;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP-2&#27169;&#22411;&#23545;&#35270;&#35273;&#35821;&#35328;&#25351;&#20196;&#35843;&#25972;&#36827;&#34892;&#20102;&#31995;&#32479;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25351;&#20196;&#35843;&#25972;&#26684;&#24335;&#24182;&#20998;&#31867;&#20026;&#20004;&#20010;&#38598;&#32676;&#65292;&#29992;&#20110;&#20445;&#25345;&#25351;&#20196;&#35843;&#25972;&#21644;&#20445;&#25345;&#38646;-shot&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25351;&#20196;&#24863;&#30693;&#30340;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25552;&#21462;&#36866;&#21512;&#20110;&#32473;&#23450;&#25351;&#20196;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#32467;&#26524;&#65292;InstructBLIP&#27169;&#22411;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
General-purpose language models that can solve various language-domain tasks have emerged driven by the pre-training and instruction-tuning pipeline. However, building general-purpose vision-language models is challenging due to the increased task discrepancy introduced by the additional visual input. Although vision-language pre-training has been widely studied, vision-language instruction tuning remains relatively less explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets, transform them into instruction tuning format and categorize them into two clusters for held-in instruction tuning and held-out zero-shot evaluation. Additionally, we introduce instruction-aware visual feature extraction, a crucial method that enables the model to extract informative features tailored to the given instruction. The resulting InstructBLIP models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03617</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segmentation of fundus vascular images based on a dual-attention mechanism. (arXiv:2305.03617v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#30340;&#30524;&#24213;&#34880;&#31649;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#22270;&#20687;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#21644;Dropout&#23618;&#35299;&#20915;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#20998;&#21106;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#34880;&#31649;&#23545;&#20110;&#26089;&#26399;&#31579;&#26597;&#12289;&#35786;&#26029;&#21644;&#35780;&#20272;&#26576;&#20123;&#30524;&#37096;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#20809;&#29031;&#21464;&#21270;&#21644;&#19981;&#22343;&#21248;&#23545;&#27604;&#24230;&#65292;&#36825;&#20351;&#24471;&#20998;&#21106;&#21464;&#24471;&#38750;&#24120;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#27880;&#24847;&#34701;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#32467;&#21512;&#20102;Transformer&#26500;&#24314;&#30340;&#36890;&#36947;&#27880;&#24847;&#21644;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#25552;&#21462;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#28040;&#38500;&#32534;&#30721;&#22120;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#22312;&#36339;&#36291;&#36830;&#25509;&#20013;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;Dropout&#23618;&#38543;&#26426;&#33293;&#24323;&#19968;&#20123;&#31070;&#32463;&#20803;&#65292;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#25311;&#21512;&#24182;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;DERIVE&#12289;STARE&#21644;CHASEDB1&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#19968;&#20123;&#26368;&#36817;&#30340;&#35270;&#32593;&#33180;&#30524;&#24213;&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately segmenting blood vessels in retinal fundus images is crucial in the early screening, diagnosing, and evaluating some ocular diseases. However, significant light variations and non-uniform contrast in these images make segmentation quite challenging. Thus, this paper employ an attention fusion mechanism that combines the channel attention and spatial attention mechanisms constructed by Transformer to extract information from retinal fundus images in both spatial and channel dimensions. To eliminate noise from the encoder image, a spatial attention mechanism is introduced in the skip connection. Moreover, a Dropout layer is employed to randomly discard some neurons, which can prevent overfitting of the neural network and improve its generalization performance. Experiments were conducted on publicly available datasets DERIVE, STARE, and CHASEDB1. The results demonstrate that our method produces satisfactory results compared to some recent retinal fundus image segmentation algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01604</link><description>&lt;p&gt;
&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#26469;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#27979;&#36712;&#36857;&#12290;&#36890;&#36807;&#26816;&#26597;&#24213;&#23618;&#39640;&#32500;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30340;&#26377;&#25928;&#20302;&#32500;&#27969;&#24418;&#12290;&#20855;&#26377;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#32593;&#32476;&#22312;&#39044;&#27979;&#31354;&#38388;&#20869;&#20301;&#20110;&#21516;&#19968;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#27969;&#24418;&#30340;&#32454;&#33410;&#65292;&#21457;&#29616;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#32593;&#32476;&#36981;&#24490;&#21487;&#21306;&#20998;&#30340;&#36712;&#36857;&#65292;&#20294;&#20854;&#20182;&#22240;&#32032;&#24433;&#21709;&#26497;&#23567;; &#26356;&#22823;&#30340;&#32593;&#32476;&#27839;&#30528;&#19982;&#36739;&#23567;&#30340;&#32593;&#32476;&#30456;&#20284;&#30340;&#27969;&#24418;&#35757;&#32451;&#65292;&#21482;&#26159;&#26356;&#24555;; &#19981;&#21516;&#37096;&#20998;&#30340;&#21021;&#22987;&#21270;&#32593;&#32476;&#22312;&#30456;&#20284;&#30340;&#27969;&#24418;&#19978;&#21521;&#35299;&#20915;&#26041;&#26696;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.01082</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#26159;&#26368;&#22522;&#26412;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25628;&#32034;&#21151;&#33021;&#20043;&#19968;&#12290;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#30340;&#29992;&#25143;&#26597;&#35810;&#19981;&#20165;&#22686;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#32780;&#19988;&#29992;&#25143;&#20063;&#26399;&#26395;&#33021;&#22815;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24191;&#27867;&#21487;&#29992;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#27604;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#31934;&#24230;&#20302;&#65292;&#35201;&#20040;&#36895;&#24230;&#22826;&#24930;&#65292;&#26080;&#27861;&#29992;&#20110;&#24310;&#36831;&#26159;&#20851;&#38190;&#35201;&#27714;&#30340;&#25628;&#32034;&#29992;&#20363;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#21019;&#26032;&#26550;&#26500;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#27809;&#26377;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#36827;&#34892;&#22521;&#35757;&#65292;&#24182;&#19988;&#26159;&#38024;&#23545;&#36739;&#38271;&#25991;&#26412;&#30340;&#25340;&#20889;&#32416;&#27491;&#36827;&#34892;&#22521;&#35757;&#65292;&#36825;&#26159;&#19982;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#25340;&#20889;&#32416;&#27491;&#19981;&#21516;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#24456;&#23569;(&#22823;&#22810;&#25968;&#26597;&#35810;&#21482;&#26377;1-2&#20010;&#21333;&#35789;)&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#20225;&#19994;&#26377;&#29420;&#29305;&#30340;&#35789;&#27719;&#65292;&#20363;&#22914;&#20135;&#21697;&#21517;&#31216;&#65292;&#29616;&#25104;&#30340;&#25340;&#20889;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.00562</link><description>&lt;p&gt;
&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Class-Balancing Diffusion Models. (arXiv:2305.00562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#35273;&#25968;&#25454;&#21516;&#26102;&#20445;&#25345;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#26377;&#20248;&#21183;&#12290;&#20294;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;&#31574;&#21010;&#22909;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21363;&#25968;&#25454;&#26679;&#26412;&#32463;&#36807;&#31934;&#24515;&#22788;&#29702;&#20197;&#22312;&#26631;&#31614;&#26041;&#38754;&#22343;&#21248;&#20998;&#24067;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#20284;&#20046;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#31181;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36824;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#24403;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#26174;&#33879;&#38477;&#20302;&#12290;&#23588;&#20854;&#26159;&#22312;&#23614;&#37096;&#31867;&#21035;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#20005;&#37325;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#19981;&#26159;&#31867;&#24179;&#34913;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such class-imbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with class-imbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#27491;&#20132;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21363;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#26469;&#32531;&#35299;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14034</link><description>&lt;p&gt;
&#27491;&#20132;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#29699;&#24418;&#24863;&#24212;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spherical Inducing Features for Orthogonally-Decoupled Gaussian Processes. (arXiv:2304.14034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35299;&#32806;&#39640;&#26031;&#36807;&#31243;&#30340;&#27491;&#20132;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21363;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#26469;&#32531;&#35299;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23398;&#20064;&#34920;&#24449;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#32463;&#24120;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#35825;&#23548;&#21464;&#37327;&#19982;&#21069;&#39304;NN&#30340;&#38544;&#34255;&#21333;&#20803;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#36328;&#22495;&#21464;&#20998;GPs&#26469;&#24357;&#21512; GPs&#21644;&#28145;&#24230;NN&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#30740;&#31350;&#27492;&#26041;&#27861;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#26041;&#27861;&#65292;&#21033;&#29992;GPs&#30340;&#27491;&#20132;&#20998;&#35299;&#26469;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#29699;&#24418;&#36328;&#22495;&#29305;&#24449;&#65292;&#26500;&#24314;&#26356;&#28789;&#27963;&#30340;&#25968;&#25454;&#20381;&#36182;&#22522;&#20989;&#25968;&#65292;&#29992;&#20110;GP&#36924;&#36817;&#30340;&#20027;&#35201;&#21644;&#27491;&#20132;&#20998;&#37327;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#27492;&#26694;&#26550;&#19979;&#21152;&#20837;NN&#28608;&#27963;&#29305;&#24449;&#65292;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#32780;&#19988;&#27604;&#20854;&#20182;&#31574;&#30053;&#26356;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their many desirable properties, Gaussian processes (GPs) are often compared unfavorably to deep neural networks (NNs) for lacking the ability to learn representations. Recent efforts to bridge the gap between GPs and deep NNs have yielded a new class of inter-domain variational GPs in which the inducing variables correspond to hidden units of a feedforward NN. In this work, we examine some practical issues associated with this approach and propose an extension that leverages the orthogonal decomposition of GPs to mitigate these limitations. In particular, we introduce spherical inter-domain features to construct more flexible data-dependent basis functions for both the principal and orthogonal components of the GP approximation and show that incorporating NN activation features under this framework not only alleviates these shortcomings but is more scalable than alternative strategies. Experiments on multiple benchmark datasets demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#26435;&#37325;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#24335;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#20687;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#21019;&#24847;&#21644;&#33402;&#26415;&#20316;&#21697;&#12290;&#22312;&#24403;&#21069;&#20027;&#23548;&#30340;&#20998;&#24067;&#25311;&#21512;&#33539;&#24335;&#19979;&#65292;&#25968;&#25454;&#38598;&#34987;&#35270;&#20026;&#35201;&#23613;&#21487;&#33021;&#25509;&#36817;&#30340;&#30495;&#23454;&#20540;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21019;&#24847;&#24212;&#29992;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#21019;&#20316;&#32773;&#32463;&#24120;&#21162;&#21147;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#31215;&#26497;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#32431;&#27169;&#24335;&#35206;&#30422;&#36716;&#21521;&#27169;&#24335;&#24179;&#34913;&#30340;&#24314;&#27169;&#30446;&#26631;&#35843;&#25972;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36866;&#24212;&#26356;&#39640;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#22312;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#35745;&#31639;&#26426;&#21019;&#24847;&#20013;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.11839</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#65288;SSA&#65289;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#12290; SSA&#33021;&#22815;&#27604;&#20856;&#22411;&#30340;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#26356;&#24555;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#26059;&#65288;&#27010;&#29575;&#27604;&#29305;&#65289;&#30340;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#26469;&#30830;&#23450;&#36229;&#21442;&#25968;&#12290;&#33258;&#26059;&#26159;SSA&#30340;&#22522;&#26412;&#35745;&#31639;&#20803;&#32032;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#19982;&#20854;&#20182;&#33258;&#26059;&#36827;&#34892;&#22270;&#24418;&#36830;&#25509;&#12290;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#21487;&#20197;&#22522;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#36827;&#34892;&#20272;&#35745;&#12290;&#22522;&#20110;CLT&#30340;&#27491;&#24577;&#20998;&#24067;&#29992;&#20110;&#30830;&#23450;&#36229;&#21442;&#25968;&#65292;&#20854;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20256;&#32479;&#26041;&#27861;&#30340;O(n^3)&#38477;&#20302;&#21040;O(1)&#12290;&#20351;&#29992;&#30830;&#23450;&#30340;&#36229;&#21442;&#25968;&#35780;&#20272;&#20102;SSA&#22312;Gset&#21644;K2000&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#26368;&#22823;&#21106;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#24179;&#22343;&#21106;&#20540;&#30340;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#22120; B-Learner&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064; CATE &#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2304.10577</link><description>&lt;p&gt;
B-Learner&#65306;&#38544;&#34255;&#28151;&#28102;&#19979;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#30340;&#20934;&#31070;&#35861;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
B-Learner: Quasi-Oracle Bounds on Heterogeneous Causal Effects Under Hidden Confounding. (arXiv:2304.10577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#22120; B-Learner&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064; CATE &#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#25919;&#31574;&#21644;&#20915;&#31574;&#32773;&#20570;&#20986;&#26356;&#22909;&#30340;&#34892;&#21160;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#65288;CATE&#65289;&#20989;&#25968;&#26041;&#38754;&#21462;&#24471;&#20102;&#40065;&#26834;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26410;&#32771;&#34385;&#38544;&#34255;&#28151;&#28102;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#22522;&#20110;&#35266;&#23519;&#25968;&#25454;&#30340;&#20219;&#20309;&#22240;&#26524;&#20272;&#35745;&#36896;&#25104;&#20219;&#24847;&#21644;&#19981;&#30693;&#24773;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;B-Learner&#30340;&#20803;&#23398;&#20064;&#22120;&#65292;&#23427;&#21487;&#20197;&#22312;&#38480;&#21046;&#38544;&#34255;&#28151;&#28102;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;CATE&#20989;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#26368;&#36817;&#38024;&#23545;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#23574;&#38160;&#19988;&#26377;&#25928;&#36793;&#30028;&#32467;&#26524;&#65288;Dorn&#31561;&#20154;&#65292;2021&#65289;&#35843;&#25972;&#20026;Kallus&#65286;Oprescu&#65288;2022&#65289;&#25152;&#25552;&#20379;&#30340;&#31283;&#20581;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#20998;&#24067;&#24335;&#27835;&#30103;&#25928;&#24212;&#23398;&#20064;&#26694;&#26550;&#65292;&#27966;&#29983;&#20986;B-Learner&#12290;B-Learner&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#20363;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating heterogeneous treatment effects from observational data is a crucial task across many fields, helping policy and decision-makers take better actions. There has been recent progress on robust and efficient methods for estimating the conditional average treatment effect (CATE) function, but these methods often do not take into account the risk of hidden confounding, which could arbitrarily and unknowingly bias any causal estimate based on observational data. We propose a meta-learner called the B-Learner, which can efficiently learn sharp bounds on the CATE function under limits on the level of hidden confounding. We derive the B-Learner by adapting recent results for sharp and valid bounds of the average treatment effect (Dorn et al., 2021) into the framework given by Kallus &amp; Oprescu (2022) for robust and model-agnostic learning of distributional treatment effects. The B-Learner can use any function estimator such as random forests and deep neural networks, and we prove its 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item><item><title>&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;</title><link>http://arxiv.org/abs/2304.08297</link><description>&lt;p&gt;
&#23545;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#35780;&#35770;&#65288;arXiv: 2304.08297v2 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08297
&lt;/p&gt;
&lt;p&gt;
&#23545;&#38472;&#31561;&#20154;&#21457;&#34920;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#30340;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;&#21644;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38472;&#31561;&#20154;&#65288;Chen2022&#65289;&#22312;&#12298;&#33258;&#28982;&#8212;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#20102;&#39064;&#20026;&#8220;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#25991;&#31456;&#12290;&#35813;&#25991;&#31456;&#20316;&#32773;&#31216;&#20854;&#26041;&#27861;&#20026;&#8220;&#32452;&#32455;&#23398;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#65292;&#31616;&#31216;SISH&#12290;&#8221;&#25105;&#20204;&#23545;SISH&#34920;&#31034;&#20102;&#20851;&#20999;&#65292;&#22240;&#20026;&#23427;&#26159;Yottixel&#30340;&#22686;&#37327;&#20462;&#25913;&#65292;&#20351;&#29992;&#20102;MinMax&#20108;&#20540;&#21270;&#20294;&#26410;&#24341;&#29992;&#21407;&#22987;&#20316;&#21697;&#65292;&#24182;&#22522;&#20110;&#19968;&#20010;&#35823;&#31216;&#8220;&#33258;&#30417;&#30563;&#22270;&#20687;&#25628;&#32034;&#8221;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#38472;&#31561;&#20154;&#36827;&#34892;&#23454;&#39564;&#21644;&#27604;&#36739;&#26102;&#23384;&#22312;&#30340;&#20960;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08177</link><description>&lt;p&gt;
&#20013;&#25991;LLaMA&#21644;Alpaca&#30340;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#26412;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. (arXiv:2304.08177v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08177
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;LLaMA&#23545;&#20013;&#25991;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#65292;&#24182;&#26174;&#31034;&#20986;&#26397;&#30528;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#23545;&#36879;&#26126;&#12289;&#21487;&#35775;&#38382;&#30340;&#23398;&#26415;&#30740;&#31350;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;LLaMA&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#65292;&#22686;&#21152;&#20102;20,000&#20010;&#20013;&#25991;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#32534;&#30721;&#25928;&#29575;&#21644;&#23545;&#27721;&#35821;&#35821;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#20013;&#25991;&#25968;&#25454;&#19978;&#36827;&#34892;&#20108;&#27425;&#39044;&#35757;&#32451;&#21644;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#29983;&#25104;&#20013;&#25991;&#25991;&#26412;&#21450;&#20854;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically transformed natural language processing research and shown promising strides towards Artificial General Intelligence (AGI). Nonetheless, the high costs associated with training and deploying LLMs present substantial obstacles to transparent, accessible academic research. While several large language models, such as LLaMA, have been open-sourced by the community, these predominantly focus on English corpora, limiting their usefulness for other languages. In this paper, we propose a method to augment LLaMA with capabilities for understanding and generating Chinese text and its ability to follow instructions. We achieve this by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens, thereby improving its encoding efficiency and semantic understanding of Chinese. We further incorporate secondary pre-training using Chinese data and fine-tune the model with Chinese instruction datasets, signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-VAE&#20316;&#20026;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06476</link><description>&lt;p&gt;
&#22522;&#20110;$\beta$-VAE&#30340;&#24515;&#30005;&#22270;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#25552;&#21462;&#32852;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Joint optimization of a $\beta$-VAE for ECG task-specific feature extraction. (arXiv:2304.06476v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-VAE&#20316;&#20026;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#26159;&#30740;&#31350;&#24515;&#33039;&#24773;&#20917;&#30340;&#26368;&#24120;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#24515;&#33039;&#33410;&#24459;&#21644;&#30005;&#27963;&#21160;&#36827;&#34892;&#35786;&#26029;&#21644;&#30417;&#27979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$\beta$-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#20449;&#21495;&#37325;&#24314;&#21644;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#24515;&#33039;&#21151;&#33021;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;7255&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiography is the most common method to investigate the condition of the heart through the observation of cardiac rhythm and electrical activity, for both diagnosis and monitoring purposes. Analysis of electrocardiograms (ECGs) is commonly performed through the investigation of specific patterns, which are visually recognizable by trained physicians and are known to reflect cardiac (dis)function. In this work we study the use of $\beta$-variational autoencoders (VAEs) as an explainable feature extractor, and improve on its predictive capacities by jointly optimizing signal reconstruction and cardiac function prediction. The extracted features are then used for cardiac function prediction using logistic regression. The method is trained and tested on data from 7255 patients, who were treated for acute coronary syndrome at the Leiden University Medical Center between 2010 and 2021. The results show that our method significantly improved prediction and explainability compared to 
&lt;/p&gt;</description></item><item><title>Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.05444</link><description>&lt;p&gt;
&#20351;&#29992;Co-ML&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#23478;&#24237;&#30340;&#21512;&#20316;&#27169;&#22411;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Collaborative Machine Learning Model Building with Families Using Co-ML. (arXiv:2304.05444v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05444
&lt;/p&gt;
&lt;p&gt;
Co-ML&#26159;&#19968;&#20010;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#32773;&#22312;&#21512;&#20316;&#20013;&#21457;&#25496;&#26032;&#30340;&#24819;&#27861;&#21644;&#26041;&#27861;&#65292;&#35299;&#20915;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#26032;&#25163;&#21451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24314;&#27169;&#24037;&#20855;&#65292;&#20391;&#37325;&#20110;&#21333;&#19968;&#29992;&#25143;&#20307;&#39564;&#65292;&#19968;&#20010;&#21333;&#19968;&#29992;&#25143;&#20165;&#25910;&#38598;&#33258;&#24049;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#24314;&#27169;&#32463;&#21382;&#38480;&#21046;&#20102;&#23398;&#20064;&#32773;&#20849;&#21516;&#24037;&#20316;&#26102;&#20250;&#36935;&#21040;&#30340;&#20132;&#26367;&#24819;&#27861;&#21644;&#26041;&#27861;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#24403;&#19981;&#21516;&#30340;&#35266;&#28857;&#20307;&#29616;&#22312;&#32676;&#20307;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#20013;&#26102;&#65292;&#24448;&#24448;&#25490;&#38500;&#20102;ML&#22260;&#32469;&#25968;&#25454;&#34920;&#29616;&#21644;&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Co-ML&#8212;&#8212;&#19968;&#20010;&#38754;&#21521;&#23398;&#20064;&#32773;&#30340;&#22522;&#20110;&#24179;&#26495;&#30005;&#33041;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#31471;&#23545;&#31471;&#30340;&#36845;&#20195;&#27169;&#22411;&#26500;&#24314;&#27969;&#31243;&#65292;&#21327;&#21516;&#26500;&#24314;ML&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#23478;&#24237;&#65288;&#30001;&#20004;&#20010;11&#21644;14&#23681;&#30340;&#23401;&#23376;&#19982;&#29238;&#27597;&#19968;&#36215;&#24037;&#20316;&#65289;&#22312;&#23478;&#20013;&#20351;&#29992;Co-ML&#36827;&#34892;&#24341;&#23548;&#24615;&#20171;&#32461;ML&#27963;&#21160;&#30340;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21327;&#20316;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;Co-ML&#31995;&#32479;&#30340;d&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML -- a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system d
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03870</link><description>&lt;p&gt;
ASPEST&#65306;&#20027;&#21160;&#23398;&#20064;&#21644;&#36873;&#25321;&#39044;&#27979;&#20043;&#38388;&#30340;&#24357;&#21512;
&lt;/p&gt;
&lt;p&gt;
ASPEST: Bridging the Gap Between Active Learning and Selective Prediction. (arXiv:2304.03870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;ASPEST&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#24037;&#20316;&#30340;&#21516;&#26102;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#39044;&#27979;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21487;&#38752;&#30340;&#27169;&#22411;&#65292;&#24403;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24456;&#39640;&#26102;&#65292;&#21487;&#20197;&#36991;&#20813;&#36827;&#34892;&#39044;&#27979;&#12290;&#38543;&#21518;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#19987;&#23478;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27979;&#35797;&#25968;&#25454;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#12290;&#36825;&#23548;&#33268;&#26356;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#38656;&#35201;&#22686;&#21152;&#20154;&#24037;&#26631;&#27880;&#65292;&#36825;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#37117;&#26159;&#22256;&#38590;&#21644;&#26114;&#36149;&#30340;&#12290;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#20165;&#26597;&#35810;&#26368;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31034;&#20363;&#26469;&#36991;&#20813;&#36825;&#31181;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#26696;&#20363;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#24635;&#20307;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#36873;&#25321;&#24615;&#39044;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#20027;&#21160;&#36873;&#25321;&#24615;&#39044;&#27979;&#65288;active selective prediction&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#22686;&#21152;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#30340;&#21516;&#26102;&#22312;&#36716;&#31227;&#30446;&#26631;&#39046;&#22495;&#20013;&#23398;&#20064;&#26597;&#35810;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#26679;&#26412;&#12290;&#23545;&#20110;&#36825;&#20010;&#26032;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;ASPEST&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#24555;&#29031;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective prediction aims to learn a reliable model that abstains from making predictions when the model uncertainty is high. These predictions can then be deferred to a human expert for further evaluation. In many real-world scenarios, however, the distribution of test data is different from the training data. This results in more inaccurate predictions, necessitating increased human labeling, which is difficult and expensive in many scenarios. Active learning circumvents this difficulty by only querying the most informative examples and, in several cases, has been shown to lower the overall labeling effort. In this work, we bridge the gap between selective prediction and active learning, proposing a new learning paradigm called active selective prediction which learns to query more informative samples from the shifted target domain while increasing accuracy and coverage. For this new problem, we propose a simple but effective solution, ASPEST, that trains ensembles of model snapshots
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#21644;&#19968;&#20010;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01669</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Re-thinking Model Inversion Attacks Against Deep Neural Networks. (arXiv:2304.01669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#21644;&#19968;&#20010;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36870;&#25512;&#65288;MI&#65289;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#28389;&#29992;&#23545;&#27169;&#22411;&#30340;&#35775;&#38382;&#26469;&#25512;&#26029;&#21644;&#37325;&#26500;&#31169;&#26377;&#22521;&#35757;&#25968;&#25454;&#12290;MI&#25915;&#20987;&#24341;&#36215;&#20102;&#26377;&#20851;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#31169;&#20154;&#38754;&#37096;&#22270;&#20687;&#65289;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#25913;&#21892;MI&#30340;&#25915;&#20987;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;MI&#65292;&#30740;&#31350;&#20102;&#25152;&#26377;&#26368;&#20808;&#36827;&#65288;SOTA&#65289; MI&#31639;&#27861;&#25152;&#28041;&#21450;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25152;&#26377;SOTA MI&#30340;&#25915;&#20987;&#34920;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;SOTA MI&#31639;&#27861;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#35748;&#20026;&#35813;&#30446;&#26631;&#23545;&#20110;&#23454;&#29616;MI&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#24615;&#33021;&#12290;2&#65289;&#25105;&#20204;&#20998;&#26512;&#20102;&#8220;MI&#36807;&#24230;&#25311;&#21512;&#8221;&#65292;&#23637;&#31034;&#20102;&#23427;&#20250;&#38459;&#27490;&#37325;&#26500;&#22270;&#20687;&#20174;&#23398;&#20064;&#22521;&#35757;&#25968;&#25454;&#30340;&#35821;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#8220;&#27169;&#22411;&#22686;&#24378;&#8221;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model inversion (MI) attacks aim to infer and reconstruct private training data by abusing access to a model. MI attacks have raised concerns about the leaking of sensitive information (e.g. private face images used in training a face recognition system). Recently, several algorithms for MI have been proposed to improve the attack performance. In this work, we revisit MI, study two fundamental issues pertaining to all state-of-the-art (SOTA) MI algorithms, and propose solutions to these issues which lead to a significant boost in attack performance for all SOTA MI. In particular, our contributions are two-fold: 1) We analyze the optimization objective of SOTA MI algorithms, argue that the objective is sub-optimal for achieving MI, and propose an improved optimization objective that boosts attack performance significantly. 2) We analyze "MI overfitting", show that it would prevent reconstructed images from learning semantics of training data, and propose a novel "model augmentation" ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#65292;&#20855;&#26377;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01329</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning the Delay Using Neural Delay Differential Equations. (arXiv:2304.01329v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#65292;&#20855;&#26377;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#21160;&#21147;&#31995;&#32479;&#30340;&#20132;&#21449;&#28857;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#20195;&#34920;&#20102;&#36825;&#20123;&#39046;&#22495;&#20043;&#38388;&#20016;&#23500;&#30340;&#20132;&#21472;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#28382;&#24494;&#20998;&#26041;&#31243;&#65288;DDEs&#65289;&#30340;&#36830;&#32493;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20276;&#38543;&#28789;&#25935;&#24230;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21644;&#26102;&#28382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;NODEs&#30340;&#21551;&#21457;&#65292;&#24182;&#25193;&#23637;&#20102;&#26089;&#26399;&#30340;&#31070;&#32463;DDE&#27169;&#22411;&#65292;&#21518;&#32773;&#20551;&#35774;&#26102;&#28382;&#30340;&#20540;&#26159;&#24050;&#30693;&#30340;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20174;&#22522;&#20934;&#31995;&#32479;&#20013;&#23398;&#20064;DDE&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35752;&#35770;&#20013;&#24471;&#20986;&#32467;&#35770;&#65292;&#25552;&#20986;&#26410;&#26469;&#21487;&#33021;&#30340;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of machine learning and dynamical systems has generated considerable interest recently. Neural Ordinary Differential Equations (NODEs) represent a rich overlap between these fields. In this paper, we develop a continuous time neural network approach based on Delay Differential Equations (DDEs). Our model uses the adjoint sensitivity method to learn the model parameters and delay directly from data. Our approach is inspired by that of NODEs and extends earlier neural DDE models, which have assumed that the value of the delay is known a priori. We perform a sensitivity analysis on our proposed approach and demonstrate its ability to learn DDE parameters from benchmark systems. We conclude our discussion with potential future directions and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.00994</link><description>&lt;p&gt;
Lean &#30340;&#26426;&#22120;&#23398;&#20064;&#21069;&#25552;&#36873;&#25321;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Machine-Learned Premise Selection for Lean. (arXiv:2304.00994v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#35813;&#24037;&#20855;&#30340;&#35774;&#35745;&#21407;&#21017;&#20026;&#65306;&#65288;1&#65289;&#19982;&#35777;&#26126;&#21161;&#25163;&#32039;&#23494;&#38598;&#25104;&#65292;&#65288;2&#65289;&#26131;&#20110;&#20351;&#29992;&#21644;&#23433;&#35013;&#65292;&#65288;3&#65289;&#37319;&#29992;&#36731;&#37327;&#32423;&#19988;&#24555;&#36895;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#30340;&#33258;&#23450;&#20041;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#30452;&#25509;&#22312; Lean &#20013;&#23454;&#29616;&#65292;&#36825;&#24471;&#30410;&#20110; Lean 4 &#20016;&#23500;&#32780;&#39640;&#25928;&#30340;&#20803;&#32534;&#31243;&#21151;&#33021;&#12290;&#38543;&#26426;&#26862;&#26519;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#20110; mathlib -- Lean &#30340;&#25968;&#23398;&#24211;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#36873;&#39033;&#26469;&#20135;&#29983;&#35757;&#32451;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24314;&#35758;&#21487;&#20197;&#36890;&#36807;&#8220;suggest_premises&#31574;&#30053;&#8221;&#20256;&#36798;&#32473;&#29992;&#25143;&#65292;&#22312;&#20132;&#20114;&#24335;&#26500;&#24314;&#35777;&#26126;&#36807;&#31243;&#20013;&#21487;&#20197;&#22312;&#32534;&#36753;&#22120;&#20013;&#35843;&#29992;&#35813;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a machine-learning-based tool for the Lean proof assistant that suggests relevant premises for theorems being proved by a user. The design principles for the tool are (1) tight integration with the proof assistant, (2) ease of use and installation, (3) a lightweight and fast approach. For this purpose, we designed a custom version of the random forest model, trained in an online fashion. It is implemented directly in Lean, which was possible thanks to the rich and efficient metaprogramming features of Lean 4. The random forest is trained on data extracted from mathlib -- Lean's mathematics library. We experiment with various options for producing training features and labels. The advice from a trained model is accessible to the user via the suggest_premises tactic which can be called in an editor while constructing a proof interactively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;FP8&#21644;INT8&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#25104;&#26524;&#65292;&#20026;&#36873;&#25321;&#27491;&#30830;&#30340;&#25968;&#23383;&#26684;&#24335;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.17951</link><description>&lt;p&gt;
FP8&#21644;INT8&#22312;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
FP8 versus INT8 for efficient deep learning inference. (arXiv:2303.17951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;FP8&#21644;INT8&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#25104;&#26524;&#65292;&#20026;&#36873;&#25321;&#27491;&#30830;&#30340;&#25968;&#23383;&#26684;&#24335;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;FP8&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25968;&#23383;&#26684;&#24335;&#30340;&#24819;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#19990;&#30028;&#20013;&#27969;&#20256;&#12290;&#37492;&#20110;&#30446;&#21069;&#22823;&#37096;&#20998;&#35757;&#32451;&#37117;&#26159;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#30340;FP32&#25110;&#32773;&#26377;&#26102;&#20351;&#29992;&#28151;&#21512;&#31934;&#24230;&#30340;FP16&#36827;&#34892;&#30340;&#65292;&#37096;&#20998;&#32593;&#32476;&#20351;&#29992;8&#20301;&#37325;&#37327;&#32423;&#30340;FP8&#21487;&#20197;&#21152;&#24555;&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#24120;&#32791;&#26102;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#27492;&#21457;&#23637;&#23545;&#20110;&#36793;&#32536;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#30340;&#24433;&#21709;&#30340;&#33258;&#28982;&#38382;&#39064;&#12290;&#22312;&#39640;&#25928;&#25512;&#29702;&#35774;&#22791;&#20013;&#65292;&#24037;&#20316;&#36127;&#36733;&#36890;&#24120;&#22312;INT8&#20013;&#25191;&#34892;&#12290;&#26377;&#26102;&#20026;&#20102;&#20445;&#35777;&#25928;&#29575;&#65292;&#29978;&#33267;&#20302;&#33267;INT4&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;FP8&#21644;INT&#26684;&#24335;&#22312;&#35774;&#22791;&#39640;&#25928;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;INT&#21644;FP&#26684;&#24335;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#21644;&#35757;&#32451;&#26102;&#37327;&#21270;&#32467;&#26524;&#26469;&#23637;&#31034;&#22914;&#20309;&#22312;&#19981;&#21516;&#26684;&#24335;&#19979;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the idea of using FP8 as a number format for neural network training has been floating around the deep learning world. Given that most training is currently conducted with entire networks in FP32, or sometimes FP16 with mixed-precision, the step to having some parts of a network run in FP8 with 8-bit weights is an appealing potential speed-up for the generally costly and time-intensive training procedures in deep learning. A natural question arises regarding what this development means for efficient inference on edge devices. In the efficient inference device world, workloads are frequently executed in INT8. Sometimes going even as low as INT4 when efficiency calls for it. In this whitepaper, we compare the performance for both the FP8 and INT formats for efficient on-device inference. We theoretically show the difference between the INT and FP formats for neural networks and present a plethora of post-training quantization and quantization-aware-training results to show how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09975</link><description>&lt;p&gt;
MedNeXt&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21464;&#21387;&#22120;&#39537;&#21160;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09975
&lt;/p&gt;
&lt;p&gt;
MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20351;&#29992;&#22522;&#20110; Transformer &#30340;&#26550;&#26500;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20854;&#24615;&#33021;&#36828;&#19981;&#22914;&#33258;&#28982;&#22270;&#20687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#35757;&#32451;&#21040;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;ConvNeXt &#26550;&#26500;&#23581;&#35797;&#36890;&#36807;&#38236;&#20687;&#21464;&#21387;&#22120;&#22359;&#26469;&#29616;&#20195;&#21270;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#19968;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837; MedNeXt&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#22823;&#26680;&#20998;&#21106;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#23436;&#20840; ConvNeXt 3D &#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#32593;&#32476;&#65292;2&#65289;&#27531;&#24046; ConvNeXt &#19978;&#19979;&#37319;&#26679;&#22359;&#65292;&#20197;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#65292;3&#65289;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19978;&#37319;&#26679;&#23567;&#26680;&#26469;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#32452;&#21512;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#19982;&#24050;&#30693;&#30340;&#21407;&#22987;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#20391;&#19981;&#31561;&#24335;&#23558;&#26368;&#20248;&#32452;&#21512;&#20540;&#20989;&#25968;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#20540;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2303.02557</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#20013;&#38480;&#21046;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounding the Optimal Value Function in Compositional Reinforcement Learning. (arXiv:2303.02557v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#32452;&#21512;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#19982;&#24050;&#30693;&#30340;&#21407;&#22987;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#21452;&#20391;&#19981;&#31561;&#24335;&#23558;&#26368;&#20248;&#32452;&#21512;&#20540;&#20989;&#25968;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#20540;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#19968;&#31995;&#21015;&#20165;&#22312;&#22870;&#21169;&#20989;&#25968;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24555;&#36895;&#33719;&#24471;&#36866;&#29992;&#20110;&#26032;&#22870;&#21169;&#20989;&#25968;&#30340;&#26410;&#30693;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#20197;&#21069;&#35299;&#20915;&#20219;&#21153;&#30340;&#21151;&#33021;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20351;&#29992;&#36825;&#31181;&#21151;&#33021;&#32452;&#21512;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#32452;&#21512;&#20989;&#25968;&#30340;&#20855;&#20307;&#23454;&#20363;&#19978;&#65292;&#36825;&#20123;&#23454;&#20363;&#30340;&#26497;&#38480;&#20551;&#35774;&#20801;&#35768;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;-shot&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32479;&#19968;&#20102;&#36825;&#20123;&#31034;&#20363;&#65292;&#24182;&#20026;&#26631;&#20934;&#21644;&#29109;&#27491;&#21017;&#21270;RL&#20013;&#30340;&#32452;&#21512;&#24615;&#25552;&#20379;&#20102;&#26356;&#19968;&#33324;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#20989;&#25968;&#65292;&#24863;&#20852;&#36259;&#30340;&#32452;&#21512;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#19982;&#24050;&#30693;&#30340;&#21407;&#22987;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#30456;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#20391;&#19981;&#31561;&#24335;&#65292;&#23558;&#26368;&#20248;&#32452;&#21512;&#20540;&#20989;&#25968;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#20540;&#20989;&#25968;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#38646;-shot&#31574;&#30053;&#30340;&#36951;&#25022;&#21487;&#20197;&#24471;&#21040;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of reinforcement learning (RL), agents are often tasked with solving a variety of problems differing only in their reward functions. In order to quickly obtain solutions to unseen problems with new reward functions, a popular approach involves functional composition of previously solved tasks. However, previous work using such functional composition has primarily focused on specific instances of composition functions whose limiting assumptions allow for exact zero-shot composition. Our work unifies these examples and provides a more general framework for compositionality in both standard and entropy-regularized RL. We find that, for a broad class of functions, the optimal solution for the composite task of interest can be related to the known primitive task solutions. Specifically, we present double-sided inequalities relating the optimal composite value function to the value functions for the primitive tasks. We also show that the regret of using a zero-shot policy can be
&lt;/p&gt;</description></item><item><title>iSAGE&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;&#65292;&#20855;&#22791;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27169;&#22411;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#20855;&#26377;&#21644;SAGE&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2303.01181</link><description>&lt;p&gt;
iSAGE&#65306;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams. (arXiv:2303.01181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01181
&lt;/p&gt;
&lt;p&gt;
iSAGE&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;&#65292;&#20855;&#22791;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27169;&#22411;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#20855;&#26377;&#21644;SAGE&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;SAGE&#31561;&#27969;&#34892;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#27979;&#37327;&#65292;&#22823;&#22810;&#38480;&#20110;&#25209;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#24212;&#29992;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#25968;&#25454;&#25345;&#32493;&#21040;&#36798;&#65292;&#24517;&#39035;&#20197;&#22312;&#32447;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;iSAGE&#65292;&#19968;&#31181;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;SAGE&#22686;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#30772;&#22351;&#65288;&#24178;&#39044;&#65289;&#21644;&#20445;&#30041;&#65288;&#35266;&#27979;&#65289;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;iSAGE&#19982;SAGE&#20855;&#26377;&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#30340;&#25968;&#25454;&#27969;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for explainable artificial intelligence (XAI), including popular feature importance measures such as SAGE, are mostly restricted to the batch learning scenario. However, machine learning is often applied in dynamic environments, where data arrives continuously and learning must be done in an online manner. Therefore, we propose iSAGE, a time- and memory-efficient incrementalization of SAGE, which is able to react to changes in the model as well as to drift in the data-generating process. We further provide efficient feature removal methods that break (interventional) and retain (observational) feature dependencies. Moreover, we formally analyze our explanation method to show that iSAGE adheres to similar theoretical properties as SAGE. Finally, we evaluate our approach in a thorough experimental analysis based on well-established data sets and data streams with concept drift.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer&#8212;&#8212;GNOT&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#21644;&#24341;&#20837;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.14376</link><description>&lt;p&gt;
GNOT: &#19968;&#31181;&#29992;&#20110;&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer
&lt;/p&gt;
&lt;p&gt;
GNOT: A General Neural Operator Transformer for Operator Learning. (arXiv:2302.14376v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31070;&#32463;&#36816;&#31639;&#31526;Transformer&#8212;&#8212;GNOT&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#21644;&#24341;&#20837;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#31639;&#23376;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#35268;&#21017;&#30340;&#32593;&#26684;&#12289;&#22810;&#20010;&#36755;&#20837;&#20989;&#25968;&#21644;&#35299;&#20915;PDE&#35299;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNOT&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24322;&#26500;&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39640;&#24230;&#28789;&#27963;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#20989;&#25968;&#21644;&#19981;&#35268;&#21017;&#32593;&#26684;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20960;&#20309;&#38376;&#25511;&#26426;&#21046;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#36719;&#22495;&#20998;&#35299;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;&#38382;&#39064;&#12290;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#27169;&#22411;&#23481;&#37327;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improv
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26500;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#20197;&#22312;&#38647;&#36798;&#39046;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21450;&#20854;&#28508;&#22312;&#34920;&#31034;&#26469;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;90.72%&#30340;AUROC&#12290;</title><link>http://arxiv.org/abs/2302.14192</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#30340;&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconstruction-based Out-of-Distribution Detection for Short-Range FMCW Radar. (arXiv:2302.14192v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26500;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#20197;&#22312;&#38647;&#36798;&#39046;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#21450;&#20854;&#28508;&#22312;&#34920;&#31034;&#26469;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#22312;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;90.72%&#30340;AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#65288;OOD&#65289;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;OOD&#26816;&#27979;&#22120;&#26088;&#22312;&#21306;&#20998;&#22521;&#35757;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#65292;&#20197;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;OOD&#25968;&#25454;&#30340;&#36807;&#24230;&#33258;&#20449;&#39044;&#27979;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;logit&#12289;&#20013;&#38388;&#29305;&#24449;&#31354;&#38388;&#12289;softmax&#20998;&#25968;&#25110;&#37325;&#26500;&#25439;&#22833;&#65292;&#23427;&#20204;&#33021;&#22815;&#20135;&#29983;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#25968;&#26159;&#38024;&#23545;&#22270;&#20687;&#39046;&#22495;&#24320;&#21457;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26500;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#20197;&#22312;&#38647;&#36798;&#39046;&#22495;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#21450;&#20854;&#28508;&#22312;&#34920;&#31034;&#26469;&#26816;&#27979;OOD&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24471;&#20998;&#65292;&#21253;&#25324;&#22522;&#20110;&#20462;&#34917;&#31243;&#24207;&#30340;&#37325;&#26500;&#25439;&#22833;&#21644;&#20174;&#27599;&#20010;&#20462;&#34917;&#31243;&#24207;&#30340;&#28508;&#22312;&#34920;&#31034;&#35745;&#31639;&#24471;&#20986;&#30340;&#33021;&#37327;&#20540;&#12290;&#22312;&#20351;&#29992;60 GHz sh&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;90.72&#65285;&#30340;AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection recently has drawn attention due to its critical role in the safe deployment of modern neural network architectures in real-world applications. The OOD detectors aim to distinguish samples that lie outside the training distribution in order to avoid the overconfident predictions of machine learning models on OOD data. Existing detectors, which mainly rely on the logit, intermediate feature space, softmax score, or reconstruction loss, manage to produce promising results. However, most of these methods are developed for the image domain. In this study, we propose a novel reconstruction-based OOD detector to operate on the radar domain. Our method exploits an autoencoder (AE) and its latent representation to detect the OOD samples. We propose two scores incorporating the patch-based reconstruction loss and the energy value calculated from the latent representations of each patch. We achieve an AUROC of 90.72% on our dataset collected by using 60 GHz sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#21160;&#24577;&#35268;&#21010;&#30340;&#20048;&#35266;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#25240;&#25187;&#32447;&#24615;&#28151;&#21512;MDPs&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#19988;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>http://arxiv.org/abs/2302.14004</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#21017;&#21270;&#21160;&#24577;&#35268;&#21010;&#30340;&#20048;&#35266;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimistic Planning by Regularized Dynamic Programming. (arXiv:2302.14004v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#21160;&#24577;&#35268;&#21010;&#30340;&#20048;&#35266;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#25240;&#25187;&#32447;&#24615;&#28151;&#21512;MDPs&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#19988;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38480;&#26102;&#27573;&#25240;&#25187;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20048;&#35266;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#22312;&#36817;&#20284;&#20540;&#36845;&#20195;&#36807;&#31243;&#30340;&#26356;&#26032;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#30340;&#24605;&#24819;&#12290;&#27492;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#20813;&#33806;&#32553;&#21644;&#21333;&#35843;&#24615;&#35770;&#35777;&#65292;&#36825;&#36890;&#24120;&#26159;&#29616;&#26377;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#20998;&#26512;&#25152;&#35201;&#27714;&#30340;&#65292;&#29305;&#21035;&#26159;&#21487;&#20197;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;MDPs&#20013;&#20351;&#29992;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#20272;&#35745;&#30340;&#36817;&#20284;&#36716;&#31227;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#24674;&#22797;&#20102;&#34920;&#26684;MDPs&#20013;&#24050;&#30693;&#30340;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#21333;&#20010;&#27969;&#32463;&#39564;&#20013;&#23398;&#20064;&#25240;&#25187;&#32447;&#24615;&#28151;&#21512;MDPs&#20013;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method for optimistic planning in infinite-horizon discounted Markov decision processes based on the idea of adding regularization to the updates of an otherwise standard approximate value iteration procedure. This technique allows us to avoid contraction and monotonicity arguments typically required by existing analyses of approximate dynamic programming methods, and in particular to use approximate transition functions estimated via least-squares procedures in MDPs with linear function approximation. We use our method to recover known guarantees in tabular MDPs and to provide a computationally efficient algorithm for learning near-optimal policies in discounted linear mixture MDPs from a single stream of experience, and show it achieves near-optimal statistical guarantees.
&lt;/p&gt;</description></item><item><title>DeAR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#23558;All-Reduce&#21407;&#35821;&#20998;&#35299;&#25104;&#20004;&#20010;&#36830;&#32493;&#25805;&#20316;&#65292;&#19982;&#21453;&#21521;&#20256;&#25773;&#21644;&#21069;&#21521;&#35745;&#31639;&#21516;&#26102;&#37325;&#21472;&#12290;&#20351;&#29992;&#23454;&#29992;&#30340;&#24352;&#37327;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeAR&#22312;&#35757;&#32451;&#36895;&#24230;&#19978;&#21487;&#36798;&#21040;83%&#21644;15%&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2302.12445</link><description>&lt;p&gt;
DeAR&#65306;&#32454;&#31890;&#24230;All-Reduce&#31649;&#36947;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining. (arXiv:2302.12445v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12445
&lt;/p&gt;
&lt;p&gt;
DeAR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;&#65292;&#23558;All-Reduce&#21407;&#35821;&#20998;&#35299;&#25104;&#20004;&#20010;&#36830;&#32493;&#25805;&#20316;&#65292;&#19982;&#21453;&#21521;&#20256;&#25773;&#21644;&#21069;&#21521;&#35745;&#31639;&#21516;&#26102;&#37325;&#21472;&#12290;&#20351;&#29992;&#23454;&#29992;&#30340;&#24352;&#37327;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DeAR&#22312;&#35757;&#32451;&#36895;&#24230;&#19978;&#21487;&#36798;&#21040;83%&#21644;15%&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#35843;&#24230;&#22312;&#21152;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#23427;&#20351;&#24471;All-Reduce&#36890;&#20449;&#19982;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#37325;&#21472;&#12290;&#36825;&#22312;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#65288;1&#65289;&#38024;&#23545;&#27599;&#20010;All-Reduce&#25805;&#20316;&#65292;&#21551;&#21160;&#24310;&#36831;&#19982;&#24037;&#20316;&#33410;&#28857;&#25968;&#25104;&#27491;&#27604;&#65307;&#65288;2&#65289;&#30001;&#20110;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#21069;&#21521;&#35745;&#31639;&#30340;&#20381;&#36182;&#21644;&#21516;&#27493;&#35201;&#27714;&#65292;&#23427;&#20165;&#33021;&#23454;&#29616;&#27425;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#24230;&#31639;&#27861;DeAR&#65292;&#23558;All-Reduce&#21407;&#35821;&#20998;&#35299;&#25104;&#20004;&#20010;&#36830;&#32493;&#25805;&#20316;&#65292;&#23427;&#20204;&#19982;&#21453;&#21521;&#20256;&#25773;&#21644;&#21069;&#21521;&#35745;&#31639;&#21516;&#26102;&#37325;&#21472;&#32780;&#26080;&#38656;&#39069;&#22806;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#24352;&#37327;&#34701;&#21512;&#31639;&#27861;&#26469;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;&#20351;&#29992;&#20116;&#31181;&#27969;&#34892;&#30340;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeAR&#22312;&#35757;&#32451;&#36895;&#24230;&#19978;&#23454;&#29616;&#20102;&#22810;&#36798;83%&#21644;15%&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication scheduling has been shown to be effective in accelerating distributed training, which enables all-reduce communications to be overlapped with backpropagation computations. This has been commonly adopted in popular distributed deep learning frameworks. However, there exist two fundamental problems: (1) excessive startup latency proportional to the number of workers for each all-reduce operation; (2) it only achieves sub-optimal training performance due to the dependency and synchronization requirement of the feed-forward computation in the next iteration. We propose a novel scheduling algorithm, DeAR, that decouples the all-reduce primitive into two continuous operations, which overlaps with both backpropagation and feed-forward computations without extra communications. We further design a practical tensor fusion algorithm to improve the training performance. Experimental results with five popular models show that DeAR achieves up to 83% and 15% training speedup over the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22810;&#31181;Gumbel-Softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MADDPG&#20013;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11793</link><description>&lt;p&gt;
&#37325;&#35775;MADDPG&#20013;&#30340;Gumbel-Softmax
&lt;/p&gt;
&lt;p&gt;
Revisiting the Gumbel-Softmax in MADDPG. (arXiv:2302.11793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22810;&#31181;Gumbel-Softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MADDPG&#20013;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MADDPG&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;DDPG&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;DDPG&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;&#29366;&#24577;-&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#30340;&#26799;&#24230;&#23384;&#22312;&#12290;&#20026;&#20102;&#20351;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65292;&#24517;&#39035;&#36827;&#34892;&#31163;&#25955;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#23545;&#20110;MADDPG&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;Gumbel-Softmax&#65288;GS&#65289;&#20272;&#31639;&#22120;--&#19968;&#31181;&#23558;&#31163;&#25955;&#20998;&#24067;&#26494;&#24347;&#21040;&#31867;&#20284;&#36830;&#32493;&#20998;&#24067;&#30340;&#20877;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#32479;&#35745;&#20559;&#24046;&#65292;&#26368;&#36817;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#35770;&#25991;&#34920;&#26126;&#65292;&#36825;&#31181;&#20559;&#24046;&#20351;&#24471;MADDPG&#22312;&#26684;&#23376;&#19990;&#30028;&#31561;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;GS&#30340;&#35768;&#22810;&#26367;&#20195;&#26041;&#27861;&#23384;&#22312;&#65292;&#20855;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20854;&#20013;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#31163;&#25955;&#26684;&#23376;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;MADDPG&#20013;&#12290;&#28982;&#21518;&#23545;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#30340;&#30456;&#24212;&#24433;&#21709;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20840;&#23616;&#26368;&#20248;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(CVAE)&#21487;&#20197;&#23398;&#20064;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#20998;&#31163;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.11756</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Manifold Dimensions with Conditional Variational Autoencoders. (arXiv:2302.11756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20840;&#23616;&#26368;&#20248;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(CVAE)&#21487;&#20197;&#23398;&#20064;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#20998;&#31163;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21450;&#20854;&#26465;&#20214;&#25193;&#23637;&#65288;CVAE&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#31934;&#30830;&#34892;&#20026;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#22312;&#25110;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;VAE&#20840;&#23616;&#26368;&#23567;&#20540;&#30830;&#23454;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#32467;&#26524;&#21040;&#26356;&#19968;&#33324;&#30340;CVAEs&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MNIST&#21644;CelebA&#65292;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#35270;&#35273;&#36136;&#37327;&#21644;&#29305;&#24449;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;MalProtect&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10739</link><description>&lt;p&gt;
MalProtect&#65306;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#26597;&#35810;&#25915;&#20987;&#30340;&#29366;&#24577;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (arXiv:2302.10739v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;MalProtect&#65292;&#23427;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26597;&#35810;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#26597;&#35810;&#20250;&#34987;&#19981;&#26029;&#25200;&#21160;&#65292;&#20197;&#26399;&#36798;&#21040;&#29305;&#23450;&#30340;&#20998;&#31867;&#30446;&#30340;&#65292;&#32780;&#19988;&#27809;&#26377;&#20851;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#20102;&#35299;&#65292;&#20165;&#20973;&#20854;&#36755;&#20986;&#12290;&#36828;&#31243;&#25176;&#31649;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#27169;&#22411;&#21644;&#38754;&#21521;&#26381;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#26222;&#36941;&#23384;&#22312;&#24847;&#21619;&#30528;&#26597;&#35810;&#25915;&#20987;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#26500;&#25104;&#20102;&#30495;&#27491;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#29366;&#24577;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#30417;&#35270;&#21644;&#20998;&#26512;&#31995;&#32479;&#25509;&#25910;&#21040;&#30340;&#26597;&#35810;&#24207;&#21015;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#24182;&#38450;&#27490;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29366;&#24577;&#38450;&#24481;&#26426;&#21046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26426;&#21046;&#20165;&#20381;&#36182;&#20110;&#30456;&#20284;&#24615;&#25110;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#22312;&#20854;&#20182;&#39046;&#22495;&#26377;&#25928;&#12290;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#65292;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#26041;&#27861;&#26412;&#36136;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26816;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#26126;&#26174;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MalProtect&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#39046;&#22495;&#35774;&#35745;&#30340;&#29366;&#24577;&#38450;&#24481;&#25216;&#26415;&#12290;MalProtect&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26597;&#35810;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20851;&#20110;&#24694;&#24847;&#21644;&#33391;&#24615;&#26597;&#35810;&#20998;&#24067;&#30340;&#30693;&#35782;&#26469;&#26816;&#27979;&#26597;&#35810;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MalProtect&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#20102;&#26597;&#35810;&#25915;&#20987;&#65292;&#24182;&#25552;&#39640;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML models are known to be vulnerable to adversarial query attacks. In these attacks, queries are iteratively perturbed towards a particular class without any knowledge of the target model besides its output. The prevalence of remotely-hosted ML classification models and Machine-Learning-as-a-Service platforms means that query attacks pose a real threat to the security of these systems. To deal with this, stateful defenses have been proposed to detect query attacks and prevent the generation of adversarial examples by monitoring and analyzing the sequence of queries received by the system. Several stateful defenses have been proposed in recent years. However, these defenses rely solely on similarity or out-of-distribution detection methods that may be effective in other domains. In the malware detection domain, the methods to generate adversarial examples are inherently different, and therefore we find that such detection mechanisms are significantly less effective. Hence, in this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10289</link><description>&lt;p&gt;
&#23558;&#40657;&#21283;&#23376;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#65306;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#65292;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#40657;&#30418;&#27169;&#22411;&#20013;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#40657;&#30418;&#27169;&#22411;&#20998;&#25104;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#28151;&#21512;&#29289;&#21644;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;&#23545;&#21487;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#22522;&#26412;&#25512;&#29702;&#12290;&#27492;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#19988;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#35201;&#20040;&#20174;&#35299;&#37322;&#24615;&#27169;&#22411;&#24320;&#22987;&#65292;&#35201;&#20040;&#20174;&#40657;&#30418;&#24320;&#22987;&#24182;&#20107;&#21518;&#35299;&#37322;&#12290;&#40657;&#30418;&#27169;&#22411;&#28789;&#27963;&#20294;&#38590;&#20197;&#35299;&#37322;&#65292;&#32780;&#35299;&#37322;&#24615;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#30693;&#35782;&#65292;&#24182;&#19988;&#24448;&#24448;&#27604;&#23427;&#20204;&#30340;&#40657;&#30418;&#21464;&#20307;&#19981;&#22815;&#28789;&#27963;&#21644;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#26088;&#22312;&#27169;&#31946;&#40657;&#30418;&#30340;&#20107;&#21518;&#35299;&#37322;&#21644;&#26500;&#24314;&#21487;&#35299;&#37322;&#27169;&#22411;&#20043;&#38388;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20174;&#40657;&#30418;&#24320;&#22987;&#65292;&#36845;&#20195;&#22320;Carve&#20986;&#19968;&#31181;&#28151;&#21512;&#35299;&#37322;&#27169;&#22411;&#65288;MoIE&#65289;&#21644;&#19968;&#20010;&#27531;&#20313;&#32593;&#32476;&#12290;&#27599;&#20010;&#21487;&#35299;&#37322;&#27169;&#22411;&#19987;&#38376;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#23376;&#38598;&#65292;&#24182;&#20351;&#29992;&#19968;&#38454;&#36923;&#36753;(FOL)&#23545;&#20854;&#36827;&#34892;&#35299;&#37322;&#65292;&#20174;&#40657;&#30418;&#20013;&#25552;&#20379;&#22522;&#26412;&#25512;&#29702;&#27010;&#24565;&#12290;&#25105;&#20204;&#36890;&#36807;&#28789;&#27963;&#30340;&#27531;&#24046;&#36335;&#30001;&#20854;&#20313;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#27531;&#36716;&#32593;&#32476;&#19978;&#37325;&#22797;&#35813;&#26041;&#27861;&#65292;&#30452;&#21040;&#25152;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#35299;&#37322;&#25152;&#38656;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#36335;&#32447;&#35268;&#21010;&#65292;&#35299;&#37322;&#21644;&#37325;&#22797;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#20960;&#31181;&#40657;&#21283;&#23376;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#22810;&#20803;&#31995;&#32479;&#24615;&#32570;&#21475;&#39118;&#38505;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21407;&#22987;&#26368;&#20248;&#21270;&#31243;&#24207;&#12289;&#23545;&#20598;&#34920;&#31034;&#30340;&#26368;&#20248;&#35299;&#20197;&#21450;&#20844;&#24179;&#39118;&#38505;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2302.10183</link><description>&lt;p&gt;
&#22810;&#20803;&#31995;&#32479;&#39118;&#38505;&#24230;&#37327;&#21450;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Multivariate Systemic Risk Measures and Computation by Deep Learning Algorithms. (arXiv:2302.10183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#22810;&#20803;&#31995;&#32479;&#24615;&#32570;&#21475;&#39118;&#38505;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#21407;&#22987;&#26368;&#20248;&#21270;&#31243;&#24207;&#12289;&#23545;&#20598;&#34920;&#31034;&#30340;&#26368;&#20248;&#35299;&#20197;&#21450;&#20844;&#24179;&#39118;&#38505;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#36890;&#36807;&#22810;&#20803;&#25928;&#29992;&#20989;&#25968;&#23450;&#20041;&#30340;&#31995;&#32479;&#24615;&#32570;&#21475;&#39118;&#38505;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#29305;&#21035;&#20851;&#27880;&#21407;&#22987;&#26368;&#20248;&#35299;&#30340;&#20844;&#24179;&#24615;&#21450;&#30456;&#24212;&#30340;&#39118;&#38505;&#20998;&#37197;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#31639;&#27861;&#20801;&#35768;&#23398;&#20064;&#21407;&#22987;&#26368;&#20248;&#21270;&#31243;&#24207;&#12289;&#23545;&#20598;&#34920;&#31034;&#30340;&#26368;&#20248;&#35299;&#20197;&#21450;&#30456;&#24212;&#30340;&#20844;&#24179;&#39118;&#38505;&#20998;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#22522;&#20110;&#37197;&#23545;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#35813;&#22522;&#20934;&#27169;&#22411;&#21487;&#25552;&#20379;&#26174;&#24335;&#20844;&#24335;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26080;&#27861;&#25552;&#20379;&#26174;&#24335;&#20844;&#24335;&#30340;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose deep learning-based algorithms for the computation of systemic shortfall risk measures defined via multivariate utility functions. We discuss the key related theoretical aspects, with a particular focus on the fairness properties of primal optima and associated risk allocations. The algorithms we provide allow for learning primal optimizers, optima for the dual representation and corresponding fair risk allocations. We test our algorithms by comparison to a benchmark model, based on a paired exponential utility function, for which we can provide explicit formulas. We also show evidence of convergence in a case for which explicit formulas are not available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20351;&#29992;&#24615;&#65292;&#33410;&#30465;&#36164;&#28304;&#21644;&#20419;&#36827;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>http://arxiv.org/abs/2302.09178</link><description>&lt;p&gt;
&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Training Stability for Multitask Ranking Models in Recommender Systems. (arXiv:2302.09178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20351;&#29992;&#24615;&#65292;&#33410;&#30465;&#36164;&#28304;&#21644;&#20419;&#36827;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#35768;&#22810;&#20869;&#23481;&#24179;&#21488;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#25512;&#33616;&#30740;&#31350;&#33268;&#21147;&#20110;&#35774;&#35745;&#26356;&#22909;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;&#20005;&#37325;&#19981;&#36275;&#12290;&#38543;&#30528;&#25512;&#33616;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23427;&#20204;&#36234;&#23481;&#26131;&#21457;&#29983;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20363;&#22914;&#25439;&#22833;&#21457;&#25955;&#65292;&#36825;&#20250;&#20351;&#27169;&#22411;&#26080;&#27861;&#20351;&#29992;&#12289;&#28010;&#36153;&#22823;&#37327;&#36164;&#28304;&#24182;&#38459;&#30861;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#25105;&#20204;&#20026;&#25552;&#39640; YouTube &#25512;&#33616;&#23454;&#38469;&#22810;&#20219;&#21153;&#25490;&#21517;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#25152;&#23398;&#20064;&#21040;&#30340;&#21457;&#29616;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#27169;&#22411;&#29305;&#24615;&#65292;&#24182;&#25512;&#26029;&#20102;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#23545;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#28857;&#38468;&#36817;&#35757;&#32451;&#21160;&#24577;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20551;&#35774;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22833;&#36133;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#32531;&#35299;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play an important role in many content platforms. While most recommendation research is dedicated to designing better models to improve user experience, we found that research on stabilizing the training for such models is severely under-explored. As recommendation models become larger and more sophisticated, they are more susceptible to training instability issues, i.e., loss divergence, which can make the model unusable, waste significant resources and block model developments. In this paper, we share our findings and best practices we learned for improving the training stability of a real-world multitask ranking model for YouTube recommendations. We show some properties of the model that lead to unstable training and conjecture on the causes. Furthermore, based on our observations of training dynamics near the point of training instability, we hypothesize why existing solutions would fail, and propose a new algorithm to mitigate the limitations of existing soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.09167</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#30340;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#19982;&#21327;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixed Traffic Control and Coordination from Pixels. (arXiv:2302.09167v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#25511;&#21046;&#26041;&#27861;&#22312;&#32531;&#35299;&#24403;&#21069;&#25317;&#22581;&#31243;&#24230;&#26041;&#38754;&#24050;&#32463;&#22833;&#25928;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#36827;&#34892;&#20132;&#36890;&#25511;&#21046;&#30340;&#24819;&#27861;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#32423;&#21035;&#33258;&#20027;&#24615;&#36710;&#36742;&#30340;&#19981;&#26029;&#28044;&#29616;&#12290;&#36825;&#24341;&#36215;&#20102;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36710;&#36742;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35843;&#33410;&#20154;&#39550;&#39542;&#36710;&#36742;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#22270;&#20687;&#35266;&#23519;&#20316;&#20026;&#28151;&#21512;&#20132;&#36890;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;1&#65289;&#22270;&#20687;&#36890;&#36807;&#21355;&#26143;&#22270;&#20687;&#12289;&#36710;&#20869;&#25668;&#20687;&#31995;&#32479;&#21644;&#20132;&#36890;&#30417;&#25511;&#31995;&#32479;&#26222;&#36941;&#23384;&#22312;&#65307;2&#65289;&#22270;&#20687;&#19981;&#38656;&#35201;&#26356;&#26032;&#29616;&#26377;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#21521;&#21487;&#33021;&#19981;&#24895;&#24847;&#37197;&#21512;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#20256;&#36882;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion is a persistent problem in our society. Existing methods for traffic control have proven futile in alleviating current congestion levels leading researchers to explore ideas with robot vehicles given the increased emergence of vehicles with different levels of autonomy on our roads. This gives rise to mixed traffic control, where robot vehicles regulate human-driven vehicles through reinforcement learning (RL). However, most existing studies use precise observations that involve global information, such as environment outflow, and local information, i.e., vehicle positions and velocities. Obtaining this information requires updating existing road infrastructure with vast sensor environments and communication to potentially unwilling human drivers. We consider image observations as the alternative for mixed traffic control via RL: 1) images are ubiquitous through satellite imagery, in-car camera systems, and traffic monitoring systems; 2) images do not require a compl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29992;&#20154;&#31867;&#21453;&#39304;&#26367;&#20195;&#20256;&#32479;&#20114;&#32852;&#32593;&#25991;&#26412;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#35757;&#32451;&#26159;&#26368;&#20248;&#21644;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.08582</link><description>&lt;p&gt;
&#29992;&#20154;&#31867;&#20559;&#22909;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Human Preferences. (arXiv:2302.08582v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#29992;&#20154;&#31867;&#21453;&#39304;&#26367;&#20195;&#20256;&#32479;&#20114;&#32852;&#32593;&#25991;&#26412;&#26469;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#35757;&#32451;&#26159;&#26368;&#20248;&#21644;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26159;&#20026;&#20102;&#27169;&#20223;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#22914;&#26524;&#30001;LMs&#29983;&#25104;&#32780;&#36829;&#21453;&#20154;&#31867;&#20559;&#22909;&#30340;&#20869;&#23481;:&#34394;&#20551;&#20449;&#24687;&#65292;&#20882;&#29359;&#24615;&#35780;&#35770;&#65292;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65292;&#36136;&#37327;&#36739;&#20302;&#25110;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#31561;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;LMs&#30340;&#22791;&#36873;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#23427;&#20204;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#19977;&#39033;&#20219;&#21153;&#20013;&#38024;&#23545;&#20154;&#31867;&#21453;&#39304;&#23545;&#20116;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;LMs&#30340;&#19968;&#33268;&#24615;&#21644;&#33021;&#21147;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#25506;&#32034;&#30340;&#26041;&#27861;&#20013;&#26377;&#19968;&#31181;&#24085;&#32047;&#25176;&#26368;&#20248;&#19988;&#31616;&#21333;&#30340;&#26041;&#27861;&#65306;&#26465;&#20214;&#35757;&#32451;&#65292;&#25110;&#23398;&#20064;&#22312;&#22870;&#21169;&#27169;&#22411;&#32473;&#20986;&#30340;&#20154;&#31867;&#20559;&#22909;&#24471;&#20998;&#26465;&#20214;&#19979;&#30340;&#20196;&#29260;&#20998;&#24067;&#12290;&#26465;&#20214;&#35757;&#32451;&#23558;&#19981;&#33391;&#20869;&#23481;&#30340;&#29983;&#25104;&#36895;&#29575;&#38477;&#20302;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#26080;&#35770;&#26159;&#22312;&#27809;&#26377;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#36824;&#26159;&#22312;&#23545;&#25239;&#36873;&#25321;&#30340;&#25552;&#31034;&#19979;&#29983;&#25104;&#65292;&#37117;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#26465;&#20214;&#35757;&#32451;&#20445;&#25345;&#20102;LMs&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#34920;&#26126;&#23427;&#26159;&#39044;&#35757;&#32451;LMs&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#30340;&#25991;&#26412;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task per
&lt;/p&gt;</description></item><item><title>LightGCL&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19981;&#36275;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#23545;&#27604;&#22686;&#24378;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08191</link><description>&lt;p&gt;
LightGCL: &#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29992;&#20110;&#25512;&#33616;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. (arXiv:2302.08191v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08191
&lt;/p&gt;
&lt;p&gt;
LightGCL&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19981;&#36275;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#23545;&#27604;&#22686;&#24378;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;GNN&#32467;&#21512;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20351;&#29992;&#65292;&#22312;&#22788;&#29702;&#39640;&#24230;&#31232;&#30095;&#30340;&#25968;&#25454;&#26041;&#38754;&#37319;&#21462;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#20854;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#29616;&#26377;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#35201;&#20040;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#19978;&#25191;&#34892;&#38543;&#26426;&#25200;&#21160;(&#20363;&#22914;&#33410;&#28857;/&#36793;&#25200;&#21160;)&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#30340;&#22686;&#24378;&#25216;&#26415;(&#20363;&#22914;&#29992;&#25143;&#32858;&#31867;)&#26469;&#29983;&#25104;&#23545;&#27604;&#35270;&#22270;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#20445;&#25345;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#19988;&#24456;&#23481;&#26131;&#21463;&#21040;&#22122;&#38899;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LightGCL&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#22240;&#22122;&#38899;&#32780;&#22833;&#21435;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#23545;&#27604;&#22686;&#24378;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the un
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07425</link><description>&lt;p&gt;
&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#25353;&#29031;&#31616;&#21333;&#30340;&#22810;&#33218;&#21163;&#21290;&#21327;&#35758;&#20849;&#21516;&#34892;&#21160;&#12290;&#20195;&#29702;&#20197;&#39034;&#24207;&#26041;&#24335;&#21040;&#36798;&#65292;&#36873;&#25321;&#27494;&#22120;&#24182;&#25509;&#25910;&#30456;&#20851;&#22870;&#21169;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#20808;&#21069;&#20195;&#29702;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65288;&#27494;&#22120;&#21644;&#22870;&#21169;&#65289;&#65292;&#19981;&#23384;&#22312;&#31169;&#26377;&#20449;&#21495;&#12290;&#23613;&#31649;&#20195;&#29702;&#20849;&#21516;&#38754;&#20020;&#24320;&#21457;&#21644;&#21033;&#29992;&#30340;&#25506;&#32034;&#25240;&#34935;&#65292;&#20294;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26159;&#19968;&#35265;&#38047;&#24773;&#30340;&#65292;&#26080;&#38656;&#32771;&#34385;&#25506;&#32034;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#31995;&#21015;&#19982;&#65288;&#21442;&#25968;&#21270;&#65289;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#21253;&#25324;&#8220;&#26080;&#20559;&#8221;&#34892;&#20026;&#21644;&#21508;&#31181;&#34892;&#20026;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#34892;&#20026;&#30340;&#26497;&#31471;&#29256;&#26412;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#21163;&#21290;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#28201;&#21644;&#30340;&#29256;&#26412;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#25506;&#32034;&#22833;&#36133;&#65292;&#22240;&#27492;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#8220;&#28201;&#21644;&#20048;&#35266;&#8221;&#30340;&#20195;&#29702;&#25552;&#20379;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#28608;&#21169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65306;&#27494;&#22120;&#25506;&#32034;&#26159;&#22266;&#26377;&#20110;&#21163;&#21290;&#38382;&#39064;&#30340;&#65292;&#21482;&#21463;&#24403;&#21069;&#20195;&#29702;&#30340;&#34892;&#21160;&#24433;&#21709;&#65292;&#32780;&#31038;&#20132;&#25506;&#32034;&#26159;&#30001;&#20808;&#21069;&#20195;&#29702;&#34892;&#20026;&#39537;&#21160;&#30340;&#65292;&#22240;&#27492;&#26377;&#21033;&#20110;&#26410;&#26469;&#20195;&#29702;&#12290;&#30001;&#20110;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#38480;&#21046;&#20102;&#31038;&#20132;&#25506;&#32034;&#65292;&#36825;&#31181;&#26435;&#34913;&#34987;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36793;&#30028;&#24863;&#30693;&#21367;&#31215;&#32593;&#32476;&#65288;B-BACN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#31934;&#30830;&#21487;&#38752;&#30340;&#35010;&#32441;&#36793;&#30028;&#26816;&#27979;&#65292;&#24182;&#21516;&#26102;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#24402;&#23646;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06827</link><description>&lt;p&gt;
B-BACN&#65306;&#36125;&#21494;&#26031;&#36793;&#30028;&#24863;&#30693;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#35010;&#32441;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
B-BACN: Bayesian Boundary-Aware Convolutional Network for Crack Characterization. (arXiv:2302.06827v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36793;&#30028;&#24863;&#30693;&#21367;&#31215;&#32593;&#32476;&#65288;B-BACN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#31934;&#30830;&#21487;&#38752;&#30340;&#35010;&#32441;&#36793;&#30028;&#26816;&#27979;&#65292;&#24182;&#21516;&#26102;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#24402;&#23646;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#26816;&#27979;&#35010;&#32441;&#36793;&#30028;&#23545;&#20110;&#32467;&#26500;&#21644;&#26448;&#26009;&#30340;&#21487;&#38752;&#24230;&#35780;&#20272;&#21644;&#39118;&#38505;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#12289;&#35786;&#26029;&#12289;&#39044;&#27979;&#21644;&#32500;&#25252;&#23433;&#25490;&#12290;&#30001;&#20110;&#21508;&#31181;&#38543;&#26426;&#22240;&#32032;&#65292;&#20363;&#22914;&#27979;&#37327;&#22122;&#22768;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#27169;&#22411;&#31616;&#21270;&#65292;&#35010;&#32441;&#26816;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#24402;&#23646;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#36793;&#30028;&#24863;&#30693;&#21367;&#31215;&#32593;&#32476;&#65288;B-BACN&#65289;&#65292;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36793;&#30028;&#32454;&#21270;&#65292;&#20197;&#29983;&#25104;&#31934;&#30830;&#21487;&#38752;&#30340;&#35010;&#32441;&#36793;&#30028;&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;dropout&#26469;&#23398;&#20064;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#39640;&#26031;&#37319;&#26679;&#20989;&#25968;&#26469;&#39044;&#27979;&#27599;&#20010;&#26679;&#26412;&#30340;&#24402;&#23646;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36793;&#30028;&#32454;&#21270;&#25439;&#22833;&#21152;&#20837;&#21040;B-BACN&#20013;&#65292;&#20197;&#22686;&#24378;&#26816;&#27979;&#32570;&#38519;&#36793;&#30028;&#30340;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately detecting crack boundaries is crucial for reliability assessment and risk management of structures and materials, such as structural health monitoring, diagnostics, prognostics, and maintenance scheduling. Uncertainty quantification of crack detection is challenging due to various stochastic factors, such as measurement noises, signal processing, and model simplifications. A machine learning-based approach is proposed to quantify both epistemic and aleatoric uncertainties concurrently. We introduce a Bayesian Boundary-Aware Convolutional Network (B-BACN) that emphasizes uncertainty-aware boundary refinement to generate precise and reliable crack boundary detections. The proposed method employs a multi-task learning approach, where we use Monte Carlo Dropout to learn the epistemic uncertainty and a Gaussian sampling function to predict each sample's aleatoric uncertainty. Moreover, we include a boundary refinement loss to B-BACN to enhance the determination of defect boundari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; iDCF&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28151;&#28102;&#22240;&#32032;&#26469;&#28040;&#38500;&#25512;&#33616;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#26377;&#25928;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.05052</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#28040;&#38500;&#25512;&#33616;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Recommendation by Learning Identifiable Latent Confounders. (arXiv:2302.05052v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; iDCF&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#35782;&#21035;&#30340;&#28151;&#28102;&#22240;&#32032;&#26469;&#28040;&#38500;&#25512;&#33616;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#26377;&#25928;&#24615;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#39044;&#27979;&#29992;&#25143;&#23545;&#26410;&#34987;&#26333;&#20809;&#30340;&#29289;&#21697;&#30340;&#21453;&#39304;&#12290;&#28151;&#28102;&#20559;&#24046;&#26159;&#30001;&#20110;&#23384;&#22312;&#26410;&#27979;&#37327;&#30340;&#21464;&#37327;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#29366;&#20917;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;&#29992;&#25143;&#30340;&#26333;&#20809;&#21644;&#21453;&#39304;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#23545;&#36825;&#20123;&#26410;&#27979;&#37327;&#21464;&#37327;&#20570;&#20986;&#19981;&#21487;&#34892;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#30452;&#25509;&#25512;&#26029;&#29992;&#25143;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#20445;&#35777;&#35782;&#21035;&#20986;&#21453;&#20107;&#23454;&#30340;&#21453;&#39304;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#24102;&#26377;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21487;&#35782;&#21035;&#30340;&#21435;&#28151;&#28102;&#65288;iDCF&#65289;&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#20195;&#29702;&#21464;&#37327;&#65288;&#20363;&#22914;&#65292;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#30340;&#38750;&#35782;&#21035;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;iDCF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#28151;&#28102;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#36817;&#31471;&#22240;&#26524;&#25512;&#26029;&#26469;&#25512;&#26029;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#24182;&#35782;&#21035;&#21453;&#20107;&#23454;&#30340;&#21453;&#39304;&#65292;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#28151;&#28102;&#20559;&#24046;&#21644;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems aim to predict users' feedback on items not exposed to them.  Confounding bias arises due to the presence of unmeasured variables (e.g., the socio-economic status of a user) that can affect both a user's exposure and feedback. Existing methods either (1) make untenable assumptions about these unmeasured variables or (2) directly infer latent confounders from users' exposure. However, they cannot guarantee the identification of counterfactual feedback, which can lead to biased predictions. In this work, we propose a novel method, i.e., identifiable deconfounder (iDCF), which leverages a set of proxy variables (e.g., observed user features) to resolve the aforementioned non-identification issue. The proposed iDCF is a general deconfounded recommendation framework that applies proximal causal inference to infer the unmeasured confounders and identify the counterfactual feedback with theoretical guarantees. Extensive experiments on various real-world and synthetic da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#31181;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#20851;&#32852;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32593;&#32476;&#32467;&#26500;&#21453;&#26144;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#24182;&#20445;&#35777;&#26412;&#22320;&#27169;&#22411;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04363</link><description>&lt;p&gt;
&#25506;&#32034;&#32593;&#32476;&#19978;&#30340;&#27169;&#22411;&#26080;&#20851;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Agnostic Federated Learning over Networks. (arXiv:2302.04363v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#31181;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#20851;&#32852;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#32593;&#32476;&#32467;&#26500;&#21453;&#26144;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#24182;&#20445;&#35777;&#26412;&#22320;&#27169;&#22411;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#21644;&#27169;&#22411;&#32593;&#32476;&#30340;&#27169;&#22411;&#26080;&#20851;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#12290;&#32593;&#32476;&#32467;&#26500;&#21453;&#26144;&#20102;&#26412;&#22320;&#25968;&#25454;&#38598;&#65288;&#32479;&#35745;&#25968;&#25454;&#65289;&#21644;&#23427;&#20204;&#30456;&#20851;&#30340;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#19968;&#31181;&#23454;&#20363;&#65292;&#20854;&#20013;&#27491;&#21017;&#21270;&#39033;&#26159;&#20174;&#25968;&#25454;&#30340;&#32593;&#32476;&#32467;&#26500;&#23548;&#20986;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35201;&#27714;&#33391;&#22909;&#36830;&#25509;&#30340;&#26412;&#22320;&#27169;&#22411;&#24418;&#25104;&#32858;&#31867;&#65292;&#22312;&#19968;&#20010;&#20844;&#20849;&#27979;&#35797;&#38598;&#19978;&#20135;&#29983;&#30456;&#20284;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#21508;&#26679;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290; &#23545;&#36825;&#20123;&#26412;&#22320;&#27169;&#22411;&#21807;&#19968;&#30340;&#38480;&#21046;&#26159;&#23427;&#20204;&#20801;&#35768;&#26377;&#25928;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;&#35757;&#32451;&#65289;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#65292;&#36825;&#26679;&#30340;&#23454;&#29616;&#37117;&#21487;&#20197;&#22312;&#39640;&#32423;&#32534;&#31243;&#24211;&#65288;&#21253;&#25324;scikit-learn&#12289;Keras&#25110;PyTorch&#65289;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a model-agnostic federated learning method for networks of heterogeneous data and models. The network structure reflects similarities between the (statistics of) local datasets and, in turn, their associated local("personal") models. Our method is an instance of empirical risk minimization, with the regularization term derived from the network structure of data. In particular, we require well-connected local models, forming clusters, to yield similar predictions on a common test set. The proposed method allows for a wide range of local models. The only restriction on these local models is that they allow for efficient implementation of regularized empirical risk minimization (training). For a wide range of models, such implementations are available in high-level programming libraries including scikit-learn, Keras or PyTorch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;SO(3)&#21367;&#31215;&#38477;&#32500;&#33267;SO(2)&#65292;&#20197;&#20943;&#23569;&#31561;&#21464;&#21367;&#31215;&#22312;&#39640;&#38454;&#24352;&#37327;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31561;&#21464;&#29699;&#24418;&#36890;&#36947;&#32593;&#32476;&#65288;eSCN&#65289;&#22312;&#22823;&#35268;&#27169;OC-2&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.03655</link><description>&lt;p&gt;
&#23558;SO(3)&#21367;&#31215;&#38477;&#32500;&#33267;SO(2)&#20197;&#23454;&#29616;&#39640;&#25928;&#31561;&#21464;GNN
&lt;/p&gt;
&lt;p&gt;
Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. (arXiv:2302.03655v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;SO(3)&#21367;&#31215;&#38477;&#32500;&#33267;SO(2)&#65292;&#20197;&#20943;&#23569;&#31561;&#21464;&#21367;&#31215;&#22312;&#39640;&#38454;&#24352;&#37327;&#19978;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#31561;&#21464;&#29699;&#24418;&#36890;&#36947;&#32593;&#32476;&#65288;eSCN&#65289;&#22312;&#22823;&#35268;&#27169;OC-2&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#28857;&#20113;&#25110;&#21407;&#23376;&#31561;3D&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#26159;SO(3)&#31561;&#21464;&#30340;&#65292;&#21363;&#23545;3D&#26059;&#36716;&#31561;&#21464;&#12290;&#28982;&#32780;&#65292;&#31561;&#21464;&#21367;&#31215;&#65288;&#26159;&#31561;&#21464;&#32593;&#32476;&#30340;&#22522;&#26412;&#25805;&#20316;&#65289;&#38543;&#30528;&#26356;&#39640;&#38454;&#24352;&#37327;&#30340;&#20351;&#29992;&#65292;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;SO(3)&#21367;&#31215;&#25110;&#24352;&#37327;&#31215;&#38477;&#32500;&#33267;SO(2)&#65292;&#20174;&#32780;&#23558;&#33410;&#28857;&#23884;&#20837;&#30340;&#20027;&#36724;&#19982;&#36793;&#21521;&#37327;&#23545;&#40784;&#65292;&#20174;&#32780;&#31232;&#30095;&#21270;&#24352;&#37327;&#31215;&#24182;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;O(L^6)&#38477;&#33267;O(L^3)&#65292;&#20854;&#20013;L&#20026;&#34920;&#31034;&#30340;&#24230;&#12290;&#36890;&#36807;&#25552;&#20986;&#21033;&#29992;&#25105;&#20204;&#26032;&#30340;&#31561;&#21464;&#21367;&#31215;&#26041;&#27861;&#23454;&#29616;&#31561;&#21464;&#29699;&#24418;&#36890;&#36947;&#32593;&#32476;&#65288;eSCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19988;&#22312;&#22823;&#35268;&#27169;OC-2&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#25913;&#36827;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be $SO(3)$ equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the $SO(3)$ convolutions or tensor products to mathematically equivalent convolutions in $SO(2)$ . This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from $O(L^6)$ to $O(L^3)$, where $L$ is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network (eSCN), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale OC-2
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#20250;&#23548;&#33268;&#31616;&#21333;&#20219;&#21153;&#30340;&#28010;&#36153;&#65292;&#36879;&#26126;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.02804</link><description>&lt;p&gt;
&#19981;&#35201;&#20877;&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#20219;&#21153;&#65292;&#36716;&#32780;&#20351;&#29992;&#36879;&#26126;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stop overkilling simple tasks with black-box models and use transparent models instead. (arXiv:2302.02804v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02804
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#20250;&#23548;&#33268;&#31616;&#21333;&#20219;&#21153;&#30340;&#28010;&#36153;&#65292;&#36879;&#26126;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#36825;&#20801;&#35768;&#36339;&#36807;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#23481;&#26131;&#20986;&#38169;&#21644;&#28902;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#22312;&#31934;&#24230;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the employment of deep learning methods has led to several significant breakthroughs in artificial intelligence. Different from traditional machine learning models, deep learning-based approaches are able to extract features autonomously from raw data. This allows for bypassing the feature engineering process, which is generally considered to be both error-prone and tedious. Moreover, deep learning strategies often outperform traditional models in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25237;&#24433;&#25439;&#22833;&#23545;&#65292;&#19982;Rademacher&#24207;&#21015;&#30456;&#20851;&#32852;&#26469;&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#35774;&#32622;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2302.02432</link><description>&lt;p&gt;
&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#26356;&#32039;&#23494;
&lt;/p&gt;
&lt;p&gt;
Tighter Information-Theoretic Generalization Bounds from Supersamples. (arXiv:2302.02432v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25237;&#24433;&#25439;&#22833;&#23545;&#65292;&#19982;Rademacher&#24207;&#21015;&#30456;&#20851;&#32852;&#26469;&#28304;&#20110;&#36229;&#21462;&#26679;&#30340;&#35774;&#32622;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#23398;&#20064;&#31639;&#27861;&#30340;&#21508;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#65292;&#28304;&#20110;Steinke&#65286;Zakynthinou&#65288;2020&#65289;&#30340;&#36229;&#21462;&#26679;&#35774;&#32622;-&#8220;&#26465;&#20214;&#20114;&#20449;&#24687;&#8221;&#26694;&#26550;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#24320;&#21457;&#21033;&#29992;&#23558;&#25439;&#22833;&#23545;&#65288;&#20174;&#35757;&#32451;&#23454;&#20363;&#21644;&#27979;&#35797;&#23454;&#20363;&#33719;&#24471;&#65289;&#25237;&#24433;&#21040;&#21333;&#20010;&#25968;&#23383;&#65292;&#24182;&#23558;&#25439;&#22833;&#20540;&#19982;Rademacher&#24207;&#21015;&#65288;&#21450;&#20854;&#31227;&#21160;&#21464;&#20307;&#65289;&#30456;&#20851;&#32852;&#12290;&#25152;&#21576;&#29616;&#30340;&#30028;&#38480;&#21253;&#25324;&#24179;&#26041;&#26681;&#30028;&#38480;&#65292;&#24555;&#36895;&#29575;&#30028;&#38480;&#65292;&#21253;&#25324;&#22522;&#20110;&#26041;&#24046;&#21644;&#23574;&#38160;&#24230;&#30340;&#30028;&#38480;&#20197;&#21450;&#25554;&#20540;&#31639;&#27861;&#30340;&#30028;&#38480;&#31561;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#25110;&#32463;&#39564;&#19978;&#35777;&#26126;&#65292;&#36825;&#20123;&#30028;&#38480;&#27604;&#21516;&#19968;&#36229;&#21462;&#26679;&#35774;&#32622;&#20013;&#36804;&#20170;&#24050;&#30693;&#30340;&#25152;&#26377;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#37117;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a variety of novel information-theoretic generalization bounds for learning algorithms, from the supersample setting of Steinke &amp; Zakynthinou (2020)-the setting of the "conditional mutual information" framework. Our development exploits projecting the loss pair (obtained from a training instance and a testing instance) down to a single number and correlating loss values with a Rademacher sequence (and its shifted variants). The presented bounds include square-root bounds, fast-rate bounds, including those based on variance and sharpness, and bounds for interpolating algorithms etc. We show theoretically or empirically that these bounds are tighter than all information-theoretic bounds known to date on the same supersample setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;&#65292;&#24182;&#33021;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2302.01308</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;
&lt;/p&gt;
&lt;p&gt;
Large language models predict human sensory judgments across six modalities. (arXiv:2302.01308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#39044;&#27979;&#20154;&#31867;&#22312;&#20845;&#20010;&#24863;&#23448;&#27169;&#24577;&#19979;&#30340;&#24863;&#30693;&#35780;&#21028;&#65292;&#24182;&#33021;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#20174;&#35821;&#35328;&#20013;&#21487;&#20197;&#24674;&#22797;&#24863;&#30693;&#19990;&#30028;&#30340;&#31243;&#24230;&#26159;&#21746;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#65292;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#20174;&#35821;&#35328;&#20013;&#25552;&#21462;&#24863;&#30693;&#20449;&#24687;&#30340;&#19979;&#38480;&#65292;&#21487;&#20197;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;GPT&#27169;&#22411;&#20013;&#24341;&#20986;&#20102;&#20845;&#20010;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#22312;&#25152;&#26377;&#39046;&#22495;&#20013;&#22343;&#19982;&#20154;&#31867;&#25968;&#25454;&#26174;&#33879;&#30456;&#20851;&#65292;&#22238;&#22797;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#34920;&#29616;&#65292;&#22914;&#39068;&#33394;&#29615;&#21644;&#38899;&#39640;&#34746;&#26059;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#19978;&#20849;&#21516;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;GPT-4&#65289;&#24182;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#23545;&#35270;&#35273;&#27169;&#24577;&#30340;&#29305;&#23450;&#25913;&#36827;&#12290;&#20026;&#20102;&#30740;&#31350;&#29305;&#23450;&#35821;&#35328;&#23545;&#24863;&#30693;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#39068;&#33394;&#21629;&#21517;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#22797;&#21046;&#20102;&#36328;&#35821;&#35328;&#24046;&#24322;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the extent to which the perceptual world can be recovered from language is a longstanding problem in philosophy and cognitive science. We show that state-of-the-art large language models can unlock new insights into this problem by providing a lower bound on the amount of perceptual information that can be extracted from language. Specifically, we elicit pairwise similarity judgments from GPT models across six psychophysical datasets. We show that the judgments are significantly correlated with human data across all domains, recovering well-known representations like the color wheel and pitch spiral. Surprisingly, we find that a model (GPT-4) co-trained on vision and language does not necessarily lead to improvements specific to the visual modality. To study the influence of specific languages on perception, we also apply the models to a multilingual color-naming task. We find that GPT-4 replicates cross-linguistic variation in English and Russian illuminating the interacti
&lt;/p&gt;</description></item><item><title>Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01128</link><description>&lt;p&gt;
Mnemosyne: &#20351;&#29992;Transformers&#26469;&#35757;&#32451;Transformers
&lt;/p&gt;
&lt;p&gt;
Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01128
&lt;/p&gt;
&lt;p&gt;
Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26550;&#26500;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#21644;&#26102;&#38388;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#20248;&#21270;&#22120;&#24182;&#35843;&#33410;&#20854;&#36229;&#21442;&#25968;&#12290;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25163;&#21160;&#35774;&#35745;ML&#20248;&#21270;&#22120;&#30340;&#26356;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mnemosyne&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;Performers: &#38544;&#24335;&#20302;&#31209;attention Transformers&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Mnemosyne&#65306;(a)&#27604;&#27969;&#34892;&#30340;LSTM&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;(b)&#29305;&#21035;&#22320;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;MLPs&#19978;&#36827;&#34892;&#20803;&#35757;&#32451;&#21518;&#25104;&#21151;&#22320;&#35757;&#32451;Vision Transformers(ViTs) (c)&#21487;&#20197;&#21021;&#22987;&#21270;&#20248;&#21270;&#22120;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#32467;&#26524;&#24320;&#21551;&#20102;&#20351;&#29992;Transformers&#26500;&#24314;&#22522;&#30784;&#20248;&#21270;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35268;&#30340;Transformer&#35757;&#32451;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>SOBER&#31639;&#27861;&#26159;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#36827;&#34892;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#65292;&#19988;&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11832</link><description>&lt;p&gt;
SOBER&#65306;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
SOBER: Highly Parallel Bayesian Optimization and Bayesian Quadrature over Discrete and Mixed Spaces. (arXiv:2301.11832v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11832
&lt;/p&gt;
&lt;p&gt;
SOBER&#31639;&#27861;&#26159;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#36827;&#34892;&#39640;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#65292;&#19988;&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#22788;&#29702;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#36125;&#21494;&#26031;&#31215;&#20998;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#38656;&#24182;&#34892;&#26597;&#35810;&#26114;&#36149;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#25191;&#34892;&#20248;&#21270;&#21644;&#31215;&#20998;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#25209;&#37327;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;SOBER&#65292;&#23427;&#20801;&#35768;&#22312;&#31163;&#25955;&#21644;&#28151;&#21512;&#31354;&#38388;&#19978;&#20351;&#29992;&#20219;&#24847;&#37319;&#38598;&#20989;&#25968;&#21644;&#20869;&#26680;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#22810;&#26679;&#21270;&#30340;&#25209;&#37327;&#20840;&#23616;&#20248;&#21270;&#21644;&#31215;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#20840;&#23616;&#20248;&#21270;&#30340;&#25209;&#37327;&#36873;&#25321;&#37325;&#26032;&#23450;&#20041;&#20026;&#31215;&#20998;&#38382;&#39064;&#65292;&#24182;&#23558;&#37319;&#38598;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#65288;&#38750;&#20984;&#65289;&#26494;&#24347;&#20026;&#20869;&#26680;&#37325;&#32452;&#65288;&#20984;&#65289;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20004;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;SOBER&#20248;&#20110;11&#20010;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#25216;&#26415;SSSD-ECG&#65292;&#22312;&#26681;&#25454;70&#22810;&#20010;&#24515;&#30005;&#22270;&#35821;&#21477;&#29983;&#25104;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2301.08227</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26465;&#20214;&#24515;&#30005;&#20449;&#21495;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Conditional ECG Generation with Structured State Space Models. (arXiv:2301.08227v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26032;&#25216;&#26415;SSSD-ECG&#65292;&#22312;&#26681;&#25454;70&#22810;&#20010;&#24515;&#30005;&#22270;&#35821;&#21477;&#29983;&#25104;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#35299;&#20915;&#25935;&#24863;&#20581;&#24247;&#25968;&#25454;&#20998;&#24067;&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20026;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24335;&#35774;&#23450;&#20102;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#26631;&#20934;&#12290;&#26368;&#36817;&#65292;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20986;&#29616;&#65292;&#25104;&#20026;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#30340;&#24378;&#22823;&#24314;&#27169;&#33539;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SSSD-ECG&#65292;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26681;&#25454;70&#22810;&#20010;&#24515;&#30005;&#22270;&#35821;&#21477;&#29983;&#25104;&#21512;&#25104;12&#23548;&#32852;&#24515;&#30005;&#22270;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#30001;&#20110;&#27809;&#26377;&#21487;&#38752;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26465;&#20214;&#21464;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#22312;&#29983;&#25104;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26469;&#24443;&#24213;&#35780;&#20272;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#35780;&#20272;&#20102;&#20165;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;SSSD-ECG&#26126;&#26174;&#20248;&#20110;&#20854;&#22522;&#20110;GAN&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation is a promising solution to address privacy issues with the distribution of sensitive health data. Recently, diffusion models have set new standards for generative models for different data modalities. Also very recently, structured state space models emerged as a powerful modeling paradigm to capture long-term dependencies in time series. We put forward SSSD-ECG, as the combination of these two technologies, for the generation of synthetic 12-lead electrocardiograms conditioned on more than 70 ECG statements. Due to a lack of reliable baselines, we also propose conditional variants of two state-of-the-art unconditional generative models. We thoroughly evaluate the quality of the generated samples, by evaluating pretrained classifiers on the generated data and by evaluating the performance of a classifier trained only on synthetic data, where SSSD-ECG clearly outperforms its GAN-based competitors. We demonstrate the soundness of our approach through further exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#24110;&#21161;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.14447</link><description>&lt;p&gt;
&#19968;&#31181;AI&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#29702;&#35770;&#26694;&#26550;&#21450;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Framework for AI Models Explainability with Application in Biomedicine. (arXiv:2212.14447v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#24110;&#21161;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21463;&#21040;&#19981;&#21516;&#26041;&#27861;&#21644;&#39046;&#22495;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#25991;&#31456;&#65292;&#20294;XAI&#20173;&#32570;&#20047;&#20849;&#20139;&#30340;&#26415;&#35821;&#21644;&#26694;&#26550;&#65292;&#26080;&#27861;&#20026;&#35299;&#37322;&#25552;&#20379;&#32467;&#26500;&#19978;&#30340;&#23436;&#25972;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#23450;&#20041;&#26159;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#30340;&#32508;&#21512;&#20307;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35299;&#37322;&#19981;&#26159;&#21407;&#23376;&#24615;&#30340;&#65292;&#32780;&#26159;&#26469;&#33258;&#20110;&#27169;&#22411;&#21644;&#20854;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#30340;&#35777;&#25454;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#36825;&#20123;&#35777;&#25454;&#30340;&#35299;&#37322;&#32452;&#21512;&#32780;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35299;&#37322;&#32435;&#20837;&#21040;&#30495;&#23454;&#24615;&#65288;&#21363;&#35299;&#37322;&#26159;&#21542;&#26159;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#30495;&#23454;&#25551;&#36848;&#65289;&#21644;&#21487;&#20449;&#24230;&#65288;&#21363;&#35299;&#37322;&#23545;&#29992;&#25143;&#30340;&#35828;&#26381;&#21147;&#65289;&#30340;&#29305;&#24615;&#20013;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#31616;&#21270;&#20102;&#36825;&#20123;&#29305;&#24615;&#30340;&#25805;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;AI&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#25512;&#36827;AI&#31639;&#27861;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#21487;&#20449;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.13381</link><description>&lt;p&gt;
MixupE&#65306;&#20174;&#26041;&#21521;&#23548;&#25968;&#35282;&#24230;&#29702;&#35299;&#21644;&#25913;&#36827;Mixup&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26041;&#21521;&#23548;&#25968;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;Mixup&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#36335;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#25913;&#36827;&#29256;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#36755;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#29983;&#25104;&#39069;&#22806;&#30340;&#26679;&#26412;&#12290;&#35813;&#25216;&#26415;&#24050;&#34987;&#35777;&#23454;&#22312;&#35768;&#22810;&#23398;&#20064;&#33539;&#24335;&#21644;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#23545;Mixup&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#38544;&#21547;&#22320;&#23545;&#25152;&#26377;&#38454;&#25968;&#30340;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#23548;&#25968;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;Mixup&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;&#29256;&#26412;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#34920;&#26684;&#25968;&#25454;&#12289;&#35821;&#38899;&#21644;&#22270;&#24418;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;Mixup&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#22312;&#20351;&#29992;&#21508;&#31181;&#26550;&#26500;&#26102;&#37117;&#34920;&#29616;&#20986;&#27604;Mixup&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#22312;ImageNet&#30340;top-1&#31934;&#24230;&#19978;&#27604;Mixup&#25552;&#39640;&#20102;0.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#25509;&#21475;&#32593;&#32476;&#65288;RINs&#65289;&#32467;&#26500;&#65292;&#23558;&#26680;&#24515;&#35745;&#31639;&#19982;&#25968;&#25454;&#32500;&#25968;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#32467;&#26500;&#23558;&#22823;&#37096;&#20998;&#35745;&#31639;&#38598;&#20013;&#22312;&#28508;&#22312;&#26631;&#35760;&#19978;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#22312;&#28508;&#22312;&#26631;&#35760;&#21644;&#25968;&#25454;&#26631;&#35760;&#20043;&#38388;&#36335;&#30001;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#24182;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#32500;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.11972</link><description>&lt;p&gt;
&#21487;&#20280;&#32553;&#33258;&#36866;&#24212;&#35745;&#31639;&#29992;&#20110;&#36845;&#20195;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scalable Adaptive Computation for Iterative Generation. (arXiv:2212.11972v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#25509;&#21475;&#32593;&#32476;&#65288;RINs&#65289;&#32467;&#26500;&#65292;&#23558;&#26680;&#24515;&#35745;&#31639;&#19982;&#25968;&#25454;&#32500;&#25968;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#36825;&#19968;&#32467;&#26500;&#23558;&#22823;&#37096;&#20998;&#35745;&#31639;&#38598;&#20013;&#22312;&#28508;&#22312;&#26631;&#35760;&#19978;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#22312;&#28508;&#22312;&#26631;&#35760;&#21644;&#25968;&#25454;&#26631;&#35760;&#20043;&#38388;&#36335;&#30001;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#24182;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#32500;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#25968;&#25454;&#26159;&#20887;&#20313;&#30340;&#65292;&#20294;&#20027;&#35201;&#30340;&#32467;&#26500;&#20173;&#28982;&#22312;&#20854;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#19978;&#32479;&#19968;&#20999;&#21106;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#25509;&#21475;&#32593;&#32476;&#65288;RINs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32467;&#26500;&#65292;&#23427;&#23558;&#20854;&#26680;&#24515;&#35745;&#31639;&#19982;&#25968;&#25454;&#30340;&#32500;&#25968;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#20280;&#32553;&#30340;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#30340;&#33258;&#36866;&#24212;&#35745;&#31639;&#12290;RINs&#23558;&#22823;&#37096;&#20998;&#35745;&#31639;&#65288;&#21363;&#20840;&#23616;&#33258;&#27880;&#24847;&#21147;&#65289;&#38598;&#20013;&#22312;&#19968;&#32452;&#28508;&#22312;&#26631;&#35760;&#19978;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#22312;&#28508;&#22312;&#26631;&#35760;&#21644;&#25968;&#25454;&#26631;&#35760;&#20043;&#38388;&#35835;&#21462;&#21644;&#20889;&#20837;&#65288;&#21363;&#36335;&#30001;&#65289;&#20449;&#24687;&#12290;&#22534;&#21472;RIN&#27169;&#22359;&#20801;&#35768;&#33258;&#19979;&#32780;&#19978;&#65288;&#20174;&#25968;&#25454;&#21040;&#28508;&#22312;&#65289;&#21644;&#33258;&#19978;&#32780;&#19979;&#65288;&#20174;&#28508;&#22312;&#21040;&#25968;&#25454;&#65289;&#21453;&#39304;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28145;&#23618;&#21644;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#36335;&#30001;&#12290;&#34429;&#28982;&#36825;&#31181;&#36335;&#30001;&#24341;&#20837;&#20102;&#25361;&#25112;&#65292;&#20294;&#22312;&#20219;&#21153;&#65288;&#21644;&#36335;&#30001;&#38382;&#39064;&#65289;&#36880;&#28176;&#21464;&#21270;&#30340;&#24490;&#29615;&#35745;&#31639;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#19981;&#37027;&#20040;&#26840;&#25163;&#20102;&#65292;&#27604;&#22914;&#25193;&#25955;&#27169;&#22411;&#30340;&#36845;&#20195;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#27599;&#27425;&#20462;&#35746;&#36807;&#31243;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#35843;&#33410;&#28508;&#22312;&#26631;&#35760;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36866;&#37197;&#22120;&#32467;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#32500;&#25968;&#25454;&#21644;&#32467;&#26500;&#30340;&#26377;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#23637;&#31034;&#20102;RINs&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;RINs&#21487;&#20197;&#20248;&#38597;&#22320;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#21313;&#19975;&#32500;&#24230;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the rev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10873</link><description>&lt;p&gt;
&#25552;&#21319;&#32447;&#24615;&#25506;&#27979;&#65306;&#36229;&#36234;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#19978;&#24182;&#19981;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#40657;&#21283;&#23376;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#32447;&#24615;&#25506;&#27979;&#33539;&#24335;&#65292;&#21363;&#22312;&#39044;&#20808;&#25552;&#21462;&#30340;&#36755;&#20837;&#34920;&#31034;&#20043;&#19978;&#35757;&#32451;&#36731;&#37327;&#32423;&#37492;&#21035;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;prompt-augmented linear probing&#65288;PALP&#65289;&#65292;&#23427;&#26159;&#32447;&#24615;&#25506;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28151;&#21512;&#20307;&#65292;&#20860;&#20855;&#20108;&#32773;&#30340;&#20248;&#28857;&#12290;PALP&#32487;&#25215;&#20102;&#32447;&#24615;&#25506;&#27979;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#20351;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23558;&#36755;&#20837;&#23450;&#21046;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#27966;&#29983;&#26356;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#28145;&#20837;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;PALP&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#31216;&#20026;data2vec 2.0&#65292;&#23427;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21644;&#20854;&#20182;&#31639;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;</title><link>http://arxiv.org/abs/2212.07525</link><description>&lt;p&gt;
&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#39640;&#25928;&#24615;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language. (arXiv:2212.07525v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39640;&#25928;&#24615;&#25913;&#36827;&#26041;&#27861;&#65292;&#31216;&#20026;data2vec 2.0&#65292;&#23427;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21644;&#20854;&#20182;&#31639;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#26159;&#27169;&#24577;&#29305;&#23450;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;data2vec&#30340;&#35757;&#32451;&#25928;&#29575;&#65292;&#35813;&#23398;&#20064;&#30446;&#26631;&#21487;&#20197;&#25512;&#24191;&#21040;&#22810;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#19981;&#23545;&#25513;&#34109;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#65292;&#20351;&#29992;&#24555;&#36895;&#21367;&#31215;&#35299;&#30721;&#22120;&#65292;&#24182;&#20998;&#25674;&#26500;&#24314;&#25945;&#24072;&#34920;&#31034;&#30340;&#24037;&#20316;&#12290;data2vec 2.0&#21463;&#21040;data2vec&#24341;&#20837;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#21270;&#30446;&#26631;&#34920;&#31034;&#30340;&#30410;&#22788;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;ImageNet-1K&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#20013;&#65292;data2vec 2.0&#22312;&#20302;16.4&#20493;&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#20869;&#19982;&#33945;&#29256;&#33258;&#32534;&#30721;&#22120;&#30340;&#20934;&#30830;&#29575;&#30456;&#21305;&#37197;&#65292;&#22312;Librispeech&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#23427;&#30340;&#34920;&#29616;&#19982;wav2vec 2.0&#30456;&#24403;&#65292;&#26102;&#38388;&#23569;10.6&#20493;&#65292;&#22312;GLUE&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#65292;&#23427;&#19982;&#37325;&#26032;&#35757;&#32451;&#30340;RoBERTa&#27169;&#22411;&#30340;&#26102;&#38388;&#30456;&#27604;&#20943;&#21322;&#12290;&#22312;&#29306;&#29298;&#19968;&#23450;&#30340;&#36895;&#24230;&#20197;&#25442;&#21462;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35757;&#32451;&#20102;150&#20010;epochs&#30340;ViT-L&#27169;&#22411;&#21487;&#20197;&#24471;&#21040;86.8\%&#30340;ImageNet-1K top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on ImageNet-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on GLUE natural language understanding it matches a retrained RoBERTa model in half the time. Trading some speed for accuracy results in ImageNet-1K top-1 accuracy of 86.8\% with a ViT-L model trained for 150 epochs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;-&#36816;&#21160;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30456;&#27604;&#20110;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22266;&#23450;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#65292;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#22522;&#32447;&#22312;&#21508;&#31181;&#31639;&#27861;&#12289;&#20219;&#21153;&#39046;&#22495;&#21644;&#25351;&#26631;&#19978;&#20855;&#26377;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2212.05749</link><description>&lt;p&gt;
&#35770;&#39044;&#35757;&#32451;&#22312;&#35270;&#35273;-&#36816;&#21160;&#25511;&#21046;&#20013;&#30340;&#20316;&#29992;&#65306;&#37325;&#35775;&#20174;&#22836;&#23398;&#20064;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
On Pre-Training for Visuo-Motor Control: Revisiting a Learning-from-Scratch Baseline. (arXiv:2212.05749v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;-&#36816;&#21160;&#25511;&#21046;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#30456;&#27604;&#20110;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22266;&#23450;&#35270;&#35273;&#34920;&#31034;&#26041;&#27861;&#65292;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#22522;&#32447;&#22312;&#21508;&#31181;&#31639;&#27861;&#12289;&#20219;&#21153;&#39046;&#22495;&#21644;&#25351;&#26631;&#19978;&#20855;&#26377;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22312;&#35270;&#35273;-&#36816;&#21160;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#37325;&#35775;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#65288;LfS&#65289;&#22522;&#32447;&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#21644;&#27973;&#23618;ConvNet&#65292;&#24182;&#21457;&#29616;&#36825;&#20010;&#22522;&#32447;&#19982;&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22266;&#23450;&#35270;&#35273;&#34920;&#31034;&#30340;&#26368;&#26032;&#26041;&#27861;&#65288;PVR&#65292;MVP&#65292;R3M&#31561;&#65289;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#21508;&#31181;&#31639;&#27861;&#12289;&#20219;&#21153;&#39046;&#22495;&#21644;&#25351;&#26631;&#19978;&#20855;&#26377;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#24403;&#21069;&#35270;&#35273;-&#36816;&#21160;&#25511;&#21046;&#22522;&#20934;&#20043;&#38388;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#30340;&#38459;&#30861;&#65292;&#36825;&#31181;&#24046;&#36317;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#26469;&#32531;&#35299;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#39044;&#35757;&#32451;&#25511;&#21046;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#65292;&#24182;&#24076;&#26395;&#25105;&#20204;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#32447;&#33021;&#22815;&#24110;&#21161;&#20934;&#30830;&#22320;&#35780;&#20272;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we examine the effectiveness of pre-training for visuo-motor control tasks. We revisit a simple Learning-from-Scratch (LfS) baseline that incorporates data augmentation and a shallow ConvNet, and find that this baseline is surprisingly competitive with recent approaches (PVR, MVP, R3M) that leverage frozen visual representations trained on large-scale vision datasets -- across a variety of algorithms, task domains, and metrics in simulation and on a real robot. Our results demonstrate that these methods are hindered by a significant domain gap between the pre-training datasets and current benchmarks for visuo-motor control, which is alleviated by finetuning. Based on our findings, we provide recommendations for future research in pre-training for control and hope that our simple yet strong baseline will aid in accurately benchmarking progress in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; TIDE&#65292;&#36890;&#36807;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#20811;&#26381;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32467;&#26500;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#22320;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#24182;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02483</link><description>&lt;p&gt;
TIDE&#65306;&#29992;&#20110;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#38388;&#23548;&#25968;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
TIDE: Time Derivative Diffusion for Deep Learning on Graphs. (arXiv:2212.02483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; TIDE&#65292;&#36890;&#36807;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#20811;&#26381;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32467;&#26500;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#22320;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#24182;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102; state-of-the-art &#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#37325;&#35201;&#33539;&#24335;&#26159;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20449;&#24687;&#36890;&#20449;&#20165;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#23454;&#29616;&#12290;&#20351;&#29992;&#36825;&#31181;&#33539;&#24335;&#30340;&#26041;&#27861;&#30340;&#25361;&#25112;&#26159;&#30830;&#20445;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#38271;&#36317;&#31163;&#36890;&#20449;&#65292;&#22240;&#20026;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#23481;&#26131;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23548;&#25968;&#22270;&#25193;&#25955;&#65288;TIDE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#36825;&#20123;&#32467;&#26500;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20248;&#21270;&#25193;&#25955;&#30340;&#31354;&#38388;&#33539;&#22260;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#32593;&#32476;&#36890;&#36947;&#65292;&#20174;&#32780;&#23454;&#29616;&#20013;&#38271;&#36317;&#31163;&#36890;&#20449;&#30340;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#20063;&#20351;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;&#26412;&#22320;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#22522;&#20934;&#21644;&#21512;&#25104;&#32593;&#26684;&#21644;&#22270;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;	state-of-the-art &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to oversmoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches. We show that on both widely used graph benchmarks and synthetic mesh and graph datasets, the proposed framework outperforms state-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21487;&#33021;&#20026;&#38750;&#20984;&#38382;&#39064;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#25042;&#24816;Hessian&#26356;&#26032;&#21644;&#37325;&#29992;&#20808;&#21069;&#30475;&#21040;&#30340;Hessian&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#24635;&#31639;&#26415;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.00781</link><description>&lt;p&gt;
&#25042;&#24816;Hessian&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Second-order optimization with lazy Hessians. (arXiv:2212.00781v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21487;&#33021;&#20026;&#38750;&#20984;&#38382;&#39064;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#25042;&#24816;Hessian&#26356;&#26032;&#21644;&#37325;&#29992;&#20808;&#21069;&#30475;&#21040;&#30340;Hessian&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#24635;&#31639;&#26415;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21487;&#33021;&#26159;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#25042;&#24816;Hessian&#26356;&#26032;&#30340;&#29275;&#39039;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#35745;&#31639;&#26041;&#27861;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#37325;&#22797;&#20351;&#29992;&#20808;&#21069;&#30475;&#21040;&#30340;Hessian&#65292;&#21516;&#26102;&#35745;&#31639;&#26032;&#30340;&#26799;&#24230;&#12290;&#36825;&#26174;&#30528;&#38477;&#20302;&#20102;&#20108;&#38454;&#20248;&#21270;&#26041;&#26696;&#30340;&#24635;&#31639;&#26415;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#31435;&#26041;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#30340;&#24555;&#36895;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#32780;Hessian&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;&#23545;&#20110;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#20108;&#27425;&#27491;&#21017;&#21270;&#30340;&#25042;&#24816;&#29275;&#39039;&#27493;&#39588;&#20855;&#26377;&#20840;&#23616;&#21644;&#23616;&#37096;&#36229;&#32447;&#24615;&#36895;&#29575;&#65292;&#36825;&#26356;&#23481;&#26131;&#35745;&#31639;&#12290;&#26356;&#26032;Hessian&#30340;&#26368;&#20339;&#39057;&#29575;&#26159;&#27599;$d$&#20010;&#36845;&#20195;&#19968;&#27425;&#65292;&#20854;&#20013;$d$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#36825;&#21487;&#20197;&#35777;&#26126;&#23558;&#20108;&#38454;&#31639;&#27861;&#30340;&#24635;&#31639;&#26415;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;$\sqrt{d}$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetical complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetical complexity of second-order algorithms by a factor $\sqrt{d}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65292;&#22522;&#20110;&#20302;&#31209;&#21322;&#27491;&#23450;&#26494;&#24347;&#25216;&#26415;&#23454;&#29616;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#20005;&#26684;&#35748;&#35777;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#37319;&#29992;&#26356;&#20415;&#23452;&#30340;SDP&#26041;&#27861;&#30456;&#24403;&#30340;&#24378;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.17244</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#20984;&#20302;&#31209;&#21322;&#27491;&#23450;&#26494;&#24347;&#23454;&#29616;&#23545;&#23545;&#25239;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#20005;&#26684;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. (arXiv:2211.17244v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65292;&#22522;&#20110;&#20302;&#31209;&#21322;&#27491;&#23450;&#26494;&#24347;&#25216;&#26415;&#23454;&#29616;&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#20005;&#26684;&#35748;&#35777;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#37319;&#29992;&#26356;&#20415;&#23452;&#30340;SDP&#26041;&#27861;&#30456;&#24403;&#30340;&#24378;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#32463;&#39564;&#19978;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#36827;&#34892;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20154;&#20204;&#36890;&#24120;&#24076;&#26395;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#30340;&#25152;&#26377;&#25915;&#20987;&#20013;&#30495;&#27491;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38754;&#23545;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#38590;&#20197;&#20570;&#20986;&#36275;&#22815;&#26377;&#25928;&#30340;&#35777;&#26126;&#12290;&#29305;&#21035;&#26159;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25216;&#26415;&#65292;&#21363;&#20351;&#32463;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#21644;&#20998;&#25903;&#23450;&#30028;&#65288;BnB&#65289;&#25216;&#26415;&#30340;&#25913;&#36827;&#65292;&#20063;&#20250;&#38754;&#20020;"&#20984;&#26494;&#24347;&#22721;&#22418;"&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20302;&#31209;&#21322;&#27491;&#23450;&#26494;&#24347;&#30340;&#38750;&#20984;&#35748;&#35777;&#25216;&#26415;&#12290;&#38750;&#20984;&#26494;&#24347;&#21487;&#20197;&#36827;&#34892;&#19982;&#26356;&#26114;&#36149;&#30340;&#21322;&#27491;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24378;&#35748;&#35777;&#65292;&#21516;&#26102;&#20248;&#21270;&#33539;&#22260;&#26356;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatica
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#19968;&#31181;&#29305;&#27530;&#30340;&#21453;&#24212;&#26041;&#31243;&#65292;&#26159;&#30446;&#21069;&#20854;&#20013;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.14208</link><description>&lt;p&gt;
GREAD: &#22522;&#20110;&#22270;&#31070;&#32463;&#21453;&#24212;&#25193;&#25955;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GREAD: Graph Neural Reaction-Diffusion Networks. (arXiv:2211.14208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#19968;&#31181;&#29305;&#27530;&#30340;&#21453;&#24212;&#26041;&#31243;&#65292;&#26159;&#30446;&#21069;&#20854;&#20013;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#20043;&#19968;&#12290;GNN&#26041;&#27861;&#36890;&#24120;&#26159;&#22522;&#20110;&#22270;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25193;&#25955;&#26041;&#31243;&#34987;&#24191;&#27867;&#29992;&#20110;&#35774;&#35745;GNN&#30340;&#26680;&#24515;&#22788;&#29702;&#23618;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#31687;&#35770;&#25991;&#27880;&#24847;&#21040;&#21453;&#24212;&#26041;&#31243;&#21644;&#25193;&#25955;&#26041;&#31243;&#30340;&#32467;&#21512;&#12290;&#19981;&#36807;&#65292;&#23427;&#20204;&#37117;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#21453;&#24212;&#26041;&#31243;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#27969;&#34892;&#30340;&#21453;&#24212;&#26041;&#31243;&#31867;&#22411;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#29305;&#27530;&#21453;&#24212;&#26041;&#31243;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#20851;&#20110;&#22522;&#20110;&#21453;&#24212;&#25193;&#25955;&#26041;&#31243;&#30340;GNN&#30340;&#26368;&#20840;&#38754;&#30340;&#30740;&#31350;&#20043;&#19968;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;9&#20010;&#25968;&#25454;&#38598;&#21644;28&#20010;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861; GREAD &#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;&#23427;&#20204;&#12290;&#36827;&#19968;&#27493;&#30340;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are one of the most popular research topics for deep learning. GNN methods typically have been designed on top of the graph signal processing theory. In particular, diffusion equations have been widely used for designing the core processing layer of GNNs, and therefore they are inevitably vulnerable to the notorious oversmoothing problem. Recently, a couple of papers paid attention to reaction equations in conjunctions with diffusion equations. However, they all consider limited forms of reaction equations. To this end, we present a reaction-diffusion equation-based GNN method that considers all popular types of reaction equations in addition to one special reaction equation designed by us. To our knowledge, our paper is one of the most comprehensive studies on reaction-diffusion equation-based GNNs. In our experiments with 9 datasets and 28 baselines, our method, called GREAD, outperforms them in a majority of cases. Further synthetic data experiments show
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#21644;&#23618;&#27425;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#29699;&#21592;&#31227;&#21160;&#65292;&#35813;&#27169;&#22411;&#23558;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#23618;&#27425;&#34701;&#21512;&#23618;&#65292;&#20197;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12217</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#21644;&#23618;&#27425;&#34701;&#21512;&#29992;&#20110;&#32701;&#27611;&#29699;&#29699;&#21592;&#31227;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Where Will Players Move Next? Dynamic Graphs and Hierarchical Fusion for Movement Forecasting in Badminton. (arXiv:2211.12217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12217
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#21644;&#23618;&#27425;&#34701;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#29699;&#21592;&#31227;&#21160;&#65292;&#35813;&#27169;&#22411;&#23558;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#23618;&#27425;&#34701;&#21512;&#23618;&#65292;&#20197;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#26512;&#24050;&#32463;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#35757;&#32451;&#31574;&#30053;&#12289;&#29699;&#21592;&#35780;&#20272;&#31561;&#25552;&#20379;&#27934;&#23519;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#39044;&#27979;&#36820;&#22238;&#25216;&#33021;&#30340;&#31867;&#22411;&#20197;&#21450;&#22522;&#20110;&#21069;&#19968;&#27425;&#20987;&#29699;&#29699;&#21592;&#23558;&#31227;&#21160;&#21040;&#21738;&#37324;&#12290;&#30001;&#20110;&#27492;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23558;&#20854;&#26500;&#36896;&#20026;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#19988;&#37319;&#29992;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#31227;&#21160;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#24573;&#30053;&#20102;&#29699;&#21592;&#20043;&#38388;&#20132;&#20114;&#30340;&#24433;&#21709;&#65292;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20851;&#20110;&#19979;&#19968;&#27425;&#31227;&#21160;&#30340;&#22810;&#26041;&#38754;&#35266;&#28857;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#21487;&#20197;&#34920;&#31034;&#29699;&#21592;&#23556;&#20987;&#31867;&#22411;&#21644;&#31227;&#21160;&#20043;&#38388;&#30340;&#25112;&#30053;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#29699;&#21592;&#31227;&#21160;&#22270;&#65288;PM&#65289;&#22270;&#30340;&#36807;&#31243;&#65292;&#20197;&#21033;&#29992;&#20855;&#26377;&#25112;&#30053;&#20851;&#31995;&#30340;&#29699;&#21592;&#30340;&#32467;&#26500;&#31227;&#21160;&#12290;&#22522;&#20110;PM&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#21644;&#23618;&#27425;&#34701;&#21512;&#65288;DGHF&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#30340;&#29699;&#21592;&#31227;&#21160;&#12290;DGHF&#27169;&#22411;&#23558;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#23618;&#27425;&#34701;&#21512;&#23618;&#65292;&#20197;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sports analytics has captured increasing attention since analysis of the various data enables insights for training strategies, player evaluation, etc. In this paper, we focus on predicting what types of returning strokes will be made, and where players will move to based on previous strokes. As this problem has not been addressed to date, movement forecasting can be tackled through sequence-based and graph-based models by formulating as a sequence prediction task. However, existing sequence-based models neglect the effects of interactions between players, and graph-based models still suffer from multifaceted perspectives on the next movement. Moreover, there is no existing work on representing strategic relations among players' shot types and movements. To address these challenges, we first introduce the procedure of the Player Movements (PM) graph to exploit the structural movements of players with strategic relations. Based on the PM graph, we propose a novel Dynamic Graphs and Hier
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11031</link><description>&lt;p&gt;
GRACE&#65306;&#31163;&#25955;&#38190;&#20540;&#36866;&#37197;&#22120;&#23454;&#29616;&#30340;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#65292;&#23427;&#36890;&#36807;&#22312;&#27969;&#24335;&#38169;&#35823;&#19978;&#25191;&#34892;&#30446;&#26631;&#32534;&#36753;&#26469;&#20462;&#22797;&#37096;&#32626;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#29983;&#25104;&#19968;&#20010;&#31163;&#25955;&#12289;&#26412;&#22320;&#30340;&#32534;&#36753;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#65292;&#22312;&#36827;&#34892;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#26102;&#34920;&#29616;&#20026;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#27169;&#22411;&#38543;&#26102;&#38388;&#25512;&#31227;&#20250;&#34928;&#36864;&#65292;&#21407;&#22240;&#26159;&#36755;&#20837;&#30340;&#21464;&#21270;&#12289;&#29992;&#25143;&#38656;&#27714;&#19981;&#26029;&#25913;&#21464;&#12289;&#25110;&#30001;&#20110;&#20986;&#29616;&#30693;&#35782;&#31354;&#32570;&#12290;&#24403;&#21457;&#29616;&#26377;&#23475;&#34892;&#20026;&#26102;&#65292;&#38656;&#35201;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#22120;&#22312;&#22810;&#27425;&#32534;&#36753;&#20013;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRACE&#65292;&#19968;&#31181;&#32456;&#36523;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#22312;&#37096;&#32626;&#27169;&#22411;&#30340;&#27969;&#24335;&#38169;&#35823;&#19978;&#23454;&#29616;&#20102;&#38382;&#39064;&#20462;&#34917;&#65292;&#30830;&#20445;&#23545;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;GRACE&#23558;&#26032;&#30340;&#26144;&#23556;&#39033;&#20889;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31163;&#25955;&#30340;&#12289;&#26412;&#22320;&#30340;&#32534;&#30721;&#26412;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#21482;&#20351;&#29992;&#27969;&#24335;&#38169;&#35823;&#23454;&#29616;&#25968;&#21315;&#20010;&#39034;&#24207;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;T5&#12289;BERT&#21644;GPT&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;GRACE&#22312;&#36827;&#34892;&#24182;&#20445;&#30041;&#32534;&#36753;&#26041;&#38754;&#30340;&#24615;&#33021;&#22788;&#20110;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;HiveNAS&#65292;&#23427;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#23601;&#33021;&#36229;&#36234;&#20854;&#20182;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2211.10250</link><description>&lt;p&gt;
HiveNAS: &#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization. (arXiv:2211.10250v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;HiveNAS&#65292;&#23427;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#23601;&#33021;&#36229;&#36234;&#20854;&#20182;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#30452;&#35273;&#21644;&#35797;&#38169;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#34987;&#24341;&#20837;&#26469;&#31283;&#20581;&#22320;&#25628;&#32034;&#32593;&#32476;&#25299;&#25169;&#65292;&#24182;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#24320;&#21457;&#12290;&#22312;NAS&#19978;&#65292;&#34429;&#28982;&#19968;&#20123;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#36951;&#20256;&#31639;&#27861;&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#31350;&#65292;&#20294;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;HiveNAS&#22312;&#19968;&#23567;&#37096;&#20998;&#26102;&#38388;&#20869;&#23601;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional Neural Network-development process requires substantial expert knowledge and relies heavily on intuition and trial-and-error. Neural Architecture Search (NAS) frameworks were introduced to robustly search for network topologies, as well as facilitate the automated development of Neural Networks. While some optimization approaches -- such as Genetic Algorithms -have been extensively explored in the NAS context, other Metaheuristic Optimization algorithms have not yet been investigated. In this study, we evaluate the viability of Artificial Bee Colony optimization for Neural Architecture Search. Our proposed framework, HiveNAS, outperforms existing state-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of the time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#21463;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#37325;&#24314;&#37325;&#21147;&#27668;&#27969;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20809;&#34928;&#20943;&#25216;&#26415;(LAT)&#24179;&#22343;&#31354;&#38388;&#23494;&#24230;&#27979;&#37327;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23569;&#37327;&#27979;&#37327;&#20540;&#19979;&#24555;&#36895;&#12289;&#31934;&#30830;&#22320;&#37325;&#24314;&#27969;&#22330;&#65292;&#24182;&#19988;&#19982;&#23454;&#39564;&#25968;&#25454;&#27604;&#36739;&#34920;&#26126;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09715</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21463;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#37325;&#24314;&#37325;&#21147;&#27668;&#27969;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks for gravity currents reconstruction from limited data. (arXiv:2211.09715v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#21463;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#37325;&#24314;&#37325;&#21147;&#27668;&#27969;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20809;&#34928;&#20943;&#25216;&#26415;(LAT)&#24179;&#22343;&#31354;&#38388;&#23494;&#24230;&#27979;&#37327;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23569;&#37327;&#27979;&#37327;&#20540;&#19979;&#24555;&#36895;&#12289;&#31934;&#30830;&#22320;&#37325;&#24314;&#27969;&#22330;&#65292;&#24182;&#19988;&#19982;&#23454;&#39564;&#25968;&#25454;&#27604;&#36739;&#34920;&#26126;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#22312;&#19977;&#32500;&#37325;&#24314;&#19981;&#31283;&#23450;&#37325;&#21147;&#27668;&#27969;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#22312;PINN&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#27969;&#22330;&#65292;&#20351;&#20854;&#30446;&#26631;&#20989;&#25968;&#24809;&#32602;&#32593;&#32476;&#39044;&#27979;&#19982;&#23454;&#38469;&#35266;&#27979;&#25968;&#25454;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#24182;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#23884;&#20837;&#22522;&#30784;&#26041;&#31243;&#12290;&#26412;&#30740;&#31350;&#20381;&#36182;&#20110;&#35268;&#33539;&#38145;&#23450;&#20132;&#25442;&#37197;&#32622;&#30340;&#39640;&#20445;&#30495;&#25968;&#20540;&#23454;&#39564;&#65292;&#21487;&#22312;&#20960;&#20010;&#35757;&#32451;&#25968;&#25454;&#24211;&#19978;&#23450;&#37327;&#27979;&#35797;PINNs&#37325;&#24314;&#33021;&#21147;&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#27169;&#25311;&#20102;&#23494;&#24230;&#21644;&#36895;&#24230;&#30340;&#26368;&#26032;&#23454;&#39564;&#27979;&#37327;&#25216;&#26415;&#12290;&#29305;&#21035;&#22320;&#65292;&#37319;&#29992;&#20809;&#34928;&#20943;&#25216;&#26415;(LAT)&#24179;&#22343;&#31354;&#38388;&#23494;&#24230;&#27979;&#37327;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#12290;&#26681;&#25454;&#20004;&#20010;&#26631;&#20934;&#25552;&#20986;&#20102;&#27969;&#37325;&#24314;&#30340;&#26368;&#20339;&#23454;&#39564;&#35774;&#32622;: &#23454;&#26045;&#22797;&#26434;&#24230;&#21644;&#37325;&#24314;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PINNs&#21487;&#20197;&#20174;&#23569;&#37327;LAT&#27979;&#37327;&#20540;&#20013;&#24555;&#36895;&#12289;&#31934;&#30830;&#22320;&#37325;&#24314;&#19981;&#31283;&#23450;&#27969;&#22330;&#65292;&#24182;&#19988;&#19982;&#31890;&#23376;&#22270;&#20687;&#27979;&#36895;&#26415;(PIV)&#21644;&#33639;&#20809;&#39063;&#31890;&#36319;&#36394;&#27979;&#36895;&#26415;(FPTV)&#30340;&#23454;&#39564;&#25968;&#25454;&#27604;&#36739;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present work investigates the use of physics-informed neural networks (PINNs) for the 3D reconstruction of unsteady gravity currents from limited data. In the PINN context, the flow fields are reconstructed by training a neural network whose objective function penalizes the mismatch between the network predictions and the observed data and embeds the underlying equations using automatic differentiation. This study relies on a high-fidelity numerical experiment of the canonical lock-exchange configuration. This allows us to benchmark quantitatively the PINNs reconstruction capabilities on several training databases that mimic state-of-the-art experimental measurement techniques for density and velocity. Notably, spatially averaged density measurements by light attenuation technique (LAT) are employed for the training procedure. An optimal experimental setup for flow reconstruction by PINNs is proposed according to two criteria : the implementation complexity and the accuracy of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#28040;&#38500;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#19979;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#20854;&#19982;&#20808;&#39564;&#30456;&#20851;&#30340;&#35823;&#35782;&#21035;&#19978;&#30028;&#65292;&#27492;&#31639;&#27861;&#20248;&#20110;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#65292;&#19982;&#26080;&#20445;&#35777;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2211.08572</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bayesian Fixed-Budget Best-Arm Identification. (arXiv:2211.08572v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#28040;&#38500;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22266;&#23450;&#39044;&#31639;&#19979;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#20854;&#19982;&#20808;&#39564;&#30456;&#20851;&#30340;&#35823;&#35782;&#21035;&#19978;&#30028;&#65292;&#27492;&#31639;&#27861;&#20248;&#20110;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#65292;&#19982;&#26080;&#20445;&#35777;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#26159;&#19968;&#31181;&#36172;&#21338;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#26368;&#22823;&#21270;&#35782;&#21035;&#26368;&#20339;&#33218;&#30340;&#27010;&#29575;&#65292;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#35266;&#23519;&#39044;&#31639;&#20869;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#28040;&#38500;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#35823;&#35782;&#21035;&#26368;&#20248;&#33218;&#30340;&#27010;&#29575;&#30340;&#19978;&#30028;&#12290;&#36825;&#20010;&#19978;&#30028;&#21453;&#26144;&#20102;&#20808;&#39564;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#26159;&#27492;&#35774;&#32622;&#20013;&#31532;&#19968;&#20010;&#19982;&#20998;&#24067;&#30456;&#20851;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#20110;&#39057;&#29575;&#23398;&#27966;&#30340;&#35770;&#35777;&#35777;&#26126;&#20102;&#23427;&#65292;&#25105;&#20204;&#19968;&#30452;&#20351;&#29992;&#20808;&#39564;&#65292;&#28982;&#21518;&#22312;&#26368;&#21518;&#23558;&#36172;&#24466;&#23454;&#20363;&#31215;&#20998;&#25481;&#65292;&#20063;&#20026;2&#20010;&#33218;&#30340;&#36125;&#21494;&#26031;&#36172;&#24466;&#25552;&#20379;&#20102;&#35823;&#35782;&#21035;&#27010;&#29575;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19978;&#30028;&#22312;&#20219;&#20309;&#39044;&#31639;&#19979;&#65288;&#20960;&#20046;&#65289;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#65292;&#36125;&#21494;&#26031;&#28040;&#38500;&#20248;&#20110;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27492;&#35774;&#32622;&#20013;&#65292;&#19982;&#26080;&#27861;&#20445;&#35777;&#30340;&#26368;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fixed-budget best-arm identification (BAI) is a bandit problem where the agent maximizes the probability of identifying the optimal arm within a fixed budget of observations. In this work, we study this problem in the Bayesian setting. We propose a Bayesian elimination algorithm and derive an upper bound on its probability of misidentifying the optimal arm. The bound reflects the quality of the prior and is the first distribution-dependent bound in this setting. We prove it using a frequentist-like argument, where we carry the prior through, and then integrate out the bandit instance at the end. We also provide a lower bound on the probability of misidentification in a $2$-armed Bayesian bandit and show that our upper bound (almost) matches it for any budget. Our experiments show that Bayesian elimination is superior to frequentist methods and competitive with the state-of-the-art Bayesian algorithms that have no guarantees in our setting.
&lt;/p&gt;</description></item><item><title>SPADE4&#26159;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24615;&#21644;&#26102;&#28382;&#23884;&#20837;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;Takens&#30340;&#26102;&#28382;&#23884;&#20837;&#23450;&#29702;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#21644;&#25429;&#25417;&#22522;&#30784;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2211.08277</link><description>&lt;p&gt;
SPADE4: &#22522;&#20110;&#31232;&#30095;&#24615;&#21644;&#26102;&#28382;&#23884;&#20837;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPADE4: Sparsity and Delay Embedding based Forecasting of Epidemics. (arXiv:2211.08277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08277
&lt;/p&gt;
&lt;p&gt;
SPADE4&#26159;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24615;&#21644;&#26102;&#28382;&#23884;&#20837;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;Takens&#30340;&#26102;&#28382;&#23884;&#20837;&#23450;&#29702;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#21644;&#25429;&#25417;&#22522;&#30784;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#30142;&#30149;&#30340;&#21457;&#23637;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#21644;&#19981;&#23436;&#25972;&#26102;&#12290;&#26368;&#21463;&#27426;&#36814;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#20256;&#26579;&#30149;&#27969;&#34892;&#30340;&#24037;&#20855;&#26159;&#38548;&#31163;&#27169;&#22411;&#12290;&#23427;&#20204;&#23558;&#20154;&#32676;&#26681;&#25454;&#20581;&#24247;&#29366;&#20917;&#20998;&#25104;&#19981;&#21516;&#30340;&#38548;&#31163;&#32676;&#20307;&#65292;&#24182;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#20123;&#38548;&#31163;&#32676;&#20307;&#30340;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30142;&#30149;&#20256;&#25773;&#21644;&#20154;&#31867;&#20114;&#21160;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#39044;&#23450;&#20041;&#31995;&#32479;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#27969;&#34892;&#30149;&#30340;&#30495;&#23454;&#21160;&#24577;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#24615;&#21644;&#26102;&#28382;&#23884;&#20837;&#30340;&#27969;&#34892;&#30149;&#39044;&#27979;&#65288;SPADE4&#65289;&#12290;SPADE4&#39044;&#27979;&#21487;&#35266;&#23519;&#21464;&#37327;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#32780;&#19981;&#38656;&#35201;&#20854;&#20182;&#21464;&#37327;&#25110;&#22522;&#30784;&#31995;&#32479;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#31232;&#30095;&#22238;&#24402;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26469;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;Takens&#30340;&#26102;&#28382;&#23884;&#20837;&#23450;&#29702;&#65292;&#20174;&#35266;&#23519;&#21464;&#37327;&#20013;&#25429;&#25417;&#22522;&#30784;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the evolution of diseases is challenging, especially when the data availability is scarce and incomplete. The most popular tools for modelling and predicting infectious disease epidemics are compartmental models. They stratify the population into compartments according to health status and model the dynamics of these compartments using dynamical systems. However, these predefined systems may not capture the true dynamics of the epidemic due to the complexity of the disease transmission and human interactions. In order to overcome this drawback, we propose Sparsity and Delay Embedding based Forecasting (SPADE4) for predicting epidemics. SPADE4 predicts the future trajectory of an observable variable without the knowledge of the other variables or the underlying system. We use random features model with sparse regression to handle the data scarcity issue and employ Takens' delay embedding theorem to capture the nature of the underlying system from the observed variable. We sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#22312;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#36890;&#36807;&#25429;&#33719;&#35828;&#35805;&#32773;&#36523;&#20221;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2211.08191</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#35821;&#38899;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder. (arXiv:2211.08191v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#22312;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#38899;&#34920;&#24449;&#65292;&#36890;&#36807;&#25429;&#33719;&#35828;&#35805;&#32773;&#36523;&#20221;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#35828;&#35805;&#32773;&#36523;&#20221;&#21644;&#35821;&#38899;&#20869;&#23481;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#21464;&#21270;&#30340;&#20107;&#23454;&#65292;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#28508;&#22312;&#21464;&#37327;&#26469;&#34920;&#31034;&#36825;&#20004;&#20010;&#23646;&#24615;&#12290;&#36890;&#36807;&#19981;&#21516;&#28508;&#22312;&#21464;&#37327;&#30340;&#20808;&#39564;&#35774;&#32622;&#26469;&#23454;&#29616;&#36825;&#20123;&#23646;&#24615;&#30340;&#20998;&#31163;&#12290;&#23545;&#20110;&#35828;&#35805;&#32773;&#36523;&#20221;&#21464;&#37327;&#30340;&#20808;&#39564;&#35774;&#32622;&#65292;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;&#23427;&#26159;&#20855;&#26377;&#21464;&#21270;&#30340;&#22343;&#20540;&#21644;&#22266;&#23450;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#35774;&#32622;&#19968;&#20010;&#36739;&#23567;&#30340;&#22266;&#23450;&#26041;&#24046;&#65292;&#35757;&#32451;&#36807;&#31243;&#20419;&#36827;&#20102;&#30456;&#21516;&#21477;&#23376;&#20869;&#30340;&#36523;&#20221;&#21464;&#37327;&#38752;&#36817;&#20854;&#20808;&#39564;&#22343;&#20540;&#12290;&#20026;&#20102;&#20351;&#21516;&#19968;&#35828;&#35805;&#32773;&#30340;&#34920;&#31034;&#26102;&#36523;&#20221;&#21464;&#37327;&#30456;&#32858;&#32780;&#19982;&#20854;&#20182;&#35828;&#35805;&#32773;&#30340;&#36523;&#20221;&#21464;&#37327;&#30456;&#38548;&#65292;&#25105;&#20204;&#22312;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#20013;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#12290;&#27169;&#22411;&#32467;&#26500;&#27809;&#26377;&#21457;&#29983;&#25913;&#21464;&#12290;&#24341;&#20837;&#23545;&#27604;&#25439;&#22833;&#21518;&#65292;&#23646;&#24615;&#20998;&#31163;&#34920;&#31034;&#26356;&#26377;&#25928;&#22320;&#25429;&#33719;&#20102;&#35828;&#35805;&#32773;&#36523;&#20221;&#12290;&#22312;TIMIT&#25968;&#25454;&#38598;&#21644;VCTK&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#27809;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#22240;&#24335;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging the fact that speaker identity and content vary on different time scales, \acrlong{fhvae} (\acrshort{fhvae}) uses different latent variables to symbolize these two attributes. Disentanglement of these attributes is carried out by different prior settings of the corresponding latent variables. For the prior of speaker identity variable, \acrshort{fhvae} assumes it is a Gaussian distribution with an utterance-scale varying mean and a fixed variance. By setting a small fixed variance, the training process promotes identity variables within one utterance gathering close to the mean of their prior. However, this constraint is relatively weak, as the mean of the prior changes between utterances. Therefore, we introduce contrastive learning into the \acrshort{fhvae} framework, to make the speaker identity variables gathering when representing the same speaker, while distancing themselves as far as possible from those of other speakers. The model structure has not been changed in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;von Mises-Fisher&#20998;&#24067;&#30340;&#26426;&#21046;&#24212;&#29992;&#26041;&#21521;&#38544;&#31169;&#26469;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#20102;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#24179;&#28369;&#36864;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.04686</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#21521;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Directional Privacy for Deep Learning. (arXiv:2211.04686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;von Mises-Fisher&#20998;&#24067;&#30340;&#26426;&#21046;&#24212;&#29992;&#26041;&#21521;&#38544;&#31169;&#26469;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#20102;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#65292;&#21487;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#24179;&#28369;&#36864;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#20445;&#25252;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#38544;&#31169;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26799;&#24230;&#21152;&#20837;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#20854;&#25928;&#29992;&#12290;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#25552;&#20379;&#22522;&#20110;&#20219;&#24847;&#24230;&#37327;&#30340;&#26367;&#20195;&#26426;&#21046;&#65292;&#36825;&#21487;&#33021;&#26356;&#36866;&#21512;&#20110;&#32500;&#25252;&#20854;&#25928;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#20110;von Mises-Fisher&#65288;VMF&#65289;&#20998;&#24067;&#30340;&#26426;&#21046;&#65292;&#37319;&#29992;\textit{&#35282;&#36317;&#31163;} &#25200;&#21160;&#26799;&#24230;&#65292;&#20174;&#32780;&#24191;&#27867;&#20445;&#30041;&#26799;&#24230;&#26041;&#21521;&#65292;&#24212;&#29992;\textit{&#26041;&#21521;&#38544;&#31169;}&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;$\epsilon$-DP&#21644;$\epsilon d$-&#38544;&#31169;&#65292;&#32780;&#19981;&#26159;&#39640;&#26031;&#26426;&#21046;&#30340;$(\epsilon,\delta)$-&#38544;&#31169;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;$\epsilon d$-&#38544;&#31169;&#20445;&#35777;&#19981;&#38656;&#35201;$\delta&gt;0$&#39033;&#65292;&#20294;&#20250;&#26681;&#25454;&#36755;&#20837;&#26799;&#24230;&#30340;&#24046;&#24322;&#32780;&#24179;&#28369;&#36864;&#21270;&#12290;&#38543;&#30528;$\epsilon$&#22312;&#20854;&#20013;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26041;&#21521;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#38544;&#31169;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#25506;&#31350;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#26576;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable for preserving utility. In this paper, we apply \textit{directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb gradients in terms of \textit{angular distance} so that gradient direction is broadly preserved. We show that this provides both $\epsilon$-DP and $\epsilon d$-privacy for deep learning training, rather than the $(\epsilon, \delta)$-privacy of the Gaussian mechanism; we observe that the $\epsilon d$-privacy guarantee does not require a $\delta&gt;0$ term but degrades smoothly according to the dissimilarity of the input gradients.  As $\epsilon$s between thes
&lt;/p&gt;</description></item><item><title>LMD&#26159;&#19968;&#31181;&#25915;&#20987;&#32773;&#29420;&#31435;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35821;&#38899;&#39564;&#35777;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20854;&#26680;&#24515;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25513;&#34109;&#35889;&#22270;&#65292;&#21033;&#29992;ASV&#20998;&#25968;&#30340;&#32477;&#23545;&#24046;&#24322;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.00825</link><description>&lt;p&gt;
LMD&#65306;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25513;&#34109;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#35821;&#38899;&#39564;&#35777;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker Verification. (arXiv:2211.00825v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00825
&lt;/p&gt;
&lt;p&gt;
LMD&#26159;&#19968;&#31181;&#25915;&#20987;&#32773;&#29420;&#31435;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35821;&#38899;&#39564;&#35777;&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20854;&#26680;&#24515;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25513;&#34109;&#35889;&#22270;&#65292;&#21033;&#29992;ASV&#20998;&#25968;&#30340;&#32477;&#23545;&#24046;&#24322;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35821;&#38899;&#39564;&#35777;&#65288;ASV&#65289;&#30340;&#23433;&#20840;&#24615;&#21463;&#21040;&#26368;&#36817;&#20986;&#29616;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20005;&#37325;&#23041;&#32961;&#65292;&#20294;&#24050;&#32463;&#26377;&#19968;&#20123;&#23545;&#31574;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#19981;&#20165;&#38656;&#35201;&#25915;&#20987;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#36824;&#26377;&#24369;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#32773;&#29420;&#31435;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#20197;&#23398;&#20064;&#30340;&#25513;&#34109;&#26816;&#27979;&#22120;&#65288;LMD&#65289;&#65292;&#20197;&#21306;&#20998;&#23545;&#25239;&#24615;&#26679;&#26412;&#21644;&#30495;&#27491;&#30340;&#26679;&#26412;&#12290;&#23427;&#21033;&#29992;&#20998;&#25968;&#21464;&#21270;&#20316;&#20026;&#25351;&#26631;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#25968;&#21464;&#21270;&#26159;&#21407;&#22987;&#38899;&#39057;&#35760;&#24405;&#21450;&#20854;&#20174;&#25513;&#34109;&#22797;&#26434;&#35889;&#22270;&#21512;&#25104;&#30340;&#36716;&#25442;&#38899;&#39057;&#30340;ASV&#20998;&#25968;&#20043;&#38388;&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#20998;&#25968;&#21464;&#21270;&#26816;&#27979;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25513;&#34109;&#35889;&#22270;&#12290;&#31070;&#32463;&#32593;&#32476;&#21482;&#38656;&#35201;&#30495;&#23454;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#25915;&#20987;&#32773;&#29420;&#31435;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the security of automatic speaker verification (ASV) is seriously threatened by recently emerged adversarial attacks, there have been some countermeasures to alleviate the threat. However, many defense approaches not only require the prior knowledge of the attackers but also possess weak interpretability. To address this issue, in this paper, we propose an attacker-independent and interpretable method, named learnable mask detector (LMD), to separate adversarial examples from the genuine ones. It utilizes score variation as an indicator to detect adversarial examples, where the score variation is the absolute discrepancy between the ASV scores of an original audio recording and its transformed audio synthesized from its masked complex spectrogram. A core component of the score variation detector is to generate the masked spectrogram by a neural network. The neural network needs only genuine examples for training, which makes it an attacker-independent approach. Its interpretab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22270;&#20687;&#19978;&#35757;&#32451;&#32534;&#30721;&#39640;&#23618;&#27425;&#30340;&#22495;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#21463;&#23376;&#37319;&#26679;&#26041;&#24335;&#30340;&#24433;&#21709;&#19979;&#39640;&#36136;&#37327;&#31283;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.13834</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#20808;&#39564;&#30340;&#31283;&#23450;&#28145;&#24230;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Stable Deep MRI Reconstruction using Generative Priors. (arXiv:2210.13834v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#22120;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22270;&#20687;&#19978;&#35757;&#32451;&#32534;&#30721;&#39640;&#23618;&#27425;&#30340;&#22495;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#21463;&#23376;&#37319;&#26679;&#26041;&#24335;&#30340;&#24433;&#21709;&#19979;&#39640;&#36136;&#37327;&#31283;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;(MRI)&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20854;&#38598;&#25104;&#21040;&#20020;&#24202;&#24120;&#35268;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#20165;&#26377;&#21442;&#32771;&#24133;&#20540;&#22270;&#20687;&#30340;&#29983;&#25104;&#29615;&#22659;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#35757;&#32451;&#21518;&#65292;&#27491;&#21017;&#21270;&#22120;&#32534;&#30721;&#20102;&#39640;&#23618;&#27425;&#30340;&#22495;&#32479;&#35745;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#26080;&#25968;&#25454;&#22270;&#20687;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21464;&#20998;&#26041;&#27861;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#19981;&#21463;&#23376;&#37319;&#26679;&#26041;&#24335;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#24403;&#38754;&#20020;&#23545;&#27604;&#24230;&#21464;&#21270;&#30340;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#65292;&#35813;&#27169;&#22411;&#21576;&#29616;&#20986;&#31283;&#23450;&#30340;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#35299;&#37322;&#25552;&#20379;&#20102;&#37325;&#24314;&#32467;&#26524;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#20801;&#35768;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches recently achieved remarkable success in magnetic resonance imaging (MRI) reconstruction, but integration into clinical routine remains challenging due to a lack of generalizability and interpretability. In this paper, we address these challenges in a unified framework based on generative image priors. We propose a novel deep neural network based regularizer which is trained in a generative setting on reference magnitude images only. After training, the regularizer encodes higher-level domain statistics which we demonstrate by synthesizing images without data. Embedding the trained model in a classical variational approach yields high-quality reconstructions irrespective of the sub-sampling pattern. In addition, the model shows stable behavior when confronted with out-of-distribution data in the form of contrast variation. Furthermore, a probabilistic interpretation provides a distribution of reconstructions and hence allows uncertainty quantification. To reconstr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERS&#30340;&#36830;&#32493;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#22522;&#26412;&#32676;&#25805;&#20316;&#19979;&#31561;&#20215;&#30340;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#32452;&#31561;&#20215;&#20219;&#21153;&#22521;&#20859;&#19968;&#31181;&#31574;&#30053;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#21807;&#19968;&#31574;&#30053;&#30340;&#25968;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12301</link><description>&lt;p&gt;
&#20855;&#26377;&#32676;&#23545;&#31216;&#30340;&#35270;&#35273;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-based Reinforcement Learning with Group Symmetries. (arXiv:2210.12301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COVERS&#30340;&#36830;&#32493;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#22522;&#26412;&#32676;&#25805;&#20316;&#19979;&#31561;&#20215;&#30340;&#20219;&#21153;&#65292;&#24182;&#20026;&#27599;&#32452;&#31561;&#20215;&#20219;&#21153;&#22521;&#20859;&#19968;&#31181;&#31574;&#30053;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#21807;&#19968;&#31574;&#30053;&#30340;&#25968;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#39034;&#24207;&#23398;&#20064;&#21508;&#31181;&#20219;&#21153;&#65292;&#20445;&#30041;&#25191;&#34892;&#20808;&#21069;&#36935;&#21040;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20026;&#26032;&#20219;&#21153;&#24320;&#21457;&#26032;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#36830;&#32493;RL&#26041;&#27861;&#24573;&#30053;&#20102;&#26576;&#20123;&#20219;&#21153;&#22312;&#22522;&#26412;&#32676;&#36816;&#31639;&#65288;&#22914;&#26059;&#36716;&#25110;&#24179;&#31227;&#65289;&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#12290;&#20182;&#20204;&#21487;&#33021;&#20250;&#20026;&#27599;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#19981;&#24517;&#35201;&#22320;&#23398;&#20064;&#21644;&#32500;&#25252;&#26032;&#31574;&#30053;&#65292;&#23548;&#33268;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;COVERS&#30340;&#21807;&#19968;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#30340;&#36830;&#32493;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20026;&#27599;&#32452;&#31561;&#20215;&#20219;&#21153;&#22521;&#20859;&#19968;&#31181;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#21046;&#23450;&#31574;&#30053;&#12290;COVERS&#37319;&#29992;&#22522;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37197;&#22791;&#31561;&#21464;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#31181;&#20381;&#36182;&#20110;&#25552;&#21462;&#30340;&#19981;&#21464;&#29305;&#24449;&#30340;&#26032;&#20219;&#21153;&#20998;&#32452;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#26700;&#38754;&#25805;&#20316;&#20219;&#21153;&#24207;&#21015;&#19978;&#35780;&#20272;COVERS&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;RL&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;COVERS&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#21807;&#19968;&#31574;&#30053;&#30340;&#25968;&#37327;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning aims to sequentially learn a variety of tasks, retaining the ability to perform previously encountered tasks while simultaneously developing new policies for novel tasks. However, current continual RL approaches overlook the fact that certain tasks are identical under basic group operations like rotations or translations, especially with visual inputs. They may unnecessarily learn and maintain a new policy for each similar task, leading to poor sample efficiency and weak generalization capability. To address this, we introduce a unique Continual Vision-based Reinforcement Learning method that recognizes Group Symmetries, called COVERS, cultivating a policy for each group of equivalent tasks rather than individual tasks. COVERS employs a proximal policy optimization-based RL algorithm with an equivariant feature extractor and a novel task grouping mechanism that relies on the extracted invariant features. We evaluate COVERS on sequences of table-top mani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#29616;&#20195;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#30340;&#20869;&#37096;&#27169;&#22411;&#26469;&#26356;&#21152;&#24555;&#36895;&#22320;&#35299;&#20915;&#26032;&#30340;&#12289;&#26126;&#26174;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36328;&#20219;&#21153;&#36716;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#36328;&#20219;&#21153;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.10763</link><description>&lt;p&gt;
&#20851;&#20110;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36328;&#20219;&#21153;&#36716;&#31227;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning. (arXiv:2210.10763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#29616;&#20195;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#30340;&#20869;&#37096;&#27169;&#22411;&#26469;&#26356;&#21152;&#24555;&#36895;&#22320;&#35299;&#20915;&#26032;&#30340;&#12289;&#26126;&#26174;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36328;&#20219;&#21153;&#36716;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#36328;&#20219;&#21153;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#20174;&#22270;&#20687;&#35266;&#27979;&#20013;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#25968;&#30334;&#19975;&#30340;&#29615;&#22659;&#20132;&#20114;&#25165;&#33021;&#20570;&#21040;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#24182;&#34917;&#20805;&#30495;&#23454;&#29615;&#22659;&#20132;&#20114;&#20197;&#36827;&#34892;&#31574;&#30053;&#25913;&#36827;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#26377;&#25928;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19982;&#20154;&#31867;&#20381;&#36182;&#20110;&#29702;&#35299;&#19990;&#30028;&#21644;&#35270;&#35273;&#32447;&#32034;&#23398;&#20064;&#26032;&#25216;&#33021;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#20195;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#23398;&#20064;&#30340;&#20869;&#37096;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#26469;&#26356;&#24555;&#22320;&#35299;&#20915;&#26032;&#30340;&#12289;&#26126;&#26174;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#36328;&#20219;&#21153;&#36716;&#31227;&#65288;XTRA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#12290;&#36890;&#36807;&#31163;&#32447;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#36328;&#20219;&#21153;&#24494;&#35843;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;XTRA&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#21487;&#25512;&#24191;&#30340;RL&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) algorithms can solve challenging control problems directly from image observations, but they often require millions of environment interactions to do so. Recently, model-based RL algorithms have greatly improved sample-efficiency by concurrently learning an internal model of the world, and supplementing real environment interactions with imagined rollouts for policy improvement. However, learning an effective model of the world from scratch is challenging, and in stark contrast to humans that rely heavily on world understanding and visual cues for learning new skills. In this work, we investigate whether internal models learned by modern model-based RL algorithms can be leveraged to solve new, distinctly different tasks faster. We propose Model-Based Cross-Task Transfer (XTRA), a framework for sample-efficient online RL with scalable pretraining and finetuning of learned world models. By offline multi-task pretraining and online cross-task finetuning, we ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24178;&#39044;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#21305;&#37197;&#25512;&#33616;&#31995;&#32479;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#22522;&#20110;&#31934;&#35843;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#26032;&#39046;&#22495;&#25968;&#25454;&#26102;&#26377;&#21453;&#25928;&#26524;&#65292;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#35299;&#37322;&#27867;&#21270;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2210.10636</link><description>&lt;p&gt;
&#20351;&#29992;&#24178;&#39044;&#26041;&#27861;&#25552;&#39640;&#25991;&#26412;&#21305;&#37197;&#25512;&#33616;&#31995;&#32479;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using Interventions to Improve Out-of-Distribution Generalization of Text-Matching Recommendation Systems. (arXiv:2210.10636v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24178;&#39044;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#21305;&#37197;&#25512;&#33616;&#31995;&#32479;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#22522;&#20110;&#31934;&#35843;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#26032;&#39046;&#22495;&#25968;&#25454;&#26102;&#26377;&#21453;&#25928;&#26524;&#65292;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#35299;&#37322;&#27867;&#21270;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#29992;&#25143;&#30340;&#36755;&#20837;&#25991;&#26412;&#65292;&#25991;&#26412;&#21305;&#37197;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#23558;&#36755;&#20837;&#25991;&#26412;&#19982;&#21487;&#29992;&#21830;&#21697;&#30340;&#25551;&#36848;&#36827;&#34892;&#27604;&#36739;&#26469;&#36755;&#20986;&#30456;&#20851;&#21830;&#21697;&#65292;&#20363;&#22914;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#21830;&#21697;&#25512;&#33616;&#12290;&#30001;&#20110;&#29992;&#25143;&#30340;&#20852;&#36259;&#21644;&#29289;&#21697;&#24211;&#23384;&#39044;&#35745;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#22240;&#27492;&#25991;&#26412;&#21305;&#37197;&#31995;&#32479;&#20855;&#26377;&#27867;&#21270;&#33267;&#25968;&#25454;&#21464;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#39033;&#31216;&#20026;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#31934;&#35843;&#22823;&#22411;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#24050;&#37197;&#23545;&#30340;&#21830;&#21697;&#30456;&#20851;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#27969;&#34892;&#26041;&#27861;&#21487;&#33021;&#23545;OOD&#27867;&#21270;&#20855;&#26377;&#21453;&#25928;&#26524;&#12290;&#23545;&#20110;&#21830;&#21697;&#25512;&#33616;&#20219;&#21153;&#65292;&#22312;&#25512;&#33616;&#26032;&#31867;&#21035;&#25110;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#21830;&#21697;&#26102;&#65292;&#24494;&#35843;&#33719;&#24471;&#30340;&#20934;&#30830;&#24615;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#31181;&#27867;&#21270;&#22833;&#36133;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#26174;&#31034;&#24494;&#35843;&#27169;&#22411;&#25429;&#25417;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#26410;&#23398;&#20064;&#30830;&#23450;&#20219;&#20309;&#20004;&#20010;&#25991;&#26412;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#22240;&#26524;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a user's input text, text-matching recommender systems output relevant items by comparing the input text to available items' description, such as product-to-product recommendation on e-commerce platforms. As users' interests and item inventory are expected to change, it is important for a text-matching system to generalize to data shifts, a task known as out-of-distribution (OOD) generalization. However, we find that the popular approach of fine-tuning a large, base language model on paired item relevance data (e.g., user clicks) can be counter-productive for OOD generalization. For a product recommendation task, fine-tuning obtains worse accuracy than the base model when recommending items in a new category or for a future time period. To explain this generalization failure, we consider an intervention-based importance metric, which shows that a fine-tuned model captures spurious correlations and fails to learn the causal features that determine the relevance between any two tex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pareto&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#25345;&#32493;&#30340;Pareto&#21069;&#27839;&#20135;&#29983;&#65292;&#24182;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#20010;&#20219;&#21153;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.09759</link><description>&lt;p&gt;
Pareto&#27969;&#24418;&#23398;&#20064;&#65306;&#36890;&#36807;&#21333;&#20219;&#21153;&#27169;&#22411;&#38598;&#25104;&#35299;&#20915;&#22810;&#20219;&#21153;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models. (arXiv:2210.09759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pareto&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#25345;&#32493;&#30340;Pareto&#21069;&#27839;&#20135;&#29983;&#65292;&#24182;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#20010;&#20219;&#21153;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;Pareto&#27969;&#24418;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#38598;&#25104;&#21333;&#20219;&#21153;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#25345;&#32493;&#30340;Pareto&#21069;&#27839;&#20135;&#29983;&#65292;&#20854;&#21487;&#20197;&#22312;&#19968;&#20010;&#27169;&#22411;&#20013;&#23454;&#29616;&#22810;&#20010;&#20219;&#21153;&#30340;&#20248;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Multi-Task Learning (MTL), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks' performance, and resort to optimality in the Pareto sense. Most MTL methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose \textit{Pareto Manifold Learning}, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single trai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Gibbs&#31639;&#27861;&#19979;&#30340;&#20266;&#26631;&#35760;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#26631;&#35760;&#21644;&#20266;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#20266;&#26631;&#35760;&#26041;&#27861;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.08188</link><description>&lt;p&gt;
&#20266;&#26631;&#35760;&#23545;&#21322;&#30417;&#30563;Gibbs&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26377;&#20309;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Pseudo-Labeling Affect the Generalization Error of the Semi-Supervised Gibbs Algorithm?. (arXiv:2210.08188v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Gibbs&#31639;&#27861;&#19979;&#30340;&#20266;&#26631;&#35760;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21457;&#29616;&#27867;&#21270;&#24615;&#33021;&#21463;&#21040;&#26631;&#35760;&#21644;&#20266;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#20266;&#26631;&#35760;&#26041;&#27861;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Gibbs&#31639;&#27861;&#23545;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#20266;&#26631;&#35760;&#36827;&#34892;&#20102;&#31934;&#30830;&#21051;&#30011;&#65292;&#32473;&#20986;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#23545;&#31216;&#21270;KL&#20449;&#24687;&#34920;&#36798;&#24335;&#12290;&#21516;&#26102;&#65292;&#33719;&#24471;&#20102;&#27867;&#21270;&#35823;&#24046;&#30340;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#19978;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20266;&#26631;&#35760;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21463;&#21040;&#36755;&#20986;&#20551;&#35774;&#21644;&#36755;&#20837;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#20449;&#24687;&#24433;&#21709;&#65292;&#36824;&#21463;&#21040;&#26631;&#35760;&#21644;&#20266;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#36825;&#20026;&#20174;&#32473;&#23450;&#30340;&#26041;&#27861;&#26063;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#20266;&#26631;&#35760;&#26041;&#27861;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#20026;&#20102;&#21152;&#28145;&#25105;&#20204;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20004;&#20010;&#20363;&#23376;&#8212;&#8212;&#22343;&#20540;&#20272;&#35745;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#29305;&#21035;&#26159;&#20998;&#26512;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#19982;&#26631;&#35760;&#25968;&#25454;&#27604;&#20363;&#955;&#23545;&#27867;&#21270;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide an exact characterization of the expected generalization error (gen-error) for semi-supervised learning (SSL) with pseudo-labeling via the Gibbs algorithm. The gen-error is expressed in terms of the symmetrized KL information between the output hypothesis, the pseudo-labeled dataset, and the labeled dataset. Distribution-free upper and lower bounds on the gen-error can also be obtained. Our findings offer new insights that the generalization performance of SSL with pseudo-labeling is affected not only by the information between the output hypothesis and input training data but also by the information {\em shared} between the {\em labeled} and {\em pseudo-labeled} data samples. This serves as a guideline to choose an appropriate pseudo-labeling method from a given family of methods. To deepen our understanding, we further explore two examples -- mean estimation and logistic regression. In particular, we analyze how the ratio of the number of unlabeled to labeled data $\lambda
&lt;/p&gt;</description></item><item><title>CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07105</link><description>&lt;p&gt;
CORL: &#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;
&lt;/p&gt;
&lt;p&gt;
CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07105
&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#28145;&#24230;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#24378;&#35843;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#20195;&#30721;&#24211;&#21644;&#29616;&#20195;&#20998;&#26512;&#36319;&#36394;&#24037;&#20855;&#12290;&#22312;CORL&#20013;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#23454;&#29616;&#38548;&#31163;&#21040;&#21333;&#29420;&#30340;&#21333;&#20010;&#25991;&#20214;&#20013;&#65292;&#20351;&#24615;&#33021;&#30456;&#20851;&#30340;&#32454;&#33410;&#26356;&#23481;&#26131;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#21487;&#29992;&#20110;&#24110;&#21161;&#35760;&#24405;&#25351;&#26631;&#12289;&#36229;&#21442;&#25968;&#12289;&#20381;&#36182;&#39033;&#31561;&#21040;&#20113;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24120;&#29992;&#30340;D4RL&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20379;&#20102;&#36879;&#26126;&#30340;&#32467;&#26524;&#28304;&#65292;&#21487;&#29992;&#20110;&#24378;&#22823;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#20363;&#22914;&#24615;&#33021;&#27010;&#35201;&#12289;&#25913;&#36827;&#27010;&#29575;&#25110;&#39044;&#26399;&#22312;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FP-Diffusion&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#27491;&#21017;&#21270;DSM&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.04296</link><description>&lt;p&gt;
FP-Diffusion: &#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation. (arXiv:2210.04296v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FP-Diffusion&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#27491;&#21017;&#21270;DSM&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#23398;&#20064;&#19968;&#32452;&#19982;&#25968;&#25454;&#23494;&#24230;&#30456;&#23545;&#24212;&#30340;&#12289;&#22122;&#22768;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#12290;&#36825;&#20123;&#25200;&#21160;&#30340;&#25968;&#25454;&#23494;&#24230;&#36890;&#36807;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65288;FPE&#65289;&#30456;&#20114;&#32852;&#31995;&#65292;&#35813;&#26041;&#31243;&#26159;&#19968;&#31181;&#25551;&#36848;&#29289;&#36136;&#25193;&#25955;&#36807;&#31243;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#23545;&#24212;&#30340;&#26041;&#31243;&#65292;&#31216;&#20026;&#24471;&#20998;FPE&#65292;&#20854;&#29305;&#24449;&#26159;&#25200;&#21160;&#30340;&#25968;&#25454;&#23494;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#26799;&#24230;&#65289;&#30340;&#22122;&#22768;&#26465;&#20214;&#24471;&#20998;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#34429;&#28982;DSM&#24471;&#20998;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#23398;&#20064;&#21040;&#30340;&#24471;&#20998;&#26410;&#33021;&#28385;&#36275;&#24213;&#23618;&#30340;&#24471;&#20998;FPE&#65292;&#36825;&#26159;&#22320;&#38754;&#30495;&#23454;&#24471;&#20998;&#30340;&#22266;&#26377;&#33258;&#19968;&#33268;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#28385;&#36275;&#24471;&#20998;FPE&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#39640;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;DSM&#30446;&#26631;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#24378;&#21046;&#28385;&#36275;&#20998;&#25968;FPE&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2210.04183</link><description>&lt;p&gt;
MAMO&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#30340;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning. (arXiv:2210.04183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#36890;&#36807;&#38544;&#24335;&#21644;&#26174;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#32852;&#21512;&#25513;&#33180;&#20449;&#21495;&#20197;&#25552;&#39640;&#32454;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#20027;&#35201;&#33268;&#21147;&#20110;&#24314;&#31435;&#20840;&#23616;&#32423;&#21035;&#30340;&#22270;&#20687;&#19982;&#35821;&#35328;&#23545;&#40784;&#65292;&#32570;&#20047;&#26377;&#25928;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#25513;&#33180;&#22810;&#27169;&#24577;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#32852;&#21512;&#25513;&#33180;&#65292;&#24182;&#38598;&#25104;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30446;&#26631;&#26469;&#24674;&#22797;&#25513;&#33180;&#20449;&#21495;&#12290;&#20854;&#20013;&#65292;&#38544;&#24335;&#30446;&#26631;&#20026;&#35270;&#35273;&#21644;&#35821;&#35328;&#25552;&#20379;&#32479;&#19968;&#19988;&#26080;&#20559;&#24046;&#30340;&#30446;&#26631;&#65292;&#27169;&#22411;&#39044;&#27979;&#26410;&#25513;&#33180;&#36755;&#20837;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#34920;&#31034;&#65307;&#26174;&#24335;&#30446;&#26631;&#21017;&#36890;&#36807;&#24674;&#22797;&#22270;&#20687;&#22359;&#30340;&#21160;&#37327;&#35270;&#35273;&#29305;&#24449;&#21644;&#21333;&#35789;&#26631;&#35760;&#30340;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#25513;&#33180;&#24314;&#27169;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#31890;&#24230;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#36824;&#33021;&#23398;&#20064;&#21040;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal representation learning has shown promising improvements on various vision-language tasks. Most existing methods excel at building global-level alignment between vision and language while lacking effective fine-grained image-text interaction. In this paper, we propose a jointly masked multimodal modeling method to learn fine-grained multimodal representations. Our method performs joint masking on image-text input and integrates both implicit and explicit targets for the masked signals to recover. The implicit target provides a unified and debiased objective for vision and language, where the model predicts latent multimodal representations of the unmasked input. The explicit target further enriches the multimodal representations by recovering high-level and semantically meaningful information: momentum visual features of image patches and concepts of word tokens. Through such a masked modeling process, our model not only learns fine-grained multimodal interaction, but also a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00069</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#65292;&#23427;&#20551;&#23450;&#25968;&#25454;&#20301;&#20110;&#25110;&#25509;&#36817;&#20110;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340;&#26410;&#30693;&#27969;&#24418;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38750;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#22855;&#24322;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#31181;&#22855;&#24322;&#24615;&#22312;&#25554;&#20540;&#21644;&#25512;&#26029;&#20219;&#21153;&#20043;&#21069;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#25299;&#25169;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#65288;i&#65289;&#37327;&#21270;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#8220;&#27431;&#20960;&#37324;&#24471;&#24615;&#8221;&#35780;&#20998;&#65292;&#29992;&#20197;&#35780;&#20272;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#25429;&#33719;&#22797;&#26434;&#31354;&#38388;&#30340;&#22855;&#24322;&#24615;&#65292;&#21516;&#26102;&#25429;&#25417;&#22855;&#24322;&#32467;&#26500;&#21644;&#23616;&#37096;&#20960;&#20309;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.05355</link><description>&lt;p&gt;
&#20998;&#31867;&#25351;&#26631;&#30340;&#20998;&#26512;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24120;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#34913;&#37327;&#30828;&#20915;&#31574;&#36136;&#37327;&#30340;&#26631;&#20934;&#21644;&#24179;&#34913;&#20934;&#30830;&#29575;&#12289;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#12289;F-beta&#20998;&#25968;&#21644;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#31561;&#25351;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#21644;&#20854;&#20182;&#25351;&#26631;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26399;&#26395;&#25104;&#26412;&#65288;EC&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#27599;&#20010;&#32479;&#35745;&#23398;&#20064;&#35838;&#31243;&#20013;&#37117;&#20171;&#32461;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#37117;&#26159;EC&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;EC&#19982;F&#20998;&#25968;&#21644;MCC&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;EC&#25351;&#26631;&#20248;&#20110;&#20256;&#32479;&#25351;&#26631;&#65292;&#22240;&#20854;&#26356;&#20855;&#26377;&#20248;&#38597;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#19988;&#22522;&#20110;&#32479;&#35745;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#26412;&#25991;&#20013;&#20171;&#32461;&#30340;&#25351;&#26631;&#22343;&#29992;&#20110;&#24230;&#37327;&#30828;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#20998;&#31867;&#31995;&#32479;&#36755;&#20986;&#36830;&#32493;&#24471;&#20998;&#65292;&#32780;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#23454;&#36341;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#36825;&#20123;&#36830;&#32493;&#24471;&#20998;&#20013;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.02167</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#20195;&#29702;&#26469;&#26368;&#23567;&#21270;&#30446;&#26631;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#24320;&#21457;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#40657;&#30418;&#29256;&#26412;&#30340;&#36825;&#20123;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#35266;&#23519;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#23558;&#30446;&#26631;&#20195;&#29702;&#35270;&#20026;&#29615;&#22659;&#30340;&#20219;&#20309;&#20854;&#20182;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#38382;&#39064;&#20013;&#30340;&#38468;&#21152;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30333;&#30418;&#25915;&#20987;&#30340;&#25991;&#29486;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#20197;&#35757;&#32451;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20854;&#28431;&#27934;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;(1)&#25105;&#20204;&#20171;&#32461;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#35266;&#23519;&#30446;&#26631;&#30340;&#20869;&#37096;&#29366;&#24577;&#21644;&#19990;&#30028;&#29366;&#24577;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#25915;&#20987;2&#20154;&#28216;&#25103;&#21644;&#29983;&#25104;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;(2)&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#40657;&#30418;&#25915;&#20987;&#30456;&#27604;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#27604;&#36739;&#22797;&#26434;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#65292;&#24182;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26356;&#36866;&#21512;&#20110;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#21487;&#33719;&#24471;&#39640;&#36798;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;</title><link>http://arxiv.org/abs/2208.08882</link><description>&lt;p&gt;
&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#30340;&#28151;&#21512;&#37327;&#23376;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early heart disease prediction using hybrid quantum classification. (arXiv:2208.08882v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#65292;&#24182;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26356;&#36866;&#21512;&#20110;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#21487;&#33719;&#24471;&#39640;&#36798;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#21457;&#30149;&#29575;&#21644;&#24515;&#33039;&#27515;&#20129;&#29575;&#30340;&#22686;&#21152;&#26126;&#26174;&#24433;&#21709;&#20102;&#20840;&#29699;&#20844;&#20849;&#20581;&#24247;&#21644;&#19990;&#30028;&#32463;&#27982;&#12290;&#26089;&#26399;&#39044;&#27979;&#23545;&#20110;&#20943;&#23569;&#24515;&#33039;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;&#24515;&#33039;&#30142;&#30149;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21644;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20998;&#21035;&#36866;&#29992;&#20110;&#39640;&#32500;&#21644;&#20302;&#32500;&#38382;&#39064;&#12290;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#24322;&#24120;&#25968;&#25454;&#25935;&#24863;&#65292;&#32780;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;&#23545;&#24322;&#24120;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#19982;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#26041;&#27861;&#26356;&#36866;&#21512;&#26089;&#26399;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#22312;Cleveland&#21644;Statlog&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;96.43&#65285;&#21644;97.78&#65285;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rate of heart morbidity and heart mortality increases significantly which affect the global public health and world economy. Early prediction of heart disease is crucial for reducing heart morbidity and mortality. This paper proposes two quantum machine learning methods i.e. hybrid quantum neural network and hybrid random forest quantum neural network for early detection of heart disease. The methods are applied on the Cleveland and Statlog datasets. The results show that hybrid quantum neural network and hybrid random forest quantum neural network are suitable for high dimensional and low dimensional problems respectively. The hybrid quantum neural network is sensitive to outlier data while hybrid random forest is robust on outlier data. A comparison between different machine learning methods shows that the proposed quantum methods are more appropriate for early heart disease prediction where 96.43% and 97.78% area under curve are obtained for Cleveland and Statlog dataset respect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.08003</link><description>&lt;p&gt;
&#30740;&#31350;&#27169;&#22411;&#23485;&#24230;&#21644;&#23494;&#24230;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise. (arXiv:2208.08003v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#65292;&#21363;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#22823;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#26159;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#36825;&#26159;&#36890;&#36807;&#21452;&#19992;&#38477;&#29616;&#35937;&#25429;&#25417;&#30340;&#65292;&#20854;&#20013;&#27979;&#35797;&#25439;&#22833;&#38543;&#30528;&#27169;&#22411;&#23485;&#24230;&#30340;&#22686;&#21152;&#21576;&#29616;&#20986;&#38477;&#20302;-&#22686;&#21152;-&#38477;&#20302;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#22122;&#22768;&#23545;&#27979;&#35797;&#25439;&#22833;&#26354;&#32447;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#21363;&#26631;&#31614;&#22122;&#22768;&#23548;&#33268;&#21407;&#26412;&#35266;&#23519;&#21040;&#30340;&#21452;&#19992;&#38477;&#26354;&#32447;&#20986;&#29616;&#20102;&#8220;&#26368;&#32456;&#19978;&#21319;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36275;&#22815;&#22823;&#30340;&#22122;&#22768;&#26679;&#26412;&#27604;&#29575;&#19979;&#65292;&#20013;&#31561;&#23485;&#24230;&#19979;&#23454;&#29616;&#26368;&#20339;&#27867;&#21270;&#24615;&#33021;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#27979;&#35797;&#25439;&#22833;&#26041;&#24046;&#24418;&#29366;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#19978;&#21319;&#29616;&#35937;&#25193;&#23637;&#21040;&#27169;&#22411;&#23494;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#29702;&#35770;&#34920;&#24449;&#65292;&#34920;&#26126;&#38543;&#26426;&#20002;&#24323;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#20943;&#23569;&#23494;&#24230;&#21487;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#25913;&#21892;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a \textit{final ascent} in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#8212;&#8212;&#31283;&#20581;&#27604;&#29575;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#20844;&#24179;&#31574;&#30053;&#22312;&#20116;&#20010;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2207.04581</link><description>&lt;p&gt;
&#20320;&#30340;&#20844;&#24179;&#27169;&#22411;&#26377;&#22810;&#31283;&#20581;&#65311;&#25506;&#32034;&#19981;&#21516;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#8212;&#8212;&#31283;&#20581;&#27604;&#29575;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#20844;&#24179;&#31574;&#30053;&#22312;&#20116;&#20010;&#20844;&#24179;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24179;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#30830;&#20445;&#31639;&#27861;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#24840;&#21457;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#23398;&#19978;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#23450;&#20041;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#32780;&#19988;&#23545;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#20581;&#24615;&#65288;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#33021;&#21147;&#65289;&#22312;&#24212;&#23545;&#26032;&#38382;&#39064;&#26102;&#24212;&#20351;&#29992;&#30340;&#31574;&#30053;&#31867;&#22411;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#34913;&#37327;&#36825;&#20123;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#26469;&#34913;&#37327;&#21508;&#31181;&#20844;&#24179;&#20248;&#21270;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615; - &#31283;&#20581;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#26368;&#24120;&#29992;&#30340;&#20844;&#24179;&#31574;&#30053;&#23545;&#20116;&#20010;&#22522;&#20934;&#20844;&#24179;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22810;&#27425;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#21644;&#19981;&#21516;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35774;&#35745;&#21644;&#36873;&#25321;&#20844;&#24179;&#31639;&#27861;&#26102;&#65292;&#24212;&#26356;&#21152;&#35880;&#24910;&#22320;&#32771;&#34385;&#31283;&#20581;&#24615;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#22987;&#32456;&#26377;&#25928;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#27979;&#35797;&#36873;&#25321;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#21152;&#36895;&#35206;&#30422;&#38381;&#21512;&#65292;&#19977;&#20010;&#27979;&#35797;&#37197;&#32622;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#27979;&#35797;&#65292;&#26368;&#22823;&#33410;&#30465;&#29575;&#36798;49.37%&#12290;</title><link>http://arxiv.org/abs/2207.00445</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#26032;&#39062;&#24615;&#30340;&#27979;&#35797;&#36873;&#25321;&#20197;&#21152;&#36895;&#21151;&#33021;&#35206;&#30422;&#38381;&#21512;
&lt;/p&gt;
&lt;p&gt;
Using Neural Networks for Novelty-based Test Selection to Accelerate Functional Coverage Closure. (arXiv:2207.00445v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#27979;&#35797;&#36873;&#25321;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#21152;&#36895;&#35206;&#30422;&#38381;&#21512;&#65292;&#19977;&#20010;&#27979;&#35797;&#37197;&#32622;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#27979;&#35797;&#65292;&#26368;&#22823;&#33410;&#30465;&#29575;&#36798;49.37%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#22522;&#20110;&#20223;&#30495;&#30340;&#39564;&#35777;&#20013;&#20351;&#29992;&#26032;&#39062;&#30340;&#27979;&#35797;&#36873;&#25321;&#22120;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35206;&#30422;&#38381;&#21512;&#65292;&#26080;&#35770;&#35206;&#30422;&#27934;&#30340;&#25968;&#37327;&#22914;&#20309;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#32622;&#28789;&#27963;&#19988;&#39640;&#24230;&#33258;&#21160;&#21270;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#39062;&#27979;&#35797;&#36873;&#25321;&#26694;&#26550;&#12290;&#20351;&#29992;&#21830;&#19994;&#20449;&#21495;&#22788;&#29702;&#21333;&#20803;&#27979;&#35797;&#20102;&#26694;&#26550;&#30340;&#19977;&#20010;&#37197;&#32622;&#12290;&#25152;&#26377;&#19977;&#20010;&#37197;&#32622;&#37117;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#27979;&#35797;&#36873;&#25321;&#65292;&#26368;&#22823;&#30340;&#20223;&#30495;&#33410;&#30465;&#29575;&#20026;49.37%&#65292;&#35206;&#30422;&#29575;&#36798;&#21040;99.5%&#12290;&#19982;&#20223;&#30495;&#20943;&#23569;&#37327;&#30456;&#27604;&#65292;&#37197;&#32622;&#30340;&#35745;&#31639;&#24320;&#38144;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#23454;&#39564;&#32467;&#26524;&#24182;&#35752;&#35770;&#20102;&#19982;&#37197;&#32622;&#24615;&#33021;&#30456;&#20851;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel test selectors used in simulation-based verification have been shown to significantly accelerate coverage closure regardless of the number of coverage holes. This paper presents a configurable and highly-automated framework for novel test selection based on neural networks. Three configurations of this framework are tested with a commercial signal processing unit. All three convincingly outperform random test selection with the largest saving of simulation being 49.37% to reach 99.5% coverage. The computational expense of the configurations is negligible compared to the simulation reduction. We compare the experimental results and discuss important characteristics related to the performance of the configurations.
&lt;/p&gt;</description></item><item><title>HRFuser&#26159;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#21644;&#26032;&#22411;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#36827;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#12290;&#22312;nuScenes&#21644;DENSE&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.15157</link><description>&lt;p&gt;
HRFuser: &#19968;&#31181;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection. (arXiv:2206.15157v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15157
&lt;/p&gt;
&lt;p&gt;
HRFuser&#26159;&#19968;&#31181;&#22810;&#20998;&#36776;&#29575;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#65292;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#21644;&#26032;&#22411;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#36827;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#12290;&#22312;nuScenes&#21644;DENSE&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38500;&#20102;&#26631;&#20934;&#30456;&#26426;&#22806;&#65292;&#36890;&#24120;&#36824;&#21253;&#25324;&#22810;&#20010;&#20854;&#20182;&#20256;&#24863;&#22120;&#65292;&#22914;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#65292;&#36825;&#20123;&#20256;&#24863;&#22120;&#24110;&#21161;&#33719;&#21462;&#24863;&#30693;&#39550;&#39542;&#22330;&#26223;&#30340;&#26356;&#20016;&#23500;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#20960;&#39033;&#24037;&#20316;&#30528;&#37325;&#20110;&#23558;&#26576;&#20123;&#20256;&#24863;&#22120;&#23545;&#36827;&#34892;&#34701;&#21512;&#65292;&#22914;&#30456;&#26426;&#19982;&#28608;&#20809;&#38647;&#36798;&#25110;&#38647;&#36798;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#19988;&#27169;&#22359;&#21270;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HRFuser&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20108;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#12290;&#23427;&#20197;&#22810;&#20998;&#36776;&#29575;&#26041;&#24335;&#34701;&#21512;&#22810;&#20010;&#20256;&#24863;&#22120;&#24182;&#21487;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#27169;&#24335;&#12290;HRFuser&#30340;&#35774;&#35745;&#22522;&#20110;&#29992;&#20110;&#20165;&#22270;&#20687;&#23494;&#38598;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#39640;&#20998;&#36776;&#29575;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#30340;&#22810;&#31383;&#21475;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#20316;&#20026;&#25191;&#34892;&#22810;&#27169;&#24577;&#22810;&#20998;&#36776;&#29575;&#34701;&#21512;&#30340;&#25163;&#27573;&#12290;&#36890;&#36807;&#23545;nuScenes&#21644;&#24694;&#21155;&#26465;&#20214;DENSE&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HRFuser&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera with lidar or radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we propose HRFuser, a modular architecture for multi-modal 2D object detection. It fuses multiple sensors in a multi-resolution fashion and scales to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. We demonstrate via extensive experiments on nuScenes and the adverse conditions DENSE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11723</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#29992;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#24335;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#20013;&#23481;&#26131;&#20986;&#29616;&#30340;&#37325;&#26500;&#24322;&#24120;&#20449;&#21495;&#32780;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#38750;&#32447;&#24615;&#38477;&#32500;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#24322;&#24120;&#30340;&#26679;&#26412;&#20248;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#26222;&#36941;&#35748;&#20026;&#30456;&#24212;&#30340;&#32593;&#32476;&#24212;&#22312;&#24212;&#29992;&#38454;&#27573;&#26410;&#33021;&#20934;&#30830;&#37325;&#26500;&#24322;&#24120;&#21306;&#22495;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#36890;&#36807;&#25511;&#21046;&#32593;&#32476;&#30340;&#23481;&#37327;&#26469;&#35299;&#20915;&#65292;&#35201;&#20040;&#36890;&#36807;&#20943;&#23569;&#29942;&#39048;&#23618;&#30340;&#22823;&#23567;&#65292;&#35201;&#20040;&#36890;&#36807;&#23545;&#20854;&#28608;&#27963;&#26045;&#21152;&#31232;&#30095;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#25216;&#26415;&#37117;&#27809;&#26377;&#26126;&#30830;&#24809;&#32602;&#24322;&#24120;&#20449;&#21495;&#30340;&#37325;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#26816;&#27979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21028;&#21035;&#20449;&#24687;&#65292;&#36890;&#36807;&#20462;&#25913;&#30340;&#37325;&#26500;&#35823;&#24046;&#38598;&#20013;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#23616;&#37096;&#19968;&#33268;&#30340;&#37325;&#26500;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep convolutional autoencoders provide an effective tool for learning non-linear dimensionality reduction in an unsupervised way. Recently, they have been used for the task of anomaly detection in the visual domain. By optimising for the reconstruction error using anomaly-free examples, the common belief is that a corresponding network should fail to accurately reconstruct anomalous regions in the application phase. This goal is typically addressed by controlling the capacity of the network by either reducing the size of the bottleneck layer or enforcing sparsity constraints on its activations. However, neither of these techniques does explicitly penalize reconstruction of anomalous signals often resulting in poor detection. We tackle this problem by adapting a self-supervised learning regime, which allows to use discriminative information during training focusing on the data manifold by means of a modified reconstruction error. This regularizes the model to produce locally consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#32852;&#37030;&#29992;&#25143;&#26412;&#22320;&#23454;&#29616;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#29992;&#25143;&#32423;&#21035;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#33258;&#28982;&#22320;&#20445;&#35777;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.03617</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#20307;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Subject Granular Differential Privacy in Federated Learning. (arXiv:2206.03617v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#32852;&#37030;&#29992;&#25143;&#26412;&#22320;&#23454;&#29616;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#29992;&#25143;&#32423;&#21035;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#33258;&#28982;&#22320;&#20445;&#35777;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#23454;&#29616;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20027;&#20307;&#26159;&#19968;&#20010;&#20010;&#20307;&#65292;&#20854;&#31169;&#26377;&#20449;&#24687;&#30001;&#21333;&#20010;&#32852;&#37030;&#29992;&#25143;&#20869;&#37096;&#25110;&#20998;&#24067;&#22312;&#22810;&#20010;&#32852;&#37030;&#29992;&#25143;&#20043;&#38388;&#30340;&#22810;&#20010;&#25968;&#25454;&#39033;&#25152;&#20307;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#32852;&#37030;&#29992;&#25143;&#26412;&#22320;&#23454;&#29616;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861;&#31216;&#20026;LocalGroupDP&#65292;&#26159;&#22312;&#27969;&#34892;&#30340;DP-SGD&#31639;&#27861;&#20013;&#24212;&#29992;&#32452;&#24046;&#20998;&#38544;&#31169;&#30340;&#30452;&#25509;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#31639;&#27861;&#26159;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#24819;&#27861;&#8212;&#8212;&#38024;&#23545;&#21442;&#21152;&#35757;&#32451;&#36855;&#20320;&#25209;&#27425;&#30340;&#20027;&#20307;&#20351;&#29992;&#20998;&#23618;&#28176;&#36827;&#24179;&#22343;&#26799;&#24230;&#65288;HiGradAvgDP&#65289;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#29992;&#25143;&#32423;&#21035;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#33258;&#28982;&#22320;&#20445;&#35777;&#20027;&#20307;&#32423;&#21035;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;FL&#20013;&#20027;&#20307;&#32423;&#38544;&#31169;&#25439;&#22833;&#30340;&#27178;&#21521;&#32452;&#21512;&#38382;&#39064;&#8212;&#8212;&#21508;&#20010;&#29992;&#25143;&#20135;&#29983;&#30340;&#20027;&#20307;&#32423;&#38544;&#31169;&#25439;&#22833;&#22312;&#32852;&#37030;&#20013;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#23545;&#20027;&#20307;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers subject level privacy in the FL setting, where a subject is an individual whose private information is embodied by several data items either confined within a single federation user or distributed across multiple federation users. We propose two new algorithms that enforce subject level DP at each federation user locally. Our first algorithm, called LocalGroupDP, is a straightforward application of group differential privacy in the popular DP-SGD algorithm. Our second algorithm is based on a novel idea of hierarchical gradient averaging (HiGradAvgDP) for subjects participating in a training mini-batch. We also show that user level Local Differential Privacy (LDP) naturally guarantees subject level DP. We observe the problem of horizontal composition of subject level privacy loss in FL - subject level privacy loss incurred at individual users composes across the federation. We formally prove the subject level DP guarantee for our algorithms, and also show their effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#30340;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#20998;&#24067;&#65292;&#21457;&#29616;&#23545;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#24418;&#29366;&#22609;&#36896;&#21487;&#20197;&#20351;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#65292;&#32780;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#21463;&#21040;&#31070;&#32463;&#21327;&#26041;&#24046;SDE&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2206.02768</link><description>&lt;p&gt;
&#31070;&#32463;&#21327;&#26041;&#24046;SDE&#65306;&#21021;&#22987;&#21270;&#26102;&#20855;&#26377;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#32593;&#32476;&#30340;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization. (arXiv:2206.02768v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#30340;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#20998;&#24067;&#65292;&#21457;&#29616;&#23545;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#24418;&#29366;&#22609;&#36896;&#21487;&#20197;&#20351;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#65292;&#32780;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#21463;&#21040;&#31070;&#32463;&#21327;&#26041;&#24046;SDE&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;logit&#36755;&#20986;&#22312;&#32473;&#23450;&#30001;&#27425;&#34920;&#23618;&#23450;&#20041;&#30340;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26465;&#20214;&#19979;&#26159;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#38543;&#26426;&#30697;&#38453;&#30340;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#65292;&#23545;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#24418;&#29366;&#22609;&#36896;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20351;&#24471;&#36825;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#38750;&#36864;&#21270;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26080;&#38480;&#23485;&#24230;&#26679;&#24335;&#30340;&#36825;&#31181;&#29702;&#35299;&#22312;&#22823;&#28145;&#24230;&#26102;&#23384;&#22312;&#19981;&#36275;&#65306;&#26080;&#38480;&#23485;&#24230;&#20998;&#26512;&#24573;&#30053;&#20102;&#20174;&#23618;&#21040;&#23618;&#30340;&#24494;&#35266;&#27874;&#21160;&#65292;&#20294;&#36825;&#20123;&#27874;&#21160;&#22312;&#35768;&#22810;&#23618;&#19978;&#31215;&#32047;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#24418;&#29366;&#22609;&#36896;&#30340;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#26497;&#38480;&#20013;&#30340;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21040;&#36798;&#38750;&#24179;&#20961;&#26497;&#38480;&#25152;&#38656;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;&#31934;&#30830;&#32553;&#25918;&#65292;&#24182;&#34920;&#26126;&#38543;&#26426;&#21327;&#26041;&#24046;&#30697;&#38453;&#21463;&#21040;&#25105;&#20204;&#31216;&#20043;&#20026;&#31070;&#32463;&#21327;&#26041;&#24046;SDE&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25511;&#21046;&#12290;&#20351;&#29992;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#21327;&#26041;&#24046;SDE&#30340;&#29702;&#35299;&#26159;&#20934;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers.  To overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaProp&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#33258;&#36866;&#24212;&#22320;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#39640;&#25928;&#12289;&#24378;&#22823;&#22320;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2205.15319</link><description>&lt;p&gt;
AdaProp&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#23398;&#20064;&#33258;&#36866;&#24212;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. (arXiv:2205.15319v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaProp&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#33258;&#36866;&#24212;&#22320;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#39640;&#25928;&#12289;&#24378;&#22823;&#22320;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#34987;&#35774;&#35745;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;(KG)&#25512;&#29702;&#12290;GNN-based KG&#25512;&#29702;&#26041;&#27861;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20256;&#25773;&#36335;&#24452;&#65292;&#23427;&#21253;&#21547;&#27599;&#20010;&#20256;&#25773;&#27493;&#39588;&#20013;&#25152;&#28041;&#21450;&#30340;&#19968;&#32452;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#20256;&#25773;&#36335;&#24452;&#65292;&#24573;&#30053;&#20102;&#23454;&#20307;&#19982;&#26597;&#35810;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26356;&#22823;&#30340;&#20256;&#25773;&#27493;&#39588;&#20013;&#65292;&#28041;&#21450;&#30340;&#23454;&#20307;&#25968;&#37327;&#20250;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20256;&#25773;&#36335;&#24452;&#65292;&#20197;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22686;&#37327;&#37319;&#26679;&#26426;&#21046;&#65292;&#23427;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#20445;&#30041;&#38468;&#36817;&#30446;&#26631;&#21644;&#20998;&#23618;&#36830;&#25509;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#22823;&#12289;&#39640;&#25928;&#65292;&#24182;&#19988;&#26159;&#35821;&#20041;&#24863;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related entities. Extensive experiments show that our method is powerful, efficient, and semantic-awa
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PLAtE&#30340;&#22823;&#35268;&#27169;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#21830;&#21697;&#21015;&#34920;&#21644;&#20135;&#21697;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2205.12386</link><description>&lt;p&gt;
PLAtE: &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PLAtE: A Large-scale Dataset for List Page Web Extraction. (arXiv:2205.12386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PLAtE&#30340;&#22823;&#35268;&#27169;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#21830;&#21697;&#21015;&#34920;&#21644;&#20135;&#21697;&#23646;&#24615;&#12290;&#25968;&#25454;&#38598;&#30001;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#27169;&#22411;&#34987;&#21033;&#29992;&#26469;&#26174;&#33879;&#25552;&#39640;&#20174;&#21322;&#32467;&#26500;&#21270;&#32593;&#31449;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#32487;&#32493;&#36827;&#27493;&#30340;&#38556;&#30861;&#26159;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#25968;&#37327;&#22826;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PLAtE &#65288;Pages of Lists Attribute Extraction&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#32593;&#32476;&#25277;&#21462;&#20219;&#21153;&#12290;PLAtE &#20027;&#35201;&#20851;&#27880;&#36141;&#29289;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#20174;&#21253;&#21547;&#22810;&#20010;&#39033;&#30446;&#30340;&#20135;&#21697;&#35780;&#35770;&#39029;&#38754;&#20013;&#25552;&#21462;&#65292;&#21253;&#21547;&#20004;&#20010;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26597;&#25214;&#20135;&#21697;&#21015;&#34920;&#20998;&#21106;&#36793;&#30028;&#21644;&#65288;2&#65289;&#25552;&#21462;&#27599;&#20010;&#20135;&#21697;&#30340;&#23646;&#24615;&#12290;PLAtE&#30001;&#26469;&#33258;6,694&#20010;&#39029;&#38754;&#30340;52,898&#20010;&#39033;&#30446;&#21644;156,014&#20010;&#23646;&#24615;&#32452;&#25104;&#65292;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21015;&#34920;&#39029;&#32593;&#32476;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#38454;&#27573;&#26041;&#27861;&#26469;&#25910;&#38598;&#21644;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#25277;&#21462;&#27169;&#22411;&#36866;&#24212;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#23450;&#37327;&#21644;&#23450;&#24615;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product-list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52, 898 items collected from 6, 694 pages and 156, 014 attributes, making it the first largescale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#20010;&#38382;&#39064;/&#39064;&#30446;&#27169;&#22411;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#38388;&#20851;&#32852;&#24615;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.09864</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;BERT&#35843;&#25972;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#20010;&#38382;&#39064;/&#39064;&#30446;&#27169;&#22411;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#38388;&#20851;&#32852;&#24615;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#26377;&#30528;&#26497;&#22823;&#30340;&#28508;&#21147;&#21487;&#20197;&#38477;&#20302;&#20154;&#24037;&#35780;&#20998;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#21644;GPT&#65289;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#35780;&#20998;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20026;&#27599;&#20010;&#38382;&#39064;/&#39064;&#30446;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#36825;&#36866;&#29992;&#20110;&#35797;&#39064;&#31181;&#31867;&#21315;&#24046;&#19975;&#21035;&#30340;&#20316;&#25991;&#35780;&#20998;&#31561;&#22330;&#26223;&#12290;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#22312;&#38405;&#35835;&#29702;&#35299;&#31561;&#19982;&#22810;&#20010;&#38382;&#39064;/&#39064;&#30446;&#30456;&#20851;&#30340;&#22330;&#26223;&#19979;&#65292;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65307;2&#65289;&#24403;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#23384;&#20648;&#27599;&#20010;&#39064;&#30446;&#29420;&#31435;&#27169;&#22411;&#21464;&#24471;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#22269;&#23478;&#25945;&#32946;&#36827;&#27493;&#35780;&#20272;&#65288;NAEP&#65289;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#30340;&#65288;&#19968;&#31561;&#22870;&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#65292;&#20135;&#29983;&#19968;&#20010;&#20849;&#29992;&#30340;&#35780;&#20998;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30701;&#26399;&#20302;&#21387;&#36127;&#33655;&#39044;&#27979;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2204.13939</link><description>&lt;p&gt;
&#20302;&#21387;&#36127;&#33655;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#30701;&#26399;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows. (arXiv:2204.13939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30701;&#26399;&#20302;&#21387;&#36127;&#33655;&#39044;&#27979;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a flexible conditional density forecasting method based on Bernstein polynomial normalizing flows for short-term low-voltage load forecasting, which outperforms traditional methods and can be used for planning and operating low-carbon energy systems.
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20840;&#38754;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#32593;&#30340;&#36716;&#22411;&#38656;&#35201;&#26356;&#22909;&#22320;&#39044;&#27979;&#20302;&#21387;&#27700;&#24179;&#30340;&#38656;&#27714;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#30830;&#20445;&#21487;&#38752;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#39640;&#27874;&#21160;&#24615;&#21644;&#19981;&#26029;&#22686;&#21152;&#30340;&#30005;&#27668;&#21270;&#23548;&#33268;&#24040;&#22823;&#30340;&#39044;&#27979;&#21464;&#24322;&#24615;&#65292;&#36825;&#22312;&#20256;&#32479;&#30340;&#28857;&#20272;&#35745;&#20013;&#27809;&#26377;&#21453;&#26144;&#20986;&#26469;&#12290;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;&#32771;&#34385;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#20801;&#35768;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#27969;&#30340;&#21442;&#25968;&#12290;&#22312;&#19968;&#39033;&#21253;&#25324;363&#20010;&#26234;&#33021;&#30005;&#34920;&#23458;&#25143;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#23494;&#24230;&#39044;&#27979;&#19982;&#39640;&#26031;&#21644;&#39640;&#26031;&#28151;&#21512;&#23494;&#24230;&#30456;&#27604;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#20204;&#22312;24&#23567;&#26102;&#21069;&#30340;&#36127;&#36733;&#39044;&#27979;&#20013;&#20248;&#20110;&#22522;&#20110;&#38024;&#29699;&#25439;&#22833;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.13695</link><description>&lt;p&gt;
&#21452;&#32447;&#24615;&#20215;&#20540;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#27969;&#26694;&#26550;&#28041;&#21450;&#20272;&#35745;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;Q&#20540;&#20989;&#25968;&#12290;&#24403;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#26102;&#65292;&#25968;&#25454;&#25928;&#29575;&#19982;Q&#20989;&#25968;&#23545;&#26032;&#30446;&#26631;&#30340;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#30446;&#21069;&#30340;&#33539;&#24335;&#26159;&#20351;&#29992;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;Q(s, a, g)&#12290;&#20026;&#20102;&#25913;&#36827;Q&#20989;&#25968;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#28857;&#31215;&#30340;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#12290;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;f(s, a)&#25429;&#25417;&#29366;&#24577;s&#22788;&#30340;&#29615;&#22659;&#23616;&#37096;&#21160;&#24577;&#65307;&#32780;&#31532;&#20108;&#20010;&#37096;&#20998;{&#981;}(s, g)&#21017;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#22788;&#20110;&#20998;&#24067;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#20855;&#26377;&#26356;&#22909;&#30340;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;Fetch&#26426;&#22120;&#20154;&#20219;&#21153;&#22871;&#20214;&#21644;DeepMind Control Suite&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.07657</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#20013;&#26816;&#27979;&#36133;&#34880;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of sepsis during emergency department triage using machine learning. (arXiv:2204.07657v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20986;&#19968;&#31181;&#26816;&#27979;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#36133;&#34880;&#30151;&#30340;&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#22120;&#23448;&#21151;&#33021;&#38556;&#30861;&#30340;&#36133;&#34880;&#30151;&#26159;&#20840;&#29699;&#27515;&#20129;&#21644;&#21361;&#37325;&#30142;&#30149;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#27604;&#36739;&#26631;&#20934;&#36133;&#34880;&#30151;&#31579;&#26597;&#31639;&#27861;&#21644;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20998;&#35786;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#24613;&#35786;&#31185;&#20998;&#35786;&#21069;&#65288;&#26410;&#20351;&#29992;&#23454;&#39564;&#23460;&#35786;&#26029;&#65289;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#24471;&#20986;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340; AUC &#20026; 0.9423&#65292;&#25935;&#24863;&#24615;&#20026; 71.09%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sepsis is a life-threatening condition with organ dysfunction and is a leading cause of death and critical illness worldwide. Even a few hours of delay in the treatment of sepsis results in increased mortality. Early detection of sepsis during emergency department triage would allow early initiation of lab analysis, antibiotic administration, and other sepsis treatment protocols. The purpose of this study was to compare sepsis detection performance at ED triage (prior to the use of laboratory diagnostics) of the standard sepsis screening algorithm (SIRS with source of infection) and a machine learning algorithm trained on EHR triage data. A machine learning model (KATE Sepsis) was developed using patient encounters with triage data from 16participating hospitals. KATE Sepsis and standard screening were retrospectively evaluated on the adult population of 512,949 medical records. KATE Sepsis demonstrates an AUC of 0.9423 (0.9401 - 0.9441) with sensitivity of 71.09% (70.12% - 71.98%) and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25299;&#25169;&#28608;&#27963;&#22270;&#26469;&#21487;&#35270;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102; DNN &#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.03528</link><description>&lt;p&gt;
&#29992;&#25299;&#25169;&#28608;&#27963;&#22270;&#26469;&#21487;&#35270;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Visualizing Deep Neural Networks with Topographic Activation Maps. (arXiv:2204.03528v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25299;&#25169;&#28608;&#27963;&#22270;&#26469;&#21487;&#35270;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102; DNN &#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35299;&#20915;&#21508;&#39046;&#22495;&#20219;&#21153;&#20013;&#24050;&#25104;&#20026;&#25104;&#21151;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20102;&#35299;&#20854;&#22914;&#20309;&#35299;&#20915;&#25152;&#23398;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#25552;&#39640; DNN &#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#31070;&#32463;&#31185;&#23398;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#22797;&#26434;&#30340;&#19981;&#36879;&#26126;&#31995;&#32479;&#12290;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#22914;&#20309;&#20351;&#29992;&#25299;&#25169;&#22270;&#21487;&#35270;&#21270;&#33041;&#27963;&#21160;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21516;&#26679;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24067;&#32622; DNN &#23618;&#20013;&#31070;&#32463;&#20803;&#30340;&#25216;&#26415;&#65292;&#20351;&#24471;&#20855;&#26377;&#31867;&#20284;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#22312;&#24444;&#27492;&#38468;&#36817;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#24182;&#27604;&#36739;&#20102;&#33719;&#24471; DNN &#23618;&#20013;&#31070;&#32463;&#20803;&#25299;&#25169;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25299;&#25169;&#28608;&#27963;&#22270;&#26469;&#35782;&#21035;&#38169;&#35823;&#25110;&#32534;&#30721;&#20559;&#35265;&#65292;&#24182;&#21487;&#35270;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#25216;&#26415;&#25552;&#39640;&#20102;&#22522;&#20110; DNN &#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with Deep Neural Networks (DNNs) has become a successful tool in solving tasks across various fields of application. However, the complexity of DNNs makes it difficult to understand how they solve their learned task. To improve the explainability of DNNs, we adapt methods from neuroscience that analyze complex and opaque systems. Here, we draw inspiration from how neuroscience uses topographic maps to visualize brain activity. To also visualize activations of neurons in DNNs as topographic maps, we research techniques to layout the neurons in a two-dimensional space such that neurons of similar activity are in the vicinity of each other. In this work, we introduce and compare methods to obtain a topographic layout of neurons in a DNN layer. Moreover, we demonstrate how to use topographic activation maps to identify errors or encoded biases and to visualize training processes. Our novel visualization technique improves the transparency of DNN-based decision-making syste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.15845</link><description>&lt;p&gt;
&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#28145;&#24230; Q &#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20174;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#25442;&#20803;&#32452;&#26356;&#26032; Q &#20540;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#24120;&#22343;&#21248;&#21644;&#38543;&#26426;&#22320;&#37319;&#26679;&#65292;&#25110;&#22522;&#20110;&#35832;&#22914;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#31561;&#24230;&#37327;&#20248;&#20808;&#12290;&#36825;&#26679;&#30340;&#37319;&#26679;&#31574;&#30053;&#22312;&#23398;&#20064; Q &#20989;&#25968;&#26102;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#21462;&#20915;&#20110;&#32487;&#25215;&#29366;&#24577;&#30340; Q &#20540;&#12290;&#22914;&#26524;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#24573;&#30053;&#20102;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#23427;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#29992;&#21644;&#24120;&#24120;&#19981;&#27491;&#30830;&#30340; Q &#20540;&#26356;&#26032;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#26126;&#30830;&#36319;&#36394;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20013;&#30340;&#27599;&#26465;&#36793;&#20195;&#34920;&#36890;&#36807;&#25191;&#34892;&#21333;&#20010;&#25805;&#20316;&#22312;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#19968;&#32452;&#32456;&#31471;&#29366;&#24577;&#24320;&#22987;&#25193;&#23637;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#36880;&#27493;&#21521;&#21518;&#31227;&#21160;&#30340;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#25191;&#34892;&#20540;&#22791;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#21387;&#32553;&#30340;&#21010;&#20998;&#24335;&#35745;&#31639;&#26041;&#26696;&#65288;SC2&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20998;&#37197;&#32473;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#65307;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#21387;&#32553;&#25968;&#25454;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2203.08875</link><description>&lt;p&gt;
SC2&#22522;&#20934;&#27979;&#35797;&#65306;&#38754;&#21521;&#21010;&#20998;&#35745;&#31639;&#30340;&#30417;&#30563;&#24335;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
SC2 Benchmark: Supervised Compression for Split Computing. (arXiv:2203.08875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#21387;&#32553;&#30340;&#21010;&#20998;&#24335;&#35745;&#31639;&#26041;&#26696;&#65288;SC2&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20998;&#37197;&#32473;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#65307;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#22312;&#21387;&#32553;&#25968;&#25454;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#20998;&#37197;&#32473;&#35774;&#22791;&#21644;&#26356;&#24378;&#22823;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#24050;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#21010;&#20998;&#35745;&#31639;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#23545;&#21387;&#32553;&#25968;&#25454;&#36827;&#34892;&#36828;&#31243;&#35745;&#31639;&#30340;&#26420;&#32032;&#22522;&#32447;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#23398;&#20064;&#21387;&#32553;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21253;&#21547;&#26356;&#22810;&#29992;&#20110;&#30417;&#30563;&#19979;&#28216;&#20219;&#21153;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#23637;&#31034;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#22823;&#23567;&#21644;&#21463;&#30417;&#30563;&#24615;&#33021;&#20043;&#38388;&#25913;&#36827;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21482;&#25552;&#20379;&#20102;&#21010;&#20998;&#35745;&#31639;&#30340;&#19981;&#23436;&#25972;&#22270;&#20687;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#38754;&#21521;&#21010;&#20998;&#35745;&#31639;&#30340;&#30417;&#30563;&#24335;&#21387;&#32553;&#65288;SC2&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26631;&#20934;&#65306;&#26368;&#23567;&#21270;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#65292;&#26368;&#23567;&#21270;&#20256;&#36755;&#25968;&#25454;&#22823;&#23567;&#65292;&#26368;&#22823;&#21270;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;10&#31181;&#22522;&#20934;&#26041;&#27861;&#12289;&#19977;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;180&#22810;&#20010;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#19979;&#26041;&#27861;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss vario
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#23454;&#25968;&#36755;&#20986;&#19988;&#28608;&#27963;&#20989;&#25968;&#21253;&#21547;&#20223;&#23556;&#27573;&#20197;&#21450;&#38544;&#34255;&#23618;&#33267;&#23569;&#26377;&#20004;&#20010;&#33410;&#28857;&#30340;&#28145;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#36825;&#31867;&#38382;&#39064;&#23545;&#20110;&#25152;&#26377;&#19981;&#26159;&#20223;&#23556;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#37117;&#23384;&#22312;&#19968;&#31995;&#21015;&#34394;&#20551;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#36825;&#26159;&#30001;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30452;&#25509;&#25512;&#23548;&#24471;&#20986;&#30340;&#12290;</title><link>http://arxiv.org/abs/2202.12262</link><description>&lt;p&gt;
&#20851;&#20110;&#26576;&#20123;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#20013;&#34394;&#20551;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems. (arXiv:2202.12262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12262
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#23454;&#25968;&#36755;&#20986;&#19988;&#28608;&#27963;&#20989;&#25968;&#21253;&#21547;&#20223;&#23556;&#27573;&#20197;&#21450;&#38544;&#34255;&#23618;&#33267;&#23569;&#26377;&#20004;&#20010;&#33410;&#28857;&#30340;&#28145;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#36825;&#31867;&#38382;&#39064;&#23545;&#20110;&#25152;&#26377;&#19981;&#26159;&#20223;&#23556;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#37117;&#23384;&#22312;&#19968;&#31995;&#21015;&#34394;&#20551;&#30340;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#36825;&#26159;&#30001;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30452;&#25509;&#25512;&#23548;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#23454;&#25968;&#36755;&#20986;&#19988;&#28608;&#27963;&#20989;&#25968;&#21253;&#21547;&#20223;&#23556;&#27573;&#20197;&#21450;&#38544;&#34255;&#23618;&#33267;&#23569;&#26377;&#20004;&#20010;&#33410;&#28857;&#30340;&#28145;&#23618;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38382;&#39064;&#25439;&#22833;&#26223;&#35266;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19981;&#26159;&#20223;&#23556;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#31867;&#38382;&#39064;&#23384;&#22312;&#19968;&#31995;&#21015;&#34394;&#20551;&#30340;&#65288;&#21363;&#38750;&#20840;&#23616;&#26368;&#20248;&#30340;&#65289;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#25152;&#26377;&#37319;&#26679;&#21644;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#19968;&#33324;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#20219;&#24847;&#36830;&#32493;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21450;&#26377;&#38480;&#32500;&#21644;&#26080;&#38480;&#32500;&#35774;&#32622;&#12290;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25152;&#32771;&#34385;&#30340;&#35757;&#32451;&#38382;&#39064;&#20013;&#34394;&#20551;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#20986;&#29616;&#26159;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#30340;&#30452;&#25509;&#32467;&#26524;&#65292;&#24213;&#23618;&#26426;&#21046;&#20063;&#23548;&#33268;&#20363;&#22914;$L^p$-&#26368;&#20339;&#36924;&#36817;&#38382;&#39064;&#23545;&#20110;&#27809;&#26377;&#23494;&#38598;&#22270;&#20687;&#30340;&#25152;&#26377;&#32593;&#32476;&#22312;Hadamard&#24847;&#20041;&#19979;&#37117;&#26159;&#19981;&#33391; posed&#12290;&#21518;&#19968;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#26080;&#38480;&#32500;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is shown that such problems possess a continuum of spurious (i.e., not globally optimal) local minima for all target functions that are not affine. In contrast to previous works, our analysis covers all sampling and parameterization regimes, general differentiable loss functions, arbitrary continuous nonpolynomial activation functions, and both the finite- and infinite-dimensional setting. It is further shown that the appearance of the spurious local minima in the considered training problems is a direct consequence of the universal approximation theorem and that the underlying mechanisms also cause, e.g., $L^p$-best approximation problems to be ill-posed in the sense of Hadamard for all networks that do not have a dense image. The latter result also holds without 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#24182;&#36890;&#36807;&#37325;&#26500;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#36991;&#20813;&#26114;&#36149;&#21644;&#36153;&#26102;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2202.07135</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#30340;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Compositional Scene Representation Learning via Reconstruction: A Survey. (arXiv:2202.07135v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07135
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#24182;&#36890;&#36807;&#37325;&#26500;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#36991;&#20813;&#26114;&#36149;&#21644;&#36153;&#26102;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#22330;&#26223;&#26159;&#30001;&#35270;&#35273;&#27010;&#24565;&#32452;&#25104;&#30340;&#65292;&#24182;&#20855;&#26377;&#32452;&#21512;&#29190;&#28856;&#30340;&#29305;&#24615;&#12290;&#20154;&#31867;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#30340;&#37325;&#35201;&#21407;&#22240;&#26159;&#20855;&#22791;&#32452;&#21512;&#24863;&#30693;&#33021;&#21147;&#65292;&#32780;&#24076;&#26395;&#20154;&#24037;&#26234;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#30340;&#33021;&#21147;&#12290;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#39033;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#37325;&#26500;&#23558;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#25512;&#36827;&#21040;&#20102;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#12290;&#36890;&#36807;&#37325;&#26500;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21183;&#22312;&#20110;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#36991;&#20813;&#26114;&#36149;&#21644;&#36153;&#26102;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#22522;&#20110;&#37325;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32452;&#21512;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#21253;&#25324;&#21457;&#23637;&#21382;&#31243;&#21644;&#29616;&#26377;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual scenes are composed of visual concepts and have the property of combinatorial explosion. An important reason for humans to efficiently learn from diverse visual scenes is the ability of compositional perception, and it is desirable for artificial intelligence to have similar abilities. Compositional scene representation learning is a task that enables such abilities. In recent years, various methods have been proposed to apply deep neural networks, which have been proven to be advantageous in representation learning, to learn compositional scene representations via reconstruction, advancing this research direction into the deep learning era. Learning via reconstruction is advantageous because it may utilize massive unlabeled data and avoid costly and laborious data annotation. In this survey, we first outline the current progress on reconstruction-based compositional scene representation learning with deep neural networks, including development history and categorizations of exi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#25968;&#25454;&#21512;&#24182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#25152;&#26377;&#27010;&#29575;&#20998;&#24067;&#19978;&#30340;&#26368;&#22823;&#25439;&#22833;&#65292;&#26469;&#26500;&#24314;&#20998;&#24067;&#24335;&#31283;&#20581;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2202.05797</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31283;&#20581;&#25968;&#25454;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Data Join. (arXiv:2202.05797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#25968;&#25454;&#21512;&#24182;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#25152;&#26377;&#27010;&#29575;&#20998;&#24067;&#19978;&#30340;&#26368;&#22823;&#25439;&#22833;&#65292;&#26469;&#26500;&#24314;&#20998;&#24067;&#24335;&#31283;&#20581;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#26377;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#24102;&#26377;&#39069;&#22806;&#36741;&#21161;&#29305;&#24449;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26368;&#21512;&#29702;&#30340;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#26500;&#24314;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#24212;&#35813;&#21462;&#20915;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#30001;&#30456;&#21516;&#30340;&#20998;&#24067;&#36824;&#26159;&#19981;&#21516;&#30340;&#20998;&#24067;&#29983;&#25104;&#30340;&#65292;&#20197;&#21450;&#27979;&#35797;&#20998;&#24067;&#19982;&#36825;&#20123;&#20998;&#24067;&#20013;&#30340;&#20219;&#19968;&#20998;&#24067;&#26377;&#22810;&#30456;&#20284;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20004;&#20010;&#25968;&#25454;&#38598;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#20998;&#24067;&#65292;&#20294;&#20004;&#32773;&#37117;&#21487;&#33021;&#25509;&#36817;&#27979;&#35797;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24314;&#31435;&#19968;&#31181;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#35813;&#39044;&#27979;&#22120;&#36890;&#36807;&#23558;&#20854;Wasserstein&#36317;&#31163;&#20998;&#21035;&#20026;$r_1$&#21644;$r_2$&#30340;&#27010;&#29575;&#20998;&#24067;&#26368;&#22823;&#21270;&#22320;&#20943;&#23569;&#25152;&#26377;&#21407;&#22987;&#29305;&#24449;&#12289;&#36741;&#21161;&#29305;&#24449;&#21644;&#20108;&#20803;&#26631;&#31614;&#19978;&#30340;&#26368;&#22823;&#25439;&#22833;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#24067;&#24335;&#31283;&#20581;&#20248;&#21270;&#30340;&#25512;&#24191;&#65292;&#29992;&#20110;&#22788;&#29702;&#25968;&#25454;&#21512;&#24182;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we are given two datasets: a labeled dataset and unlabeled dataset which also has additional auxiliary features not present in the first dataset. What is the most principled way to use these datasets together to construct a predictor?  The answer should depend upon whether these datasets are generated by the same or different distributions over their mutual feature sets, and how similar the test distribution will be to either of those distributions. In many applications, the two datasets will likely follow different distributions, but both may be close to the test distribution. We introduce the problem of building a predictor which minimizes the maximum loss over all probability distributions over the original features, auxiliary features, and binary labels, whose Wasserstein distance is $r_1$ away from the empirical distribution over the labeled dataset and $r_2$ away from that of the unlabeled dataset. This can be thought of as a generalization of distributionally robust opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#36890;&#36807;&#32593;&#32476;&#12289;&#24322;&#26500;&#24615;&#21644;&#25509;&#36817;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#32447;&#35774;&#22791;&#38598;&#21512;&#19978;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.02947</link><description>&lt;p&gt;
&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#19978;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#36890;&#36807;&#32593;&#32476;&#12289;&#24322;&#26500;&#24615;&#21644;&#25509;&#36817;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#32447;&#35774;&#22791;&#38598;&#21512;&#19978;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FedL)&#24050;&#25104;&#20026;&#19968;&#31181;&#23558;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#22312;&#19968;&#32452;&#26080;&#32447;&#35774;&#22791;&#19978;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#30340;&#36845;&#20195;&#26412;&#22320;&#26356;&#26032;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#32858;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#23558;FedL&#26550;&#26500;&#27839;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#25193;&#23637;&#65306;&#65288;i&#65289;&#32593;&#32476;&#65292;&#36890;&#36807;&#35774;&#22791;&#38388;&#36890;&#20449;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#65307;&#65288;ii&#65289;&#24322;&#26500;&#24615;&#65292;&#22312;&#19977;&#20010;&#23618;&#27425;&#36827;&#34892;&#35299;&#37322;&#65306;&#65288;ii-a&#65289;&#23398;&#20064;&#65306;PSL&#32771;&#34385;&#21040;&#35774;&#22791;&#19978;&#20855;&#26377;&#19981;&#21516;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#24322;&#26500;&#25968;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65307;&#65288;ii-b&#65289;&#25968;&#25454;&#65306;PSL&#20551;&#35774;&#25968;&#25454;&#21040;&#36798;&#21644;&#31163;&#24320;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;/&#27010;&#24565;&#28418;&#31227;&#24230;&#37327;&#26469;&#25429;&#33719;&#65307;&#65288;ii-c&#65289;&#35774;&#22791;&#65306;PSL&#32771;&#34385;&#21040;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#33021;&#21147;&#30340;&#35774;&#22791;&#65307;&#65288;iii&#65289;&#25509;&#36817;&#24615;&#65292;&#35774;&#22791;&#20043;&#38388;&#20197;&#21450;&#35775;&#38382;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#21516;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#30456;&#22270;&#65292;&#25506;&#31350;&#20102;&#31364;&#32593;&#32476;&#21644;&#36807;&#21442;&#25968;&#21270;&#27973;&#23618;&#32593;&#32476;&#20043;&#38388;&#30340;&#20132;&#30028;&#22788;&#65292;&#24182;&#30740;&#31350;&#20102;&#19977;&#20010;&#26041;&#38754;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24037;&#20316;&#24314;&#31435;&#22312;&#32479;&#35745;&#29289;&#29702;&#30340;&#26694;&#26550;&#19979;&#12290;</title><link>http://arxiv.org/abs/2202.00293</link><description>&lt;p&gt;
&#39640;&#32500;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#30456;&#22270;
&lt;/p&gt;
&lt;p&gt;
Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks. (arXiv:2202.00293v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#30456;&#22270;&#65292;&#25506;&#31350;&#20102;&#31364;&#32593;&#32476;&#21644;&#36807;&#21442;&#25968;&#21270;&#27973;&#23618;&#32593;&#32476;&#20043;&#38388;&#30340;&#20132;&#30028;&#22788;&#65292;&#24182;&#30740;&#31350;&#20102;&#19977;&#20010;&#26041;&#38754;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24037;&#20316;&#24314;&#31435;&#22312;&#32479;&#35745;&#29289;&#29702;&#30340;&#26694;&#26550;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38750;&#20984;&#20248;&#21270;&#26223;&#35266;&#65292;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#27973;&#23618;&#32593;&#32476;&#20013;&#65292;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#20294;&#24773;&#20917;&#23545;&#20110;&#31364;&#32593;&#32476;&#21364;&#21487;&#33021;&#23436;&#20840;&#19981;&#21516;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#34987;&#22256;&#22312;&#20855;&#26377;&#31967;&#31957;&#27867;&#21270;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#36825;&#20004;&#20010;&#33539;&#30068;&#20043;&#38388;&#30340;&#20132;&#30028;&#22788;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#35843;&#26597;&#20102;&#25152;&#35859;&#30340;&#24179;&#22343;&#22330;/&#27969;&#20307;&#21147;&#23398;&#33539;&#30068;&#19982;Saad&#21644;Solla &#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#24577;&#20013;&#65292;&#23398;&#20064;&#29575;&#12289;&#26102;&#38388;&#23610;&#24230;&#21644;&#38544;&#21547;&#23618;&#25968;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#39640;&#26031;&#25968;&#25454;&#20026;&#20363;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#20174;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#23545;&#20110;&#39640;&#32500;&#24230;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#30830;&#23450;&#24615;&#25551;&#36848;&#65292;&#25105;&#20204;&#21152;&#20197;&#25299;&#23637;&#24182;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#25910;&#25947;&#36895;&#29575;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad &amp; Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#21363;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;</title><link>http://arxiv.org/abs/2201.08163</link><description>&lt;p&gt;
&#35748;&#30693;&#36134;&#26412;&#39033;&#30446;&#65306;&#36890;&#36807;&#35748;&#30693;&#21306;&#22359;&#38142;&#26500;&#24314;&#20010;&#20154;&#25968;&#23383;&#21270;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain. (arXiv:2201.08163v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#21363;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36134;&#26412;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#35774;&#35745;&#22312;&#20854;&#26680;&#24515;&#37319;&#29992;&#20102;&#35748;&#30693;&#21306;&#22359;&#38142;&#65288;&#35748;&#30693;&#36134;&#26412;&#65289;&#12290;&#26550;&#26500;&#21253;&#25324;&#22810;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#36716;&#21270;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#23545;&#35937;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cognitive Ledger Project is an effort to develop a modular system for turning users' personal data into structured information and machine learning models based on a blockchain-based infrastructure. In this work-in-progress paper, we propose a cognitive architecture for cognitive digital twins. The suggested design embraces a cognitive blockchain (Cognitive ledger) at its core. The architecture includes several modules that turn users' activities in the digital environment into reusable knowledge objects and artificial intelligence that one day can work together to form the cognitive digital twin of users.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30452;&#25509;&#35745;&#31639;&#20302;&#28201;&#19979;&#30340;&#28909;&#29109;&#26469;&#25552;&#21462;&#20108;&#32500;&#33258;&#26059;&#26497;&#21270;&#30005;&#23376;&#27668;&#20307;&#30340;&#26377;&#25928;&#36136;&#37327;$m^\ast$&#65292;&#20854;&#25581;&#31034;&#20102;&#26377;&#25928;&#36136;&#37327;&#22312;&#20302;&#23494;&#24230;&#24378;&#32806;&#21512;&#21306;&#22495;&#30340;&#26356;&#21152;&#26126;&#26174;&#30340;&#25233;&#21046;&#20316;&#29992;&#12290;&#35813;&#39044;&#27979;&#38656;&#35201;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2201.03156</link><description>&lt;p&gt;
&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#30340;&#26377;&#25928;&#36136;&#37327;$m^\ast$&#65306;&#19968;&#39033;&#31070;&#32463;&#35268;&#33539;&#21464;&#25442;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
$m^\ast$ of two-dimensional electron gas: a neural canonical transformation study. (arXiv:2201.03156v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30452;&#25509;&#35745;&#31639;&#20302;&#28201;&#19979;&#30340;&#28909;&#29109;&#26469;&#25552;&#21462;&#20108;&#32500;&#33258;&#26059;&#26497;&#21270;&#30005;&#23376;&#27668;&#20307;&#30340;&#26377;&#25928;&#36136;&#37327;$m^\ast$&#65292;&#20854;&#25581;&#31034;&#20102;&#26377;&#25928;&#36136;&#37327;&#22312;&#20302;&#23494;&#24230;&#24378;&#32806;&#21512;&#21306;&#22495;&#30340;&#26356;&#21152;&#26126;&#26174;&#30340;&#25233;&#21046;&#20316;&#29992;&#12290;&#35813;&#39044;&#27979;&#38656;&#35201;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#30005;&#23376;&#30340;&#20934;&#31890;&#23376;&#26377;&#25928;&#36136;&#37327;$m^\ast$&#26159;&#36153;&#31859;&#28082;&#20307;&#29702;&#35770;&#20013;&#30340;&#22522;&#26412;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#30740;&#31350;&#65292;&#22343;&#21248;&#30005;&#23376;&#27668;&#20307;&#30340;&#26377;&#25928;&#36136;&#37327;&#30340;&#31934;&#30830;&#20540;&#20173;&#28982;&#38590;&#20197;&#30830;&#23450;&#12290;&#26032;&#24320;&#21457;&#30340;&#31070;&#32463;&#35268;&#33539;&#21464;&#25442;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#35745;&#31639;&#20302;&#28201;&#19979;&#30340;&#28909;&#29109;&#26469;&#25552;&#21462;&#30005;&#23376;&#27668;&#20307;&#26377;&#25928;&#36136;&#37327;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#23545;&#21464;&#20998;&#22810;&#30005;&#23376;&#23494;&#24230;&#30697;&#38453;&#36827;&#34892;&#24314;&#27169;&#65306;&#19968;&#20010;&#21160;&#37327;&#21344;&#25454;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#19968;&#20010;&#30005;&#23376;&#22352;&#26631;&#30340;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#25581;&#31034;&#20102;&#20108;&#32500;&#33258;&#26059;&#26497;&#21270;&#30005;&#23376;&#27668;&#20307;&#20013;&#26377;&#25928;&#36136;&#37327;&#30340;&#25233;&#21046;&#20316;&#29992;&#65292;&#36825;&#27604;&#20808;&#21069;&#22312;&#20302;&#23494;&#24230;&#24378;&#32806;&#21512;&#21306;&#22495;&#30340;&#25253;&#21578;&#26356;&#21152;&#26126;&#26174;&#12290;&#36825;&#19968;&#39044;&#27979;&#38656;&#35201;&#22312;&#20108;&#32500;&#30005;&#23376;&#27668;&#20307;&#23454;&#39564;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quasiparticle effective mass $m^\ast$ of interacting electrons is a fundamental quantity in the Fermi liquid theory. However, the precise value of the effective mass of uniform electron gas is still elusive after decades of research. The newly developed neural canonical transformation approach [Xie et al., J. Mach. Learn. 1, (2022)] offers a principled way to extract the effective mass of electron gas by directly calculating the thermal entropy at low temperature. The approach models a variational many-electron density matrix using two generative neural networks: an autoregressive model for momentum occupation and a normalizing flow for electron coordinates. Our calculation reveals a suppression of effective mass in the two-dimensional spin-polarized electron gas, which is more pronounced than previous reports in the low-density strong-coupling region. This prediction calls for verification in two-dimensional electron gas experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; "&#32511;&#33394;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;" &#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#25972;&#20010; AutoML &#36807;&#31243;&#26356;&#21152;&#29615;&#20445;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#37327;&#21270; AutoML &#24037;&#20855;&#30340;&#29615;&#22659;&#36275;&#36857;&#65292;&#24182;&#24635;&#32467;&#20102;&#26377;&#20851;&#22914;&#20309;&#38024;&#23545;&#20854; "&#32511;&#33394;&#24615;"&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#65292;&#35774;&#35745;&#21644;&#22522;&#20934;&#27979;&#35797; AutoML &#24037;&#20855;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2111.05850</link><description>&lt;p&gt;
&#32511;&#33394;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards Green Automated Machine Learning: Status Quo and Future Directions. (arXiv:2111.05850v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.05850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; "&#32511;&#33394;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;" &#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#25972;&#20010; AutoML &#36807;&#31243;&#26356;&#21152;&#29615;&#20445;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22914;&#20309;&#37327;&#21270; AutoML &#24037;&#20855;&#30340;&#29615;&#22659;&#36275;&#36857;&#65292;&#24182;&#24635;&#32467;&#20102;&#26377;&#20851;&#22914;&#20309;&#38024;&#23545;&#20854; "&#32511;&#33394;&#24615;"&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#65292;&#35774;&#35745;&#21644;&#22522;&#20934;&#27979;&#35797; AutoML &#24037;&#20855;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064; (AutoML) &#26088;&#22312;&#33258;&#21160;&#37197;&#32622;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#36866;&#29992;&#20110;&#25163;&#22836;&#30340;&#23398;&#20064;&#20219;&#21153; (&#25968;&#25454;&#38598;) &#30340;&#25972;&#20307; (&#36719;&#20214;) &#35299;&#20915;&#26041;&#26696; - &#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;AutoML &#24050;&#32463;&#21457;&#23637;&#25104;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26377;&#30528;&#25968;&#30334;&#31687;&#30340;&#30456;&#20851;&#35770;&#25991;&#12290;&#21516;&#26102;&#65292;AutoML &#22240;&#20854;&#39640;&#36164;&#28304;&#28040;&#32791;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#35768;&#22810;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340; (&#26114;&#36149;&#30340;) &#35780;&#20272;&#65292;&#20197;&#21450;&#36328;&#35768;&#22810;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#30340;&#26114;&#36149;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#32511;&#33394;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064; (Green AutoML) &#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#25972;&#20010; AutoML &#36807;&#31243;&#26356;&#21152;&#29615;&#20445;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#38416;&#36848;&#20102;&#22914;&#20309;&#37327;&#21270; AutoML &#24037;&#20855;&#30340;&#29615;&#22659;&#36275;&#36857;&#12290;&#25509;&#30528;&#65292;&#24635;&#32467;&#20102;&#26377;&#20851;&#22914;&#20309;&#38024;&#23545;&#20854; "&#32511;&#33394;&#24615;"&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#65292;&#35774;&#35745;&#21644;&#22522;&#20934;&#27979;&#35797; AutoML &#24037;&#20855;&#30340;&#19981;&#21516;&#31574;&#30053;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Automated machine learning (AutoML) strives for the automatic configuration of machine learning algorithms and their composition into an overall (software) solution - a machine learning pipeline - tailored to the learning task (dataset) at hand. Over the last decade, AutoML has developed into an independent research field with hundreds of contributions. At the same time, AutoML is being criticised for its high resource consumption as many approaches rely on the (costly) evaluation of many machine learning pipelines, as well as the expensive large scale experiments across many datasets and approaches. In the spirit of recent work on Green AI, this paper proposes Green AutoML, a paradigm to make the whole AutoML process more environmentally friendly. Therefore, we first elaborate on how to quantify the environmental footprint of an AutoML tool. Afterward, different strategies on how to design and benchmark an AutoML tool wrt. their "greenness", i.e. sustainability, are summarized. Finall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.03837</link><description>&lt;p&gt;
&#38598;&#20013;&#20851;&#27880;&#28508;&#22312;&#21629;&#21517;&#23454;&#20307;&#30340;&#20027;&#21160;&#26631;&#27880;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#20851;&#27880;&#28508;&#22312;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;NER&#27880;&#37322;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26088;&#22312;&#35782;&#21035;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#21629;&#21517;&#23454;&#20307;&#30340;&#25552;&#21450;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#21629;&#21517;&#23454;&#20307;&#31867;&#21035;&#20013;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#22312;NER&#20013;&#23454;&#29616;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#35768;&#22810;&#29305;&#23450;&#39046;&#22495;&#30340;NER&#24212;&#29992;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20027;&#21160;&#23398;&#20064;(AL)&#26159;&#35299;&#20915;&#26631;&#31614;&#33719;&#21462;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24050;&#29992;&#20110;NER&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#27880;&#37322;&#25104;&#26412;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#30340;&#20005;&#37325;&#19981;&#22343;&#21248;&#31867;&#20998;&#24067;&#24341;&#20837;&#20102;&#35774;&#35745;&#26377;&#25928;&#30340;NER&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;AL&#21477;&#23376;&#26597;&#35810;&#35780;&#20272;&#20989;&#25968;&#65292;&#26356;&#22810;&#20851;&#27880;&#28508;&#22312;&#30340;&#27491;&#38754;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#21644;&#26631;&#35760;&#25104;&#26412;&#35780;&#20272;&#31574;&#30053;&#26469;&#35780;&#20272;&#36825;&#20123;&#25552;&#35758;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24120;&#21270;&#26041;&#27861;&#65292;&#20197;&#24809;&#32602;&#36807;&#38271;&#25110;&#36807;&#30701;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
&lt;/p&gt;</description></item><item><title>QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;</title><link>http://arxiv.org/abs/2110.11331</link><description>&lt;p&gt;
QuantumNAT&#65306;&#27880;&#37325;&#37327;&#23376;&#22122;&#22768;&#30340;&#22122;&#22768;&#27880;&#20837;&#12289;&#37327;&#21270;&#21644;&#24402;&#19968;&#21270;&#30340;&#37327;&#23376;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization. (arXiv:2110.11331v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11331
&lt;/p&gt;
&lt;p&gt;
QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26159;&#23454;&#29616;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#20248;&#21183;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#36739;&#22823;&#30340;&#37327;&#23376;&#22122;&#22768;&#65288;&#35823;&#24046;&#65289;&#65292;&#22312;&#23454;&#38469;&#30340;&#37327;&#23376;&#35774;&#22791;&#19978;&#65292;PQC&#27169;&#22411;&#30340;&#24615;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QuantumNAT&#65292;&#19968;&#20010;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#30340;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#25105;&#20204;&#21457;&#29616;&#65292;&#37327;&#23376;&#22122;&#22768;&#23545;PQC&#27979;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#26159;&#20174;&#26080;&#22122;&#22768;&#32467;&#26524;&#32463;&#36807;&#19968;&#20010;&#32553;&#25918;&#21644;&#20559;&#31227;&#22240;&#23376;&#24471;&#21040;&#30340;&#32447;&#24615;&#26144;&#23556;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#27979;&#37327;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized Quantum Circuits (PQC) are promising towards quantum advantage on near-term quantum hardware. However, due to the large quantum noises (errors), the performance of PQC models has a severe degradation on real quantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy gap between noise-free simulation and noisy results on IBMQ-Yorktown for MNIST-4 classification is over 60%. Existing noise mitigation methods are general ones without leveraging unique characteristics of PQC; on the other hand, existing PQC work does not consider noise effect. To this end, we present QuantumNAT, a PQC-specific framework to perform noise-aware optimizations in both training and inference stages to improve robustness. We experimentally observe that the effect of quantum noise to PQC measurement outcome is a linear map from noise-free outcome with a scaling and a shift factor. Motivated by that, we propose post-measurement normalization to mitigate the feature distribution di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;OOD&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#23519;&#35282;&#24230;&#65292;&#21363;&#23558;&#24179;&#22343;&#20284;&#28982;&#20998;&#35299;&#20026;KL&#25955;&#24230;&#39033;&#21644;&#29109;&#39033;&#12290;&#21518;&#32773;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;OOD&#25968;&#25454;&#39640;&#20284;&#28982;&#20540;&#30340;&#29616;&#35937;&#65292;&#22240;&#20026;&#23427;&#25233;&#21046;&#20855;&#26377;&#26356;&#39640;&#29109;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20284;&#28982;&#20540;&#12290;</title><link>http://arxiv.org/abs/2109.10794</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;OOD&#26816;&#27979;&#20013;&#30340;&#29109;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Entropic Issues in Likelihood-Based OOD Detection. (arXiv:2109.10794v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;OOD&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#23519;&#35282;&#24230;&#65292;&#21363;&#23558;&#24179;&#22343;&#20284;&#28982;&#20998;&#35299;&#20026;KL&#25955;&#24230;&#39033;&#21644;&#29109;&#39033;&#12290;&#21518;&#32773;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;OOD&#25968;&#25454;&#39640;&#20284;&#28982;&#20540;&#30340;&#29616;&#35937;&#65292;&#22240;&#20026;&#23427;&#25233;&#21046;&#20855;&#26377;&#26356;&#39640;&#29109;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20284;&#28982;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#26159;&#20851;&#20110;&#25968;&#25454;&#27010;&#29575;&#25512;&#29702;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#23427;&#20204;&#21487;&#33021;&#20250;&#20998;&#37197;&#27604;&#27491;&#21521;&#20998;&#24067;&#25968;&#25454;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#36136;&#30097;&#36825;&#20123;&#20284;&#28982;&#20540;&#30340;&#21547;&#20041;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#23519;&#35282;&#24230;&#65292;&#23558;&#24179;&#22343;&#20284;&#28982;&#20998;&#35299;&#20026;KL&#25955;&#24230;&#39033;&#21644;&#29109;&#39033;&#12290;&#25105;&#20204;&#35748;&#20026;&#21518;&#32773;&#21487;&#20197;&#35299;&#37322;&#19978;&#36848;&#22855;&#24618;&#30340;OOD&#34892;&#20026;&#65292;&#25233;&#21046;&#20855;&#26377;&#26356;&#39640;&#29109;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#20284;&#28982;&#20540;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#24605;&#36335;&#24456;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#36824;&#27809;&#26377;&#30475;&#21040;&#23427;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#25506;&#35752;&#12290;&#36825;&#31181;&#20998;&#26512;&#36827;&#19968;&#27493;&#35299;&#37322;&#20102;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#25104;&#21151;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#38382;&#39064;&#29109;&#39033;&#22312;&#26399;&#26395;&#20013;&#20250;&#25269;&#28040;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#22914;&#20309;&#19982;&#26368;&#36817;&#22312;&#27969;&#24418;&#25903;&#25345;&#27169;&#22411;&#20013;&#30340;OOD&#26816;&#27979;&#25104;&#21151;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#38656;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#30340;&#36328; silo &#32852;&#37030;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169; ISRL-DP&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30830;&#20445;&#26469;&#33258;&#27599;&#20010;&#20154;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#34987;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2106.09779</link><description>&lt;p&gt;
&#26080;&#38656;&#20449;&#20219;&#30340;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65306;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses. (arXiv:2106.09779v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#38656;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#30340;&#36328; silo &#32852;&#37030;&#23398;&#20064;&#65292;&#32771;&#34385;&#20102;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169; ISRL-DP&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30830;&#20445;&#26469;&#33258;&#27599;&#20010;&#20154;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#34987;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#36328;&#25968;&#25454;&#28304;&#65288;&#36328; silo&#65289;FL&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#20027;&#20154;&#37117;&#19981;&#20449;&#20219;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182; silos&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#25968;&#25454;&#28304;&#65288;&#20363;&#22914;&#21307;&#38498;&#65289;&#37117;&#26377;&#26469;&#33258;&#19981;&#21516;&#20154;&#65288;&#20363;&#22914;&#24739;&#32773;&#65289;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#24517;&#39035;&#32500;&#25252;&#27599;&#20010;&#20154;&#65288;&#20363;&#22914;&#21307;&#30103;&#35760;&#24405;&#65289;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#21363;&#20351;&#26381;&#21153;&#22120;&#25110;&#20854;&#20182;&#25968;&#25454;&#28304;&#26159;&#24694;&#24847;&#30417;&#21548;&#32773;&#12290;&#36825;&#31181;&#35201;&#27714;&#20419;&#36827;&#20102;&#23545;&#36328; silo &#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;ISRL-DP&#65289;&#30340;&#30740;&#31350;&#65292;&#23427;&#35201;&#27714; silo i &#30340;&#36890;&#20449;&#28385;&#36275;&#35760;&#24405; / &#39033;&#30446;&#32423;&#24046;&#20998;&#38544;&#31169; (DP)&#12290;ISRL-DP &#30830;&#20445; silo i &#20013;&#27599;&#20010;&#20154;&#65288;&#20363;&#22914;&#24739;&#32773;&#65289;&#30340;&#25968;&#25454;&#37117;&#19981;&#20250;&#27844;&#28431;&#12290;ISRL-DP &#19981;&#21516;&#20110;&#21508;&#31181;&#24050;&#26377;&#30340;&#38544;&#31169;&#27010;&#24565;&#12290;&#20013;&#24515;&#21644;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#20551;&#23450;&#20154;&#20204;&#20449;&#20219;&#26381;&#21153;&#22120;/&#20854;&#20182;&#25968;&#25454;&#28304;&#12290;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#26412;&#22320;DP &#20551;&#23450;&#20154;&#20204;&#26681;&#26412;&#19981;&#20449;&#20219;&#20219;&#20309;&#20154;&#65288;&#29978;&#33267;&#26159;&#20182;&#20204;&#33258;&#24049;&#30340;&#25968;&#25454;&#28304;&#65289;&#12290;ISRL-DP &#22788;&#20110;&#20013;&#24515;&#21644;&#26412;&#22320;DP &#20043;&#38388;&#65292;&#20351;&#24471;&#22312;&#36328; silo &#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#20855;&#26377;&#29616;&#23454;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies federated learning (FL)--especially cross-silo FL--with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person's data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo i's communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo i (e.g. hospital i) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-sil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#26426;&#30340;&#35268;&#33539;&#21270;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;&#35268;&#33539;&#21270;&#33258;&#32534;&#30721;&#26426;&#65288;NAE&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25233;&#21046;&#36127;&#26679;&#26412;&#30340;&#37325;&#26500;&#26469;&#24378;&#21046;&#25191;&#34892;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.05735</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#30340;&#33258;&#32534;&#30721;&#26426;
&lt;/p&gt;
&lt;p&gt;
Autoencoding Under Normalization Constraints. (arXiv:2105.05735v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#26426;&#30340;&#35268;&#33539;&#21270;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;&#35268;&#33539;&#21270;&#33258;&#32534;&#30721;&#26426;&#65288;NAE&#65289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25233;&#21046;&#36127;&#26679;&#26412;&#30340;&#37325;&#26500;&#26469;&#24378;&#21046;&#25191;&#34892;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20284;&#28982;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#24322;&#24120;&#26816;&#27979;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#26426;&#30340;&#26631;&#20934;&#21270;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;&#35268;&#33539;&#21270;&#33258;&#32534;&#30721;&#26426;&#65288;NAE&#65289;&#12290;NAE&#30340;&#27010;&#29575;&#23494;&#24230;&#26159;&#36890;&#36807;&#23545;&#33258;&#32534;&#30721;&#26426;&#30340;&#37325;&#26500;&#35823;&#24046;&#36827;&#34892;&#23450;&#20041;&#30340;&#65292;&#35813;&#35823;&#24046;&#22312;&#20256;&#32479;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20013;&#34987;&#19981;&#21516;&#22320;&#23450;&#20041;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#25233;&#21046;&#36127;&#26679;&#26412;&#30340;&#37325;&#26500;&#26469;&#24378;&#21046;&#25191;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#30830;&#35748;NAE&#30340;&#26377;&#25928;&#24615;&#65292;&#19981;&#20165;&#21487;&#20197;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#36824;&#21487;&#20197;&#29983;&#25104;&#20998;&#24067;&#20869;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Likelihood is a standard estimate for outlier detection. The specific role of the normalization constraint is to ensure that the out-of-distribution (OOD) regime has a small likelihood when samples are learned using maximum likelihood. Because autoencoders do not possess such a process of normalization, they often fail to recognize outliers even when they are obviously OOD. We propose the Normalized Autoencoder (NAE), a normalized probabilistic model constructed from an autoencoder. The probability density of NAE is defined using the reconstruction error of an autoencoder, which is differently defined in the conventional energy-based model. In our model, normalization is enforced by suppressing the reconstruction of negative samples, significantly improving the outlier detection performance. Our experimental results confirm the efficacy of NAE, both in detecting outliers and in generating in-distribution samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#21152;&#26435;&#26694;&#26550;&#26041;&#27861;&#20197;&#32531;&#35299;&#22240;&#35268;&#23450;&#30340;&#20989;&#25968;&#31867;&#19981;&#21253;&#21547;&#26368;&#20248;&#35268;&#21017;&#32780;&#23548;&#33268;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2105.00581</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#26679;&#26412;&#21152;&#26435;&#26041;&#27861;&#20197;&#20415;&#20026;&#30446;&#26631;&#20154;&#32676;&#23398;&#20064;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Robust Sample Weighting to Facilitate Individualized Treatment Rule Learning for a Target Population. (arXiv:2105.00581v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.00581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#21152;&#26435;&#26694;&#26550;&#26041;&#27861;&#20197;&#32531;&#35299;&#22240;&#35268;&#23450;&#30340;&#20989;&#25968;&#31867;&#19981;&#21253;&#21547;&#26368;&#20248;&#35268;&#21017;&#32780;&#23548;&#33268;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#23545;&#20110;&#31934;&#20934;&#21307;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#20174;&#21333;&#20010;&#26469;&#28304;&#20154;&#32676;&#20013;&#23548;&#20986;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#24403;&#26469;&#28304;&#20154;&#32676;&#19982;&#30446;&#26631;&#20154;&#32676;&#19981;&#21516;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#32771;&#34385;&#35266;&#23519;&#25968;&#25454;&#35774;&#32622;&#12290;&#19982;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#22240;&#26524;&#25512;&#24191;&#19981;&#21516;&#65292;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#30340;&#25512;&#24191;&#30001;&#20110;&#38656;&#35201;&#22522;&#20110;&#19968;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#20989;&#25968;&#31867;&#23545;&#35268;&#21017;&#36827;&#34892;&#24314;&#27169;&#21644;&#25512;&#24191;&#65292;&#22240;&#27492;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#35813;&#20989;&#25968;&#31867;&#21487;&#33021;&#19981;&#21253;&#21547;&#26080;&#32422;&#26463;&#30495;&#27491;&#26368;&#20248;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#21152;&#26435;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#35268;&#23450;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26469;&#28304;&#20154;&#32676;&#21040;&#30446;&#26631;&#20154;&#32676;&#30340;&#26368;&#20248;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23547;&#27714;&#22312;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#34920;&#24449;&#30340;&#38750;&#21442;&#25968;&#20989;&#25968;&#31867;&#19978;&#36827;&#34892;&#21327;&#21464;&#37327;&#24179;&#34913;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#36827;&#35768;&#22810;&#20381;&#36182;&#20110;&#26435;&#37325;&#30340;&#20010;&#20307;&#21270;&#27835;&#30103;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26679;&#26412;&#21152;&#26435;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning individualized treatment rules (ITRs) is an important topic in precision medicine. Current literature mainly focuses on deriving ITRs from a single source population. We consider the observational data setting when the source population differs from a target population of interest. Compared with causal generalization for the average treatment effect which is a scalar quantity, ITR generalization poses new challenges due to the need to model and generalize the rules based on a prespecified class of functions which may not contain the unrestricted true optimal ITR. The aim of this paper is to develop a weighting framework to mitigate the impact of such misspecification and thus facilitate the generalizability of optimal ITRs from a source population to a target population. Our method seeks covariate balance over a non-parametric function class characterized by a reproducing kernel Hilbert space and can improve many ITR learning methods that rely on weights. We show that the prop
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#27169;&#22411;WARM&#26469;&#35299;&#20915;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#23398;&#39064;&#12290;&#36890;&#36807;&#20165;&#29992;&#26399;&#26395;&#31572;&#26696;&#20316;&#20026;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#26041;&#31243;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#26080;&#38656;&#20351;&#29992;&#26041;&#31243;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2104.06722</link><description>&lt;p&gt;
&#19968;&#31181;&#24369;&#30417;&#30563;&#27169;&#22411;WARM&#29992;&#20110;&#35299;&#20915;&#25968;&#23398;&#39064;
&lt;/p&gt;
&lt;p&gt;
WARM: A Weakly (+Semi) Supervised Model for Solving Math word Problems. (arXiv:2104.06722v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.06722
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#27169;&#22411;WARM&#26469;&#35299;&#20915;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25968;&#23398;&#39064;&#12290;&#36890;&#36807;&#20165;&#29992;&#26399;&#26395;&#31572;&#26696;&#20316;&#20026;&#30417;&#30563;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#26041;&#31243;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#26080;&#38656;&#20351;&#29992;&#26041;&#31243;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#20013;&#38388;&#26041;&#31243;&#33719;&#24471;&#20840;&#37096;&#30417;&#30563;&#65292;&#32780;&#26631;&#27880;&#27599;&#20010;&#25968;&#23398;&#39064;&#30340;&#22353;&#20154;&#20195;&#20215;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#26041;&#31243;&#27880;&#37322;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#38656;&#35201;&#26399;&#26395;&#31572;&#26696;&#20316;&#20026;&#30417;&#30563;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solve MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the state-of-the-art weakly supervised approach, on the stan
&lt;/p&gt;</description></item><item><title>ppAURORA&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110; MPC &#30340;&#26041;&#27861;&#21512;&#24182;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#21015;&#34920;&#65292;&#23454;&#29616;&#23545; ROC &#26354;&#32447;&#21644; Precision-Recall &#26354;&#32447;&#19979;&#38754;&#31215;&#30340;&#20934;&#30830;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2102.08788</link><description>&lt;p&gt;
ppAURORA&#65306;&#38544;&#31169;&#20445;&#25252;&#19979;&#30340; ROC &#26354;&#32447;&#21644; Precision-Recall &#26354;&#32447;&#19979;&#38754;&#31215;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves. (arXiv:2102.08788v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.08788
&lt;/p&gt;
&lt;p&gt;
ppAURORA&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110; MPC &#30340;&#26041;&#27861;&#21512;&#24182;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#21015;&#34920;&#65292;&#23454;&#29616;&#23545; ROC &#26354;&#32447;&#21644; Precision-Recall &#26354;&#32447;&#19979;&#38754;&#31215;&#30340;&#20934;&#30830;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#30740;&#31350;&#39033;&#30446;&#20013;&#65292;&#20351;&#29992;AUC&#20316;&#20026;&#24615;&#33021;&#35780;&#20215;&#25351;&#26631;&#65292;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#26368;&#21518;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#35768;&#22810;&#26041;&#27861;&#37117;&#26159;&#22312;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#38598;&#19981;&#33021;&#20849;&#20139;&#25110;&#32852;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#21644;/&#25110;&#27979;&#35797;&#65292;&#21017;&#26377;&#22810;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22914; &#949;-&#24046;&#20998;&#38544;&#31169;&#12289;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23494;&#30721;&#23398;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#20840;&#23616; AUC &#21487;&#33021;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#26631;&#31614;&#21487;&#33021;&#20063;&#21253;&#21547;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#22522;&#20110; &#949;-&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#24341;&#20837;&#30830;&#20999;&#30340;&#38544;&#31169;&#20445;&#25252;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; MPC &#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026; ppAURORA&#65292;&#23427;&#33021;&#22815;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#24182;&#25490;&#24207;&#21015;&#34920;&#65292;&#36827;&#34892;&#38544;&#31169;&#21512;&#24182;&#65292;&#35745;&#31639;&#19982;&#22312;&#27719;&#38598;&#30340;&#21407;&#22987;&#27979;&#35797;&#26679;&#26412;&#19978;&#33719;&#24471;&#30340;&#23436;&#20840;&#30456;&#21516;&#30340; AUC&#12290;&#20351;&#29992; ppAURORA&#65292;&#35745;&#31639;&#26102;&#38388;&#21482;&#20250;&#27604;&#21407;&#22987;&#30340;&#35745;&#31639;&#30053;&#26377;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing an AUC as a performance measure to compare the quality of different machine learning models is one of the final steps of many research projects. Many of these methods are trained on privacy-sensitive data and there are several different approaches like $\epsilon$-differential privacy, federated machine learning and cryptography if the datasets cannot be shared or used jointly at one place for training and/or testing. In this setting, it can also be a problem to compute the global AUC, since the labels might also contain privacy-sensitive information. There have been approaches based on $\epsilon$-differential privacy to address this problem, but to the best of our knowledge, no exact privacy preserving solution has been introduced. In this paper, we propose an MPC-based solution, called ppAURORA, with private merging of individually sorted lists from multiple sources to compute the exact AUC as one could obtain on the pooled original test samples. With ppAURORA, the computati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2011.07442</link><description>&lt;p&gt;
&#21033;&#29992;&#32972;&#26223;&#38899;&#32032;&#31867;&#20449;&#24687;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information. (arXiv:2011.07442v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.07442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;&#36890;&#36807;&#29992;&#21457;&#38899;&#30340;&#20301;&#32622;&#21644;&#26041;&#24335;&#29305;&#24449;&#22686;&#24378;&#22768;&#23398;&#29305;&#24449;&#65292;&#35821;&#38899;&#22686;&#24378;(SE)&#36807;&#31243;&#21487;&#20197;&#25351;&#23548;&#32771;&#34385;&#36755;&#20837;&#35821;&#38899;&#30340;&#24191;&#20041;&#35821;&#38899;&#23646;&#24615;&#65292;&#20197;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#32467;&#26500;&#23646;&#24615;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20316;&#20026;&#36827;&#19968;&#27493;&#21463;&#30410;&#30340;SE&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(E2E-ASR)&#27169;&#22411;&#20013;&#39044;&#27979;&#30340;&#24191;&#20041;&#35821;&#38899;&#31867;&#21035;&#24207;&#21015;&#30340;&#25439;&#22833;&#26469;&#25552;&#39640;SE&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22810;&#30446;&#26631;&#35757;&#32451;&#65292;&#21033;&#29992;ASR&#21644;&#24863;&#30693;&#25439;&#22833;&#22522;&#20110;&#22522;&#20110;BPC&#30340;E2E-ASR&#26469;&#35757;&#32451;SE&#31995;&#32479;&#12290;&#26469;&#33258;&#35821;&#38899;&#38477;&#22122;&#65292;&#35821;&#38899;&#21435;&#28151;&#21709;&#21644;&#21463;&#25439;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;BPC&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;SE&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22522;&#20110;BPC&#30340;E2E-ASR&#35757;&#32451;&#30340;SE&#27169;&#22411;&#32988;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this paper, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#20855;&#26377;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#20102;DP&#26080;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#65292;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#25552;&#20379;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2010.12112</link><description>&lt;p&gt;
&#30740;&#31350;&#22312;&#25968;&#25454;&#20381;&#36182;&#24615;&#19979;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Investigating Membership Inference Attacks under Data Dependencies. (arXiv:2010.12112v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#20855;&#26377;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#20102;DP&#26080;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#65292;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#30340;&#38450;&#24481;&#31574;&#30053;&#26469;&#25552;&#20379;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#23454;&#36341;&#65292;&#25512;&#21160;&#30528;&#19981;&#26029;&#25193;&#22823;&#30340;&#39046;&#22495;&#20013;&#30340;&#21019;&#26032;&#12290;&#36825;&#25171;&#24320;&#20102;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#21487;&#33021;&#20250;&#24102;&#26469;&#20005;&#37325;&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;&#19968;&#31181;&#36825;&#26679;&#30340;&#25915;&#20987;&#26159;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#25581;&#31034;&#20102;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#31639;&#27861;&#20316;&#20026;&#38450;&#24481;&#36825;&#31181;&#25915;&#20987;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#35780;&#20272;&#38450;&#24481;&#31574;&#30053;&#26102;&#37117;&#20351;&#29992;&#20102;&#19968;&#31181;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#38598;&#21644;&#38750;&#25104;&#21592;&#29420;&#31435;&#19988;&#28385;&#36275;&#30456;&#21516;&#20998;&#24067;&#12290;&#36825;&#31181;&#20551;&#35774;&#22312;&#24456;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#23545;&#25104;&#21592;&#25512;&#26029;&#30340;&#24433;&#21709;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#26356;&#26222;&#36941;&#30340;&#24773;&#20917;&#19979;DP&#19981;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#65288;&#38544;&#31169;&#21442;&#25968;$\epsilon$&#38543;&#35757;&#32451;&#38598;&#22823;&#23567;$n$&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;DP&#22312;&#25968;&#25454;&#20381;&#36182;&#24615;&#19979;&#23545;MIA&#30340;&#33030;&#24369;&#24615;&#65292;&#31361;&#26174;&#20986;&#38656;&#35201;&#23547;&#25214;&#21487;&#25552;&#20379;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#30340;&#26367;&#20195;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#19981;&#21516;&#31181;&#31867;&#30340;&#22270;&#20687;&#36716;&#25442;&#21518;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#20419;&#36827;&#20102;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2007.00112</link><description>&lt;p&gt;
&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#65306;&#40065;&#26834;&#24615;&#26159;&#21542;&#30001;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#39537;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.00112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#19981;&#21516;&#31181;&#31867;&#30340;&#22270;&#20687;&#36716;&#25442;&#21518;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#26159;&#21542;&#20419;&#36827;&#20102;&#36328;&#31867;&#21035;&#30340;&#22270;&#20687;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#35782;&#21035;&#29289;&#20307;&#22312;&#21464;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#20363;&#22914;&#27169;&#31946;&#25110;&#22122;&#38899;&#65289;&#65292;&#24403;&#36825;&#20123;&#21464;&#25442;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#38598;&#20013;&#26102;&#12290;&#19968;&#20010;&#35299;&#37322;&#36825;&#31181;&#40065;&#26834;&#24615;&#30340;&#20551;&#35774;&#26159;&#65292;DCNNs&#21457;&#23637;&#20986;&#30340;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#22312;&#22270;&#20687;&#36716;&#25442;&#26102;&#19981;&#21457;&#29983;&#25913;&#21464;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#30340;&#30495;&#23454;&#31243;&#24230;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#40065;&#26834;&#24615;&#21487;&#33021;&#26159;&#36890;&#36807;&#19982;&#19981;&#21464;&#24615;&#19981;&#21516;&#30340;&#29305;&#24615;&#23454;&#29616;&#30340;&#65292;&#20363;&#22914;&#65292;&#32593;&#32476;&#30340;&#26576;&#20123;&#37096;&#20998;&#21487;&#33021;&#19987;&#38376;&#29992;&#20110;&#35782;&#21035;&#36716;&#25442;&#25110;&#38750;&#36716;&#25442;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#20419;&#36827;&#23545;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#20102;&#19981;&#21464;&#31070;&#32463;&#34920;&#31034;&#20986;&#29616;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#35813;&#35757;&#32451;&#33539;&#24335;&#20013;&#65292;&#21482;&#26377;&#19968;&#20123;&#23545;&#35937;&#31867;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#34987;&#21464;&#25442;&#65292;&#28982;&#21518;&#35780;&#20215;DCNN&#26159;&#21542;&#23545;&#25152;&#26377;&#31867;&#21035;&#30340;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#37027;&#20123;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36328;&#31867;&#21035;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#34920;&#31034;&#25152;&#20419;&#36827;&#30340;&#65292;&#25903;&#25345;&#40065;&#26834;&#24615;&#26159;&#30001;&#19981;&#21464;&#24615;&#39537;&#21160;&#32780;&#19981;&#26159;&#19987;&#38376;&#30340;&#23376;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#20998;&#23376;&#32467;&#21512;&#34507;&#30333;&#36136;&#30340;&#27969;&#34892;&#35745;&#31639;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#30446;&#21069;&#22522;&#20110;&#22270;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26032;&#22411;&#33647;&#29289;&#35774;&#35745;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2006.16955</link><description>&lt;p&gt;
&#25105;&#20204;&#33267;&#23569;&#24212;&#35813;&#33021;&#22815;&#35774;&#35745;&#20986;&#22909;&#30340;&#20998;&#23376;&#32467;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v5 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.16955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#20998;&#23376;&#32467;&#21512;&#34507;&#30333;&#36136;&#30340;&#27969;&#34892;&#35745;&#31639;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#30446;&#21069;&#22522;&#20110;&#22270;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26032;&#22411;&#33647;&#29289;&#35774;&#35745;&#19978;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#29305;&#24615;&#30340;&#21270;&#21512;&#29289;&#26159;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#29616;&#23454;&#30340;&#22238;&#39038;&#24615;&#22522;&#20934;&#21644;&#21069;&#30651;&#24615;&#39564;&#35777;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#34913;&#37327;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25509;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20998;&#23376;&#32467;&#21512;&#34507;&#30333;&#36136;&#30340;&#27969;&#34892;&#35745;&#31639;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#24471;&#20998;&#39640;&#30340;&#33647;&#29289;&#26679;&#20998;&#23376;&#65292;&#36825;&#20123;&#33647;&#29289;&#23646;&#20110;SMINA&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#23545;&#25509;&#36719;&#20214;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20351;&#29992;&#30495;&#23454;&#22823;&#23567;&#30340;&#35757;&#32451;&#38598;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#32467;&#21512;&#24471;&#20998;&#30340;&#20998;&#23376;&#12290;&#36825;&#34920;&#26126;&#20102;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#26032;&#22411;&#33647;&#29289;&#35774;&#35745;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31616;&#21270;&#35780;&#20998;&#20989;&#25968;&#30340;&#22522;&#20934;&#27979;&#35797;&#29256;&#26412;&#65292;&#24182;&#23637;&#31034;&#27979;&#35797;&#27169;&#22411;&#33021;&#22815;&#37096;&#20998;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#35813;&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#21253;&#21457;&#24067;&#65292;&#21487;&#22312; https://github.com &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing compounds with desired properties is a key element of the drug discovery process. However, measuring progress in the field has been challenging due to the lack of realistic retrospective benchmarks, and the large cost of prospective validation. To close this gap, we propose a benchmark based on docking, a popular computational method for assessing molecule binding to a protein. Concretely, the goal is to generate drug-like molecules that are scored highly by SMINA, a popular docking software. We observe that popular graph-based generative models fail to generate molecules with a high docking score when trained using a realistically sized training set. This suggests a limitation of the current incarnation of models for de novo drug design. Finally, we propose a simplified version of the benchmark based on a simpler scoring function, and show that the tested models are able to partially solve it. We release the benchmark as an easy to use package available at https://github.com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Volterra&#28388;&#27874;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#36755;&#20837;&#25968;&#25454;&#30340;&#24310;&#36831;&#37319;&#26679;&#20043;&#38388;&#30340;&#20132;&#20114;&#24341;&#20837;&#21463;&#25511;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20018;&#34892;&#23454;&#29616;Volterra&#28388;&#27874;&#22120;&#65292;&#20943;&#23569;&#20102;&#36827;&#34892;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#23545;&#35270;&#39057;&#30340;RGB&#20449;&#24687;&#21644;&#20809;&#27969;&#20449;&#24687;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/1910.09616</link><description>&lt;p&gt;
Volterra&#31070;&#32463;&#32593;&#32476;&#65288;VNNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Volterra Neural Networks (VNNs). (arXiv:1910.09616v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Volterra&#28388;&#27874;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#36755;&#20837;&#25968;&#25454;&#30340;&#24310;&#36831;&#37319;&#26679;&#20043;&#38388;&#30340;&#20132;&#20114;&#24341;&#20837;&#21463;&#25511;&#30340;&#38750;&#32447;&#24615;&#65292;&#20197;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20018;&#34892;&#23454;&#29616;Volterra&#28388;&#27874;&#22120;&#65292;&#20943;&#23569;&#20102;&#36827;&#34892;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#23545;&#35270;&#39057;&#30340;RGB&#20449;&#24687;&#21644;&#20809;&#27969;&#20449;&#24687;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#23548;&#33268;&#20102;ML&#30340;&#22823;&#37327;&#25552;&#26696;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#12290;&#20026;&#20102;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Volterra&#28388;&#27874;&#22120;&#21551;&#21457;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36755;&#20837;&#25968;&#25454;&#30340;&#24310;&#36831;&#37319;&#26679;&#20043;&#38388;&#30340;&#20132;&#20114;&#24341;&#20837;&#20102;&#21463;&#25511;&#38750;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Volterra&#28388;&#27874;&#22120;&#30340;&#20018;&#32423;&#23454;&#29616;&#65292;&#20197;&#26174;&#33879;&#38477;&#20302;&#36827;&#34892;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;Volterra&#31070;&#32463;&#32593;&#32476;&#65288;VNN&#65289;&#30340;&#39640;&#25928;&#24182;&#34892;&#23454;&#29616;&#65292;&#20197;&#21450;&#20854;&#22312;&#20445;&#25345;&#30456;&#23545;&#36739;&#31616;&#21333;&#19988;&#21487;&#33021;&#26356;&#26131;&#20110;&#22788;&#29702;&#30340;&#32467;&#26500;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32593;&#32476;&#23545;&#35270;&#39057;&#30340;RGB&#65288;&#31354;&#38388;&#65289;&#20449;&#24687;&#21644;&#20809;&#27969;&#65288;&#26102;&#38388;&#65289;&#20449;&#24687;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#30340;&#27604;&#36739;&#22797;&#26434;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video 
&lt;/p&gt;</description></item></channel></rss>