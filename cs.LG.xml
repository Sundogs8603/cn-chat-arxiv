<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2311.00519</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#37492;&#21035;&#20986;&#30340;&#27491;&#26679;&#26412;&#23545;&#65292;&#24403;&#23427;&#20204;&#34987;&#25512;&#21040;&#23884;&#20837;&#31354;&#38388;&#26102;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#30340;&#19979;&#28216;&#20219;&#21153;&#32534;&#30721;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#21487;&#33021;&#20250;&#30772;&#22351;&#21407;&#22987;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#25105;&#20204;&#33021;&#20174;&#19968;&#20010;&#23376;&#24207;&#21015;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#25104;&#21151;&#37325;&#24314;&#21478;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#37027;&#20040;&#23427;&#20204;&#24212;&#35813;&#26159;&#19968;&#20010;&#27491;&#26679;&#26412;&#23545;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;REBAR&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#20132;&#21449;&#27880;&#24847;&#21147;&#26550;&#26500;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;REBAR&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;REBAR&#35823;&#24046;&#26159;&#20114;&#30456;&#31867;&#21035;&#25104;&#21592;&#30340;&#39044;&#27979;&#22120;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#19968;&#26086;&#38598;&#25104;&#21040;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08224</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#28508;&#22312;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37197;&#22791;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#28508;&#22312;&#31354;&#38388;&#20013;&#22352;&#26631;$\vec{x}$&#30340;&#24179;&#26041;&#25351;&#25968;&#22686;&#38271;&#65292;&#35825;&#23548;&#20986;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#25105;&#20204;&#25551;&#36848;&#30340;&#29616;&#35937;&#26159;&#24050;&#30693;&#30340;&#19968;&#31181;&#34987;&#31216;&#20026;"&#31070;&#32463;&#23849;&#28291;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20986;&#29616;&#65292;&#24182;&#23548;&#33268;&#28508;&#22312;&#31867;&#22343;&#20540;&#23849;&#28291;&#20026;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#21152;&#36895;&#20102;&#25910;&#25947;&#21040;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;</title><link>http://arxiv.org/abs/2310.02563</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#21512;&#20316;&#20215;&#20540;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Practical, Private Assurance of the Value of Collaboration. (arXiv:2310.02563v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#26041;&#21521;&#24076;&#26395;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#24444;&#27492;&#36879;&#38706;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#21452;&#26041;&#24076;&#26395;&#33021;&#22815;&#24471;&#21040;&#21512;&#20316;&#23558;&#26159;&#23500;&#26377;&#25104;&#26524;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#26041;&#34987;&#25215;&#35834;&#36890;&#36807;&#21512;&#24182;&#26469;&#33258;&#21478;&#19968;&#26041;&#30340;&#25968;&#25454;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#27169;&#22411;&#12290;&#21482;&#26377;&#24403;&#26356;&#26032;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#26102;&#65292;&#21452;&#26041;&#25165;&#24076;&#26395;&#36827;&#19968;&#27493;&#21512;&#20316;&#12290;&#22312;&#30830;&#23450;&#36825;&#19968;&#28857;&#20043;&#21069;&#65292;&#21452;&#26041;&#19981;&#24076;&#26395;&#36879;&#38706;&#20182;&#20204;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Torus&#19978;&#30340;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#65288;TFHE&#65289;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#20854;&#20013;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#29992;&#20110;&#30830;&#20445;&#35745;&#31639;&#19981;&#23436;&#20840;&#22312;&#21152;&#23494;&#39046;&#22495;&#36827;&#34892;&#65292;&#36825;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two parties wish to collaborate on their datasets. However, before they reveal their datasets to each other, the parties want to have the guarantee that the collaboration would be fruitful. We look at this problem from the point of view of machine learning, where one party is promised an improvement on its prediction model by incorporating data from the other party. The parties would only wish to collaborate further if the updated model shows an improvement in accuracy. Before this is ascertained, the two parties would not want to disclose their models and datasets. In this work, we construct an interactive protocol for this problem based on the fully homomorphic encryption scheme over the Torus (TFHE) and label differential privacy, where the underlying machine learning model is a neural network. Label differential privacy is used to ensure that computations are not done entirely in the encrypted domain, which is a significant bottleneck for neural network training according to the cu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06240</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#20114;&#34917;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Consistency and adaptivity are complementary targets for the validation of variance-based uncertainty quantification metrics in machine learning regression tasks. (arXiv:2309.06240v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06240
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#20013;&#22522;&#20110;&#26041;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24230;&#37327;&#30340;&#39564;&#35777;&#65292;&#21457;&#29616;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#26448;&#26009;&#21644;&#21270;&#23398;&#31185;&#23398;&#20013;&#35768;&#22810;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#30446;&#21069;&#24050;&#32463;&#35748;&#35782;&#21040;&#24179;&#22343;&#26657;&#20934;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20351;&#29992;&#39069;&#22806;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#26465;&#20214;&#26657;&#20934;&#65292;&#21363;&#19968;&#33268;&#24615;&#12290;&#19968;&#33268;&#24615;&#20027;&#35201;&#36890;&#36807;&#21487;&#38752;&#24615;&#22270;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#24179;&#22343;&#26657;&#20934;&#20043;&#22806;&#36824;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#30340;&#26465;&#20214;&#26657;&#20934;&#65292;&#20063;&#23601;&#26159;&#36866;&#24212;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#36866;&#24212;&#24615;&#26159;ML-UQ&#26041;&#27861;&#30340;&#26368;&#32456;&#29992;&#25143;&#20851;&#27880;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#20182;&#20204;&#23547;&#27714;&#23545;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20219;&#20309;&#28857;&#30340;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#31034;&#19968;&#33268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26159;&#20114;&#34917;&#30340;&#39564;&#35777;&#30446;&#26631;&#65292;&#24182;&#19988;&#22909;&#30340;&#19968;&#33268;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#24182;&#22312;&#19968;&#20010;&#20856;&#22411;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#39564;&#35777;&#26041;&#27861;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty quantification (UQ) in machine learning (ML) regression tasks is becoming the focus of many studies in materials and chemical science. It is now well understood that average calibration is insufficient, and most studies implement additional methods testing the conditional calibration with respect to uncertainty, i.e. consistency. Consistency is assessed mostly by so-called reliability diagrams. There exists however another way beyond average calibration, which is conditional calibration with respect to input features, i.e. adaptivity. In practice, adaptivity is the main concern of the final users of a ML-UQ method, seeking for the reliability of predictions and uncertainties for any point in features space. This article aims to show that consistency and adaptivity are complementary validation targets, and that a good consistency does not imply a good adaptivity. Adapted validation methods are proposed and illustrated on a representative example.
&lt;/p&gt;</description></item><item><title>TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.03755</link><description>&lt;p&gt;
TSGBench&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03755
&lt;/p&gt;
&lt;p&gt;
TSGBench&#26159;&#39318;&#20010;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#22522;&#20934;&#65292;&#29992;&#20110;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#35780;&#20272;&#12289;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;(TSG)&#22312;&#25968;&#25454;&#22686;&#24378;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#32463;&#24120;&#38024;&#23545;&#31867;&#20284;&#30340;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#35282;&#12290;(2)&#20351;&#29992;&#19987;&#38376;&#30340;&#21512;&#25104;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#20559;&#20506;&#65292;&#38459;&#30861;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;(3)&#27169;&#31946;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24448;&#24448;&#19982;&#33258;&#23450;&#20041;&#32593;&#32476;&#25110;&#19979;&#28216;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#38459;&#30861;&#20102;&#19968;&#33268;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;\textsf {TSGBench}&#65292;&#20316;&#20026;&#39318;&#20010;TSG&#22522;&#20934;&#65292;&#26088;&#22312;&#32479;&#19968;&#21644;&#20840;&#38754;&#35780;&#20272;TSG&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;(1)&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#12289;&#38754;&#21521;TSG&#30340;&#20844;&#24320;&#23454;&#38469;&#25968;&#25454;&#38598;&#25910;&#38598;&#65292;&#20197;&#21450;&#26631;&#20934;&#21270;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#65307;(2)&#19968;&#22871;&#32508;&#21512;&#30340;&#35780;&#20272;&#25351;&#26631;&#22871;&#20214;&#65292;&#21253;&#25324;&#22522;&#26412;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Synthetic Time Series Generation (TSG) is crucial in a range of applications, including data augmentation, anomaly detection, and privacy preservation. Although significant strides have been made in this field, existing methods exhibit three key limitations: (1) They often benchmark against similar model types, constraining a holistic view of performance capabilities. (2) The use of specialized synthetic and private datasets introduces biases and hampers generalizability. (3) Ambiguous evaluation measures, often tied to custom networks or downstream tasks, hinder consistent and fair comparison.  To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural TSG Benchmark, designed for a unified and comprehensive assessment of TSG methods. It comprises three modules: (1) a curated collection of publicly available, real-world datasets tailored for TSG, together with a standardized preprocessing pipeline; (2) a comprehensive evaluation measures suite including vanilla measur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09778</link><description>&lt;p&gt;
&#36861;&#27714;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#23454;&#38469;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models. (arXiv:2308.09778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#37319;&#29992;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#35745;&#25968;&#12289;&#25351;&#28041;&#34920;&#36798;&#21644;&#19968;&#33324;&#30340;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65289;&#19978;&#30340;&#34920;&#29616;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#37325;&#28857;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#65292;&#20154;&#20204;&#23581;&#35797;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;Liu, Emerson, and Collier 2022) &#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20294;&#37117;&#34920;&#29616;&#20986;&#24615;&#33021;&#19981;&#20339;&#24182;&#19988;&#19982;&#20154;&#31867;&#24615;&#33021;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#32452;&#21512;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#30784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#26469;&#23545;&#31354;&#38388;&#20174;&#21477;&#36827;&#34892;&#25490;&#21517;&#24182;&#35780;&#20272;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#32467;&#21512;&#21644;&#22320;&#38754;&#21270;&#29289;&#20307;&#23545;&#24212;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#23427;&#20204;&#30340;&#20301;&#32622;&#30340;&#35777;&#25454;&#26469;&#35745;&#31639;&#31354;&#38388;&#20174;&#21477;&#30340;&#26368;&#32456;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.14023</link><description>&lt;p&gt;
&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#20351;&#29992;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#26159;&#21542;&#26159;&#36890;&#29992;&#36924;&#36817;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;Transformer&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#20998;&#26512;&#35201;&#27714;&#36807;&#28145;&#30340;&#23618;&#25968;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#35760;&#24518;&#65292;&#23548;&#33268;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;Transformer&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23558;softmax&#20989;&#25968;&#35299;&#37322;&#20026;hardmax&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;&#36890;&#36807;&#28548;&#28165;softmax&#20989;&#25968;&#19982;Boltzmann&#31639;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#23618;&#20855;&#26377;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#33258;&#27880;&#24847;&#21147;&#20855;&#22791;&#23436;&#20840;&#25429;&#33719;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21333;&#23618;Transformer&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20004;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26500;&#25104;&#30340;&#21333;&#23618;&#33258;&#27880;&#24847;&#21147;Transformer&#26159;&#32039;&#20945;&#22495;&#19978;&#36830;&#32493;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;</title><link>http://arxiv.org/abs/2307.13423</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#23545;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#28165;&#26224;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations. (arXiv:2307.13423v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#24212;&#29992;&#20110;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#20010;&#20307;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034; (SSSRs) &#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#20316;&#20026;&#35821;&#38899;&#36136;&#37327; (SQ) &#39044;&#27979;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#27491;&#24120;&#25110;&#26377;&#21548;&#21147;&#38556;&#30861;&#30340;&#29992;&#25143;&#30340;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#21644;&#22914;&#20309;&#23558;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#23884;&#20837;&#21040;&#36825;&#26679;&#30340;&#34920;&#31034;&#20013;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38750;&#20405;&#20837;&#24335; SQ &#35780;&#32423;&#39044;&#27979;&#25216;&#26415;&#34987;&#25193;&#23637;&#21040;&#39044;&#27979;&#21548;&#21147;&#38556;&#30861;&#29992;&#25143;&#30340;&#21487;&#29702;&#35299;&#24615;&#12290;&#21457;&#29616;&#33258;&#23398;&#20064;&#34920;&#31034;&#20316;&#20026;&#38750;&#20405;&#20837;&#24335;&#39044;&#27979;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#38750;&#24120;&#26377;&#29992;&#65292;&#20854;&#24615;&#33021;&#31454;&#20105;&#21147;&#24378;&#20110;&#26356;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#38024;&#23545; Clarity Prediction Challenge 1 &#21463;&#35797;&#32773;&#21644;&#22686;&#24378;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#25165;&#33021;&#25512;&#24191;&#21040;&#26410;&#30693;&#31995;&#32479;&#21644;&#65288;&#21548;&#21147;&#21463;&#25439;&#30340;&#65289;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.08360</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#28176;&#21464;&#21270;&#30340;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#22810;&#23618;&#22312;&#32447;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Online Learning with Gradual Variations: A Multi-layer Online Ensemble Approach. (arXiv:2307.08360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#20855;&#26377;&#22810;&#31181;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#20998;&#26512;&#20013;&#30452;&#25509;&#24212;&#29992;&#20110;&#23567;&#25439;&#22833;&#30028;&#12290;&#21516;&#26102;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#32423;&#21035;&#33258;&#36866;&#24212;&#24615;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;&#12290;&#22312;&#26356;&#39640;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20855;&#20307;&#31867;&#22411;&#21644;&#26354;&#29575;&#19981;&#30693;&#24773;&#65292;&#32780;&#22312;&#26356;&#20302;&#32423;&#21035;&#19978;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#29615;&#22659;&#30340;&#33391;&#22909;&#24615;&#36136;&#24182;&#33719;&#24471;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#24378;&#20984;&#12289;&#25351;&#25968;&#20985;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#20998;&#21035;&#33719;&#24471;&#20102;$O(\ln V_T)$&#12289;$O(d \ln V_T)$&#21644;$\hat{O}(\sqrt{V_T})$&#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#32500;&#24230;&#65292;$V_T$&#34920;&#31034;&#38382;&#39064;&#30456;&#20851;&#30340;&#26799;&#24230;&#21464;&#21270;&#65292;$\hat{O}(\cdot)$&#34920;&#31034;&#22312;$V_T$&#19978;&#30465;&#30053;&#23545;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#12290;&#23427;&#19981;&#20165;&#20445;&#35777;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#36824;&#30452;&#25509;&#23548;&#20986;&#20102;&#20998;&#26512;&#20013;&#30340;&#23567;&#25439;&#22833;&#30028;&#12290;&#27492;&#22806;&#65292;&#23427;&#19982;&#23545;&#25239;&#24615;/&#38543;&#26426;&#20984;&#20248;&#21270;&#21644;&#21338;&#24328;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#20854;&#23454;&#38469;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an online convex optimization method with two different levels of adaptivity. On a higher level, our method is agnostic to the specific type and curvature of the loss functions, while at a lower level, it can exploit the niceness of the environments and attain problem-dependent guarantees. To be specific, we obtain $\mathcal{O}(\ln V_T)$, $\mathcal{O}(d \ln V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and $\hat{\mathcal{O}}(\cdot)$-notation omits logarithmic factors on $V_T$. Our result finds broad implications and applications. It not only safeguards the worst-case guarantees, but also implies the small-loss bounds in analysis directly. Besides, it draws deep connections with adversarial/stochastic convex optimization and game theory, further validating its practical potential. Our method is based
&lt;/p&gt;</description></item><item><title>XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12816</link><description>&lt;p&gt;
XAI-TRIS&#65306;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12816
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21487;&#35299;&#37322;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#24341;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20915;&#31574;&#8220;&#21487;&#29702;&#35299;&#8221;&#32473;&#20154;&#31867;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#8220;&#37325;&#35201;&#24615;&#8221;&#35780;&#20998;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27491;&#24335;&#30340;&#22522;&#30784;&#65292;&#20351;&#24471;&#26080;&#27861;&#20174;&#32473;&#23450;XAI&#26041;&#27861;&#30340;&#32467;&#26524;&#20013;&#23433;&#20840;&#22320;&#24471;&#20986;&#32467;&#35770;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20063;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#35777;&#39564;&#35777;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#30446;&#21069;&#32570;&#20047;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#36890;&#24120;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#24773;&#26223;&#21046;&#20316;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#36890;&#36807;&#35774;&#35745;&#24050;&#30693;&#37325;&#35201;&#30340;&#31867;&#26465;&#20214;&#29305;&#24449;&#65292;&#20316;&#20026;&#22320;&#38754;&#23454;&#20917;&#35299;&#37322;&#12290;&#21033;&#29992;&#26032;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#19978;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;XAI&#26041;&#27861;&#30340;&#24456;&#22810;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#26469;&#35299;&#20915;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.05566</link><description>&lt;p&gt;
&#25968;&#25454;&#33258;&#36866;&#24212;&#27010;&#29575;&#20284;&#28982;&#36924;&#36817;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Adaptive Probabilistic Likelihood Approximation for Ordinary Differential Equations. (arXiv:2306.05566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#26469;&#35299;&#20915;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26080;&#27861;&#35299;&#20915;&#30340;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#21442;&#25968;&#25512;&#26029;&#22312;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;ODE&#35299;&#36890;&#24120;&#30001;&#30830;&#23450;&#24615;&#31639;&#27861;&#36817;&#20284;&#65292;&#20294;&#26377;&#20851;&#27010;&#29575;&#27714;&#35299;&#22120;&#30340;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#36890;&#36807;&#26356;&#22909;&#22320;&#32771;&#34385;&#25968;&#23383;&#35823;&#24046;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;ODE&#31995;&#32479;&#23545;&#20854;&#21442;&#25968;&#20540;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#22312;&#20284;&#28982;&#20989;&#25968;&#20013;&#20135;&#29983;&#20102;&#28145;&#23618;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#8212;&#8212;&#29616;&#26377;&#30340;&#27010;&#29575;&#27714;&#35299;&#22120;&#23578;&#26410;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;ODE&#35299;&#30340;&#36125;&#21494;&#26031;&#28388;&#27874;&#33539;&#24335;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#22024;&#26434;&#30340;ODE&#35266;&#23519;&#32467;&#26524;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#26410;&#35266;&#27979;&#20998;&#37327;&#21644;&#20219;&#24847;&#38750;&#39640;&#26031;&#22122;&#22768;&#30340;ODEs&#12290;&#20960;&#20010;&#20363;&#23376;&#34920;&#26126;&#65292;&#23427;&#27604;&#29616;&#26377;&#30340;&#27010;&#29575;ODE&#27714;&#35299;&#22120;&#26356;&#31934;&#30830;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#27604;&#31934;&#30830;ODE&#20284;&#28982;&#20989;&#25968;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter inference for ordinary differential equations (ODEs) is of fundamental importance in many scientific applications. While ODE solutions are typically approximated by deterministic algorithms, new research on probabilistic solvers indicates that they produce more reliable parameter estimates by better accounting for numerical errors. However, many ODE systems are highly sensitive to their parameter values. This produces deep local minima in the likelihood function -- a problem which existing probabilistic solvers have yet to resolve. Here, we show that a Bayesian filtering paradigm for probabilistic ODE solution can dramatically reduce sensitivity to parameters by learning from the noisy ODE observations in a data-adaptive manner. Our method is applicable to ODEs with partially unobserved components and with arbitrary non-Gaussian noise. Several examples demonstrate that it is more accurate than existing probabilistic ODE solvers, and even in some cases than the exact ODE likel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#19981;&#21487;&#20449;&#37327;&#23376;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04843</link><description>&lt;p&gt;
&#37327;&#23376;&#23398;&#20064;&#30340;&#32463;&#20856;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Classical Verification of Quantum Learning. (arXiv:2306.04843v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#19981;&#21487;&#20449;&#37327;&#23376;&#35777;&#26126;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25968;&#25454;&#35775;&#38382;&#21644;&#37327;&#23376;&#22788;&#29702;&#21487;&#20197;&#20351;&#26576;&#20123;&#32463;&#20856;&#38590;&#20197;&#22788;&#29702;&#30340;&#23398;&#20064;&#20219;&#21153;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#33021;&#21147;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21482;&#33021;&#25552;&#20379;&#32473;&#23569;&#25968;&#20154;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#26041;&#26696;&#65292;&#20801;&#35768;&#32463;&#20856;&#23458;&#25143;&#22996;&#25176;&#23398;&#20064;&#32473;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#26381;&#21153;&#22120;&#65292;&#20197;&#20419;&#36827;&#24191;&#27867;&#33719;&#24471;&#37327;&#23376;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#21476;&#20856;&#26426;&#22120;&#23398;&#20064;&#20132;&#20114;&#35777;&#26126;&#31995;&#32479;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#20856;&#39564;&#35777;&#37327;&#23376;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#32463;&#20856;&#23398;&#20064;&#32773;&#26080;&#27861;&#33258;&#34892;&#39640;&#25928;&#27714;&#35299;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#20294;&#24403;&#19982;&#19981;&#21487;&#20449;&#30340;&#37327;&#23376;&#35777;&#26126;&#32773;&#20132;&#20114;&#26102;&#65292;&#20182;&#20204;&#21487;&#20197;&#39640;&#25928;&#19988;&#21487;&#38752;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20851;&#20110;&#20855;&#26377;&#22343;&#21248;&#36755;&#20837;&#36793;&#32536;&#23494;&#24230;&#30340;&#23545;&#20183;&#23398;&#20064;&#22855;&#20598;&#24615;&#21644;&#20613;&#37324;&#21494;&#31232;&#30095;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#25968;&#25454;&#35775;&#38382;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#21472;&#21152;&#28151;&#21512;&#37327;&#23376;&#26679;&#20363;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum data access and quantum processing can make certain classically intractable learning tasks feasible. However, quantum capabilities will only be available to a select few in the near future. Thus, reliable schemes that allow classical clients to delegate learning to untrusted quantum servers are required to facilitate widespread access to quantum learning advantages. Building on a recently introduced framework of interactive proof systems for classical machine learning, we develop a framework for classical verification of quantum learning. We exhibit learning problems that a classical learner cannot efficiently solve on their own, but that they can efficiently and reliably solve when interacting with an untrusted quantum prover. Concretely, we consider the problems of agnostic learning parities and Fourier-sparse functions with respect to distributions with uniform input marginal. We propose a new quantum data access model that we call "mixture-of-superpositions" quantum example
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#25439;&#22833;&#26368;&#23567;&#21270;&#20998;&#31867;&#26641;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#36923;&#36753;&#26031;&#33922;&#22238;&#24402;&#30340;&#35299;&#20915;&#26041;&#24335;&#20855;&#26377;&#20855;&#26377;&#35299;&#37322;&#24615;&#29305;&#24449;&#21644;&#31454;&#20105;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00857</link><description>&lt;p&gt;
&#25439;&#22833;&#26368;&#23567;&#21270;&#20998;&#31867;&#26641;&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#21644;&#36923;&#36753;&#26031;&#33922;&#22238;&#24402;&#24773;&#20917;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Loss-Optimal Classification Trees: A Generalized Framework and the Logistic Case. (arXiv:2306.00857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#25439;&#22833;&#26368;&#23567;&#21270;&#20998;&#31867;&#26641;&#30340;&#35299;&#20915;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#36923;&#36753;&#26031;&#33922;&#22238;&#24402;&#30340;&#35299;&#20915;&#26041;&#24335;&#20855;&#26377;&#20855;&#26377;&#35299;&#37322;&#24615;&#29305;&#24449;&#21644;&#31454;&#20105;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#26641;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#23613;&#31649;&#36825;&#26679;&#30340;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#36138;&#23146;&#31574;&#30053;&#26500;&#24314;&#65292;&#20294;&#26159;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#27714;&#35299;&#22120;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#31934;&#30830;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#34920;&#36848;&#12290;&#26412;&#25991;&#35748;&#20026;&#20854;&#20013;&#19968;&#20123;&#26368;&#30456;&#20851;&#30340;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#23553;&#35013;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#65292;&#20854;&#23454;&#20363;&#30001;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#22120;&#30340;&#35268;&#23450;&#22609;&#36896;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#26032;&#39062;&#23454;&#29616;&#65306;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#36923;&#36753;&#25439;&#22833;&#65292;&#23427;&#22312;MIP&#35774;&#32622;&#20013;&#36890;&#36807;&#32447;&#24615;&#20998;&#27573;&#36924;&#36817;&#22788;&#29702;&#65292;&#24182;&#19982;$\ell_1$-&#35268;&#21017;&#21270;&#39033;&#30456;&#32467;&#21512;&#12290;&#26368;&#32456;&#30340;&#26368;&#20248;&#36923;&#36753;&#26641;&#27169;&#22411;&#22312;&#25968;&#20540;&#19978;&#34987;&#35777;&#26126;&#33021;&#22815;&#35825;&#23548;&#20855;&#26377;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#29305;&#24449;&#21644;&#31454;&#20105;&#24615;&#27867;&#21270;&#33021;&#21147;&#30340;&#26641;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;MIP&#30340;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Classification Tree (CT) is one of the most common models in interpretable machine learning. Although such models are usually built with greedy strategies, in recent years, thanks to remarkable advances in Mixer-Integer Programming (MIP) solvers, several exact formulations of the learning problem have been developed. In this paper, we argue that some of the most relevant ones among these training models can be encapsulated within a general framework, whose instances are shaped by the specification of loss functions and regularizers. Next, we introduce a novel realization of this framework: specifically, we consider the logistic loss, handled in the MIP setting by a linear piece-wise approximation, and couple it with $\ell_1$-regularization terms. The resulting Optimal Logistic Tree model numerically proves to be able to induce trees with enhanced interpretability features and competitive generalization capabilities, compared to the state-of-the-art MIP-based approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.00245</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#29992;&#25143;&#30028;&#38754;&#25805;&#20316;&#65306;&#36890;&#36807;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. (arXiv:2306.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#30028;&#38754;&#21644;&#28151;&#21512;&#21160;&#20316;&#31354;&#38388;&#30340;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20026;&#20102;&#26500;&#24314;&#25805;&#20316;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#30340;&#25968;&#23383;&#21270;&#20195;&#29702;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#20381;&#36182;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65288;&#20174;HTML&#25110;&#20854;&#20182;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#27966;&#29983;&#65289;&#65292;&#36825;&#20123;&#34920;&#31034;&#24182;&#19981;&#24635;&#26159;&#23481;&#26131;&#33719;&#21462;&#12290;&#36825;&#20123;&#36755;&#20837;&#34920;&#31034;&#36890;&#24120;&#19982;&#33258;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#32852;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#20351;&#29992;&#19982;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#30340;&#30456;&#21516;&#27010;&#24565;&#30028;&#38754;-&#36890;&#36807;&#22522;&#20110;&#20687;&#32032;&#30340;&#23631;&#24149;&#25130;&#22270;&#21644;&#23545;&#24212;&#20110;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#30340;&#36890;&#29992;&#21160;&#20316;&#31354;&#38388;&#19982;&#25968;&#23383;&#19990;&#30028;&#20132;&#20114;&#30340;&#20195;&#29702;&#12290;&#22312;&#36817;&#26399;&#20851;&#20110;&#20687;&#32032;&#32423;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#20195;&#29702;&#22312;GUI&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#30340;MiniWob ++&#22522;&#20934;&#27979;&#35797;&#20013;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much of the previous work towards digital agents for graphical user interfaces (GUIs) has relied on text-based representations (derived from HTML or other structured data sources), which are not always readily available. These input representations have been often coupled with custom, task-specific action spaces. This paper focuses on creating agents that interact with the digital world using the same conceptual interface that humans commonly use -via pixel-based screenshots and a generic action space corresponding to keyboard and mouse actions. Building upon recent progress in pixel-based pretraining, we show, for the first time, that it is possible for such agents to outperform human crowdworkers on the MiniWob++ benchmark of GUI-based instruction following tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.04120</link><description>&lt;p&gt;
&#19968;&#31181;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Latent Diffusion Model for Protein Structure Generation. (arXiv:2305.04120v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#22797;&#26434;&#30340;&#29983;&#29289;&#20998;&#23376;&#65292;&#33021;&#22312;&#29983;&#29289;&#20307;&#20869;&#25191;&#34892;&#22810;&#31181;&#20851;&#38190;&#21151;&#33021;&#12290;&#35774;&#35745;&#21644;&#29983;&#25104;&#26032;&#22411;&#34507;&#30333;&#36136;&#21487;&#20026;&#26410;&#26469;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#24212;&#29992;&#65288;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#20294;&#30001;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#24314;&#27169;&#31354;&#38388;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20943;&#23569;&#34507;&#30333;&#36136;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#28789;&#27963;&#22320;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#21464;&#34507;&#30333;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#34507;&#30333;&#36136;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#28508;&#22312;&#34507;&#30333;&#36136;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;</title><link>http://arxiv.org/abs/2305.03514</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Transform Computational Social Science?. (arXiv:2305.03514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#27809;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20170;&#21518;&#21487;&#20197;&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#20351;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#25104;&#21151;&#22320;&#22312;&#35768;&#22810;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot&#25805;&#20316;&#65288;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65289;&#12290;&#22914;&#26524;&#36825;&#31181;&#33021;&#21147;&#20063;&#36866;&#29992;&#20110;&#23545;&#35828;&#26381;&#21147;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#31561;&#31038;&#20250;&#29616;&#35937;&#30340;&#32534;&#30721;&#65292;&#37027;&#20040;LLMs&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21464;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;(CSS)&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;CSS&#24037;&#20855;&#30340;&#36335;&#32447;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20248;&#31168;&#30340;&#25552;&#31034;&#23454;&#36341;&#20197;&#21450;&#19968;&#20010;&#24191;&#27867;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20197;&#27979;&#37327;13&#31181;&#35821;&#35328;&#27169;&#22411;&#22312;24&#20010;&#20195;&#34920;&#24615;&#30340;CSS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;LLMs&#26080;&#27861;&#36229;&#36234;&#26368;&#20339;&#24494;&#35843;&#27169;&#22411;&#65292;&#20294;&#20173;&#28982;&#19982;&#20154;&#31867;&#36798;&#25104;&#20102;&#20844;&#24179;&#30340;&#21327;&#35758;&#27700;&#24179;&#12290;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#32534;&#30721;&#20219;&#21153;&#65288;&#29983;&#25104;&#65289;&#19978;&#65292;LLMs&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#36229;&#36807;&#20102;&#24037;&#20316;&#32773;&#30340;&#40644;&#37329;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20170;&#22825;&#30340;LLMs&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#20174;&#26681;&#26412;&#19978;&#22686;&#24378;CSS&#30740;&#31350;&#27969;&#31243;&#65306;(1)&#20316;&#20026;&#38646;-shot&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#26080;&#32541;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02041</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#20302;&#22797;&#26434;&#24230;&#30340;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#30340;&#23545;&#31216;&#27491;&#23450;&#27969;&#24418;&#19978;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#21644;&#36991;&#20813;&#26114;&#36149;&#30697;&#38453;&#25805;&#20316;&#21644;&#35745;&#31639;&#40654;&#26364;&#26799;&#24230;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;&#40654;&#26364;&#23376;&#31354;&#38388;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#19978;&#23545;&#20989;&#25968;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#19982;&#29616;&#26377;&#30340;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#21464;&#20307;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992; carefully chosen &#30340;&#23376;&#31354;&#38388;&#65292;&#20351;&#24471;&#26356;&#26032;&#21487;&#20197;&#20889;&#25104;&#36845;&#20195;&#30340; Cholesky &#22240;&#23376;&#21644;&#19968;&#20010;&#31232;&#30095;&#30697;&#38453;&#30340;&#20056;&#31215;&#24418;&#24335;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26356;&#26032;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#30697;&#38453;&#25805;&#20316;&#65292;&#22914;&#30697;&#38453;&#25351;&#25968;&#21644;&#23494;&#38598;&#30697;&#38453;&#20056;&#27861;&#65292;&#36825;&#20123;&#25805;&#20316;&#36890;&#24120;&#22312;&#20960;&#20046;&#25152;&#26377;&#20854;&#20182; Riemannian &#20248;&#21270;&#31639;&#27861;&#20013;&#37117;&#26159;&#24517;&#38656;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work puts forth low-complexity Riemannian subspace descent algorithms for the minimization of functions over the symmetric positive definite (SPD) manifold. Different from the existing Riemannian gradient descent variants, the proposed approach utilizes carefully chosen subspaces that allow the update to be written as a product of the Cholesky factor of the iterate and a sparse matrix. The resulting updates avoid the costly matrix operations like matrix exponentiation and dense matrix multiplication, which are generally required in almost all other Riemannian optimization algorithms on SPD manifold. We further identify a broad class of functions, arising in diverse applications, such as kernel matrix learning, covariance estimation of Gaussian distributions, maximum likelihood parameter estimation of elliptically contoured distributions, and parameter estimation in Gaussian mixture model problems, over which the Riemannian gradients can be calculated efficiently. The proposed uni-
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36882;&#24402;&#30005;&#36335;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#26159;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#20998;&#24067;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.18187</link><description>&lt;p&gt;
&#21033;&#29992;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#21069;&#21521;&#36807;&#31243;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Spiking Neural Systems with the Event-Driven Forward-Forward Process. (arXiv:2303.18187v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36882;&#24402;&#30005;&#36335;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#26159;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#20998;&#24067;&#34920;&#31034;&#30340;&#26377;&#21069;&#36884;&#30340;&#19968;&#31181;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20351;&#29992;&#23574;&#23792;&#31070;&#32463;&#20803;&#36827;&#34892;&#20449;&#24687;&#22788;&#29702;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20998;&#20998;&#37197;&#31639;&#27861;&#65292;&#26080;&#38656;&#21453;&#39304;&#31361;&#35302;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;-&#21069;&#21521;&#21644;&#39044;&#27979;&#24335;&#21069;&#21521;-&#21069;&#21521;&#23398;&#20064;&#36807;&#31243;&#30340;&#36890;&#29992;&#21270;&#26041;&#26696;&#65292;&#29992;&#20110;&#36845;&#20195;&#22788;&#29702;&#24863;&#35273;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#36882;&#24402;&#30005;&#36335;&#20250;&#26681;&#25454;&#23616;&#37096;&#33258;&#19979;&#21521;&#19978;&#12289;&#33258;&#19978;&#32780;&#19979;&#21644;&#20391;&#38754;&#30340;&#20449;&#21495;&#35745;&#31639;&#27599;&#23618;&#20013;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#65292;&#20419;&#36827;&#19968;&#31181;&#21160;&#24577;&#30340;&#12289;&#36880;&#23618;&#24182;&#34892;&#30340;&#31070;&#32463;&#35745;&#31639;&#24418;&#24335;&#12290;&#19982;&#20381;&#36182;&#21453;&#39304;&#31361;&#35302;&#35843;&#25972;&#31070;&#32463;&#30005;&#27963;&#21160;&#30340;&#23574;&#23792;&#31070;&#32463;&#32534;&#30721;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32431;&#22312;&#32447;&#24182;&#19988;&#26102;&#38388;&#21521;&#21069;&#65292;&#36825;&#26679;&#23601;&#33021;&#22815;&#23398;&#20064;&#24102;&#26377;&#26102;&#38388;&#23574;&#23792;&#20449;&#21495;&#30340;&#24863;&#35273;&#25968;&#25454;&#27169;&#24335;&#30340;&#20998;&#24067;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20107;&#20214;&#39537;&#21160;&#30340;&#21069;&#21521;&#21069;&#21521;&#65288;ED-FF&#65289;&#26694;&#26550;&#24037;&#20316;&#27491;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel credit assignment algorithm for information processing with spiking neurons without requiring feedback synapses. Specifically, we propose an event-driven generalization of the forward-forward and the predictive forward-forward learning processes for a spiking neural system that iteratively processes sensory input over a stimulus window. As a result, the recurrent circuit computes the membrane potential of each neuron in each layer as a function of local bottom-up, top-down, and lateral signals, facilitating a dynamic, layer-wise parallel form of neural computation. Unlike spiking neural coding, which relies on feedback synapses to adjust neural electrical activity, our model operates purely online and forward in time, offering a promising way to learn distributed representations of sensory data patterns with temporal spike signals. Notably, our experimental results on several pattern datasets demonstrate that the even-driven forward-forward (ED-FF) framework works we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16343</link><description>&lt;p&gt;
&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20174;&#38754;&#37096;&#22270;&#20687;&#20013;&#26174;&#31034;&#25919;&#27835;&#21462;&#21521;&#65292;&#21363;&#20351;&#25511;&#21046;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21644;&#33258;&#25105;&#34920;&#29616;&#12290;(arXiv: 2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992;&#38754;&#37096;&#35782;&#21035;&#31639;&#27861;&#65292;&#20174;&#23454;&#39564;&#23460;&#35774;&#32622;&#19979;&#25293;&#25668;&#30340;591&#24352;&#20013;&#24615;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#25551;&#36848;&#31526;&#12290;&#22312;&#25511;&#21046;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#22312;&#25919;&#27835;&#21462;&#21521;&#37327;&#34920;&#19978;&#30340;&#24471;&#20998;(Cronbach&#30340;&#945;=0.94)&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;r = 0.20&#65292;&#36828;&#20248;&#20110;&#20154;&#31867;&#35780;&#20998;&#32773;&#65292;&#19982;&#24037;&#20316;&#38754;&#35797;&#39044;&#27979;&#24037;&#20316;&#25104;&#21151;&#12289;&#37202;&#31934;&#39537;&#21160;&#25915;&#20987;&#24615;&#25110;&#24515;&#29702;&#27835;&#30103;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20174;&#26631;&#20934;&#21270;&#22270;&#20687;&#34893;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;3,401&#21517;&#26469;&#33258;&#32654;&#22269;&#12289;&#33521;&#22269;&#21644;&#21152;&#25343;&#22823;&#30340;&#25919;&#27835;&#20154;&#29289;&#30340;&#33258;&#28982;&#22270;&#20687;&#26679;&#26412;&#20013;&#34920;&#29616;&#33391;&#22909;(r = 0.12)&#65292;&#34920;&#26126;&#38754;&#37096;&#22806;&#35980;&#21644;&#25919;&#27835;&#21462;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#25512;&#24191;&#21040;&#25105;&#20204;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#38754;&#37096;&#29305;&#24449;&#19982;&#25919;&#27835;&#21462;&#21521;&#30456;&#20851;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#30340;&#19979;&#21322;&#33080;&#37096;&#20998;&#26356;&#22823;&#65292;&#34429;&#28982;&#25919;&#27835;&#21462;&#21521;&#19981;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#20010;&#20307;&#38754;&#37096;&#29305;&#24449;&#30340;&#25152;&#26377;&#21464;&#21270;&#65292;&#20294;&#26159;&#36825;&#31181;&#21457;&#29616;&#36824;&#26159;&#23500;&#26377;&#21551;&#21457;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.11835</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#30340;&#25628;&#32034;&#26041;&#27861;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs. (arXiv:2302.11835v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#21442;&#25968;&#26657;&#20934;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#22312;&#25552;&#39640;&#25928;&#29575;&#30340;&#21516;&#26102;&#19981;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#35843;&#25972;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#23398;&#21644;&#37329;&#34701;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#21442;&#25968;&#26657;&#20934;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#38750;&#24120;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#26080;&#23548;&#25968;&#25628;&#32034;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23545;&#20247;&#25152;&#21608;&#30693;&#30340;&#23439;&#35266;&#32463;&#27982;&#20195;&#29702;&#27169;&#22411;&#30340;&#33509;&#24178;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#26041;&#27861;&#25152;&#20570;&#20986;&#30340;&#8220;&#28151;&#21512;&#31574;&#30053;&#8221;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;&#29305;&#21035;&#39640;&#25928;&#65292;&#24182;&#19988;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#36890;&#24120;&#20250;&#22686;&#21152;&#24615;&#33021;&#65292;&#22240;&#20026;&#20219;&#20309;&#21333;&#19968;&#26041;&#27861;&#30340;&#20559;&#24046;&#37117;&#20250;&#34987;&#32531;&#35299;&#12290;&#36890;&#36807;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#22312;&#26657;&#20934;&#36816;&#34892;&#36807;&#31243;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#32452;&#21512;&#25628;&#32034;&#26041;&#27861;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20154;&#21482;&#26377;&#22312;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#26102;&#25165;&#32487;&#32493;&#21033;&#29992;&#29305;&#23450;&#26041;&#27861;&#65292;&#20294;&#22312;&#35813;&#26041;&#27861;&#36798;&#21040;&#24615;&#33021;&#24179;&#21488;&#26102;&#25506;&#32034;&#26032;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#25628;&#32034;&#26041;&#26696;&#22312;&#20219;&#20309;&#20854;&#20182;&#27979;&#35797;&#30340;&#26041;&#27861;&#25110;&#26041;&#27861;&#32452;&#21512;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#19987;&#19994;&#30340;&#39046;&#22495;&#30693;&#35782;&#25110;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating agent-based models (ABMs) in economics and finance typically involves a derivative-free search in a very large parameter space. In this work, we benchmark a number of search methods in the calibration of a well-known macroeconomic ABM on real data, and further assess the performance of "mixed strategies" made by combining different methods. We find that methods based on random-forest surrogates are particularly efficient, and that combining search methods generally increases performance since the biases of any single method are mitigated. Moving from these observations, we propose a reinforcement learning (RL) scheme to automatically select and combine search methods on-the-fly during a calibration run. The RL agent keeps exploiting a specific method only as long as this keeps performing well, but explores new strategies when the specific method reaches a performance plateau. The resulting RL search scheme outperforms any other method or method combination tested, and does 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.03684</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#20013;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Temporal Robustness against Data Poisoning. (arXiv:2302.03684v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#30340;&#26102;&#38388;&#25139;&#65292;&#24341;&#20837;&#20102;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#36825;&#20004;&#20010;&#25351;&#26631;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#32771;&#34385;&#20102;&#36890;&#36807;&#24694;&#24847;&#35757;&#32451;&#25968;&#25454;&#25805;&#32437;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#27745;&#26579;&#23041;&#32961;&#27169;&#22411;&#37117;&#22260;&#32469;&#30528;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65292;&#21363;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#33021;&#22815;&#20197;&#21487;&#25215;&#21463;&#30340;&#20195;&#20215;&#27745;&#26579;&#27604;&#39044;&#26399;&#26356;&#22810;&#30340;&#26679;&#26412;&#65292;&#23601;&#20687;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19968;&#26679;&#65292;&#20182;&#20204;&#21487;&#33021;&#33021;&#22815;&#22312;&#24456;&#30701;&#30340;&#26102;&#38388;&#20869;&#20351;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#22833;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#30340;&#20986;&#29983;&#26085;&#26399;&#26102;&#38388;&#25139;&#65292;&#36825;&#20123;&#26102;&#38388;&#25139;&#36890;&#24120;&#26159;&#21487;&#29992;&#30340;&#20294;&#36807;&#21435;&#34987;&#24573;&#30053;&#12290;&#21033;&#29992;&#36825;&#20123;&#26102;&#38388;&#25139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#26032;&#22411;&#25351;&#26631;&#65288;&#25552;&#21069;&#26102;&#38388;&#21644;&#25345;&#32493;&#26102;&#38388;&#65289;&#30340;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#23041;&#32961;&#27169;&#22411;&#65292;&#20998;&#21035;&#34913;&#37327;&#25915;&#20987;&#25552;&#21069;&#24320;&#22987;&#30340;&#26102;&#38388;&#21644;&#25915;&#20987;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#25351;&#26631;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#25968;&#25454;&#27745;&#26579;&#30340;&#26102;&#24207;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#34987;&#27745;&#26579;&#30340;&#26679;&#26412;&#65292;&#20063;&#33021;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20445;&#25252;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data poisoning considers cases when an adversary manipulates the behavior of machine learning algorithms through malicious training data. Existing threat models of data poisoning center around a single metric, the number of poisoned samples. In consequence, if attackers can poison more samples than expected with affordable overhead, as in many practical scenarios, they may be able to render existing defenses ineffective in a short time. To address this issue, we leverage timestamps denoting the birth dates of data, which are often available but neglected in the past. Benefiting from these timestamps, we propose a temporal threat model of data poisoning with two novel metrics, earliness and duration, which respectively measure how long an attack started in advance and how long an attack lasted. Using these metrics, we define the notions of temporal robustness against data poisoning, providing a meaningful sense of protection even with unbounded amounts of poisoned samples. We present a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;</title><link>http://arxiv.org/abs/2210.12583</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Discrete-Time Dynamics for Uncertainty-Aware Model Predictive Control. (arXiv:2210.12583v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#25511;&#21046;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#36827;&#34892;&#20934;&#30830;&#24314;&#27169;&#65292;&#20197;&#20415;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#31934;&#30830;&#19988;&#23433;&#20840;&#22320;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#27492;&#22806;&#65292;&#22312;&#25805;&#20316;&#26465;&#20214;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#24212;&#35813;&#19981;&#26029;&#35843;&#25972;&#20197;&#24357;&#34917;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#20027;&#21160;&#24314;&#27169;&#38750;&#32447;&#24615;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#31163;&#32447;&#23398;&#20064;&#20197;&#24448;&#32463;&#39564;&#21644;&#22312;&#32447;&#23398;&#20064;&#24403;&#21069;&#26426;&#22120;&#20154;&#19982;&#26410;&#30693;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20013;&#20934;&#30830;&#25512;&#26029;&#27169;&#22411;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#22823;&#22823;&#19981;&#21516;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#25805;&#20316;&#33539;&#22260;&#20869;&#20063;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#30340;aleatoric&#65288;&#25968;&#25454;&#65289;&#19981;&#30830;&#23450;&#24615;&#21551;&#21457;&#24335;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#20027;&#21160;&#36873;&#25321;&#26368;&#20248;&#30340;&#25511;&#21046;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based control requires an accurate model of the system dynamics for precisely and safely controlling the robot in complex and dynamic environments. Moreover, in the presence of variations in the operating conditions, the model should be continuously refined to compensate for dynamics changes. In this paper, we present a self-supervised learning approach that actively models the dynamics of nonlinear robotic systems. We combine offline learning from past experience and online learning from current robot interaction with the unknown environment. These two ingredients enable a highly sample-efficient and adaptive learning process, capable of accurately inferring model dynamics in real-time even in operating regimes that greatly differ from the training distribution. Moreover, we design an uncertainty-aware model predictive controller that is heuristically conditioned to the aleatoric (data) uncertainty of the learned dynamics. This controller actively chooses the optimal control act
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11407</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#20256;&#36882;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Architectures Based on Input Gradient Transferability. (arXiv:2210.11407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#32780;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30456;&#20284;&#25110;&#19981;&#21516;&#65292;&#20197;&#21450;&#20160;&#20040;&#22240;&#32032;&#24433;&#21709;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#25110;&#19981;&#21516;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#20855;&#26377;&#19982;&#36755;&#20837;&#26799;&#24230;&#21644;&#20915;&#31574;&#36793;&#30028;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#23545;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20174;&#32780;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#31070;&#32463;&#26550;&#26500;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#22810;&#26679;&#24615;&#21487;&#20197;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20026;&#20160;&#20040;&#24320;&#21457;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#30340;&#22810;&#26679;&#21270;&#31070;&#32463;&#26550;&#26500;&#26159;&#24517;&#35201;&#30340;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09943</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20559;&#35265;&#32531;&#35299;&#65306;&#26356;&#20844;&#24179;&#30340;&#26550;&#26500;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09943
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#25191;&#27861;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#32500;&#24230;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#27169;&#22411;&#20559;&#35265;&#28304;&#20110;&#26377;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#20851;&#20110;&#20559;&#35265;&#32531;&#35299;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#24809;&#32602;&#39033;&#20197;&#38450;&#27490;&#20559;&#35265;&#24433;&#21709;&#27169;&#22411;&#65292;&#25110;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#21518;&#22788;&#29702;&#20197;&#28040;&#38500;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#23454;&#38469;&#19978;&#26681;&#28304;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#12290;&#22522;&#20110;&#36825;&#19968;&#37325;&#26032;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#36755;&#20986;&#20102;&#19968;&#31995;&#21015;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#39640;&#24615;&#33021;&#26550;&#26500;&#21644;&#29616;&#26377;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
&lt;/p&gt;</description></item></channel></rss>