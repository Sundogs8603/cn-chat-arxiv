<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39118;&#38505;&#30340;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#22312;&#20256;&#36755;&#36895;&#29575;&#21644;&#20256;&#36755;&#38169;&#35823;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21160;&#24577;&#39118;&#38505;&#24863;&#30693;&#30340;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#20197;&#26368;&#20339;&#39034;&#24207;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2401.09267</link><description>&lt;p&gt;
&#32771;&#34385;&#39118;&#38505;&#30340;&#24322;&#26500;&#23458;&#25143;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#30340;&#21152;&#36895;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Accelerated Wireless Federated Learning with Heterogeneous Clients. (arXiv:2401.09267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39118;&#38505;&#30340;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#22312;&#20256;&#36755;&#36895;&#29575;&#21644;&#20256;&#36755;&#38169;&#35823;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#21160;&#24577;&#39118;&#38505;&#24863;&#30693;&#30340;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#20197;&#26368;&#20339;&#39034;&#24207;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#31227;&#21160;&#23458;&#25143;&#31471;&#19978;&#23384;&#22312;&#26426;&#23494;&#21644;&#31169;&#23494;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#20256;&#36755;&#36895;&#29575;&#21644;&#20256;&#36755;&#38169;&#35823;&#30340;&#20301;&#32622;&#30456;&#20851;&#24615;&#65292;&#23545;&#20110;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#22312;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#39564;&#35777;&#23458;&#25143;&#31471;&#25968;&#25454;&#36136;&#37327;&#21644;&#23433;&#20840;&#24615;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#25932;&#23545;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#25361;&#25112;&#26356;&#20026;&#20005;&#23803;&#12290;&#38024;&#23545;&#36825;&#19968;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#38505;&#24863;&#30693;&#21152;&#36895;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#22312;&#25317;&#26377;&#25968;&#25454;&#37327;&#12289;&#20256;&#36755;&#36895;&#29575;&#12289;&#20256;&#36755;&#38169;&#35823;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#24322;&#36136;&#24615;&#12290;&#26681;&#25454;&#23458;&#25143;&#31471;&#30340;&#20301;&#32622;&#30456;&#20851;&#24615;&#24615;&#33021;&#21644;&#21487;&#20449;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39118;&#38505;&#24863;&#30693;&#30340;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#26041;&#26696;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#20197;&#20256;&#36755;&#36895;&#29575;&#38477;&#24207;&#21644;&#20256;&#36755;&#38169;&#35823;&#21319;&#24207;&#30340;&#26041;&#24335;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless Federated Learning (FL) is an emerging distributed machine learning paradigm, particularly gaining momentum in domains with confidential and private data on mobile clients. However, the location-dependent performance, in terms of transmission rates and susceptibility to transmission errors, poses major challenges for wireless FL's convergence speed and accuracy. The challenge is more acute for hostile environments without a metric that authenticates the data quality and security profile of the clients. In this context, this paper proposes a novel risk-aware accelerated FL framework that accounts for the clients heterogeneity in the amount of possessed data, transmission rates, transmission errors, and trustworthiness. Classifying clients according to their location-dependent performance and trustworthiness profiles, we propose a dynamic risk-aware global model aggregation scheme that allows clients to participate in descending order of their transmission rates and an ascending
&lt;/p&gt;</description></item><item><title>MSHyper&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#36229;&#22270;&#21644;&#36229;&#36793;&#22270;&#65292;&#24182;&#20351;&#29992;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09261</link><description>&lt;p&gt;
MSHyper&#65306;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series Forecasting. (arXiv:2401.09261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09261
&lt;/p&gt;
&lt;p&gt;
MSHyper&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#36229;&#22270;&#21644;&#36229;&#36793;&#22270;&#65292;&#24182;&#20351;&#29992;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#26512;&#19981;&#21516;&#23610;&#24230;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#32570;&#20047;&#23545;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#65288;MSHyper&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25552;&#20379;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36229;&#36793;&#35270;&#20026;&#33410;&#28857;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36793;&#22270;&#26469;&#22686;&#24378;&#36229;&#22270;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#32858;&#21512;&#27169;&#24335;&#20449;&#24687;&#24182;&#23398;&#20064;&#19981;&#21516;&#23610;&#24230;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24378;&#24230;&#12290;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MSHyper&#22312;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;(MAE)&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#20943;&#23567;&#20102;8.73%&#21644;7.15%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demystifying interactions between temporal patterns of different scales is fundamental to precise long-range time series forecasting. However, previous works lack the ability to model high-order interactions. To promote more comprehensive pattern interaction modeling for long-range time series forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper) framework. Specifically, a multi-scale hypergraph is introduced to provide foundations for modeling high-order pattern interactions. Then by treating hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph modeling. In addition, a tri-stage message passing mechanism is introduced to aggregate pattern information and learn the interaction strength between temporal patterns of different scales. Extensive experiments on five real-world datasets demonstrate that MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 8.73% and 7.15% over the best baseline in MSE and MAE, respec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#20840;&#26223;&#22270;&#20687;&#30340;&#21333;&#22270;&#12289;&#21452;&#22270;&#25110;&#22810;&#22270;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#12290;&#20027;&#35201;&#21253;&#25324;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#12289;&#20840;&#26223;&#22270;&#20687;&#33719;&#21462;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12289;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#12289;&#31435;&#20307;&#21305;&#37197;&#22312;&#29699;&#38754;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#19979;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;</title><link>http://arxiv.org/abs/2401.09252</link><description>&lt;p&gt;
&#20174;360&#24230;&#22270;&#20687;&#20013;&#20272;&#35745;3D&#22330;&#26223;&#20960;&#20309;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey. (arXiv:2401.09252v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#20840;&#26223;&#22270;&#20687;&#30340;&#21333;&#22270;&#12289;&#21452;&#22270;&#25110;&#22810;&#22270;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#12290;&#20027;&#35201;&#21253;&#25324;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#12289;&#20840;&#26223;&#22270;&#20687;&#33719;&#21462;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12289;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#12289;&#31435;&#20307;&#21305;&#37197;&#22312;&#29699;&#38754;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#19979;&#30340;&#31435;&#20307;&#21305;&#37197;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#21333;&#20010;&#12289;&#20004;&#20010;&#25110;&#22810;&#20010;&#22312;&#20840;&#26223;&#20809;&#23398;&#19979;&#25429;&#33719;&#30340;&#22270;&#20687;&#30340;&#20808;&#39537;&#21644;&#26368;&#26032;&#30340;3D&#22330;&#26223;&#20960;&#20309;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#29699;&#38754;&#30456;&#26426;&#27169;&#22411;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#36866;&#29992;&#20110;&#20840;&#26223;&#65288;&#20063;&#31216;&#20026;360&#24230;&#12289;&#29699;&#24418;&#25110;&#20840;&#26223;&#65289;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26368;&#24120;&#35265;&#30340;&#37319;&#38598;&#25216;&#26415;&#21644;&#34920;&#31034;&#26684;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21333;&#30524;&#24067;&#23616;&#21644;&#28145;&#24230;&#25512;&#26029;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#36866;&#29992;&#20110;&#29699;&#24418;&#25968;&#25454;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25509;&#30528;&#65292;&#22312;&#29699;&#38754;&#39046;&#22495;&#23545;&#32463;&#20856;&#30340;&#31435;&#20307;&#21305;&#37197;&#36827;&#34892;&#20102;&#20462;&#35746;&#65292;&#20854;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#31232;&#30095;&#21644;&#31264;&#23494;&#29305;&#24449;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#65292;&#31435;&#20307;&#21305;&#37197;&#27010;&#24565;&#34987;&#25512;&#24191;&#21040;&#22810;&#35270;&#35282;&#25668;&#20687;&#26426;&#35774;&#32622;&#20013;&#65292;&#23558;&#23427;&#20204;&#24402;&#31867;&#20026;&#20809;&#22330;&#12289;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#21644;&#32467;&#26500;&#36816;&#21160;&#65288;&#25110;&#35270;&#35273;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65289;&#12290;&#25105;&#20204;&#36824;&#32534;&#21046;&#20102;&#19968;&#20010;&#20851;&#20110;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#23610;&#24230;&#30340;&#32508;&#21512;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey on pioneer and state-of-the-art 3D scene geometry estimation methodologies based on single, two, or multiple images captured under the omnidirectional optics. We first revisit the basic concepts of the spherical camera model, and review the most common acquisition technologies and representation formats suitable for omnidirectional (also called 360$^\circ$, spherical or panoramic) images and videos. We then survey monocular layout and depth inference approaches, highlighting the recent advances in learning-based solutions suited for spherical data. The classical stereo matching is then revised on the spherical domain, where methodologies for detecting and describing sparse and dense features become crucial. The stereo matching concepts are then extrapolated for multiple view camera setups, categorizing them among light fields, multi-view stereo, and structure from motion (or visual simultaneous localization and mapping). We also compile and di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20984;&#20307;&#32422;&#26463;&#20998;&#35299;&#20026;&#19979;&#20984;&#20984;&#20307;&#21644;&#19968;&#33324;&#20984;&#20307;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#24179;&#28369;&#25554;&#20540;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09251</link><description>&lt;p&gt;
&#22312;&#23376;&#27169;&#27867;&#21270;&#26368;&#22823;&#21270;&#20013;&#65292;&#24357;&#21512;&#19968;&#33324;&#19982;&#19979;&#20984;&#38598;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap Between General and Down-Closed Convex Sets in Submodular Maximization. (arXiv:2401.09251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20984;&#20307;&#32422;&#26463;&#20998;&#35299;&#20026;&#19979;&#20984;&#20984;&#20307;&#21644;&#19968;&#33324;&#20984;&#20307;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#24179;&#28369;&#25554;&#20540;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#20984;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;DR-&#23376;&#27169;&#20989;&#25968;&#30340;&#20248;&#21270;&#22312;&#36817;&#26399;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#38024;&#23545;&#38750;&#21333;&#35843;DR-&#23376;&#27169;&#20989;&#25968;&#22312;&#19968;&#33324;&#65288;&#19981;&#19968;&#23450;&#26159;&#19979;&#20984;&#65289;&#32422;&#26463;&#38598;&#19978;&#36827;&#34892;&#26368;&#22823;&#21270;&#12290;&#28982;&#32780;&#65292;Mualem&#21644;Feldman&#30340;&#26368;&#26032;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20219;&#24847;&#21487;&#34892;&#35299;&#30340;&#26368;&#23567;l&#8734;&#33539;&#25968;&#20316;&#20026;&#21442;&#25968;&#30340;&#26041;&#27861;&#26080;&#27861;&#22312;&#19979;&#20984;&#21644;&#38750;&#19979;&#20984;&#32422;&#26463;&#20043;&#38388;&#23454;&#29616;&#24179;&#28369;&#25554;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#31639;&#27861;&#65292;&#22522;&#20110;&#23558;&#20984;&#20307;&#32422;&#26463;&#20998;&#35299;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#20984;&#20307;&#65306;&#19968;&#20010;&#19979;&#20984;&#20984;&#20307;&#21644;&#19968;&#20010;&#19968;&#33324;&#20984;&#20307;&#65292;&#21487;&#35777;&#26126;&#22320;&#25552;&#20379;&#36825;&#31181;&#25554;&#20540;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25554;&#20540;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of DR-submodular functions has experienced a notable surge in significance in recent times, marking a pivotal development within the domain of non-convex optimization. Motivated by real-world scenarios, some recent works have delved into the maximization of non-monotone DR-submodular functions over general (not necessarily down-closed) convex set constraints. Up to this point, these works have all used the minimum $\ell_\infty$ norm of any feasible solution as a parameter. Unfortunately, a recent hardness result due to Mualem \&amp; Feldman~\cite{mualem2023resolving} shows that this approach cannot yield a smooth interpolation between down-closed and non-down-closed constraints. In this work, we suggest novel offline and online algorithms that provably provide such an interpolation based on a natural decomposition of the convex body constraint into two distinct convex bodies: a down-closed convex body and a general convex body. We also empirically demonstrate the superiority o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09243</link><description>&lt;p&gt;
DiffClone: &#20351;&#29992;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#34892;&#20026;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven Policy Learning. (arXiv:2401.09243v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#39537;&#21160;&#30340;&#31574;&#30053;&#23398;&#20064;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#12290;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;MOCO&#24494;&#35843;&#30340;ResNet50&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#23494;&#38598;&#19988;&#30828;&#20214;&#29305;&#23450;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#25805;&#20316;&#20195;&#29702;&#65292;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#24335;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#24320;&#28304;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30001;&#19987;&#23478;&#25968;&#25454;&#32452;&#25104;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DiffClone&#65292;&#19968;&#31181;&#22686;&#24378;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#22312;&#30495;&#23454;&#30340;&#22312;&#32447;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#22312;NeurIPS 2023&#20030;&#21150;&#30340;Train-Offline-Test-Online&#65288;TOTO&#65289;&#22522;&#20934;&#25361;&#25112;&#36187;&#20013;&#30340;&#23448;&#26041;&#25552;&#20132;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20195;&#29702;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MOCO&#24494;&#35843;&#30340;ResNet50&#30456;&#27604;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning tasks are extremely compute-intensive and hardware-specific. Thus the avenues of tackling these challenges, using a diverse dataset of offline demonstrations that can be used to train robot manipulation agents, is very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a well-curated open-source dataset for offline training comprised mostly of expert data and also benchmark scores of the common offline-RL and behaviour cloning agents. In this paper, we introduce DiffClone, an offline algorithm of enhanced behaviour cloning agent with diffusion-based policy learning, and measured the efficacy of our method on real online physical robots at test time. This is also our official submission to the Train-Offline-Test-Online (TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both pre-trained visual representation and agent policies. In our experiments, we find that MOCO finetuned ResNet50 performs the best in comparison to other finetuned
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20998;&#31867;&#21644;&#37325;&#24314;&#36807;&#31243;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#31867;&#39537;&#21160;&#30340;&#20449;&#24687;&#19982;&#37325;&#24314;&#39537;&#21160;&#30340;&#20449;&#24687;&#22312;&#20849;&#20139;&#23618;&#23384;&#22312;&#30456;&#20114;&#25233;&#21046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.09237</link><description>&lt;p&gt;
&#28145;&#24230;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#20013;&#30340;&#20998;&#31867;&#21644;&#37325;&#24314;&#36807;&#31243;&#65306;&#23545;&#25163;&#36824;&#26159;&#30431;&#21451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Classification and Reconstruction Processes in Deep Predictive Coding Networks: Antagonists or Allies?. (arXiv:2401.09237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20998;&#31867;&#21644;&#37325;&#24314;&#36807;&#31243;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20998;&#31867;&#39537;&#21160;&#30340;&#20449;&#24687;&#19982;&#37325;&#24314;&#39537;&#21160;&#30340;&#20449;&#24687;&#22312;&#20849;&#20139;&#23618;&#23384;&#22312;&#30456;&#20114;&#25233;&#21046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#21551;&#21457;&#30340;&#35270;&#35273;&#35745;&#31639;&#28145;&#24230;&#32593;&#32476;&#23558;&#20998;&#31867;&#21644;&#37325;&#24314;&#36807;&#31243;&#25972;&#21512;&#22312;&#20849;&#20139;&#30340;&#20013;&#38388;&#23618;&#20013;&#12290;&#23613;&#31649;&#36890;&#24120;&#35748;&#20026;&#36825;&#20123;&#36807;&#31243;&#20043;&#38388;&#23384;&#22312;&#21327;&#21516;&#25928;&#24212;&#65292;&#20294;&#23578;&#26410;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#35777;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#20998;&#31867;&#21644;&#37325;&#24314;&#20132;&#20114;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#31867;&#20284;&#20110;&#33258;&#32534;&#30721;&#22120;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#37197;&#22791;&#20102;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#20998;&#31867;&#22836;&#37096;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#22359;&#21644;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#32454;&#33268;&#22320;&#20998;&#26512;&#20102;&#20998;&#31867;&#39537;&#21160;&#21644;&#37325;&#24314;&#39537;&#21160;&#20449;&#24687;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#20849;&#20139;&#28508;&#22312;&#23618;&#20013;&#26080;&#32541;&#20849;&#23384;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20986;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#20998;&#31867;&#39537;&#21160;&#30340;&#20449;&#24687;&#21066;&#24369;&#20102;&#20013;&#38388;&#23618;&#20849;&#20139;&#34920;&#31034;&#20013;&#30340;&#37325;&#24314;&#39537;&#21160;&#20449;&#24687;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20849;&#20139;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive coding-inspired deep networks for visual computing integrate classification and reconstruction processes in shared intermediate layers. Although synergy between these processes is commonly assumed, it has yet to be convincingly demonstrated. In this study, we take a critical look at how classifying and reconstructing interact in deep learning architectures. Our approach utilizes a purposefully designed family of model architectures reminiscent of autoencoders, each equipped with an encoder, a decoder, and a classification head featuring varying modules and complexities. We meticulously analyze the extent to which classification- and reconstruction-driven information can seamlessly coexist within the shared latent layer of the model architectures. Our findings underscore a significant challenge: Classification-driven information diminishes reconstruction-driven information in intermediate layers' shared representations and vice versa. While expanding the shared representation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.09235</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#28857;&#29366;&#28608;&#27963;&#30340;&#31561;&#21464;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Characterization Theorem for Equivariant Networks with Point-wise Activations. (arXiv:2401.09235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#23545;&#31216;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20294;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#30340;&#23545;&#31216;&#24615;&#12289;&#34920;&#31034;&#21644;&#22352;&#26631;&#36873;&#25321;&#65292;&#26368;&#24120;&#35265;&#30340;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#65289;&#24182;&#19981;&#20855;&#22791;&#31561;&#21464;&#24615;&#65292;&#22240;&#27492;&#19981;&#33021;&#29992;&#20110;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#23450;&#29702;&#25551;&#36848;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#26377;&#38480;&#32500;&#34920;&#31034;&#12289;&#22352;&#26631;&#36873;&#25321;&#21644;&#28857;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#20010;&#23436;&#20840;&#31561;&#21464;&#30340;&#23618;&#65292;&#20174;&#32780;&#25512;&#24191;&#21644;&#21152;&#24378;&#20102;&#29616;&#26377;&#30340;&#29305;&#24449;&#23450;&#29702;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24773;&#20917;&#20316;&#20026;&#25512;&#35770;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26059;&#36716;&#31561;&#21464;&#32593;&#32476;&#21482;&#33021;&#26159;&#19981;&#21464;&#30340;&#65292;&#23601;&#20687;&#23545;&#20110;&#20219;&#20309;&#23545;&#36830;&#36890;&#32039;&#33268;&#32676;&#31561;&#21464;&#30340;&#32593;&#32476;&#19968;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20110;&#37325;&#35201;&#30340;&#23436;&#20840;&#31561;&#21464;&#32593;&#32476;&#23454;&#20363;&#26102;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks have shown improved performance, expressiveness and sample complexity on symmetrical domains. But for some specific symmetries, representations, and choice of coordinates, the most common point-wise activations, such as ReLU, are not equivariant, hence they cannot be employed in the design of equivariant neural networks. The theorem we present in this paper describes all possible combinations of finite-dimensional representations, choice of coordinates and point-wise activations to obtain an exactly equivariant layer, generalizing and strengthening existing characterizations. Notable cases of practical relevance are discussed as corollaries. Indeed, we prove that rotation-equivariant networks can only be invariant, as it happens for any network which is equivariant with respect to connected compact groups. Then, we discuss implications of our findings when applied to important instances of exactly equivariant networks. First, we completely characterize permu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21476;&#20856;&#22768;&#20048;&#34920;&#28436;&#30340;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#38899;&#39640;&#21644;&#38899;&#32032;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;&#27468;&#35789;&#23545;&#40784;&#31639;&#27861;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;&#20102;&#33298;&#20271;&#29305;&#20908;&#20043;&#26053;&#25968;&#25454;&#38598;&#65292;&#20026;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#27169;&#22411;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26631;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09200</link><description>&lt;p&gt;
&#19968;&#20010;&#20351;&#29992;&#38899;&#39640;&#21644;&#38899;&#32032;&#29305;&#24449;&#36827;&#34892;&#21476;&#20856;&#22768;&#20048;&#34920;&#28436;&#30340;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features For Classical Vocal Performance. (arXiv:2401.09200v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09200
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21476;&#20856;&#22768;&#20048;&#34920;&#28436;&#30340;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;&#65292;&#36890;&#36807;&#20248;&#21270;&#38899;&#39640;&#21644;&#38899;&#32032;&#29305;&#24449;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;&#27468;&#35789;&#23545;&#40784;&#31639;&#27861;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;&#20102;&#33298;&#20271;&#29305;&#20908;&#20043;&#26053;&#25968;&#25454;&#38598;&#65292;&#20026;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#27169;&#22411;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26631;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23558;&#29616;&#22330;&#28436;&#21809;&#30340;&#38899;&#39057;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#28436;&#21809;&#36807;&#31243;&#20013;&#20934;&#30830;&#25214;&#21040;&#32473;&#23450;&#27468;&#35789;&#30340;&#20301;&#32622;&#12290;&#35813;&#20219;&#21153;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21463;&#30410;&#65292;&#27604;&#22914;&#33258;&#21160;&#20026;&#29616;&#22330;&#38899;&#20048;&#20250;&#25110;&#27468;&#21095;&#29983;&#25104;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#23454;&#26102;&#27169;&#22411;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21482;&#33021;&#20351;&#29992;&#36807;&#21435;&#30340;&#36755;&#20837;&#24182;&#22312;&#26368;&#23567;&#24310;&#36831;&#20869;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#29992;&#20110;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#20351;&#29992;&#31169;&#20154;&#20869;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#23548;&#33268;&#32570;&#20047;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21476;&#20856;&#22768;&#20048;&#34920;&#28436;&#30340;&#23454;&#26102;&#27468;&#35789;&#23545;&#40784;&#31995;&#32479;&#65292;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#38899;&#39640;&#35889;&#22270;&#21644;&#38899;&#32032;&#21518;&#39564;&#22270;&#65288;PPG&#65289;&#30340;&#26368;&#20339;&#32452;&#21512;&#26469;&#25913;&#36827;&#27468;&#35789;&#23545;&#40784;&#31639;&#27861;&#65292;&#20998;&#21035;&#25429;&#25417;&#21809;&#27468;&#22768;&#38899;&#30340;&#26059;&#24459;&#21644;&#35821;&#38899;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37325;&#32452;&#20102;&#33298;&#20271;&#29305;&#20908;&#20043;&#26053;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The goal of real-time lyrics alignment is to take live singing audio as input and to pinpoint the exact position within given lyrics on the fly. The task can benefit real-world applications such as the automatic subtitling of live concerts or operas. However, designing a real-time model poses a great challenge due to the constraints of only using past input and operating within a minimal latency. Furthermore, due to the lack of datasets for real-time models for lyrics alignment, previous studies have mostly evaluated with private in-house datasets, resulting in a lack of standard evaluation methods. This paper presents a real-time lyrics alignment system for classical vocal performances with two contributions. First, we improve the lyrics alignment algorithm by finding an optimal combination of chromagram and phonetic posteriorgram (PPG) that capture melodic and phonetics features of the singing voice, respectively. Second, we recast the Schubert Winterreise Dataset (SWD) which contain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09198</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#36830;&#32493;&#30340;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29289;&#29702;&#27169;&#25311;&#65292;&#24182;&#35299;&#20915;&#20102;&#22522;&#20110;&#22266;&#23450;&#25903;&#25345;&#32593;&#26684;&#30340;&#20256;&#32479;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29289;&#29702;&#27169;&#25311;&#25216;&#26415;&#20381;&#36182;&#20110;&#25968;&#20540;&#26041;&#26696;&#21644;&#32593;&#26684;&#32454;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#36825;&#20123;&#25163;&#24037;&#35299;&#20915;&#26041;&#26696;&#32321;&#29712;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#22522;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#26356;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#38598;&#25104;&#38271;&#36317;&#31163;&#20381;&#36182;&#26469;&#23454;&#29616;&#39640;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#37096;&#20998;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#21644;&#39044;&#27979;&#24418;&#24335;&#20026;&#24120;&#35268;&#25110;&#38750;&#35268;&#21017;&#32593;&#26684;&#30340;&#22266;&#23450;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#22312;&#36830;&#32493;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#22312;&#31232;&#30095;&#35266;&#27979;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#21452;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#22312;&#31232;&#30095;&#20301;&#32622;&#21644;&#36830;&#32493;&#22495;&#19978;&#23450;&#20041;&#20102;&#20004;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#21021;&#22987;&#26465;&#20214;&#36827;&#34892;&#39044;&#27979;&#21644;&#25554;&#20540;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
&lt;/p&gt;</description></item><item><title>GNN-LoFI&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#37051;&#22495;&#33410;&#28857;&#29305;&#24449;&#30340;&#20998;&#24067;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#36890;&#36807;&#30452;&#26041;&#22270;&#20132;&#38598;&#26680;&#20989;&#25968;&#23558;&#30456;&#20284;&#24615;&#20449;&#24687;&#20256;&#25773;&#21040;&#20854;&#20182;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#26426;&#21046;&#65292;&#24182;&#22312;&#22270;&#20998;&#31867;&#21644;&#22238;&#24402;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.09193</link><description>&lt;p&gt;
GNN-LoFI: &#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23616;&#37096;&#30452;&#26041;&#22270;&#20132;&#38598;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based Histogram Intersection. (arXiv:2401.09193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09193
&lt;/p&gt;
&lt;p&gt;
GNN-LoFI&#26159;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#37051;&#22495;&#33410;&#28857;&#29305;&#24449;&#30340;&#20998;&#24067;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#36890;&#36807;&#30452;&#26041;&#22270;&#20132;&#38598;&#26680;&#20989;&#25968;&#23558;&#30456;&#20284;&#24615;&#20449;&#24687;&#20256;&#25773;&#21040;&#20854;&#20182;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#26426;&#21046;&#65292;&#24182;&#22312;&#22270;&#20998;&#31867;&#21644;&#22238;&#24402;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#25104;&#20026;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#39318;&#36873;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#29992;&#23616;&#37096;&#37051;&#22495;&#33410;&#28857;&#29305;&#24449;&#30340;&#20998;&#24067;&#20998;&#26512;&#26367;&#20195;&#20102;&#20256;&#32479;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#27599;&#20010;&#23616;&#37096;&#37051;&#22495;&#30340;egonet&#20013;&#29305;&#24449;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#19982;&#19968;&#32452;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20998;&#24067;&#36827;&#34892;&#30452;&#26041;&#22270;&#20132;&#38598;&#26680;&#20989;&#25968;&#30340;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#23558;&#30456;&#20284;&#24615;&#20449;&#24687;&#20256;&#25773;&#21040;&#32593;&#32476;&#20013;&#30340;&#20854;&#20182;&#33410;&#28857;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#28040;&#24687;&#30001;&#29305;&#24449;&#30340;&#38598;&#21512;&#30830;&#23450;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#32593;&#32476;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#22270;&#20998;&#31867;&#21644;&#22238;&#24402;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#26680;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning. In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network's performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#22810;&#31867;&#20998;&#31867;&#20013;&#23545;&#25239;&#35757;&#32451;&#19979;&#30028;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#35745;&#31639;&#26368;&#20248;&#23545;&#25239;&#39118;&#38505;&#19979;&#30028;&#21644;&#30830;&#23450;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#25130;&#26029;&#31867;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#36991;&#20813;&#20102;&#32452;&#21512;&#36816;&#34892;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09191</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#22810;&#31867;&#20998;&#31867;&#20013;&#23545;&#25239;&#35757;&#32451;&#19979;&#30028;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Optimal Transport Approach for Computing Adversarial Training Lower Bounds in Multiclass Classification. (arXiv:2401.09191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#22810;&#31867;&#20998;&#31867;&#20013;&#23545;&#25239;&#35757;&#32451;&#19979;&#30028;&#30340;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#35745;&#31639;&#26368;&#20248;&#23545;&#25239;&#39118;&#38505;&#19979;&#30028;&#21644;&#30830;&#23450;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#25130;&#26029;&#31867;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#36991;&#20813;&#20102;&#32452;&#21512;&#36816;&#34892;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#24378;&#21046;&#40065;&#26834;&#24615;&#30340;&#27969;&#34892;&#33539;&#24335;&#26159;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#65292;&#28982;&#32780;&#65292;&#36825;&#24341;&#20837;&#20102;&#35768;&#22810;&#35745;&#31639;&#21644;&#29702;&#35770;&#19978;&#30340;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#22810;&#31867;&#20998;&#31867;&#35774;&#32622;&#21644;&#22810;&#36793;&#38469;&#26368;&#20248;&#36755;&#36816;&#65288;MOT&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#65292;&#20026;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#22871;&#26032;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;MOT&#30340;&#36830;&#25509;&#65292;&#25552;&#20986;&#20102;&#35745;&#31639;&#19978;&#26368;&#31616;&#20415;&#21487;&#34892;&#30340;&#25968;&#20540;&#31639;&#27861;&#26469;&#35745;&#31639;&#26368;&#20248;&#23545;&#25239;&#39118;&#38505;&#30340;&#26222;&#36941;&#19979;&#30028;&#65292;&#24182;&#30830;&#23450;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#21644;&#29109;&#27491;&#21017;&#21270;&#65288;Sinkhorn&#65289;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#21487;&#20197;&#26080;&#23475;&#22320;&#25130;&#26029;&#31867;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22312;MOT&#38382;&#39064;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#32452;&#21512;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MNIST&#21644;CI &#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Despite the success of deep learning-based algorithms, it is widely known that neural networks may fail to be robust. A popular paradigm to enforce robustness is adversarial training (AT), however, this introduces many computational and theoretical difficulties. Recent works have developed a connection between AT in the multiclass classification setting and multimarginal optimal transport (MOT), unlocking a new set of tools to study this problem. In this paper, we leverage the MOT connection to propose computationally tractable numerical algorithms for computing universal lower bounds on the optimal adversarial risk and identifying optimal classifiers. We propose two main algorithms based on linear programming (LP) and entropic regularization (Sinkhorn). Our key insight is that one can harmlessly truncate the higher order interactions between classes, preventing the combinatorial run times typically encountered in MOT problems. We validate these results with experiments on MNIST and CI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#20013;&#30340;&#20316;&#29992;&#65292;&#24378;&#35843;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#25552;&#21462;&#35786;&#26029;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.09190</link><description>&lt;p&gt;
&#25506;&#32034;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#20013;&#30340;&#20316;&#29992;&#65306;&#19968;&#39033;&#32508;&#21512;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Role of Convolutional Neural Networks (CNN) in Dental Radiography Segmentation: A Comprehensive Systematic Literature Review. (arXiv:2401.09190v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29273;&#31185;&#25918;&#23556;&#23398;&#20998;&#21106;&#20013;&#30340;&#20316;&#29992;&#65292;&#24378;&#35843;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#25552;&#21462;&#35786;&#26029;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29273;&#31185;&#39046;&#22495;&#65292;&#23545;&#35786;&#26029;&#24037;&#20855;&#30340;&#31934;&#24230;&#35201;&#27714;&#36234;&#26469;&#36234;&#39640;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12289;&#38181;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12289;&#30913;&#20849;&#25391;&#25104;&#20687;&#12289;&#36229;&#22768;&#21644;&#20256;&#32479;&#21475;&#33108;&#26681;&#23574;X&#32447;&#31561;&#20808;&#36827;&#25104;&#20687;&#25216;&#26415;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#21160;&#20998;&#21106;&#25216;&#26415;&#65292;&#20174;&#32780;&#25552;&#21462;&#24517;&#35201;&#30340;&#35786;&#26029;&#25968;&#25454;&#12290;&#36825;&#31181;&#20808;&#36827;&#25216;&#26415;&#30340;&#25972;&#21512;&#35299;&#20915;&#20102;&#23545;&#29273;&#31185;&#30142;&#30149;&#30340;&#26377;&#25928;&#31649;&#29702;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#22914;&#26524;&#19981;&#21450;&#26102;&#21457;&#29616;&#65292;&#21487;&#33021;&#23545;&#20154;&#20307;&#20581;&#24247;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#29273;&#31185;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#20986;&#33394;&#25104;&#32489;&#31361;&#20986;&#20102;&#23427;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#26089;&#26399;&#21457;&#29616;&#21644;&#27835;&#30103;&#20013;&#30340;&#28508;&#21147;&#12290;&#30446;&#26631;&#65306;&#36890;&#36807;&#23637;&#31034;&#22312;&#35786;&#26029;&#21644;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#26524;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20195;&#34920;&#20102;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of dentistry, there is a growing demand for increased precision in diagnostic tools, with a specific focus on advanced imaging techniques such as computed tomography, cone beam computed tomography, magnetic resonance imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep learning has emerged as a pivotal tool in this context, enabling the implementation of automated segmentation techniques crucial for extracting essential diagnostic data. This integration of cutting-edge technology addresses the urgent need for effective management of dental conditions, which, if left undetected, can have a significant impact on human health. The impressive track record of deep learning across various domains, including dentistry, underscores its potential to revolutionize early detection and treatment of oral health issues. Objective: Having demonstrated significant results in diagnosis and prediction, deep convolutional neural networks (CNNs) represent an emerging field 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.09184</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#23610;&#24230;&#22797;&#26434;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#32500;&#24230;&#30340;&#32479;&#35745;&#27169;&#22411;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#12290;&#36825;&#20010;&#26032;&#30340;&#25968;&#37327;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;2sED&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#19979;&#26041;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#36817;&#20284;&#23545;&#19981;&#21516;&#30340;&#31361;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09181</link><description>&lt;p&gt;
&#36229;&#36234;&#21453;&#36951;&#24536;: &#24102;&#26377;&#27491;&#21521;&#20256;&#36882;&#30340;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. (arXiv:2401.09181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#65288;MCIT&#65289;&#20351;&#24471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#28385;&#36275;&#19981;&#26029;&#20986;&#29616;&#30340;&#38656;&#27714;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;MCIT&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65306;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;&#26087;&#30693;&#35782;&#34987;&#36951;&#24536;&#65289;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#65288;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65289;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22823;&#22823;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#28982;&#36973;&#21463;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#36755;&#20837;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#19982;&#26087;&#30340;&#21644;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fwd-Prompt&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#26799;&#24230;&#25237;&#24433;&#21040;&#27531;&#24046;&#31354;&#38388;&#20013;&#65292;&#20197;&#20943;&#23567;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#24182;&#25237;&#24433;&#21040;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#20197;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21463;&#25511;&#35299;&#32544;&#30340;&#20004;&#20010;&#28508;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20165;&#19982;&#39046;&#22495;&#26377;&#20851;&#65292;&#21478;&#19968;&#20010;&#19982;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09180</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25511;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#21463;&#25511;&#35299;&#32544;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multiple Domain Translation through Controlled Disentanglement in Variational Autoencoder. (arXiv:2401.09180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09180
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#21463;&#25511;&#35299;&#32544;&#30340;&#20004;&#20010;&#28508;&#21464;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20165;&#19982;&#39046;&#22495;&#26377;&#20851;&#65292;&#21478;&#19968;&#20010;&#19982;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#39046;&#22495;&#32763;&#35793;&#26159;&#23558;&#25968;&#25454;&#20174;&#19968;&#20010;&#39046;&#22495;&#36716;&#25442;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#26469;&#35757;&#32451;&#31995;&#32479;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#23436;&#20840;&#20381;&#36182;&#20110;&#20462;&#25913;&#36807;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#36825;&#31181;&#20462;&#25913;&#21253;&#25324;&#36890;&#36807;&#35774;&#35745;&#20197;&#25511;&#21046;&#26041;&#24335;&#35299;&#32544;&#20004;&#20010;&#28508;&#21464;&#37327;&#12290;&#20854;&#20013;&#19968;&#20010;&#28508;&#21464;&#37327;&#34987;&#24378;&#21046;&#20165;&#20381;&#36182;&#20110;&#39046;&#22495;&#65292;&#32780;&#21478;&#19968;&#20010;&#28508;&#21464;&#37327;&#24517;&#39035;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#39046;&#22495;&#28508;&#21464;&#37327;&#30340;&#26465;&#20214;&#38480;&#21046;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21644;&#29702;&#35299;&#28508;&#31354;&#38388;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#20102;&#20854;&#20182;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#38469;&#19978;&#19968;&#20010;&#28508;&#21464;&#37327;&#23384;&#20648;&#20102;&#19982;&#39046;&#22495;&#21644;&#20854;&#20182;&#21464;&#21270;&#22240;&#32032;&#26377;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Multiple Domain Translation is the task of transforming data from one domain to other domains without having paired data to train the systems. Typically, methods based on Generative Adversarial Networks (GANs) are used to address this task. However, our proposal exclusively relies on a modified version of a Variational Autoencoder. This modification consists of the use of two latent variables disentangled in a controlled way by design. One of this latent variables is imposed to depend exclusively on the domain, while the other one must depend on the rest of the variability factors of the data. Additionally, the conditions imposed over the domain latent variable allow for better control and understanding of the latent space. We empirically demonstrate that our approach works on different vision datasets improving the performance of other well known methods. Finally, we prove that, indeed, one of the latent variables stores all the information related to the domain and the o
&lt;/p&gt;</description></item><item><title>ADCNet&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#25239;&#20307;&#33647;&#29289;&#22797;&#21512;&#29289;&#30340;&#27963;&#24615;&#12290;&#20854;&#22312;&#25163;&#21160;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2401.09176</link><description>&lt;p&gt;
ADCNet&#65306;&#39044;&#27979;&#25239;&#20307;&#33647;&#29289;&#22797;&#21512;&#29289;&#27963;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ADCNet: a unified framework for predicting the activity of antibody-drug conjugates. (arXiv:2401.09176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09176
&lt;/p&gt;
&lt;p&gt;
ADCNet&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#25239;&#20307;&#33647;&#29289;&#22797;&#21512;&#29289;&#30340;&#27963;&#24615;&#12290;&#20854;&#22312;&#25163;&#21160;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#33647;&#29289;&#22797;&#21512;&#29289;&#65288;ADC&#65289;&#30001;&#20110;&#20854;&#31934;&#30830;&#38774;&#21521;&#30284;&#32454;&#32990;&#24182;&#37322;&#25918;&#39640;&#25928;&#33647;&#29289;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#31934;&#20934;&#21307;&#23398;&#26102;&#20195;&#24443;&#24213;&#25913;&#21464;&#20102;&#30284;&#30151;&#27835;&#30103;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;ADC&#30340;&#32467;&#26500;&#19982;&#27963;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#38590;&#20197;&#29702;&#35299;&#65292;&#22240;&#27492;&#21512;&#29702;&#35774;&#35745;ADC&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ADCNet&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#24110;&#21161;&#35774;&#35745;&#28508;&#22312;&#30340;ADC&#12290;ADCNet&#38598;&#25104;&#20102;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;ESM-2&#21644;&#23567;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;FG-BERT&#65292;&#36890;&#36807;&#23398;&#20064;ADC&#30340;&#25239;&#21407;&#21644;&#25239;&#20307;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#36830;&#25509;&#29289;&#21644;&#33647;&#29289;&#30340;SMILES&#24207;&#21015;&#65292;&#20197;&#21450;&#33647;&#29289;-&#25239;&#20307;&#27604;&#65288;DAR&#65289;&#20540;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#27963;&#24615;&#39044;&#27979;&#12290;&#26681;&#25454;&#31934;&#24515;&#35774;&#35745;&#21644;&#25163;&#21160;&#35843;&#25972;&#30340;ADC&#25968;&#25454;&#38598;&#65292;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;ADCNet&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibody-drug conjugate (ADC) has revolutionized the field of cancer treatment in the era of precision medicine due to their ability to precisely target cancer cells and release highly effective drug. Nevertheless, the realization of rational design of ADC is very difficult because the relationship between their structures and activities is difficult to understand. In the present study, we introduce a unified deep learning framework called ADCNet to help design potential ADCs. The ADCNet highly integrates the protein representation learning language model ESM-2 and small-molecule representation learning language model FG-BERT models to achieve activity prediction through learning meaningful features from antigen and antibody protein sequences of ADC, SMILES strings of linker and payload, and drug-antibody ratio (DAR) value. Based on a carefully designed and manually tailored ADC data set, extensive evaluation results reveal that ADCNet performs best on the test set compared to baseline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.09135</link><description>&lt;p&gt;
&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Local&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(Local-SGD)&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#24179;&#22343;&#65292;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#22312;&#36890;&#20449;&#20013;&#25191;&#34892;&#22810;&#20010;SGD&#26356;&#26032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24322;&#27493;Local-SGD&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#39564;&#35777;&#30740;&#31350;&#65307;&#21363;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#22312;&#23436;&#25104;&#20854;SGD&#27493;&#39588;&#21518;&#31435;&#21363;&#26356;&#26032;&#20840;&#23616;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#23519;&#24037;&#20316;&#33410;&#28857;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#20248;&#21270;&#22120;&#31561;&#22240;&#32032;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26356;&#39057;&#32321;&#22320;&#26356;&#26032;&#65288;&#20840;&#23616;&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#24322;&#27493;Local-SGD&#27604;&#20854;&#21516;&#27493;&#23545;&#24212;&#29289;&#38656;&#35201;&#26356;&#22810;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#24037;&#20316;&#33410;&#28857;&#26799;&#24230;&#38472;&#26087;&#26102;&#20840;&#23616;&#21442;&#25968;&#30340;&#21160;&#37327;&#21152;&#36895;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#65292;&#26681;&#25454;&#24037;&#20316;&#33410;&#28857;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#65292;&#24182;&#19988;&#25299;&#25169;&#22122;&#22768;&#23545;&#20998;&#31867;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.09125</link><description>&lt;p&gt;
&#29702;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Heterophily for Graph Neural Networks. (arXiv:2401.09125v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#65292;&#24182;&#19988;&#25299;&#25169;&#22122;&#22768;&#23545;&#20998;&#31867;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38754;&#20020;&#25361;&#25112;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#33410;&#28857;&#36890;&#36807;&#21508;&#31181;&#27169;&#24335;&#19982;&#19981;&#21516;&#30340;&#37051;&#23621;&#30456;&#36830;&#25509;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#21367;&#31215;&#65288;GC&#65289;&#25805;&#20316;&#21512;&#24182;&#21040;&#23436;&#20840;&#36830;&#25509;&#30340;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#24322;&#36136;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;HSBM&#65289;&#26469;&#25552;&#20379;&#23545;&#19981;&#21516;&#24322;&#36136;&#24615;&#27169;&#24335;&#23545;GNNs&#24433;&#21709;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;HSBM&#26159;&#19968;&#20010;&#21487;&#20197;&#23481;&#32435;&#22810;&#26679;&#30340;&#24322;&#36136;&#24615;&#27169;&#24335;&#30340;&#36890;&#29992;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24212;&#29992;GC&#25805;&#20316;&#65292;&#21487;&#20998;&#24615;&#22686;&#30410;&#21462;&#20915;&#20110;&#20004;&#20010;&#22240;&#32032;&#65292;&#21363;&#37051;&#22495;&#20998;&#24067;&#30340;&#27431;&#27663;&#36317;&#31163;&#21644;$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$&#65292;&#20854;&#20013;$\mathbb{E}\left[\operatorname{deg}\right]$&#26159;&#24179;&#22343;&#33410;&#28857;&#24230;&#12290;&#23427;&#25581;&#31034;&#20102;&#24322;&#36136;&#24615;&#23545;&#20998;&#31867;&#30340;&#24433;&#21709;&#38656;&#35201;&#19982;&#24179;&#22343;&#33410;&#28857;&#24230;&#19968;&#36215;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25299;&#25169;&#22122;&#22768;&#20855;&#26377;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where $\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimenta
&lt;/p&gt;</description></item><item><title>RWKV-TS&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;RNN&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20855;&#26377;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#39062;&#26550;&#26500;&#12289;&#22686;&#24378;&#30340;&#25429;&#25417;&#38271;&#26399;&#24207;&#21015;&#20449;&#24687;&#33021;&#21147;&#20197;&#21450;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#29305;&#28857;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#25110;CNN&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09093</link><description>&lt;p&gt;
RWKV-TS&#65306;&#36229;&#36234;&#20256;&#32479;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks. (arXiv:2401.09093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09093
&lt;/p&gt;
&lt;p&gt;
RWKV-TS&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;RNN&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20855;&#26377;&#20302;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#39062;&#26550;&#26500;&#12289;&#22686;&#24378;&#30340;&#25429;&#25417;&#38271;&#26399;&#24207;&#21015;&#20449;&#24687;&#33021;&#21147;&#20197;&#21450;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#29305;&#28857;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;Transformer&#25110;CNN&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#27169;&#22411;&#22914;LSTM&#21644;GRU&#22312;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#23427;&#20204;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#30340;&#22320;&#20301;&#26377;&#25152;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30740;&#31350;&#24050;&#32463;&#21457;&#29983;&#20102;&#26126;&#26174;&#30340;&#36716;&#21464;&#65292;&#20174;RNN&#21521;Transformers&#12289;MLPs&#21644;CNNs&#31561;&#20854;&#20182;&#26550;&#26500;&#36716;&#21464;&#12290;&#20026;&#20102;&#36229;&#36234;&#20256;&#32479;RNN&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RWKV-TS&#30340;&#39640;&#25928;RNN&#27169;&#22411;&#65292;&#20855;&#26377;&#20197;&#19979;&#19977;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;&#65288;i&#65289;&#20855;&#26377;$O(L)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#26032;&#39062;RNN&#26550;&#26500;&#65307;&#65288;ii&#65289;&#30456;&#36739;&#20110;&#20256;&#32479;RNN&#65292;&#26356;&#33021;&#22815;&#25429;&#25417;&#38271;&#26399;&#30340;&#24207;&#21015;&#20449;&#24687;&#65307;&#65288;iii&#65289;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25193;&#23637;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;RWKV-TS&#27169;&#22411;&#22312;&#19982;&#22522;&#20110;Transformer&#31867;&#22411;&#25110;CNN&#31867;&#22411;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CN
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#30340;&#31574;&#30053;DP-BAI&#65292;&#24182;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#30340;&#25351;&#25968;&#34928;&#20943;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09073</link><description>&lt;p&gt;
&#22266;&#23450;&#39044;&#31639;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Differentially Private Best Arm Identification. (arXiv:2401.09073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#30340;&#31574;&#30053;DP-BAI&#65292;&#24182;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#21644;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#30340;&#25351;&#25968;&#34928;&#20943;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#19979;&#30740;&#31350;&#20102;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#20854;&#20013;&#33218;&#30340;&#22870;&#21169;&#22312;&#21333;&#20301;&#21306;&#38388;&#19978;&#12290;&#32473;&#23450;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;$T$&#21644;&#38544;&#31169;&#21442;&#25968;$\varepsilon&gt;0$&#65292;&#30446;&#26631;&#26159;&#22312;$T$&#20010;&#37319;&#26679;&#36718;&#21518;&#26368;&#23567;&#21270;&#23547;&#25214;&#24179;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#21516;&#26102;&#28385;&#36275;&#20915;&#31574;&#32773;&#31574;&#30053;&#28385;&#36275;&#29305;&#23450;&#30340;$\varepsilon$-&#24046;&#20998;&#38544;&#31169;($\varepsilon$-DP)&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#8220;&#26368;&#22823;&#32477;&#23545;&#34892;&#21015;&#24335;&#8221;&#21407;&#21017;&#26500;&#24314;&#28385;&#36275;$\varepsilon$-DP&#32422;&#26463;&#30340;&#31574;&#30053;(&#31216;&#20026;DP-BAI)&#65292;&#24182;&#32473;&#20986;&#20854;&#38169;&#35823;&#27010;&#29575;&#30340;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#38169;&#35823;&#27010;&#29575;&#30340;&#26368;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#36825;&#20004;&#20010;&#30028;&#22312;$T$&#19978;&#25353;&#25351;&#25968;&#34928;&#20943;&#65292;&#30028;&#20013;&#30340;&#25351;&#25968;&#19982;(a)&#33218;&#30340;&#27425;&#20248;&#38388;&#38553;&#65292;(b)$\varepsilon$&#21644;(c)&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification (BAI) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\varepsilon&gt;0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain {\em $\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by proposing the principle of {\em maximum absolute determinants}, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and (c) t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>DTMM&#26159;&#19968;&#20010;&#24211;&#65292;&#26088;&#22312;&#22312;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#21644;&#39640;&#25928;&#25191;&#34892;&#30340;&#30446;&#26631;&#65292;&#32780;DTMM&#36890;&#36807;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09068</link><description>&lt;p&gt;
DTMM&#65306;&#21033;&#29992;&#20462;&#21098;&#22312;&#26497;&#24369;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#37096;&#32626;TinyML&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09068
&lt;/p&gt;
&lt;p&gt;
DTMM&#26159;&#19968;&#20010;&#24211;&#65292;&#26088;&#22312;&#22312;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#22312;&#19981;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#21644;&#39640;&#25928;&#25191;&#34892;&#30340;&#30446;&#26631;&#65292;&#32780;DTMM&#36890;&#36807;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DTMM&#26159;&#19968;&#20010;&#20026;&#24369;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65289;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#32780;&#35774;&#35745;&#30340;&#24211;&#12290;&#35774;&#35745;DTMM&#30340;&#21160;&#26426;&#26469;&#33258;&#20110;&#26032;&#20852;&#39046;&#22495;&#30340;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#65292;&#23427;&#25506;&#32034;&#23558;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#21040;&#35768;&#22810;&#20302;&#31471;&#29289;&#32852;&#32593;&#35774;&#22791;&#20197;&#23454;&#29616;&#26222;&#36941;&#26234;&#33021;&#12290;&#30001;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#33021;&#21147;&#36739;&#24369;&#65292;&#38656;&#35201;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#20462;&#21098;&#36275;&#22815;&#30340;&#26435;&#37325;&#26469;&#21387;&#32553;&#27169;&#22411;&#12290;&#23613;&#31649;&#20462;&#21098;&#24050;&#22312;&#35768;&#22810;&#35745;&#31639;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20462;&#21098;&#26041;&#27861;&#22312;MCUs&#19978;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#21152;&#21095;&#65306;&#38656;&#35201;&#22312;&#19981;&#26174;&#33879;&#25439;&#23475;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#28145;&#24230;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#19988;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;&#25191;&#34892;&#25928;&#29575;&#19978;&#24212;&#20855;&#22791;&#39640;&#25928;&#24615;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21482;&#33021;&#23454;&#29616;&#20854;&#20013;&#19968;&#20010;&#30446;&#26631;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#23454;&#29616;&#20004;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20462;&#21098;&#21518;&#30340;&#27169;&#22411;&#22312;MCUs&#19978;&#20855;&#26377;&#39640;&#25928;&#37096;&#32626;&#21644;&#25191;&#34892;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20462;&#21098;&#21333;&#20803;&#36873;&#25321;&#30340;DTMM&#12290;
&lt;/p&gt;
&lt;p&gt;
DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19988;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#24658;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#38480;&#21046;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.09067</link><description>&lt;p&gt;
&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Towards Continual Learning Desiderata via HSIC-Bottleneck Orthogonalization and Equiangular Embedding. (arXiv:2401.09067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;HSIC-Bottleneck&#27491;&#20132;&#21270;&#21644;&#24179;&#22343;&#35282;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;&#25345;&#32493;&#23398;&#20064;&#20013;&#36951;&#24536;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#20808;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19988;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#24658;&#23450;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#38480;&#21046;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20219;&#21153;&#35757;&#32451;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26679;&#26412;&#32531;&#20914;&#21306;&#21644;/&#25110;&#32593;&#32476;&#25193;&#23637;&#65292;&#20197;&#24179;&#34913;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65292;&#20294;&#36825;&#20250;&#25439;&#23475;&#20854;&#23454;&#38469;&#20215;&#20540;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#38544;&#31169;&#21644;&#20869;&#23384;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#20005;&#26684;&#20294;&#29616;&#23454;&#30340;&#35774;&#32622;&#65292;&#21363;&#20197;&#21069;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#19988;&#22312;&#39034;&#24207;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#30340;&#22823;&#23567;&#20445;&#25345;&#30456;&#23545;&#24658;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#36951;&#24536;&#24402;&#22240;&#20110;&#36880;&#23618;&#21442;&#25968;&#35206;&#30422;&#21644;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#36793;&#30028;&#30072;&#21464;&#12290;&#36825;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#23454;&#29616;&#65306;HSIC-Bottleneck&#27491;&#20132;&#21270;&#65288;HBO&#65289;&#22312;&#27491;&#20132;&#31354;&#38388;&#20013;&#23454;&#29616;&#38750;&#35206;&#30422;&#21442;&#25968;&#30340;&#26356;&#26032;&#65292;&#36890;&#36807;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;&#36827;&#34892;&#20013;&#20171;&#65307;&#32780;&#24179;&#22343;&#35282;&#23884;&#20837;&#65288;EAE&#65289;&#21017;&#22686;&#24378;&#20102;&#20915;&#31574;&#36793;&#30028;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to catastrophic forgetting when trained on sequential tasks. Various continual learning (CL) methods often rely on exemplar buffers or/and network expansion for balancing model stability and plasticity, which, however, compromises their practical value due to privacy and memory concerns. Instead, this paper considers a strict yet realistic setting, where the training data from previous tasks is unavailable and the model size remains relatively constant during sequential training. To achieve such desiderata, we propose a conceptually simple yet effective method that attributes forgetting to layer-wise parameter overwriting and the resulting decision boundary distortion. This is achieved by the synergy between two key components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten parameter updates mediated by Hilbert-Schmidt independence criterion in an orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Consistent3D&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#32441;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09050</link><description>&lt;p&gt;
Consistent3D: &#23454;&#29616;&#19968;&#33268;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior. (arXiv:2401.09050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Consistent3D&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#32441;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#21450;&#20854;&#21464;&#31181;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20294;&#23481;&#26131;&#20986;&#29616;&#20960;&#20309;&#23849;&#28291;&#21644;&#36136;&#37327;&#24046;&#30340;&#32441;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#20998;&#26512;&#20102;SDS&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#33976;&#39311;&#37319;&#26679;&#36807;&#31243;&#23454;&#38469;&#19978;&#23545;&#24212;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#36712;&#36857;&#37319;&#26679;&#65306;SDS&#27839;&#30528;SDE&#36712;&#36857;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#20135;&#29983;&#19968;&#20010;&#26356;&#23569;&#24102;&#22122;&#22768;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#26679;&#26412;&#21017;&#20316;&#20026;&#20248;&#21270;3D&#27169;&#22411;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;SDE&#37319;&#26679;&#20013;&#30340;&#38543;&#26426;&#24615;&#32463;&#24120;&#23548;&#33268;&#26679;&#26412;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#65292;&#19981;&#24635;&#26159;&#26356;&#23569;&#24102;&#22122;&#22768;&#65292;&#22240;&#27492;&#19981;&#26159;&#19968;&#20010;&#19968;&#33268;&#27491;&#30830;&#30340;&#25351;&#23548;&#65292;&#36825;&#35299;&#37322;&#20102;SDS&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#30001;&#20110;&#23545;&#20110;&#20219;&#20309;SDE&#65292;&#24635;&#26159;&#23384;&#22312;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#20854;&#36712;&#36857;&#37319;&#26679;&#21487;&#20197;&#30830;&#23450;&#24615;&#21644;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#25152;&#38656;&#30446;&#26631;&#28857;&#20316;&#20026;SDE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;Consistent3D&#8221;&#26041;&#27861;&#65292;&#25506;&#32034;ODE&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#20808;&#39564;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score distillation sampling (SDS) and its variants have greatly boosted the development of text-to-3D generation, but are vulnerable to geometry collapse and poor textures yet. To solve this issue, we first deeply analyze the SDS and find that its distillation sampling process indeed corresponds to the trajectory sampling of a stochastic differential equation (SDE): SDS samples along an SDE trajectory to yield a less noisy sample which then serves as a guidance to optimize a 3D model. However, the randomness in SDE sampling often leads to a diverse and unpredictable sample which is not always less noisy, and thus is not a consistently correct guidance, explaining the vulnerability of SDS. Since for any SDE, there always exists an ordinary differential equation (ODE) whose trajectory sampling can deterministically and consistently converge to the desired target point as the SDE, we propose a novel and effective "Consistent3D" method that explores the ODE deterministic sampling prior for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09031</link><description>&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65306;&#26102;&#38388;&#27493;&#24341;&#36215;&#30340;&#23545;&#24433;&#21709;&#20272;&#35745;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Data Attribution for Diffusion Models: Timestep-induced Bias in Influence Estimation. (arXiv:2401.09031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#65292;&#20854;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#65292;&#23548;&#33268;&#22312;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#21487;&#20197;&#23558;&#27169;&#22411;&#34892;&#20026;&#36861;&#28335;&#21040;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20026;&#29702;&#35299;&#8220;&#40657;&#31665;&#8221;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#24314;&#31435;&#20102;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#37327;&#21270;&#32852;&#31995;&#65292;&#20294;&#22312;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#25193;&#25955;&#27169;&#22411;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#38754;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#31995;&#21015;&#26102;&#38388;&#27493;&#39588;&#32780;&#19981;&#26159;&#20043;&#21069;&#30340;&#30636;&#26102;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#25805;&#20316;&#65292;&#23545;&#30452;&#25509;&#23558;&#29616;&#26377;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diffusion-TracIn&#65292;&#23427;&#21253;&#21547;&#20102;&#36825;&#31181;&#26102;&#38388;&#21160;&#21147;&#23398;&#65292;&#24182;&#35266;&#23519;&#21040;&#26679;&#26412;&#30340;&#25439;&#22833;&#26799;&#24230;&#33539;&#25968;&#39640;&#24230;&#20381;&#36182;&#20110;&#26102;&#38388;&#27493;&#39588;&#12290;&#36825;&#31181;&#36235;&#21183;&#23548;&#33268;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#23545;&#20110;&#22312;&#24341;&#21457;&#22823;&#33539;&#25968;&#26102;&#38388;&#27493;&#39588;&#19978;&#35757;&#32451;&#30340;&#26679;&#26412;&#23588;&#20026;&#26126;&#26174;&#65292;&#23548;&#33268;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion-ReTr&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution methods trace model behavior back to its training dataset, offering an effective approach to better understand ``black-box'' neural networks. While prior research has established quantifiable links between model output and training data in diverse settings, interpreting diffusion model outputs in relation to training samples remains underexplored. In particular, diffusion models operate over a sequence of timesteps instead of instantaneous input-output relationships in previous contexts, posing a significant challenge to extend existing frameworks to diffusion models directly. Notably, we present Diffusion-TracIn that incorporates this temporal dynamics and observe that samples' loss gradient norms are highly dependent on timestep. This trend leads to a prominent bias in influence estimation, and is particularly noticeable for samples trained on large-norm-inducing timesteps, causing them to be generally influential. To mitigate this effect, we introduce Diffusion-ReTr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#21270;&#27531;&#24046;&#27169;&#22359;&#24182;&#27979;&#37327;&#20854;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#25581;&#31034;&#20102;ResNet&#26550;&#26500;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#21253;&#25324;&#20013;&#38388;&#34920;&#31034;&#30340;&#31561;&#38388;&#38548;&#20998;&#24067;&#12289;&#27531;&#24046;&#38597;&#21487;&#27604;&#30340;&#23545;&#40784;&#20197;&#21450;&#19982;&#31867;&#21035;&#25968;&#30456;&#20851;&#30340;&#27531;&#24046;&#38597;&#21487;&#27604;&#31209;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.09018</link><description>&lt;p&gt;
&#27531;&#24046;&#23545;&#40784;&#65306;&#25581;&#31034;&#27531;&#24046;&#32593;&#32476;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Alignment: Uncovering the Mechanisms of Residual Networks. (arXiv:2401.09018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32447;&#24615;&#21270;&#27531;&#24046;&#27169;&#22359;&#24182;&#27979;&#37327;&#20854;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#25581;&#31034;&#20102;ResNet&#26550;&#26500;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#21253;&#25324;&#20013;&#38388;&#34920;&#31034;&#30340;&#31561;&#38388;&#38548;&#20998;&#24067;&#12289;&#27531;&#24046;&#38597;&#21487;&#27604;&#30340;&#23545;&#40784;&#20197;&#21450;&#19982;&#31867;&#21035;&#25968;&#30456;&#20851;&#30340;&#27531;&#24046;&#38597;&#21487;&#27604;&#31209;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#36807;&#31616;&#21333;&#30340;&#36339;&#36291;&#36830;&#25509;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;ResNet&#26550;&#26500;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28982;&#32780;&#23548;&#33268;&#20854;&#25104;&#21151;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#38597;&#21487;&#27604;&#21644;&#27979;&#37327;&#20854;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#23545;ResNet&#26550;&#26500;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32447;&#24615;&#21270;&#20854;&#32452;&#25104;&#30340;&#27531;&#24046;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#27979;&#37327;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#21517;&#20026;&#27531;&#24046;&#23545;&#40784;&#30340;&#36807;&#31243;&#65292;&#20854;&#20855;&#26377;&#20197;&#19979;&#22235;&#20010;&#29305;&#24615;&#65306;&#65288;RA1&#65289;&#32473;&#23450;&#36755;&#20837;&#30340;&#20013;&#38388;&#34920;&#31034;&#22312;&#23884;&#20837;&#39640;&#32500;&#31354;&#38388;&#30340;&#32447;&#19978;&#31561;&#38388;&#38548;&#65292;&#22914;Gai&#21644;Zhang [2021]&#35266;&#23519;&#21040;&#30340;&#19968;&#26679;&#65307;&#65288;RA2&#65289;&#27531;&#24046;&#38597;&#21487;&#27604;&#30340;&#24038;&#19978;&#21644;&#21491;&#19978;&#22855;&#24322;&#21521;&#37327;&#30456;&#20114;&#23545;&#40784;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#28145;&#24230;&#19978;&#20063;&#30456;&#20114;&#23545;&#40784;&#65307;&#65288;RA3&#65289;&#23545;&#20110;&#20840;&#36830;&#25509;&#30340;ResNet&#26469;&#35828;&#65292;&#27531;&#24046;&#38597;&#21487;&#27604;&#33267;&#22810;&#26159;&#31209;C&#65292;&#20854;&#20013;C&#26159;&#31867;&#21035;&#25968;&#65307;&#65288;RA4&#65289;&#27531;&#24046;&#38597;&#21487;&#27604;&#30340;&#21069;N&#20010;&#22855;&#24322;&#20540;&#19982;N&#30456;&#20851;&#32852;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#25104;&#21453;&#27604;&#20363;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties:  (RA1) intermediate representations of a given input are equispaced on a line, embedded in high dimensional space, as observed by Gai and Zhang [2021];  (RA2) top left and right singular vectors of Residual Jacobians align with each other and across different depths;  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes; and  (RA4) top singular values of Residual Jacobians scale inversely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#19981;&#36275;&#65292;&#25552;&#20986;AI&#31995;&#32479;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09011</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24402;&#32435;&#33021;&#21147;&#19981;&#36275;&#65292;&#32570;&#20047;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Inductive Models for Artificial Intelligence Systems are Insufficient without Good Explanations. (arXiv:2401.09011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#19981;&#36275;&#65292;&#25552;&#20986;AI&#31995;&#32479;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#23427;&#20204;&#22312;&#36924;&#36817;&#22797;&#26434;&#20989;&#25968;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#24120;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#24402;&#32435;&#38382;&#39064;&#65306;&#21363;&#36807;&#21435;&#30340;&#35266;&#23519;&#19981;&#19968;&#23450;&#33021;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65292;&#36825;&#26159;ML&#27169;&#22411;&#22312;&#36973;&#36935;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20027;&#24352;&#37325;&#35201;&#30340;&#19981;&#20165;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#36824;&#35201;&#25552;&#20379;&#33391;&#22909;&#30340;&#35299;&#37322;&#65292;&#32780;&#24403;&#21069;&#27169;&#22411;&#24448;&#24448;&#26410;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#23427;&#24314;&#35758;&#20026;&#20102;AI&#30340;&#36827;&#27493;&#65292;&#25105;&#20204;&#24517;&#39035;&#23547;&#27714;&#33021;&#22815;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the limitations of machine learning (ML), particularly deep artificial neural networks (ANNs), which are effective at approximating complex functions but often lack transparency and explanatory power. It highlights the `problem of induction' : the philosophical issue that past observations may not necessarily predict future events, a challenge that ML models face when encountering new, unseen data. The paper argues for the importance of not just making predictions but also providing good explanations, a feature that current models often fail to deliver. It suggests that for AI to progress, we must seek models that offer insights and explanations, not just predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CTCS-HRRL&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19982;&#20195;&#29702;&#33021;&#21147;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.08999</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#65288;CTCS-HRRL&#65289;&#65306;&#26397;&#21521;&#29983;&#29289;&#33258;&#20027;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Continuous Time Continuous Space Homeostatic Reinforcement Learning (CTCS-HRRL) : Towards Biological Self-Autonomous Agent. (arXiv:2401.08999v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;&#30340;&#31283;&#24577;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;CTCS-HRRL&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19982;&#20195;&#29702;&#33021;&#21147;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#24577;&#26159;&#29983;&#29289;&#32500;&#25345;&#20869;&#37096;&#24179;&#34913;&#30340;&#29983;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#24577;&#26159;&#19968;&#31181;&#23398;&#20064;&#34892;&#20026;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31283;&#24577;&#35843;&#33410;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;HRRL&#65289;&#26694;&#26550;&#35797;&#22270;&#36890;&#36807;&#23558;&#39537;&#21160;&#20943;&#23569;&#29702;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#35299;&#37322;&#36825;&#31181;&#23398;&#20064;&#30340;&#31283;&#24577;&#34892;&#20026;&#12290;&#36825;&#31181;&#38142;&#25509;&#24050;&#32463;&#22312;&#31163;&#25955;&#26102;&#38388;&#31354;&#38388;&#19978;&#24471;&#21040;&#35777;&#26126;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#26102;&#38388;&#31354;&#38388;&#19978;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;HRRL&#26694;&#26550;&#25512;&#24191;&#21040;&#36830;&#32493;&#26102;&#38388;&#31354;&#38388;&#29615;&#22659;&#65292;&#24182;&#39564;&#35777;&#36830;&#32493;&#26102;&#38388;&#36830;&#32493;&#31354;&#38388;HRRL&#65288;CTCS-HRRL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#27169;&#22411;&#26469;&#27169;&#25311;&#30495;&#23454;&#29983;&#29289;&#20195;&#29702;&#20013;&#30340;&#31283;&#24577;&#26426;&#21046;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;Hamilton-Jacobian Bellman&#26041;&#31243;&#20197;&#21450;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20989;&#25968;&#36817;&#20284;&#12290;&#36890;&#36807;&#22522;&#20110;&#27169;&#25311;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20195;&#29702;&#33021;&#21147;&#26377;&#20851;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homeostasis is a biological process by which living beings maintain their internal balance. Previous research suggests that homeostasis is a learned behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning (HRRL) framework attempts to explain this learned homeostatic behavior by linking Drive Reduction Theory and Reinforcement Learning. This linkage has been proven in the discrete time-space, but not in the continuous time-space. In this work, we advance the HRRL framework to a continuous time-space environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL) framework. We achieve this by designing a model that mimics the homeostatic mechanisms in a real-world biological agent. This model uses the Hamilton-Jacobian Bellman Equation, and function approximation based on neural networks and Reinforcement Learning. Through a simulation-based experiment we demonstrate the efficacy of this model and uncover the evidence linked to the agent's ability to dy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#21644;&#37325;&#32622;&#29992;&#20110;&#36951;&#24536;&#30340;&#26041;&#27861;&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#24615;&#22122;&#22768;&#29983;&#25104;&#21442;&#25968;&#33945;&#29256;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#37325;&#32622;&#27169;&#22411;&#20013;&#30340;&#29305;&#23450;&#21442;&#25968;&#65292;&#20351;&#20854;&#26080;&#27861;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38754;&#37096;&#26426;&#22120;&#36951;&#24536;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#26159;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.08998</link><description>&lt;p&gt;
&#25915;&#20987;&#19982;&#37325;&#32622;&#29992;&#20110;&#36951;&#24536;&#65306;&#36890;&#36807;&#21442;&#25968;&#37325;&#26032;&#21021;&#22987;&#21270;&#21033;&#29992;&#23545;&#25239;&#24615;&#22122;&#22768;&#23454;&#29616;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization. (arXiv:2401.08998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#21644;&#37325;&#32622;&#29992;&#20110;&#36951;&#24536;&#30340;&#26041;&#27861;&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#24615;&#22122;&#22768;&#29983;&#25104;&#21442;&#25968;&#33945;&#29256;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#37325;&#32622;&#27169;&#22411;&#20013;&#30340;&#29305;&#23450;&#21442;&#25968;&#65292;&#20351;&#20854;&#26080;&#27861;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22312;&#38754;&#37096;&#26426;&#22120;&#36951;&#24536;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#26159;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#21644;&#30417;&#31649;&#21512;&#35268;&#26041;&#38754;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#36951;&#24536;&#30340;&#27010;&#24565;&#26085;&#30410;&#37325;&#35201;&#65292;&#26088;&#22312;&#36873;&#25321;&#24615;&#22320;&#24536;&#35760;&#25110;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#23450;&#23398;&#20064;&#20449;&#24687;&#12290;&#38024;&#23545;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#25915;&#20987;&#21644;&#37325;&#32622;&#29992;&#20110;&#36951;&#24536;&#65288;ARU&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#29983;&#25104;&#21442;&#25968;&#33945;&#29256;&#65292;&#26377;&#25928;&#22320;&#37325;&#32622;&#26576;&#20123;&#21442;&#25968;&#24182;&#20351;&#20854;&#26080;&#27861;&#23398;&#20064;&#12290;ARU&#22312;&#20004;&#20010;&#38754;&#37096;&#26426;&#22120;&#36951;&#24536;&#22522;&#20934;&#25968;&#25454;&#38598;MUFAC&#21644;MUCAC&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#29305;&#21035;&#20171;&#32461;&#20102;&#25915;&#20987;&#21644;&#33945;&#29256;&#30340;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#36807;&#28388;&#21644;&#37325;&#32622;&#23545;&#24536;&#35760;&#38598;&#26377;&#20559;&#35265;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#24615;&#22122;&#22768;&#26469;&#35774;&#35745;&#33945;&#29256;&#65292;&#22312;&#36890;&#36807;&#21442;&#25968;&#37325;&#26032;&#21021;&#22987;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20351;&#25968;&#25454;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21487;&#21033;&#29992;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing concerns surrounding privacy and regulatory compliance, the concept of machine unlearning has gained prominence, aiming to selectively forget or erase specific learned information from a trained model. In response to this critical need, we introduce a novel approach called Attack-and-Reset for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. ARU outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC. In particular, we present the steps involved in attacking and masking that strategically filter and re-initialize network parameters biased towards the forget set. Our work represents a significant advancement in rendering data unexploitable to deep learning models through parameter re-initialization, achieved by harnessing adversarial noise to craft a mask.
&lt;/p&gt;</description></item><item><title>MicroNAS&#26159;&#19968;&#20010;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;MCU&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#20284;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.08996</link><description>&lt;p&gt;
MicroNAS: &#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;MCUs
&lt;/p&gt;
&lt;p&gt;
MicroNAS: Zero-Shot Neural Architecture Search for MCUs. (arXiv:2401.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08996
&lt;/p&gt;
&lt;p&gt;
MicroNAS&#26159;&#19968;&#20010;&#38024;&#23545;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;MCU&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#20284;&#30340;&#31934;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#21487;&#20197;&#26377;&#25928;&#22320;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#31934;&#24230;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#36229;&#32423;&#32593;&#32476;&#19978;&#36827;&#34892;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#25110;&#24191;&#27867;&#30340;&#26550;&#26500;&#35780;&#20272;&#65292;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MicroNAS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803; (MCUs) &#35774;&#35745;&#30340;&#30828;&#20214;&#24863;&#30693;&#38646;&#26679;&#26412;NAS&#26694;&#26550;&#12290;MicroNAS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#30446;&#26631;&#30828;&#20214;&#20248;&#21270;&#24615;&#33021;&#65292;&#21033;&#29992;&#19987;&#38376;&#30340;&#24615;&#33021;&#25351;&#26631;&#26469;&#35782;&#21035;&#26368;&#20339;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#32780;&#19981;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;MicroNAS&#22312;&#25628;&#32034;&#25928;&#29575;&#26041;&#38754;&#25552;&#39640;&#20102;1104&#20493;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#22312;&#32500;&#25345;&#30456;&#20284;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;MCU&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;3.23&#20493;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) effectively discovers new Convolutional Neural Network (CNN) architectures, particularly for accuracy optimization. However, prior approaches often require resource-intensive training on super networks or extensive architecture evaluations, limiting practical applications. To address these challenges, we propose MicroNAS, a hardware-aware zero-shot NAS framework designed for microcontroller units (MCUs) in edge computing. MicroNAS considers target hardware optimality during the search, utilizing specialized performance indicators to identify optimal neural architectures without high computational costs. Compared to previous works, MicroNAS achieves up to 1104x improvement in search efficiency and discovers models with over 3.23x faster MCU inference while maintaining similar accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#37319;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;0.4%&#12290;&#35813;&#26041;&#27861;&#22312;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08992</link><description>&lt;p&gt;
&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#38024;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#39640;&#25928;&#36866;&#37197;&#22120;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#37319;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;0.4%&#12290;&#35813;&#26041;&#27861;&#22312;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;&#24773;&#26223;&#20013;&#65292;&#20154;&#20204;&#36890;&#24120;&#24076;&#26395;&#20351;&#29992;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#65292;&#22240;&#20026;&#36825;&#26679;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#21463;&#30410;&#65292;&#20363;&#22914;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#24322;&#36136;&#24615;&#21644;&#25968;&#25454;&#20016;&#23500;&#24230;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#20986;&#29616;&#24322;&#27493;&#23792;&#20540;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36793;&#32536;&#35821;&#31181;&#12290;&#26377;&#26102;&#65292;&#25968;&#25454;&#26412;&#36523;&#29978;&#33267;&#21487;&#33021;&#22240;&#20026;&#21152;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#32780;&#19981;&#21487;&#29992;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20542;&#21521;&#20110;&#26174;&#33879;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#26469;&#21333;&#29420;&#36866;&#24212;&#27599;&#31181;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;Language-Dependent Adapter (LDA)&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36816;&#29992;&#20102;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#23545;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#30340;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#23436;&#25972;&#27169;&#22411;&#30340;0.4%&#12290;&#23427;&#34987;&#25554;&#20837;&#21040;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ElliDock&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26925;&#25243;&#20307;&#26469;&#34920;&#31034;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#30028;&#38754;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;ElliDock&#22312;&#25152;&#26377;&#27604;&#36739;&#26041;&#27861;&#20013;&#25317;&#26377;&#26368;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30340;&#23545;&#25509;&#26041;&#27861;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08986</link><description>&lt;p&gt;
&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#65306;&#22522;&#20110;&#31561;&#21464;&#26925;&#25243;&#25509;&#21475;&#39044;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction. (arXiv:2401.08986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ElliDock&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#26925;&#25243;&#20307;&#26469;&#34920;&#31034;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#30028;&#38754;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;ElliDock&#22312;&#25152;&#26377;&#27604;&#36739;&#26041;&#27861;&#20013;&#25317;&#26377;&#26368;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30340;&#23545;&#25509;&#26041;&#27861;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24615;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#30340;&#30740;&#31350;&#22312;&#33647;&#29289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#24037;&#31243;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#23545;&#25509;&#20219;&#21153;&#65292;&#30456;&#27604;&#35745;&#31639;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#23545;&#25509;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ElliDock&#30340;&#26032;&#39062;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#26925;&#25243;&#20307;&#26469;&#34920;&#31034;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#23545;&#25509;&#30028;&#38754;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20998;&#21035;&#20026;&#20004;&#20010;&#36755;&#20837;&#34507;&#30333;&#36136;&#20272;&#35745;&#26925;&#25243;&#20307;&#25509;&#21475;&#65292;&#24182;&#36890;&#36807;&#20351;&#20004;&#20010;&#25509;&#21475;&#37325;&#21512;&#26469;&#33719;&#24471;&#23545;&#25509;&#30340;&#26059;&#36716;&#24179;&#31227;&#21464;&#25442;&#12290;&#36890;&#36807;&#20854;&#35774;&#35745;&#65292;ElliDock&#22312;&#34507;&#30333;&#36136;&#30340;&#20219;&#24847;&#26059;&#36716;/&#24179;&#31227;&#19979;&#37117;&#26159;&#31561;&#21464;&#30340;&#65292;&#36825;&#26159;&#30830;&#20445;&#23545;&#25509;&#36807;&#31243;&#30340;&#27867;&#21270;&#24615;&#30340;&#24517;&#35201;&#24615;&#36136;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;ElliDock&#22312;&#25152;&#26377;&#27604;&#36739;&#26041;&#27861;&#20013;&#25317;&#26377;&#26368;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30340;&#23545;&#25509;&#26041;&#27861;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called ElliDock, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, ElliDock is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that ElliDock achieves the fastest inference time among all compared methods and is strongly competitive with current 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#65288;P-GAN&#65289;&#65292;&#29992;&#20110;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GAN&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#21518;&#36890;&#36807;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.08984</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A GAN-based data poisoning framework against anomaly detection in vertical federated learning. (arXiv:2401.08984v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08984
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25968;&#25454;&#27745;&#26579;&#26694;&#26550;&#65288;P-GAN&#65289;&#65292;&#29992;&#20110;&#23545;&#25239;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;GAN&#29983;&#25104;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#26368;&#21518;&#36890;&#36807;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#24320;&#21457;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32437;&#21521;&#32852;&#21512;&#23398;&#20064; (VFL) &#20013;&#65292;&#21830;&#19994;&#23454;&#20307;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#30340;&#27745;&#26579;&#25915;&#20987;&#21487;&#33021;&#20250;&#38477;&#20302;&#36825;&#20010;&#21327;&#20316;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#29616;&#27745;&#26579;&#25915;&#20987;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#23545;&#26381;&#21153;&#22120;&#31471;&#39030;&#23618;&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20351;&#24471;&#24694;&#24847;&#21442;&#19982;&#32773;&#27809;&#26377;&#26126;&#30830;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31471;&#21040;&#31471;&#27745;&#26579;&#26694;&#26550; P-GAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#26368;&#21021;&#37319;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#35813;&#21442;&#19982;&#32773;&#37319;&#29992;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#20135;&#29983;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#20197;&#38477;&#20302;&#26367;&#20195;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#22120;&#34987;&#33719;&#24471;&#24182;&#38024;&#23545;VFL&#27745;&#26579;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120; (DAE) &#24320;&#21457;&#20102;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#20026;VFL&#22330;&#26223;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
In vertical federated learning (VFL), commercial entities collaboratively train a model while preserving data privacy. However, a malicious participant's poisoning attack may degrade the performance of this collaborative model. The main challenge in achieving the poisoning attack is the absence of access to the server-side top model, leaving the malicious participant without a clear target model. To address this challenge, we introduce an innovative end-to-end poisoning framework P-GAN. Specifically, the malicious participant initially employs semi-supervised learning to train a surrogate target model. Subsequently, this participant employs a GAN-based method to produce adversarial perturbations to degrade the surrogate target model's performance. Finally, the generator is obtained and tailored for VFL poisoning. Besides, we develop an anomaly detection algorithm based on a deep auto-encoder (DAE), offering a robust defense mechanism to VFL scenarios. Through extensive experiments, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08977</link><description>&lt;p&gt;
FedLoGe: &#38271;&#23614;&#25968;&#25454;&#19979;&#30340;&#26412;&#22320;&#21644;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoGe: Joint Local and Generic Federated Learning under Long-tailed Data. (arXiv:2401.08977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#25552;&#39640;&#21306;&#22495;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#20013;&#24573;&#35270;&#26412;&#22320;&#32423;&#21035;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#26159;&#19968;&#31181;&#22312;&#21435;&#20013;&#24515;&#21270;&#30340;&#26412;&#22320;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#25968;&#25454;&#21576;&#29616;&#20840;&#29699;&#26222;&#36941;&#23384;&#22312;&#30340;&#38271;&#23614;&#20998;&#24067;&#30340;&#33539;&#20363;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;Fed-LT&#30340;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#36890;&#29992;&#20840;&#23616;&#27169;&#22411;&#30340;&#25928;&#33021;&#65292;&#32780;&#24573;&#35270;&#20102;&#26412;&#22320;&#32423;&#21035;&#30340;&#24615;&#33021;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24120;&#35268;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#25216;&#26415;&#20027;&#35201;&#26159;&#22312;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#65292;&#20248;&#21270;&#20010;&#24615;&#21270;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLoGe&#30340;&#26041;&#27861;&#65292;&#22312;Fed-LT&#20013;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26694;&#26550;&#20013;&#38598;&#25104;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#65292;&#25552;&#39640;&#26412;&#22320;&#21644;&#36890;&#29992;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#20351;&#29992;&#20849;&#20139;&#39592;&#24178;&#20316;&#20026;&#22522;&#30784;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of Fed-LT, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning (pFL) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in Fed-LT (FedLoGe), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Our investigation reveals the feasibility of employing a shared backbone as a foundational framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26080;&#32447;&#30005;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32858;&#21512;&#19978;&#19979;&#25991;&#36716;&#25442;&#22359;&#12289;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36716;&#32622;&#21367;&#31215;&#22359;&#20110;&#29983;&#25104;&#22120;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26080;&#32447;&#30005;&#22270;&#30340;&#37325;&#24314;&#20934;&#30830;&#24615;&#21644;&#23616;&#37096;&#32441;&#29702;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08976</link><description>&lt;p&gt;
ACT-GAN&#65306;&#22522;&#20110;ACT&#22359;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#30340;&#26080;&#32447;&#30005;&#22270;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
ACT-GAN: Radio map construction based on generative adversarial networks with ACT blocks. (arXiv:2401.08976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26080;&#32447;&#30005;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32858;&#21512;&#19978;&#19979;&#25991;&#36716;&#25442;&#22359;&#12289;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36716;&#32622;&#21367;&#31215;&#22359;&#20110;&#29983;&#25104;&#22120;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26080;&#32447;&#30005;&#22270;&#30340;&#37325;&#24314;&#20934;&#30830;&#24615;&#21644;&#23616;&#37096;&#32441;&#29702;&#65292;&#24182;&#22312;&#22810;&#20010;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#22270;&#20316;&#20026;&#30005;&#30913;&#31354;&#38388;&#29305;&#24449;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#22312;&#35780;&#20272;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#21644;&#26080;&#32447;&#30005;&#30417;&#27979;&#35206;&#30422;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#26080;&#32447;&#30005;&#22270;&#26500;&#24314;&#23384;&#22312;&#30340;&#20302;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26032;&#22411;&#26080;&#32447;&#30005;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#20854;&#20013;&#24212;&#29992;&#20102;&#32858;&#21512;&#19978;&#19979;&#25991;&#36716;&#25442;&#65288;AOT&#65289;&#22359;&#12289;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CBAM&#65289;&#21644;&#36716;&#32622;&#21367;&#31215;&#65288;T-Conv&#65289;&#22359;&#20110;&#29983;&#25104;&#22120;&#65292;&#24182;&#21629;&#21517;&#20026;ACT-GAN&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26080;&#32447;&#30005;&#22270;&#30340;&#37325;&#24314;&#20934;&#30830;&#24615;&#21644;&#23616;&#37096;&#32441;&#29702;&#12290;&#22312;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#65292;ACT-GAN&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#31232;&#30095;&#31163;&#25955;&#35266;&#27979;&#30340;&#22330;&#26223;&#20013;&#65292;&#19982;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#38477;&#20302;&#20102;14.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
The radio map, serving as a visual representation of electromagnetic spatial characteristics, plays a pivotal role in assessment of wireless communication networks and radio monitoring coverage. Addressing the issue of low accuracy existing in the current radio map construction, this paper presents a novel radio map construction method based on generative adversarial network (GAN) in which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied to the generator, and we name it as ACT-GAN. It significantly improves the reconstruction accuracy and local texture of the radio maps. The performance of ACT-GAN across three different scenarios is demonstrated. Experiment results reveal that in the scenario without sparse discrete observations, the proposed method reduces the root mean square error (RMSE) by 14.6% in comparison to the state-of-the-art models. In the scenario with sparse discrete ob
&lt;/p&gt;</description></item><item><title>DOO-RE&#26159;&#19968;&#20010;&#26469;&#33258;&#20250;&#35758;&#23460;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#27969;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#27880;&#37322;&#36807;&#31243;&#33719;&#24471;&#20102;9&#31181;&#27963;&#21160;&#31867;&#22411;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.08962</link><description>&lt;p&gt;
DOO-RE: &#19968;&#20010;&#20250;&#35758;&#23460;&#29615;&#22659;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DOO-RE: A dataset of ambient sensors in a meeting room for activity recognition. (arXiv:2401.08962v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08962
&lt;/p&gt;
&lt;p&gt;
DOO-RE&#26159;&#19968;&#20010;&#26469;&#33258;&#20250;&#35758;&#23460;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#27969;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#27880;&#37322;&#36807;&#31243;&#33719;&#24471;&#20102;9&#31181;&#27963;&#21160;&#31867;&#22411;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#29992;&#25143;&#27963;&#21160;&#26159;&#25552;&#20379;&#21508;&#31181;&#26234;&#33021;&#26381;&#21153;&#32473;&#29992;&#25143;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#36825;&#26679;&#30340;&#26381;&#21153;&#38656;&#35201;&#20855;&#22791;&#36136;&#37327;&#39640;&#19988;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#12290;&#21608;&#22260;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#27969;&#38750;&#24120;&#36866;&#21512;&#36825;&#20010;&#38656;&#27714;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#29615;&#22659;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#21482;&#25903;&#25345;&#21463;&#38480;&#30340;&#31169;&#20154;&#31354;&#38388;&#65292;&#23545;&#20110;&#20844;&#20849;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#23613;&#31649;&#23545;&#27492;&#31867;&#30740;&#31350;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20174;&#35013;&#26377;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#20250;&#35758;&#23460;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;DOO-RE&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#29615;&#22659;&#20256;&#24863;&#22120;&#31867;&#22411;(&#22914;&#22768;&#38899;&#21644;&#25237;&#24433;&#20202;)&#30340;&#25968;&#25454;&#27969;&#12290;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#37117;&#34987;&#21010;&#20998;&#20026;&#27963;&#21160;&#21333;&#20803;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#27880;&#37322;&#36807;&#31243;&#65292;&#22810;&#20010;&#27880;&#37322;&#32773;&#25552;&#20379;&#20102;&#27963;&#21160;&#26631;&#31614;&#20197;&#25552;&#39640;&#27880;&#37322;&#36136;&#37327;&#12290;&#26368;&#32456;&#25105;&#20204;&#24471;&#21040;&#20102;9&#31181;&#31867;&#22411;&#30340;&#27963;&#21160;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DOO-RE&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#20844;&#20849;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of IoT technology, recognizing user activities with machine learning methods is a promising way to provide various smart services to users. High-quality data with privacy protection is essential for deploying such services in the real world. Data streams from surrounding ambient sensors are well suited to the requirement. Existing ambient sensor datasets only support constrained private spaces and those for public spaces have yet to be explored despite growing interest in research on them. To meet this need, we build a dataset collected from a meeting room equipped with ambient sensors. The dataset, DOO-RE, includes data streams from various ambient sensor types such as Sound and Projector. Each sensor data stream is segmented into activity units and multiple annotators provide activity labels through a cross-validation annotation process to improve annotation quality. We finally obtain 9 types of activities. To our best knowledge, DOO-RE is the first dataset to su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2401.08961</link><description>&lt;p&gt;
&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32423;&#32852;&#36172;&#21338;&#26426;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#22312;&#32423;&#32852;&#36172;&#21338;&#26426;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#26102;&#21051;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#20174;&#19968;&#32452;&#20855;&#26377;&#26410;&#30693;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#20013;&#25512;&#33616;&#19968;&#20010;&#26377;&#24207;&#30340;&#39033;&#30446;&#23376;&#38598;&#65288;&#31216;&#20026;&#39033;&#30446;&#21015;&#34920;&#65289;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#26816;&#26597;&#21015;&#34920;&#65292;&#24182;&#28857;&#20987;&#31532;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#39033;&#30446;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#65292;&#20043;&#21518;&#65292;&#20195;&#29702;&#25910;&#21040;&#19968;&#20010;&#22870;&#21169;&#12290;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#32423;&#32852;&#36172;&#21338;&#26426;&#25991;&#29486;&#24573;&#30053;&#20102;&#29992;&#25143;&#29366;&#24577;&#65288;&#20363;&#22914;&#21382;&#21490;&#34892;&#20026;&#65289;&#23545;&#25512;&#33616;&#30340;&#24433;&#21709;&#20197;&#21450;&#20250;&#35805;&#36827;&#34892;&#36807;&#31243;&#20013;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#22312;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.08959</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#25490;&#21517;&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Off-Policy Reinforcement Learning for Ranking Policies with Human Feedback. (arXiv:2401.08959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#65292;&#22312;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#23398;&#20064;&#25490;&#21517;&#65288;LTR&#65289;&#19968;&#30452;&#26159;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#26368;&#22823;&#21270;&#38271;&#26399;&#22238;&#25253;&#12290;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26469;&#23558;&#25512;&#33616;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#65292;&#20197;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;LTR&#26041;&#27861;&#30456;&#27604;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#21644;&#25490;&#21517;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#20215;&#20540;&#25490;&#21517;&#65288;VR&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32479;&#19968;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#26694;&#26550;&#19979;&#21516;&#26102;&#26368;&#22823;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#22238;&#25253;&#21644;&#20248;&#21270;&#25490;&#21517;&#25351;&#26631;&#65292;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;EM&#36807;&#31243;&#24341;&#23548;&#23398;&#20064;&#31574;&#30053;&#20139;&#21463;&#26410;&#26469;&#22238;&#25253;&#21644;&#25490;&#21517;&#25351;&#26631;&#34701;&#21512;&#30340;&#22909;&#22788;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#22312;&#32447;&#20132;&#20114;&#12290;&#22823;&#37327;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic learning to rank (LTR) has been the dominating approach for optimizing the ranking metric, but cannot maximize long-term rewards. Reinforcement learning models have been proposed to maximize user long-term rewards by formulating the recommendation as a sequential decision-making problem, but could only achieve inferior accuracy compared to LTR counterparts, primarily due to the lack of online interactions and the characteristics of ranking. In this paper, we propose a new off-policy value ranking (VR) algorithm that can simultaneously maximize user long-term rewards and optimize the ranking metric offline for improved sample efficiency in a unified Expectation-Maximization (EM) framework. We theoretically and empirically show that the EM process guides the leaned policy to enjoy the benefit of integration of the future reward and ranking metric, and learn without any online interactions. Extensive offline and online experiments demonstrate the effectiveness of our methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;LSTM-based&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.08947</link><description>&lt;p&gt;
AntiPhishStack&#65306;&#22522;&#20110;LSTM&#30340;&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized Phishing URLs Detection. (arXiv:2401.08947v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;LSTM-based&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#29992;&#20110;&#20248;&#21270;&#32593;&#32476;&#38035;&#40060;URL&#30340;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38761;&#21629;&#24615;&#30340;&#22312;&#32447;&#32593;&#32476;&#26381;&#21153;&#30340;&#19981;&#26029;&#20381;&#36182;&#24341;&#20837;&#20102;&#26356;&#39640;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#23613;&#31649;&#26377;&#24191;&#27867;&#30340;&#23433;&#20840;&#25514;&#26045;&#65292;&#32593;&#32476;&#38035;&#40060;&#20173;&#28982;&#24102;&#26469;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#25163;&#21160;&#29305;&#24449;&#30340;&#32593;&#32476;&#38035;&#40060;&#31995;&#32479;&#22312;&#24212;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#31574;&#30053;&#19978;&#24456;&#22256;&#38590;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#35299;&#20915;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#21644;&#24694;&#24847;URL&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AntiPhishStack&#30340;&#20004;&#38454;&#27573;&#22534;&#21472;&#27867;&#21270;&#27169;&#22411;&#65292;&#26088;&#22312;&#26816;&#27979;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#35813;&#27169;&#22411;&#23545;URL&#21644;&#23383;&#31526;&#32423;TF-IDF&#29305;&#24449;&#36827;&#34892;&#23545;&#31216;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23545;&#26032;&#22411;&#32593;&#32476;&#38035;&#40060;&#23041;&#32961;&#30340;&#24212;&#23545;&#33021;&#21147;&#12290;&#31532;&#19968;&#38454;&#27573;&#22312;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#19978;&#35757;&#32451;&#29305;&#24449;&#65292;&#37319;&#29992;K&#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#22343;&#20540;&#39044;&#27979;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#31532;&#20108;&#38454;&#27573;&#37319;&#29992;&#20004;&#23618;&#22534;&#21472;LSTM&#32593;&#32476;&#65292;&#37197;&#21512;&#20116;&#20010;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#36827;&#34892;&#21160;&#24577;&#32534;&#35793;&#65292;&#30830;&#20445;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#33719;&#24471;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#21644;&#23545;&#25239;&#21018;&#24615;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating reliance on revolutionary online web services has introduced heightened security risks, with persistent challenges posed by phishing despite extensive security measures. Traditional phishing systems, reliant on machine learning and manual features, struggle with evolving tactics. Recent advances in deep learning offer promising avenues for tackling novel phishing challenges and malicious URLs. This paper introduces a two-phase stack generalized model named AntiPhishStack, designed to detect phishing sites. The model leverages the learning of URLs and character-level TF-IDF features symmetrically, enhancing its ability to combat emerging phishing threats. In Phase I, features are trained on a base machine learning classifier, employing K-fold cross-validation for robust mean prediction. Phase II employs a two-layered stacked-based LSTM network with five adaptive optimizers for dynamic compilation, ensuring premier prediction on these features. Additionally, the symmetrica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEL&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24809;&#32602;&#37325;&#35201;&#21442;&#25968;&#30340;&#25913;&#21464;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#30142;&#30149;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08940</link><description>&lt;p&gt;
CEL&#65306;&#36890;&#36807;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#26469;&#36827;&#34892;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CEL: A Continual Learning Model for Disease Outbreak Prediction by Leveraging Domain Adaptation via Elastic Weight Consolidation. (arXiv:2401.08940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEL&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24809;&#32602;&#37325;&#35201;&#21442;&#25968;&#30340;&#25913;&#21464;&#26469;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#30142;&#30149;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#24536;&#35760;&#20043;&#21069;&#30693;&#35782;&#24182;&#33021;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#22312;&#30142;&#30149;&#29190;&#21457;&#39044;&#27979;&#31561;&#21160;&#24577;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;LSTM&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#32780;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24377;&#24615;&#26435;&#37325;&#25972;&#21512;&#65288;EWC&#65289;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CEL&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#32531;&#35299;&#39046;&#22495;&#22686;&#37327;&#35774;&#32622;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#12290;&#21033;&#29992;EWC&#26500;&#24314;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#20197;&#24320;&#21457;&#20986;&#19968;&#20010;&#24809;&#32602;&#23545;&#37325;&#35201;&#21442;&#25968;&#21363;&#37325;&#35201;&#20808;&#21069;&#30693;&#35782;&#30340;&#25913;&#21464;&#30340;&#27491;&#21017;&#21270;&#39033;&#12290;&#22312;&#35780;&#20272;&#21644;&#37325;&#26032;&#35780;&#20272;&#20013;&#65292;CEL&#30340;&#24615;&#33021;&#20351;&#29992;&#19981;&#21516;&#25351;&#26631;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#30142;&#30149;&#65288;&#27969;&#24863;&#65292;&#30168;&#30137;&#21644;&#40635;&#30137;&#65289;&#19978;&#24471;&#21040;&#20102;&#24456;&#39640;&#30340;R-squared&#20540;&#65292;&#32988;&#36807;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#19978;&#19979;&#25991;&#20013;&#34920;&#26126;CEL&#23545;&#22686;&#37327;&#25968;&#25454;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning, the ability of a model to learn over time without forgetting previous knowledge and, therefore, be adaptive to new data, is paramount in dynamic fields such as disease outbreak prediction. Deep neural networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This study introduces a novel CEL model for continual learning by leveraging domain adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate the catastrophic forgetting phenomenon in a domain incremental setting. The Fisher Information Matrix (FIM) is constructed with EWC to develop a regularization term that penalizes changes to important parameters, namely, the important previous knowledge. CEL's performance is evaluated on three distinct diseases, Influenza, Mpox, and Measles, with different metrics. The high R-squared values during evaluation and reevaluation outperform the other state-of-the-art models in several contexts, indicating that CEL adapts to incremental data w
&lt;/p&gt;</description></item><item><title>DeLF&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;DeLF&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.08936</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340; DeLF
&lt;/p&gt;
&lt;p&gt;
DeLF: Designing Learning Environments with Foundation Models. (arXiv:2401.08936v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08936
&lt;/p&gt;
&lt;p&gt;
DeLF&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;DeLF&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#22522;&#26412;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#21147;&#24378;&#22823;&#19988;&#30452;&#35266;&#30340;&#32467;&#26500;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31361;&#30772;&#65292;&#20294;&#22312;&#35768;&#22810;&#31616;&#21333;&#24212;&#29992;&#20013;&#23454;&#38469;&#24212;&#29992;RL&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#29992;&#20110;&#20026;&#32473;&#23450;&#30340;&#12289;&#29992;&#25143;&#39044;&#26399;&#30340;&#24212;&#29992;&#35774;&#35745;RL&#29615;&#22659;&#32452;&#20214;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RL&#32452;&#20214;&#35774;&#35745;&#38382;&#39064;&#30340;&#21021;&#22987;&#24418;&#24335;&#21270;&#65292;&#37325;&#28857;&#26159;&#35774;&#35745;&#35266;&#23519;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeLF&#65306;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#35774;&#35745;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35774;&#35745;&#21644;&#32534;&#30721;&#29992;&#25143;&#39044;&#26399;&#30340;&#23398;&#20064;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29615;&#22659;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeLF&#21487;&#20197;&#20026;&#30456;&#24212;&#30340;RL&#38382;&#39064;&#33719;&#24471;&#21487;&#25191;&#34892;&#30340;&#29615;&#22659;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) offers a capable and intuitive structure for the fundamental sequential decision-making problem. Despite impressive breakthroughs, it can still be difficult to employ RL in practice in many simple applications. In this paper, we try to address this issue by introducing a method for designing the components of the RL environment for a given, user-intended application. We provide an initial formalization for the problem of RL component design, that concentrates on designing a good representation for observation and action space. We propose a method named DeLF: Designing Learning Environments with Foundation Models, that employs large language models to design and codify the user's intended learning scenario. By testing our method on four different learning environments, we demonstrate that DeLF can obtain executable environment codes for the corresponding RL problems.
&lt;/p&gt;</description></item><item><title>&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.08919</link><description>&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#23545;&#27604;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08919
&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#26631;&#21270;&#22312;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#26412;&#21487;&#35835;&#24615;&#21644;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#26631;&#35760;&#27599;&#20010;&#31526;&#21512;&#26465;&#20214;&#30340;&#23383;&#31526;&#65288;&#20840;&#38899;&#26631;&#21270;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;PD&#65289;&#26159;&#36873;&#25321;&#26631;&#35760;&#23376;&#38598;&#20197;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24110;&#21161;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#22810;&#30340;&#38899;&#26631;&#31526;&#21495;&#20250;&#22952;&#30861;&#29087;&#32451;&#35835;&#32773;&#65292;&#38477;&#20302;&#38405;&#35835;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#37096;&#20998;&#26631;&#35760;&#30340;&#25991;&#26412;&#36890;&#24120;&#27604;&#23436;&#20840;&#26631;&#35760;&#30340;&#25991;&#26412;&#26356;&#23481;&#26131;&#38405;&#35835;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#32431;&#25991;&#26412;&#26356;&#23481;&#26131;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;-&#19968;&#31181;&#19982;&#29616;&#26377;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;CCPD&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#20004;&#27425;&#22788;&#29702;&#65292;&#19968;&#27425;&#26377;&#19978;&#19979;&#25991;&#65292;&#19968;&#27425;&#27809;&#26377;&#65292;&#24182;&#19988;&#21482;&#23545;&#20004;&#27425;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23383;&#31526;&#36827;&#34892;&#38899;&#26631;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.08909</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20934;&#30830;&#24615;&#20272;&#35745;&#19979;&#20998;&#24067;&#20559;&#31227;&#30340;&#26799;&#24230;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift. (arXiv:2401.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21270;&#30340;&#27979;&#35797;&#29615;&#22659;&#19979;&#65292;&#26080;&#27861;&#35775;&#38382;&#30495;&#23454;&#27979;&#35797;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23433;&#20840;&#37096;&#32626;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#26497;&#20854;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#25110;&#25552;&#21462;&#29305;&#24449;&#30340;&#20449;&#24687;&#26469;&#24314;&#31435;&#19982;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#20272;&#35745;&#20998;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#25506;&#35752;&#20102;&#26799;&#24230;&#20449;&#24687;&#22914;&#20309;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#32463;&#36807;&#19968;&#27425;&#26799;&#24230;&#27493;&#38271;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21518;&#21453;&#21521;&#20256;&#25773;&#30340;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#22312;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#24403;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#65292;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach en
&lt;/p&gt;</description></item><item><title>LLaMaS&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26032;&#35774;&#22791;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#22312;&#36816;&#34892;&#26102;&#20570;&#20986;&#25805;&#20316;&#31995;&#32479;&#20915;&#31574;&#65292;&#20026;&#26032;&#35774;&#22791;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#25805;&#20316;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.08908</link><description>&lt;p&gt;
&#39535;&#26381;LLaMaS&#65306;&#23558;LLMs&#29992;&#20316;&#25805;&#20316;&#31995;&#32479;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Herding LLaMaS: Using LLMs as an OS Module. (arXiv:2401.08908v1 [cs.OS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08908
&lt;/p&gt;
&lt;p&gt;
LLaMaS&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26032;&#35774;&#22791;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#22312;&#36816;&#34892;&#26102;&#20570;&#20986;&#25805;&#20316;&#31995;&#32479;&#20915;&#31574;&#65292;&#20026;&#26032;&#35774;&#22791;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#25805;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#30340;&#23384;&#20648;&#25216;&#26415;&#21644;&#35745;&#31639;&#35774;&#22791;&#30340;&#20986;&#29616;&#65292;&#35745;&#31639;&#26426;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24322;&#26500;&#21270;&#12290; GPU&#21644;CPU&#24182;&#29992;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#32780;CXL&#26377;&#26395;&#25104;&#20026;&#20113;&#31995;&#32479;&#30340;&#20027;&#35201;&#25903;&#26609;&#12290;&#25805;&#20316;&#31995;&#32479;&#36127;&#36131;&#31649;&#29702;&#36825;&#20123;&#30828;&#20214;&#36164;&#28304;&#65292;&#27599;&#27425;&#21457;&#24067;&#26032;&#35774;&#22791;&#26102;&#37117;&#38656;&#35201;&#36827;&#34892;&#20462;&#25913;&#12290;&#22810;&#24180;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25237;&#20837;&#21040;&#35843;&#25972;&#25805;&#20316;&#31995;&#32479;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#19982;&#27599;&#20010;&#26032;&#30340;&#24322;&#26500;&#35774;&#22791;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#20869;&#23384;&#25216;&#26415;&#21644;&#39046;&#22495;&#29305;&#23450;&#21152;&#36895;&#22120;&#30340;&#29190;&#21457;&#65292;&#26377;&#19968;&#20010;&#21487;&#20197;&#36731;&#26494;&#20026;&#26032;&#35774;&#22791;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#25805;&#20316;&#31995;&#32479;&#23558;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLaMaS&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#35774;&#22791;&#12290;LLaMaS&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#26032;&#35774;&#22791;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#22312;&#36816;&#34892;&#26102;&#20570;&#20986;&#25805;&#20316;&#31995;&#32479;&#20915;&#31574;&#12290;&#20026;LLaMaS&#25552;&#20379;&#23545;&#26032;&#35774;&#22791;&#30340;&#25903;&#25345;&#23601;&#20687;&#25551;&#36848;&#31995;&#32479;&#19968;&#26679;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer systems are becoming increasingly heterogeneous with the emergence of new memory technologies and compute devices. GPUs alongside CPUs have become commonplace and CXL is poised to be a mainstay of cloud systems. The operating system is responsible for managing these hardware resources, requiring modification every time a new device is released. Years of research and development are sunk into tuning the OS for high performance with each new heterogeneous device. With the recent explosion in memory technologies and domain-specific accelerators, it would be beneficial to have an OS that could provide high performance for new devices without significant effort.  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large Language Models (LLMs) to extract the useful features of new devices from their textual description and uses these features to make operating system decisions at runtime. Adding support to LLaMaS for a new device is as simple as describing the syste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08903</link><description>&lt;p&gt;
PPR: &#22312;&#32500;&#25345;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#21516;&#26102;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#21644;&#36530;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25104;&#21151;&#36827;&#34892;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#24182;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22312;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#25104;&#21151;&#36827;&#34892;&#36530;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#35757;&#32451;&#20462;&#21098;&#24674;&#22797;&#25915;&#20987;&#65288;PPR&#65289;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#21487;&#20197;&#23558;&#19968;&#37096;&#20998;&#23545;&#25239;&#25200;&#21160;&#35774;&#20026;&#38646;&#65292;&#24182;&#20542;&#21521;&#20110;&#20445;&#25345;&#25915;&#20987;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#26377;&#36873;&#25321;&#24615;&#22320;&#37322;&#25918;&#26576;&#20123;&#23545;&#25239;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25200;&#21160;&#23884;&#20837;&#21040;&#20462;&#21098;&#21306;&#22495;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#38899;&#39057;&#23884;&#20837;&#20013;&#25805;&#20316;&#33410;&#22863;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#32500;&#25345;&#20854;&#20182;&#23646;&#24615;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#20986;&#22312;&#33410;&#22863;&#19978;&#30456;&#20284;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#19981;&#21516;&#30340;&#38899;&#20048;&#26354;&#30446;&#12290;</title><link>http://arxiv.org/abs/2401.08902</link><description>&lt;p&gt;
&#31867;&#20284;&#20294;&#26356;&#24555;&#65306;&#38899;&#20048;&#38899;&#39057;&#23884;&#20837;&#20013;&#30340;&#33410;&#22863;&#25805;&#20316;&#29992;&#20110;&#33410;&#22863;&#39044;&#27979;&#21644;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for Tempo Prediction and Search. (arXiv:2401.08902v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#38899;&#39057;&#23884;&#20837;&#20013;&#25805;&#20316;&#33410;&#22863;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#32500;&#25345;&#20854;&#20182;&#23646;&#24615;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#32034;&#20986;&#22312;&#33410;&#22863;&#19978;&#30456;&#20284;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#19981;&#21516;&#30340;&#38899;&#20048;&#26354;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23884;&#20837;&#20351;&#24471;&#38899;&#39057;&#25991;&#20214;&#30340;&#30456;&#20284;&#24615;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#29992;&#20110;&#25628;&#32034;&#21644;&#25512;&#33616;&#31561;&#24212;&#29992;&#12290;&#30001;&#20110;&#38899;&#39057;&#30456;&#20284;&#24615;&#30340;&#20027;&#35266;&#24615;&#65292;&#35774;&#35745;&#33021;&#22815;&#22238;&#31572;&#38899;&#39057;&#26159;&#21542;&#30456;&#20284;&#20197;&#21450;&#30456;&#20284;&#30340;&#26041;&#24335;&#65288;&#20363;&#22914;&#65292;&#19982;&#33410;&#22863;&#12289;&#24773;&#32490;&#25110;&#27969;&#27966;&#30456;&#20851;&#65289;&#30340;&#31995;&#32479;&#21487;&#33021;&#26356;&#21152;&#29702;&#24819;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#33021;&#22815;&#23545;&#29305;&#23450;&#23646;&#24615;&#36827;&#34892;&#21152;&#26435;&#20197;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#24378;&#35843;&#36825;&#20123;&#23646;&#24615;&#30340;&#20998;&#31163;&#23884;&#20837;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#23376;&#31354;&#38388;&#30340;&#29420;&#31435;&#24615;&#20197;&#21450;&#23545;&#20854;&#36827;&#34892;&#25805;&#20316;&#20197;&#26816;&#32034;&#22312;&#29305;&#23450;&#26041;&#24335;&#19979;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#26354;&#30446;&#30340;&#30740;&#31350;&#23578;&#26410;&#36827;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25805;&#20316;&#33410;&#22863;&#20316;&#20026;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33410;&#22863;&#36716;&#25442;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#23884;&#20837;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25805;&#20316;&#33410;&#22863;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#27966;&#31561;&#20854;&#20182;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio embeddings enable large scale comparisons of the similarity of audio files for applications such as search and recommendation. Due to the subjectivity of audio similarity, it can be desirable to design systems that answer not only whether audio is similar, but similar in what way (e.g., wrt. tempo, mood or genre). Previous works have proposed disentangled embedding spaces where subspaces representing specific, yet possibly correlated, attributes can be weighted to emphasize those attributes in downstream tasks. However, no research has been conducted into the independence of these subspaces, nor their manipulation, in order to retrieve tracks that are similar but different in a specific way. Here, we explore the manipulation of tempo in embedding spaces as a case-study towards this goal. We propose tempo translation functions that allow for efficient manipulation of tempo within a pre-existing embedding space whilst maintaining other properties such as genre. As this translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08895</link><description>&lt;p&gt;
cedar&#65306;&#21487;&#32452;&#21512;&#21644;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08895
&lt;/p&gt;
&lt;p&gt;
cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#26159;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#36127;&#36131;&#35835;&#21462;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#21464;&#25442;&#22788;&#29702;&#26679;&#26412;&#25209;&#27425;&#65292;&#24182;&#20197;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#23558;&#20854;&#21152;&#36733;&#21040;&#35757;&#32451;&#33410;&#28857;&#19978;&#12290;&#39640;&#24615;&#33021;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#30340;&#24615;&#33021;&#20248;&#21270;&#65292;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#26497;&#20302;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#22320;&#65292;&#28010;&#36153;&#26114;&#36149;&#30340;&#21152;&#36895;&#22120;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cedar&#65292;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;cedar&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#26469;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#30340;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
&lt;/p&gt;</description></item><item><title>MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08893</link><description>&lt;p&gt;
MADA: &#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#30340;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
MADA: Meta-Adaptive Optimizers through hyper-gradient Descent. (arXiv:2401.08893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08893
&lt;/p&gt;
&lt;p&gt;
MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Adam&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#36890;&#24120;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;Adam&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;(MADA)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#24050;&#30693;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;MADA&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#20248;&#21270;&#22120;&#30340;&#31354;&#38388;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#20351;&#29992;&#36229;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#25628;&#32034;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;MADA&#23545;&#20110;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;Adam&#12289;Lion&#21644;Adan&#65292;&#29978;&#33267;&#22312;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;AVGrad&#65292;&#23427;&#26159;AMSGrad&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#22312;&#20854;&#20013;&#23558;&#26368;&#22823;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#24179;&#22343;&#25805;&#20316;&#31526;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#22312;MADA&#20013;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#20197;&#34920;&#26126;&#20248;&#21270;&#22120;&#30340;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;AVGrad&#21644;Adam&#65289;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#38899;&#39057;&#20013;&#30340;&#20840;&#23616;&#33410;&#22863;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#38899;&#39057;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.08891</link><description>&lt;p&gt;
&#21160;&#24577;&#20272;&#35745;&#20316;&#20026;&#23436;&#20840;&#33258;&#30417;&#30563;&#30340;&#20108;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Tempo estimation as fully self-supervised binary classification. (arXiv:2401.08891v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#38899;&#39057;&#20013;&#30340;&#20840;&#23616;&#33410;&#22863;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#38899;&#39057;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#38899;&#20048;&#38899;&#39057;&#20013;&#20840;&#23616;&#33410;&#22863;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#26631;&#27880;&#33410;&#22863;&#26159;&#32791;&#26102;&#19988;&#38656;&#35201;&#19968;&#23450;&#30340;&#38899;&#20048;&#19987;&#19994;&#30693;&#35782;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#28304;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#29992;&#65288;&#38899;&#20048;&#65289;&#38899;&#39057;&#23884;&#20837;&#24050;&#32463;&#21253;&#21547;&#20102;&#21508;&#31181;&#23646;&#24615;&#30340;&#20107;&#23454;&#65292;&#21253;&#25324;&#20851;&#20110;&#33410;&#22863;&#30340;&#20449;&#24687;&#65292;&#20351;&#20854;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#33410;&#22863;&#20272;&#35745;&#30340;&#30740;&#31350;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#29305;&#23450;&#33410;&#22863;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#20998;&#31867;&#22120;&#65292;&#20294;&#25105;&#20204;&#23558;&#35813;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#30446;&#26631;&#26354;&#30446;&#19982;&#21442;&#32771;&#26354;&#30446;&#30340;&#33410;&#22863;&#26159;&#21542;&#30456;&#21516;&#12290;&#34429;&#28982;&#21069;&#32773;&#20173;&#28982;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#26368;&#32456;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#38899;&#39057;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of global tempo estimation in musical audio. Given that annotating tempo is time-consuming and requires certain musical expertise, few publicly available data sources exist to train machine learning models for this task. Towards alleviating this issue, we propose a fully self-supervised approach that does not rely on any human labeled data. Our method builds on the fact that generic (music) audio embeddings already encode a variety of properties, including information about tempo, making them easily adaptable for downstream tasks. While recent work in self-supervised tempo estimation aimed to learn a tempo specific representation that was subsequently used to train a supervised classifier, we reformulate the task into the binary classification problem of predicting whether a target track has the same or a different tempo compared to a reference. While the former still requires labeled training data for the final classification model, our approach uses a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#21457;&#29616;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#20943;&#23569;&#19968;&#39318;&#26354;&#30446;&#20013;&#22343;&#21248;&#30340;&#38899;&#20048;&#29305;&#24615;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26412;&#22320;&#21270;&#65292;&#24182;&#25552;&#39640;&#20854;&#20182;&#23646;&#24615;&#30340;&#26412;&#22320;&#21270;&#12290;&#36825;&#23545;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#24212;&#29992;&#26159;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.08889</link><description>&lt;p&gt;
&#23545;&#25968;&#25454;&#22686;&#24378;&#22312;&#23545;&#27604;&#23398;&#20064;&#38899;&#20048;&#38899;&#39057;&#34920;&#31034;&#20013;&#30340;&#23616;&#37096;&#23884;&#20837;&#29305;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Data-Augmentation on Local Embedding Properties in the Contrastive Learning of Music Audio Representations. (arXiv:2401.08889v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#21457;&#29616;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#20943;&#23569;&#19968;&#39318;&#26354;&#30446;&#20013;&#22343;&#21248;&#30340;&#38899;&#20048;&#29305;&#24615;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26412;&#22320;&#21270;&#65292;&#24182;&#25552;&#39640;&#20854;&#20182;&#23646;&#24615;&#30340;&#26412;&#22320;&#21270;&#12290;&#36825;&#23545;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#26368;&#36817;&#37051;&#31639;&#27861;&#24212;&#29992;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#23884;&#20837;&#26159;&#29702;&#35299;&#22823;&#37327;&#38899;&#20048;&#30446;&#24405;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#36890;&#24120;&#65292;&#23884;&#20837;&#26159;&#26681;&#25454;&#23427;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#30340;&#65292;&#28982;&#32780;&#23569;&#26377;&#30740;&#31350;&#35843;&#26597;&#23884;&#20837;&#31354;&#38388;&#26412;&#36523;&#30340;&#23616;&#37096;&#29305;&#24615;&#65292;&#32780;&#36825;&#20123;&#23616;&#37096;&#29305;&#24615;&#23545;&#20110;&#26368;&#36817;&#37051;&#31639;&#27861;&#22312;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#30340;&#38899;&#39057;&#34920;&#31034;&#26102;&#65292;&#36890;&#24120;&#22312;&#19968;&#39318;&#26354;&#30446;&#20013;&#26159;&#22343;&#21248;&#30340;&#38899;&#20048;&#29305;&#24615;&#65288;&#22914;&#38899;&#35843;&#21644;&#36895;&#24230;&#65289;&#22312;&#25152;&#24471;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#37051;&#22495;&#30340;&#23616;&#37096;&#24615;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;&#36890;&#36807;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36825;&#20123;&#29305;&#24615;&#30340;&#26412;&#22320;&#21270;&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#65292;&#32780;&#19988;&#20854;&#20182;&#23646;&#24615;&#30340;&#26412;&#22320;&#21270;&#20063;&#20250;&#22686;&#21152;&#12290;&#20363;&#22914;&#65292;&#19982;&#38750;&#19987;&#19994;&#21548;&#20247;&#20851;&#31995;&#36739;&#23567;&#30340;&#38899;&#39640;&#21644;&#36895;&#24230;&#31561;&#29305;&#24449;&#30340;&#23616;&#37096;&#24615;&#21487;&#33021;&#20250;&#24471;&#21040;&#32531;&#35299;&#65292;&#21516;&#26102;&#25913;&#21892;&#26356;&#26174;&#33879;&#29305;&#24449;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio embeddings are crucial tools in understanding large catalogs of music. Typically embeddings are evaluated on the basis of the performance they provide in a wide range of downstream tasks, however few studies have investigated the local properties of the embedding spaces themselves which are important in nearest neighbor algorithms, commonly used in music search and recommendation. In this work we show that when learning audio representations on music datasets via contrastive learning, musical properties that are typically homogeneous within a track (e.g., key and tempo) are reflected in the locality of neighborhoods in the resulting embedding space. By applying appropriate data augmentation strategies, localisation of such properties can not only be reduced but the localisation of other attributes is increased. For example, locality of features such as pitch and tempo that are less relevant to non-expert listeners, may be mitigated while improving the locality of more salient fea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#39640;&#36895;&#27969;&#21160;&#20013;&#30340;Riemann&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;DeepONet&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#22312;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.08886</link><description>&lt;p&gt;
RiemannONets: &#21487;&#35299;&#37322;&#30340;&#29992;&#20110;Riemann&#38382;&#39064;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
RiemannONets: Interpretable Neural Operators for Riemann Problems. (arXiv:2401.08886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#39640;&#36895;&#27969;&#21160;&#20013;&#30340;Riemann&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;DeepONet&#36827;&#34892;&#31616;&#21333;&#20462;&#25913;&#65292;&#22312;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#20540;&#20998;&#26512;&#20013;&#65292;&#22914;&#20309;&#20026;&#27169;&#25311;&#20855;&#26377;&#24378;&#28608;&#27874;&#12289;&#31232;&#30095;&#21270;&#21644;&#25509;&#35302;&#38388;&#26029;&#30340;&#39640;&#36895;&#27969;&#21160;&#24320;&#21457;&#21512;&#36866;&#30340;&#34920;&#31034;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#22312;&#21487;&#21387;&#32553;&#27969;&#20013;&#36935;&#21040;&#30340;Riemann&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#26497;&#31471;&#21387;&#21147;&#36339;&#36291;&#65288;&#39640;&#36798;$10^{10}$&#30340;&#21387;&#21147;&#27604;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;DeepONet&#65292;&#23427;&#26159;&#22312;Lee&#21644;Shin&#30340;&#26368;&#26032;&#24037;&#20316;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20174;&#20027;&#24178;&#32593;&#32476;&#20013;&#25552;&#21462;&#20102;&#19968;&#20010;&#22522;&#30784;&#24182;&#20351;&#20854;&#27491;&#20132;&#21270;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#23427;&#26469;&#35757;&#32451;&#20998;&#25903;&#32593;&#32476;&#12290;&#36825;&#20010;&#23545;DeepONet&#30340;&#31616;&#21333;&#20462;&#25913;&#23545;&#20854;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#26377;&#30528;&#28145;&#36828;&#24433;&#21709;&#65292;&#30456;&#27604;&#21407;&#22987;&#29256;&#26412;&#65292;&#23427;&#25552;&#20379;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;Riemann&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29289;&#29702;&#19978;&#23545;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#22240;&#20026;&#23618;&#27425;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#30340;&#22522;&#30784;&#21453;&#26144;&#20102;&#25152;&#26377;&#27969;&#21160;&#29305;&#24449;&#65292;&#21542;&#21017;&#23558;&#34987;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the proper representations for simulating high-speed flows with strong shock waves, rarefactions, and contact discontinuities has been a long-standing question in numerical analysis. Herein, we employ neural operators to solve Riemann problems encountered in compressible flows for extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we first consider the DeepONet that we train in a two-stage process, following the recent work of Lee and Shin, wherein the first stage, a basis is extracted from the trunk net, which is orthonormalized and subsequently is used in the second stage in training the branch net. This simple modification of DeepONet has a profound effect on its accuracy, efficiency, and robustness and leads to very accurate solutions to Riemann problems compared to the vanilla version. It also enables us to interpret the results physically as the hierarchical data-driven produced basis reflects all the flow features that would otherwise be introduce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08876</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#23427;&#20204;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#34920;&#31034;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#39044;&#27880;&#20876;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#21644;&#26174;&#31034;Top-1&#21644;Top-k&#39044;&#27979;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#30340;&#20934;&#30830;&#24615;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30456;&#24403;&#25110;&#31245;&#20302;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22270;&#20687;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#65292;&#39044;&#27979;&#38598;&#22312;&#36741;&#21161;&#20154;&#31867;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23454;&#36341;&#20013;&#24378;&#35843;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
&lt;/p&gt;</description></item><item><title>DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.08875</link><description>&lt;p&gt;
DCRMTA: &#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#30340;&#22240;&#26524;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DCRMTA: Unbiased Causal Representation for Multi-touch Attribution. (arXiv:2401.08875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08875
&lt;/p&gt;
&lt;p&gt;
DCRMTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#21644;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#26469;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35302;&#28857;&#24402;&#22240;&#65288;MTA&#65289;&#22312;&#23454;&#29616;&#23545;&#27599;&#20010;&#24191;&#21578;&#35302;&#28857;&#23545;&#20110;&#36716;&#21270;&#34892;&#20026;&#30340;&#36129;&#29486;&#30340;&#20844;&#27491;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28145;&#21051;&#24433;&#21709;&#39044;&#31639;&#20998;&#37197;&#21644;&#24191;&#21578;&#25512;&#33616;&#12290;&#20256;&#32479;&#30340;&#22810;&#35302;&#28857;&#24402;&#22240;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#23398;&#20064;&#35302;&#28857;&#24207;&#21015;&#21644;&#29992;&#25143;&#36141;&#20080;&#34892;&#20026;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20174;&#21407;&#22987;&#24207;&#21015;&#23376;&#38598;&#20013;&#26500;&#24314;&#23545;&#29031;&#35302;&#28857;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#36716;&#21270;&#65292;&#20174;&#32780;&#35745;&#31639;&#24191;&#21578;&#36129;&#29486;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#36716;&#21270;&#39044;&#27979;&#27169;&#22411;&#30340;&#26080;&#20559;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#20559;&#22909;&#21644;&#20114;&#32852;&#32593;&#25512;&#33616;&#26426;&#21046;&#65288;&#22914;&#36807;&#21435;&#30340;&#36141;&#29289;&#35760;&#24405;&#23548;&#33268;&#30340;&#24191;&#21578;&#25512;&#33616;&#21516;&#36136;&#21270;&#65289;&#24341;&#36215;&#30340;&#28151;&#26434;&#21464;&#37327;&#22240;&#32032;&#65292;&#36716;&#21270;&#20013;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-touch attribution (MTA) currently plays a pivotal role in achieving a fair estimation of the contributions of each advertising touchpoint to-wards conversion behavior, deeply influencing budget allocation and advertising recommenda-tion. Traditional multi-touch attribution methods initially build a conversion prediction model, an-ticipating learning the inherent relationship be-tween touchpoint sequences and user purchasing behavior through historical data. Based on this, counterfactual touchpoint sequences are con-structed from the original sequence subset, and conversions are estimated using the prediction model, thus calculating advertising contributions. A covert assumption of these methods is the un-biased nature of conversion prediction models. However, due to confounding variables factors arising from user preferences and internet recom-mendation mechanisms such as homogenization of ad recommendations resulting from past shop-ping records, bias can easily occur in conversi
&lt;/p&gt;</description></item><item><title>MambaTab&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#29305;&#24615;&#65292;&#22312;&#23569;&#37327;&#21442;&#25968;&#21644;&#26368;&#23567;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08867</link><description>&lt;p&gt;
MambaTab&#65306;&#19968;&#31181;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MambaTab: A Simple Yet Effective Approach for Handling Tabular Data. (arXiv:2401.08867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08867
&lt;/p&gt;
&lt;p&gt;
MambaTab&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#29305;&#24615;&#65292;&#22312;&#23569;&#37327;&#21442;&#25968;&#21644;&#26368;&#23567;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;transformers&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35843;&#20248;&#21644;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;MambaTab&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12290;SSM&#23545;&#20110;&#20174;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#25552;&#21462;&#26377;&#25928;&#34920;&#31034;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;MambaTab&#21033;&#29992;&#20102;Mamba&#65292;&#19968;&#31181;&#26032;&#20852;&#30340;SSM&#21464;&#20307;&#65292;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;MambaTab&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#36739;&#23569;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;MambaTab&#30340;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#21363;&#24320;&#21363;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data remains ubiquitous across domains despite growing use of images and texts for machine learning. While deep learning models like convolutional neural networks and transformers achieve strong performance on tabular data, they require extensive data preprocessing, tuning, and resources, limiting accessibility and scalability. This work develops an innovative approach based on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have strong capabilities for efficiently extracting effective representations from data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM variant, for end-to-end supervised learning on tables. Compared to state-of-the-art baselines, MambaTab delivers superior performance while requiring significantly fewer parameters and minimal preprocessing, as empirically validated on diverse benchmark datasets. MambaTab's efficiency, scalability, generalizability, and predictive gains signify it as a lightweight, "out-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.08865</link><description>&lt;p&gt;
Intrinsic Dataset Properties&#23545;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65306;&#25581;&#31034;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#23398;&#20064;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#36825;&#22312;&#20174;&#33258;&#28982;&#22270;&#20687;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#22270;&#20687;&#65289;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26102;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#38598;&#30340;&#22266;&#26377;&#32500;&#24230;($d_{data}$)&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#38169;&#35823;&#19968;&#33324;&#20250;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#65288;&#25918;&#23556;&#23398;&#65289;&#21644;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#20043;&#38388;&#30340;&#36825;&#31181;&#20851;&#31995;&#30340;&#38497;&#23789;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;&#26080;&#29616;&#26377;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24182;&#32463;&#39564;&#35777;&#19968;&#20010;&#19982;$d_{data}$&#30456;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#26469;&#35299;&#20915;&#36825;&#20010;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#32771;&#34385;&#21040;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;($K_F$)&#36825;&#19968;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#37096;&#20998;&#35299;&#37322;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#26174;&#33879;&#32553;&#25918;&#24046;&#24322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#27979;&#37327;&#36825;&#19968;&#25351;&#26631;&#21487;&#20197;&#25552;&#20379;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#32819;&#35282;&#20998;&#31163;&#32593;&#32476;&#65292;&#33021;&#22815;&#20351;&#29992;&#20004;&#20010;&#40614;&#20811;&#39118;&#22312;&#19981;&#21516;&#30340;&#35282;&#24230;&#21306;&#22495;&#20998;&#31163;&#30446;&#26631;&#35821;&#38899;&#28304;&#21644;&#24178;&#25200;&#28304;&#65292;&#22312;&#21508;&#31181;&#28151;&#21709;&#29615;&#22659;&#19979;&#20445;&#25345;&#31283;&#20581;&#65292;&#24182;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08864</link><description>&lt;p&gt;
&#21452;&#32819;&#35282;&#20998;&#31163;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Binaural Angular Separation Network. (arXiv:2401.08864v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08864
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#32819;&#35282;&#20998;&#31163;&#32593;&#32476;&#65292;&#33021;&#22815;&#20351;&#29992;&#20004;&#20010;&#40614;&#20811;&#39118;&#22312;&#19981;&#21516;&#30340;&#35282;&#24230;&#21306;&#22495;&#20998;&#31163;&#30446;&#26631;&#35821;&#38899;&#28304;&#21644;&#24178;&#25200;&#28304;&#65292;&#22312;&#21508;&#31181;&#28151;&#21709;&#29615;&#22659;&#19979;&#20445;&#25345;&#31283;&#20581;&#65292;&#24182;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#20004;&#20010;&#40614;&#20811;&#39118;&#22312;&#19981;&#21516;&#30340;&#35282;&#24230;&#21306;&#22495;&#20998;&#31163;&#30446;&#26631;&#35821;&#38899;&#28304;&#21644;&#24178;&#25200;&#28304;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#27169;&#25311;&#30340;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;RIRs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#25910;&#38598;&#30495;&#23454;&#30340;RIRs&#12290;&#36890;&#36807;&#20381;&#36182;&#29305;&#23450;&#30340;&#35282;&#24230;&#21306;&#22495;&#21644;&#22810;&#20010;&#25151;&#38388;&#27169;&#25311;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#19968;&#33268;&#30340;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#32447;&#32034;&#65292;&#25110;&#32773;&#25105;&#20204;&#31216;&#20043;&#20026;&#24310;&#36831;&#23545;&#27604;&#65292;&#23558;&#30446;&#26631;&#21644;&#24178;&#25200;&#28304;&#20998;&#31163;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#28151;&#21709;&#29615;&#22659;&#19979;&#20445;&#25345;&#31283;&#20581;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#19981;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#30053;&#26377;&#19981;&#21516;&#40614;&#20811;&#39118;&#20960;&#20309;&#24418;&#29366;&#30340;&#21830;&#29992;&#35774;&#22791;&#65292;&#32780;&#19988;&#32988;&#36807;&#20102;&#25105;&#20204;&#20043;&#21069;&#22312;&#30456;&#21516;&#35774;&#22791;&#19978;&#20351;&#29992;&#30340;&#39069;&#22806;&#40614;&#20811;&#39118;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#35774;&#22791;&#19978;&#23454;&#26102;&#36816;&#34892;&#65292;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#27969;&#23186;&#20307;&#24212;&#29992;&#65292;&#22914;&#30005;&#35805;&#21644;&#35270;&#39057;&#20250;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural network model that can separate target speech sources from interfering sources at different angular regions using two microphones. The model is trained with simulated room impulse responses (RIRs) using omni-directional microphones without needing to collect real RIRs. By relying on specific angular regions and multiple room simulations, the model utilizes consistent time difference of arrival (TDOA) cues, or what we call delay contrast, to separate target and interference sources while remaining robust in various reverberation environments. We demonstrate the model is not only generalizable to a commercially available device with a slightly different microphone geometry, but also outperforms our previous work which uses one additional microphone on the same device. The model runs in real-time on-device and is suitable for low-latency streaming applications such as telephony and video conferencing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#23545;&#36710;&#36742;&#20013;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08863</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#30340;&#36890;&#36947;&#33033;&#20914;&#21709;&#24212;&#26469;&#23454;&#29616;&#38053;&#21273;&#26080;&#32447;&#36827;&#20837;&#31995;&#32479;&#20013;&#30340;&#23494;&#38053;&#25187;&#30340;&#40065;&#26834;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Robust Localization of Key Fob Using Channel Impulse Response of Ultra Wide Band Sensors for Keyless Entry Systems. (arXiv:2401.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#36229;&#23485;&#24102;&#20256;&#24863;&#22120;&#23545;&#36710;&#36742;&#20013;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#23450;&#20301;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#36710;&#36742;&#20869;&#37096;&#21644;&#21608;&#22260;&#30340;&#38053;&#21273;&#25187;&#36827;&#34892;&#23450;&#20301;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#36710;&#36742;&#23433;&#20840;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#23485;&#24102;&#65288;UWB&#65289;&#23450;&#20301;&#20998;&#31867;&#30340;&#39044;&#35745;&#31639;&#29305;&#24449;&#30340;&#24615;&#33021;&#20316;&#20026;&#25105;&#20204;&#23454;&#39564;&#30340;&#22522;&#20934;&#12290;2&#65289;&#30740;&#31350;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#27809;&#26377;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#30340;&#23545;&#25239;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;3&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27809;&#26377;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#22522;&#20934;&#31070;&#32463;&#32593;&#32476;&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#26041;&#27861;&#30340;&#26576;&#20123;&#23545;&#25239;&#24378;&#24230;&#33539;&#22260;&#20869;&#25552;&#39640;&#20102;67&#65285;&#65292;&#23545;&#20110;&#22522;&#26412;&#36845;&#20195;&#26041;&#27861;&#21644;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20998;&#21035;&#25552;&#39640;&#20102;37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using neural networks for localization of key fob within and surrounding a car as a security feature for keyless entry is fast emerging. In this paper we study: 1) the performance of pre-computed features of neural networks based UWB (ultra wide band) localization classification forming the baseline of our experiments. 2) Investigate the inherent robustness of various neural networks; therefore, we include the study of robustness of the adversarial examples without any adversarial training in this work. 3) Propose a multi-head self-supervised neural network architecture which outperforms the baseline neural networks without any adversarial training. The model's performance improved by 67% at certain ranges of adversarial magnitude for fast gradient sign method and 37% each for basic iterative method and projected gradient descent method.
&lt;/p&gt;</description></item><item><title>Shabari&#26159;&#19968;&#20010;&#24310;&#36831;&#20915;&#31574;&#30340;&#26080;&#26381;&#21153;&#22120;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20989;&#25968;&#36755;&#20837;&#30340;&#24310;&#36831;&#26469;&#20943;&#36731;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08859</link><description>&lt;p&gt;
Shabari: &#24310;&#36831;&#20915;&#31574;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Shabari: Delayed Decision-Making for Faster and Efficient Serverless Function. (arXiv:2401.08859v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08859
&lt;/p&gt;
&lt;p&gt;
Shabari&#26159;&#19968;&#20010;&#24310;&#36831;&#20915;&#31574;&#30340;&#26080;&#26381;&#21153;&#22120;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20989;&#25968;&#36755;&#20837;&#30340;&#24310;&#36831;&#26469;&#20943;&#36731;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#20943;&#36731;&#20102;&#24320;&#21457;&#20154;&#21592;&#23545;&#36164;&#28304;&#31649;&#29702;&#30340;&#36127;&#25285;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#26131;&#29992;&#24615;&#65292;&#24182;&#20026;&#25552;&#20379;&#32773;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20170;&#22825;&#30340;&#26080;&#26381;&#21153;&#22120;&#31995;&#32479;&#22312;&#20989;&#25968;&#35843;&#29992;&#26041;&#38754;&#32570;&#20047;&#24615;&#33021;&#20445;&#35777;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#30340;&#25903;&#25345;&#65306;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#24615;&#33021;&#21464;&#24322;&#24615;&#65288;&#39640;&#36798;6&#20493;&#65289;&#12290;&#25552;&#20379;&#32773;&#32570;&#20047;&#23545;&#29992;&#25143;&#20989;&#25968;&#30340;&#21487;&#35265;&#24615;&#65292;&#22240;&#27492;&#24456;&#38590;&#23545;&#20854;&#36827;&#34892;&#21512;&#36866;&#30340;&#36164;&#28304;&#35268;&#27169;&#21270;&#65306;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#36164;&#28304;&#20302;&#21033;&#29992;&#29575;&#65288;&#39640;&#36798;80%&#65289;&#12290;&#20026;&#20102;&#29702;&#35299;&#24615;&#33021;&#21464;&#24322;&#24615;&#21644;&#20302;&#21033;&#29992;&#29575;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#37096;&#32626;&#30340;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#36827;&#34892;&#20102;&#27979;&#37327;&#30740;&#31350;&#65292;&#24182;&#20102;&#35299;&#21040;&#20989;&#25968;&#24615;&#33021;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#20851;&#38190;&#21462;&#20915;&#20110;&#20989;&#25968;&#35821;&#20041;&#21644;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35748;&#35782;&#26159;&#22312;&#20989;&#25968;&#36755;&#20837;&#21487;&#29992;&#20043;&#21518;&#24310;&#36831;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Shabari&#65292;&#19968;&#20010;&#29992;&#20110;&#26080;&#26381;&#21153;&#22120;&#30340;&#36164;&#28304;&#31649;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serverless computing relieves developers from the burden of resource management, thus providing ease-of-use to the users and the opportunity to optimize resource utilization for the providers. However, today's serverless systems lack performance guarantees for function invocations, thus limiting support for performance-critical applications: we observed severe performance variability (up to 6x). Providers lack visibility into user functions and hence find it challenging to right-size them: we observed heavy resource underutilization (up to 80%). To understand the causes behind the performance variability and underutilization, we conducted a measurement study of commonly deployed serverless functions and learned that the function performance and resource utilization depend crucially on function semantics and inputs. Our key insight is to delay making resource allocation decisions until after the function inputs are available. We introduce Shabari, a resource management framework for ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#30340;&#31454;&#20105;&#21147;&#21644;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.08851</link><description>&lt;p&gt;
&#20351;&#29992;i&#21521;&#37327;&#36827;&#34892;&#20027;&#20307;&#26080;&#20851;&#30340;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#30340;&#31454;&#20105;&#21147;&#21644;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#26159;&#26681;&#25454;&#29983;&#29702;&#27979;&#37327;&#22914;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#33258;&#21160;&#30830;&#23450;&#20010;&#20307;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#21033;&#29992;&#24037;&#20316;&#35760;&#24518;&#36164;&#28304;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#22788;&#29702;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#35821;&#26009;&#24211;&#20316;&#20026;&#31532;&#19968;&#27425;&#34987;&#21160;&#33041;&#26426;&#25509;&#21475;&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;2021&#24180;&#20844;&#24320;&#21457;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25253;&#36947;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination.
&lt;/p&gt;</description></item><item><title>REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08850</link><description>&lt;p&gt;
REValueD: &#23545;&#21487;&#20998;&#35299;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes. (arXiv:2401.08850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08850
&lt;/p&gt;
&lt;p&gt;
REValueD&#26159;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#38598;&#21512;&#20540;&#20998;&#35299;&#30340;&#26032;&#31639;&#27861;&#65292;&#38024;&#23545;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#36943;&#21046;&#20102;Q-learning&#31639;&#27861;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#24182;&#20943;&#36731;&#20102;&#30446;&#26631;&#26041;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21487;&#33021;&#30340;&#21160;&#20316;&#25968;&#37327;&#24222;&#22823;&#65292;&#31163;&#25955;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20855;&#26377;&#39640;&#32500;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#8212;&#8212;&#20540;&#20998;&#35299;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20540;&#20998;&#35299;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23427;&#34429;&#28982;&#21487;&#20197;&#36943;&#21046;Q&#23398;&#20064;&#31639;&#27861;&#22266;&#26377;&#30340;&#39640;&#20272;&#20559;&#24046;&#65292;&#20294;&#20063;&#20250;&#25918;&#22823;&#30446;&#26631;&#26041;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;&#30340;&#38598;&#21512;&#20197;&#20943;&#36731;&#30446;&#26631;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19968;&#20010;&#32500;&#24230;&#19978;&#30340;&#25506;&#32034;&#24615;&#21160;&#20316;&#23545;&#20854;&#20182;&#32500;&#24230;&#19978;&#26368;&#20248;&#21160;&#20316;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#31639;&#27861;REValueD&#65292;&#22312;&#32463;&#36807;&#31163;&#25955;&#21270;&#30340;DeepMind&#25511;&#21046;&#22871;&#20214;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#20154;&#24418;&#21644;&#29399;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24433;&#21709;REValueD&#34920;&#29616;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance
&lt;/p&gt;</description></item><item><title>RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08847</link><description>&lt;p&gt;
RIDGE: &#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08847
&lt;/p&gt;
&lt;p&gt;
RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#24212;&#29992;&#12290;&#22270;&#20687;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#65292;&#38656;&#35201;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;/&#20307;&#31215;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RIDGE&#28165;&#21333;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;&#35813;&#28165;&#21333;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#20854;&#24037;&#20316;&#30340;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#31185;&#23398;&#30340;&#21487;&#38752;&#24615;&#65292;&#36824;&#20855;&#26377;&#20020;&#24202;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#23376;&#32593;&#32476;&#36864;&#28779;&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#20462;&#21098;&#23376;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#27599;&#27425;&#21069;&#21521;&#20256;&#25773;&#26102;&#20351;&#29992;&#38543;&#26426;&#25513;&#30721;&#34920;&#31034;&#21442;&#25968;&#30340;&#21253;&#21547;&#25110;&#25490;&#38500;&#27010;&#29575;&#65292;&#20197;&#36991;&#20813;&#20462;&#21098;&#36807;&#22810;&#21442;&#25968;&#23548;&#33268;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#21644;&#23376;&#32593;&#32476;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08830</link><description>&lt;p&gt;
&#38543;&#26426;&#23376;&#32593;&#32476;&#36864;&#28779;&#65306;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#20462;&#21098;&#23376;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Pruned Subnetworks. (arXiv:2401.08830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08830
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23376;&#32593;&#32476;&#36864;&#28779;&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#20462;&#21098;&#23376;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#27599;&#27425;&#21069;&#21521;&#20256;&#25773;&#26102;&#20351;&#29992;&#38543;&#26426;&#25513;&#30721;&#34920;&#31034;&#21442;&#25968;&#30340;&#21253;&#21547;&#25110;&#25490;&#38500;&#27010;&#29575;&#65292;&#20197;&#36991;&#20813;&#20462;&#21098;&#36807;&#22810;&#21442;&#25968;&#23548;&#33268;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#21644;&#23376;&#32593;&#32476;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20462;&#21098;&#26041;&#27861;&#22240;&#20854;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#24615;&#32780;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#32463;&#36807;&#23569;&#25968;&#35757;&#32451;&#36718;&#27425;&#21518;&#65292;&#22823;&#37327;&#21442;&#25968;&#21487;&#20197;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#31227;&#38500;&#65292;&#32780;&#20934;&#30830;&#24230;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#20462;&#21098;&#22826;&#22810;&#21442;&#25968;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24230;&#30340;&#38497;&#28982;&#19979;&#38477;&#65292;&#36825;&#21487;&#33021;&#30772;&#22351;&#25910;&#25947;&#36136;&#37327;&#12290;&#36845;&#20195;&#20462;&#21098;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#36718;&#27425;&#20013;&#36880;&#28176;&#31227;&#38500;&#23569;&#37327;&#21442;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20173;&#21487;&#33021;&#23548;&#33268;&#23376;&#32593;&#32476;&#36807;&#25311;&#21512;&#25439;&#22833;&#20989;&#25968;&#30340;&#23616;&#37096;&#21306;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#38543;&#26426;&#23376;&#32593;&#32476;&#36864;&#28779;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35843;&#25972;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25513;&#30721;&#26469;&#34920;&#31034;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;&#27599;&#20010;&#21442;&#25968;&#22312;&#20219;&#20309;&#21069;&#21521;&#20256;&#25773;&#20013;&#37117;&#26377;&#34987;&#21253;&#21547;&#25110;&#25490;&#38500;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning methods have recently grown in popularity as an effective way to reduce the size and computational complexity of deep neural networks. Large numbers of parameters can be removed from trained models with little discernible loss in accuracy after a small number of continued training epochs. However, pruning too many parameters at once often causes an initial steep drop in accuracy which can undermine convergence quality. Iterative pruning approaches mitigate this by gradually removing a small number of parameters over multiple epochs. However, this can still lead to subnetworks that overfit local regions of the loss landscape. We introduce a novel and effective approach to tuning subnetworks through a regularization technique we call Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete manner, we instead represent subnetworks with stochastic masks where each parameter has a probabilistic chance of being included or excluded on any given forward pass. We a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;AiGen-FoodReview&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#37492;&#21035;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35780;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#23646;&#24615;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08825</link><description>&lt;p&gt;
AiGen-FoodReview:&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;AiGen-FoodReview&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#37492;&#21035;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35780;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#23646;&#24615;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35780;&#35770;&#20316;&#20026;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65288;UGC&#65289;&#26174;&#33879;&#24433;&#21709;&#28040;&#36153;&#32773;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19981;&#20165;&#20154;&#20026;&#21046;&#36896;&#30340;&#34394;&#20551;&#20869;&#23481;&#65292;&#36824;&#26377;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#37117;&#25361;&#25112;&#20102;UGC&#30340;&#21487;&#38752;&#24615;&#12290;&#20511;&#21161;&#20110;OpenAI&#30340;GPT-4-Turbo&#21644;DALL-E-2&#27169;&#22411;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AiGen-FoodReview&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;20,144&#20010;&#39184;&#21381;&#35780;&#35770;-&#22270;&#20687;&#23545;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#30495;&#23454;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#22312;FLAVA&#19978;&#23454;&#29616;&#20102;99.80%&#30340;&#22810;&#27169;&#24577;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#20998;&#21035;&#23545;&#35780;&#35770;&#21644;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#20854;&#24615;&#33021;&#19982;&#23545;&#27604;&#30456;&#24403;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#26816;&#27979;&#22120;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25512;&#33616;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#20197;&#22686;&#24378;&#21487;&#38752;&#30340;UGC&#12290;
&lt;/p&gt;
&lt;p&gt;
Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25289;&#26364;&#31995;&#32479;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37325;&#24314;&#25163;&#26415;&#21306;&#22495;&#20013;&#32959;&#30244;&#30340;&#20301;&#32622;&#21644;&#36793;&#30028;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39564;&#35777;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08821</link><description>&lt;p&gt;
&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#25163;&#26415;&#21306;&#22495;&#20934;&#30830;&#37325;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward Accurate Reconstruction of the Surgical Zone. (arXiv:2401.08821v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25289;&#26364;&#31995;&#32479;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37325;&#24314;&#25163;&#26415;&#21306;&#22495;&#20013;&#32959;&#30244;&#30340;&#20301;&#32622;&#21644;&#36793;&#30028;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39564;&#35777;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25289;&#26364;&#20809;&#35889;&#26159;&#19968;&#31181;&#22522;&#20110;&#30456;&#24178;&#20809;&#30340;&#38750;&#24377;&#24615;&#32972;&#21521;&#25955;&#23556;&#30340;&#20809;&#23376;&#27169;&#24577;&#65292;&#23545;&#20110;&#26415;&#20013;&#24863;&#30693;&#31354;&#38388;&#26469;&#35828;&#26159;&#19968;&#31181;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#25552;&#20379;&#20102;&#38750;&#30005;&#31163;&#28508;&#21147;&#21644;&#39640;&#24230;&#29305;&#24322;&#24615;&#30340;&#20998;&#23376;&#25351;&#32441;&#26679;&#20809;&#35889;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21160;&#24577;&#25163;&#26415;&#22330;&#20013;&#30149;&#29702;&#32452;&#32455;&#30340;&#35786;&#26029;&#12290;&#23613;&#31649;&#25289;&#26364;&#20809;&#35889;&#22312;&#24378;&#24230;&#19978;&#23384;&#22312;&#32570;&#38519;&#65292;&#20294;&#21033;&#29992;&#37329;&#23646;&#32435;&#31859;&#32467;&#26500;&#26469;&#25918;&#22823;&#25289;&#26364;&#20449;&#21495;&#30340;&#34920;&#38754;&#22686;&#24378;&#25289;&#26364;&#20809;&#35889;&#65288;SERS&#65289;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#20809;&#23376;&#27169;&#24577;&#30456;&#23218;&#32654;&#30340;&#26816;&#27979;&#28789;&#25935;&#24230;&#12290;&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#19968;&#31181;&#33021;&#21487;&#38752;&#22320;&#30830;&#23450;&#20581;&#24247;&#32452;&#32455;&#20013;&#23884;&#20837;&#32959;&#30244;&#30340;&#20301;&#32622;&#21644;&#36793;&#30028;&#30340;&#26426;&#22120;&#20154;&#25289;&#26364;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20197;&#32452;&#32455;&#27169;&#25311;&#24187;&#24433;&#20026;&#27169;&#22411;&#65292;&#20854;&#20013;&#26377;&#36873;&#25321;&#22320;&#27880;&#20837;&#20102;&#37329;&#32435;&#31859;&#26143;&#21306;&#22495;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25910;&#38598;&#30340;&#29983;&#29289;SERS&#25110;&#25289;&#26364;&#25968;&#25454;&#30456;&#23545;&#19981;&#36275;&#65292;&#25105;&#20204;&#23454;&#26045;&#36801;&#31227;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#19982;&#37329;&#32435;&#31859;&#26143;&#30456;&#27604;&#30340;100%&#39564;&#35777;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Raman spectroscopy, a photonic modality based on the inelastic backscattering of coherent light, is a valuable asset to the intraoperative sensing space, offering non-ionizing potential and highly-specific molecular fingerprint-like spectroscopic signatures that can be used for diagnosis of pathological tissue in the dynamic surgical field. Though Raman suffers from weakness in intensity, Surface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to amplify Raman signals, can achieve detection sensitivities that rival traditional photonic modalities. In this study, we outline a robotic Raman system that can reliably pinpoint the location and boundaries of a tumor embedded in healthy tissue, modeled here as a tissue-mimicking phantom with selectively infused Gold Nanostar regions. Further, due to the relative dearth of collected biological SERS or Raman data, we implement transfer learning to achieve 100% validation classification accuracy for Gold Nanostars compared to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08819</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#20174;&#31232;&#30095;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#65292;CDE&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22806;&#25512;&#38169;&#35823;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#31232;&#30095;&#22870;&#21169;&#25110;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20445;&#23432;&#23494;&#24230;&#20272;&#35745;&#65288;CDE&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#32422;&#26463;&#29366;&#24577;-&#34892;&#20026;&#21344;&#25454;&#31283;&#24577;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;CDE&#36890;&#36807;&#35299;&#20915;&#36793;&#38469;&#37325;&#35201;&#24615;&#25277;&#26679;&#20013;&#30340;&#25903;&#25345;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#31283;&#24577;&#20998;&#24067;&#26657;&#27491;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CDE&#22312;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#25110;&#19981;&#36275;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#25345;&#32493;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22806;&#25512;&#38169;&#35823;&#38382;&#39064;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31038;&#20132;&#38899;&#20048;&#25512;&#33616;&#20013;&#24433;&#21709;&#38899;&#20048;&#20114;&#21160;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#12289;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#12289;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#20855;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#20197;&#21450;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#65292;&#36825;&#20123;&#22240;&#32032;&#37117;&#20250;&#22686;&#21152;&#25509;&#25910;&#32773;&#19982;&#26032;&#33402;&#26415;&#23478;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08818</link><description>&lt;p&gt;
Link Me Baby One More Time: &#22312; Spotify &#19978;&#30340;&#31038;&#20132;&#38899;&#20048;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Link Me Baby One More Time: Social Music Discovery on Spotify. (arXiv:2401.08818v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31038;&#20132;&#38899;&#20048;&#25512;&#33616;&#20013;&#24433;&#21709;&#38899;&#20048;&#20114;&#21160;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#12289;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#12289;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#20855;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#20197;&#21450;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#65292;&#36825;&#20123;&#22240;&#32032;&#37117;&#20250;&#22686;&#21152;&#25509;&#25910;&#32773;&#19982;&#26032;&#33402;&#26415;&#23478;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#24433;&#21709;&#20010;&#20154;&#20043;&#38388;&#38899;&#20048;&#25512;&#33616;&#21644;&#21457;&#29616;&#32467;&#26524;&#30340;&#31038;&#20132;&#21644;&#29615;&#22659;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992; Spotify &#30340;&#25968;&#25454;&#26469;&#30740;&#31350;&#29992;&#25143;&#20043;&#38388;&#21457;&#36865;&#38142;&#25509;&#23548;&#33268;&#25509;&#25910;&#32773;&#19982;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#30340;&#38899;&#20048;&#20114;&#21160;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#21487;&#33021;&#24433;&#21709;&#36825;&#19968;&#36807;&#31243;&#30340;&#22240;&#32032;&#65292;&#22914;&#21457;&#36865;&#32773;&#19982;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#24378;&#24230;&#65292;&#29992;&#25143;&#22312; Spotify &#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#35282;&#33394;&#65292;&#20182;&#20204;&#30340;&#38899;&#20048;&#31038;&#20132;&#20957;&#32858;&#21147;&#65292;&#20197;&#21450;&#26032;&#33402;&#26415;&#23478;&#19982;&#25509;&#25910;&#32773;&#30340;&#21697;&#21619;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#25509;&#25910;&#32773;&#19982;&#21457;&#36865;&#32773;&#30340;&#38899;&#20048;&#21697;&#21619;&#30456;&#20284;&#19988;&#20998;&#20139;&#30340;&#38899;&#36712;&#36866;&#21512;&#20182;&#20204;&#30340;&#21697;&#21619;&#26102;&#65292;&#20182;&#20204;&#26356;&#26377;&#21487;&#33021;&#19982;&#26032;&#33402;&#26415;&#23478;&#20114;&#21160;&#65307;&#24403;&#20182;&#20204;&#19982;&#21457;&#36865;&#32773;&#26377;&#26356;&#24378;&#21644;&#26356;&#20146;&#23494;&#30340;&#32852;&#31995;&#26102;&#65292;&#20063;&#26356;&#26377;&#21487;&#33021;&#20114;&#21160;&#65307;&#20197;&#21450;&#24403;&#20998;&#20139;&#30340;&#33402;&#26415;&#23478;&#22312;&#25509;&#25910;&#32773;&#30340;&#20851;&#31995;&#20013;&#21463;&#27426;&#36814;&#26102;&#65292;&#20063;&#26356;&#26377;&#21487;&#33021;&#20114;&#21160;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#26500;&#24314;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#20139;&#30340;&#38899;&#20048;&#36712;&#36947;&#26159;&#21542;&#20250;&#23548;&#33268;&#25509;&#25910;&#32773;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the social and contextual factors that influence the outcome of person-to-person music recommendations and discovery. Specifically, we use data from Spotify to investigate how a link sent from one user to another results in the receiver engaging with the music of the shared artist. We consider several factors that may influence this process, such as the strength of the sender-receiver relationship, the user's role in the Spotify social network, their music social cohesion, and how similar the new artist is to the receiver's taste. We find that the receiver of a link is more likely to engage with a new artist when (1) they have similar music taste to the sender and the shared track is a good fit for their taste, (2) they have a stronger and more intimate tie with the sender, and (3) the shared artist is popular with the receiver's connections. Finally, we use these findings to build a Random Forest classifier to predict whether a shared music track will result in the receiver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#30417;&#30563;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;ALDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#21106;&#21028;&#21035;&#22120;&#21644;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#21644;&#29983;&#25104;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08815</link><description>&lt;p&gt;
&#23545;&#25239;&#30417;&#30563;&#20351;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#34028;&#21187;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive. (arXiv:2401.08815v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#30417;&#30563;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;ALDM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#21106;&#21028;&#21035;&#22120;&#21644;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#21644;&#29983;&#25104;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#30340;&#36827;&#23637;&#36739;&#23567;&#12290;&#24403;&#21069;&#30340;&#24067;&#23616;&#21040;&#22270;&#20687;&#27169;&#22411;&#35201;&#20040;&#22312;&#25991;&#26412;&#32534;&#36753;&#33021;&#21147;&#19978;&#27424;&#20339;&#65292;&#35201;&#20040;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#19978;&#36739;&#24369;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23545;&#25239;&#30417;&#30563;&#38598;&#25104;&#21040;&#20256;&#32479;&#24067;&#23616;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#65288;ALDM&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20998;&#21106;&#30340;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#20687;&#32032;&#32423;&#21035;&#19978;&#20026;&#25193;&#25955;&#29983;&#25104;&#22120;&#25552;&#20379;&#26126;&#30830;&#30340;&#21453;&#39304;&#65292;&#29992;&#20110;&#25351;&#23548;&#21435;&#22122;&#22270;&#20687;&#19982;&#36755;&#20837;&#24067;&#23616;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#40723;&#21169;&#22312;&#37319;&#26679;&#27493;&#39588;&#20013;&#23545;&#36755;&#20837;&#24067;&#23616;&#30340;&#19968;&#33268;&#20381;&#20174;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22810;&#27493;&#23637;&#24320;&#31574;&#30053;&#12290;&#25105;&#20204;&#19981;&#26159;&#21482;&#20851;&#27880;&#21333;&#20010;&#26102;&#38388;&#27493;&#65292;&#32780;&#26159;&#36882;&#24402;&#22320;&#23637;&#24320;&#20960;&#20010;&#27493;&#39588;&#26469;&#27169;&#25311;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#35201;&#27714;&#21028;&#21035;&#22120;&#22312;&#19968;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#35780;&#20272;&#21435;&#22122;&#22270;&#20687;&#19982;&#24067;&#23616;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680; (lpNTK)&#65292;&#24182;&#25506;&#35752;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.08808</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#21147;&#23398;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#65306;&#20174;&#26679;&#26412;&#20851;&#31995;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08808
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680; (lpNTK)&#65292;&#24182;&#25506;&#35752;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#25552;&#20986;&#26032;&#27169;&#22411;&#25110;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#35757;&#32451;&#25968;&#25454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#21364;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26679;&#26412;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23545;&#20854;&#20182;&#26679;&#26412;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#30417;&#30563;&#23398;&#20064;&#20013;&#28041;&#21450;&#30340;&#26435;&#37325;&#26356;&#26032;&#39033;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#31614;&#20250;&#24433;&#21709;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;lpNTK&#65289;&#65292;&#22312;&#27979;&#37327;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26102;&#32771;&#34385;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;lpNTK&#22312;Frobenius&#33539;&#25968;&#19979;&#28176;&#36817;&#25910;&#25947;&#20110;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#20808;&#21069;&#24037;&#20316;&#20013;&#21457;&#29616;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#26679;&#26412;&#23398;&#20064;&#22256;&#38590;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2401.08788</link><description>&lt;p&gt;
&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Differential Feature Under-reporting on Algorithmic Fairness. (arXiv:2401.08788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#37096;&#38376;&#30340;&#39044;&#27979;&#39118;&#38505;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26356;&#23436;&#25972;&#30340;&#34892;&#25919;&#25968;&#25454;&#26469;&#24320;&#21457;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#26356;&#22823;&#31243;&#24230;&#20381;&#36182;&#20844;&#20849;&#26381;&#21153;&#30340;&#20122;&#32676;&#20307;&#26356;&#20026;&#23436;&#25972;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#23545;&#20110;&#30001;&#21307;&#30103;&#34917;&#21161;&#21644;&#21307;&#30103;&#20445;&#38505;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#25919;&#24220;&#26426;&#26500;&#24120;&#24120;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#21033;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#31169;&#20154;&#20445;&#38505;&#30340;&#20154;&#21017;&#27809;&#26377;&#12290;&#23545;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#30340;&#25209;&#35780;&#25351;&#20986;&#65292;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23548;&#33268;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#19981;&#20844;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#20559;&#35265;&#22312;&#25216;&#26415;&#35270;&#35282;&#19979;&#20173;&#28982;&#30740;&#31350;&#19981;&#36275;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#28155;&#21152;&#29305;&#24449;&#22122;&#22768;&#21644;&#26126;&#30830;&#26631;&#35760;&#20026;&#32570;&#22833;&#30340;&#29305;&#24449;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#32570;&#22833;&#25351;&#26631;&#30340;&#25968;&#25454;&#32570;&#22833;&#24773;&#20917;&#65288;&#21363;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#65289;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to charac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#27979;&#30456;&#20851;&#24615;&#30340;&#20272;&#35745;&#65292;&#24182;&#25512;&#24191;&#20102;&#21435;&#30456;&#20851;&#24615;&#21040;&#22810;&#32972;&#26223;&#35774;&#32622;&#20013;&#12290;&#22312;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#32500;&#31890;&#23376;&#34928;&#21464;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08777</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#31890;&#23376;&#29289;&#29702;&#30340;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Anomaly Detection for Particle Physics Using Multi-Background Representation Learning. (arXiv:2401.08777v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#40065;&#26834;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#27979;&#30456;&#20851;&#24615;&#30340;&#20272;&#35745;&#65292;&#24182;&#25512;&#24191;&#20102;&#21435;&#30456;&#20851;&#24615;&#21040;&#22810;&#32972;&#26223;&#35774;&#32622;&#20013;&#12290;&#22312;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#30340;&#39640;&#32500;&#31890;&#23376;&#34928;&#21464;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#26816;&#27979;&#26159;&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#21457;&#29616;&#26032;&#31890;&#23376;&#25110;&#36807;&#31243;&#30340;&#26377;&#26395;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#25913;&#36827;&#39640;&#33021;&#29289;&#29702;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#20010;&#34987;&#24573;&#35270;&#30340;&#26426;&#20250;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#19981;&#20877;&#23545;&#21333;&#19968;&#26368;&#20027;&#35201;&#30340;&#32972;&#26223;&#36807;&#31243;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#22810;&#20010;&#32972;&#26223;&#31867;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#26500;&#24314;&#26816;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#21033;&#29992;&#26356;&#22810;&#20449;&#24687;&#26469;&#25552;&#39640;&#26816;&#27979;&#30456;&#20851;&#24615;&#30340;&#20272;&#35745;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#21435;&#30456;&#20851;&#24615;&#25512;&#24191;&#21040;&#22810;&#32972;&#26223;&#35774;&#32622;&#20013;&#65292;&#20174;&#32780;&#30452;&#25509;&#24378;&#21270;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#26356;&#23436;&#25972;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#19978;&#30340;&#39640;&#32500;&#31890;&#23376;&#34928;&#21464;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#40065;&#26834;&#22810;&#32972;&#26223;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly, or out-of-distribution, detection is a promising tool for aiding discoveries of new particles or processes in particle physics. In this work, we identify and address two overlooked opportunities to improve anomaly detection for high-energy physics. First, rather than train a generative model on the single most dominant background process, we build detection algorithms using representation learning from multiple background types, thus taking advantage of more information to improve estimation of what is relevant for detection. Second, we generalize decorrelation to the multi-background setting, thus directly enforcing a more complete definition of robustness for anomaly detection. We demonstrate the benefit of the proposed robust multi-background anomaly detection algorithms on a high-dimensional dataset of particle decays at the Large Hadron Collider.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22826;&#38451;&#31995;&#22825;&#20307;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#23547;&#25214;&#26377;&#36259;&#30340;&#22825;&#20307;&#12290;&#35813;&#26041;&#27861;&#22312;&#23547;&#25214;&#26143;&#38469;&#29289;&#20307;&#31561;&#26377;&#36259;&#31867;&#21035;&#30340;&#20363;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#23545;&#32463;&#20856;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#20197;&#32771;&#34385;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#20197;&#22686;&#21152;&#21457;&#29616;&#30340;&#24322;&#24120;&#31181;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.08763</link><description>&lt;p&gt;
&#22826;&#38451;&#31995;&#20013;&#30340;&#22855;&#24618;&#21644;&#22855;&#22937;&#20043;&#22788;&#65306;&#22312;&#26102;&#31354;&#36951;&#20135;&#35843;&#26597;&#20013;&#23547;&#25214;&#24847;&#22806;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
The weird and the wonderful in our Solar System: Searching for serendipity in the Legacy Survey of Space and Time. (arXiv:2401.08763v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22826;&#38451;&#31995;&#22825;&#20307;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#23547;&#25214;&#26377;&#36259;&#30340;&#22825;&#20307;&#12290;&#35813;&#26041;&#27861;&#22312;&#23547;&#25214;&#26143;&#38469;&#29289;&#20307;&#31561;&#26377;&#36259;&#31867;&#21035;&#30340;&#20363;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#20063;&#23545;&#32463;&#20856;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#20197;&#32771;&#34385;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#20197;&#22686;&#21152;&#21457;&#29616;&#30340;&#24322;&#24120;&#31181;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22826;&#38451;&#31995;&#22825;&#20307;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20026;&#26102;&#31354;&#36951;&#20135;&#35843;&#26597;&#20570;&#20934;&#22791;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#26469;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25628;&#32034;&#20854;&#20182;&#26377;&#36259;&#30340;&#22825;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#26143;&#38469;&#29289;&#20307;&#31561;&#26377;&#36259;&#30340;&#20363;&#23376;&#26469;&#35777;&#26126;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#25214;&#21040;&#26356;&#22810;&#26377;&#36259;&#31867;&#21035;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#24322;&#24120;&#26469;&#25506;&#31350;&#32463;&#20856;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#32771;&#34385;&#36890;&#36807;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#26469;&#22686;&#21152;&#22312;&#35843;&#26597;&#20013;&#21487;&#20197;&#21457;&#29616;&#30340;&#24322;&#24120;&#30340;&#31181;&#31867;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for anomaly detection in Solar System object data, in preparation for the Legacy Survey of Space and Time. We train a deep autoencoder for anomaly detection and use the learned latent space to search for other interesting objects. We demonstrate the efficacy of the autoencoder approach by finding interesting examples, such as interstellar objects, and show that using the autoencoder, further examples of interesting classes can be found. We also investigate the limits of classic unsupervised approaches to anomaly detection through the generation of synthetic anomalies and evaluate the feasibility of using a supervised learning approach. Future work should consider expanding the feature space to increase the variety of anomalies that can be uncovered during the survey using an autoencoder.
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#21644;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.08741</link><description>&lt;p&gt;
&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fixed Point Diffusion Models. (arXiv:2401.08741v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08741
&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#24341;&#20837;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#20837;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#21644;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22266;&#23450;&#28857;&#25193;&#25955;&#27169;&#22411;&#65288;FPDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22266;&#23450;&#28857;&#27714;&#35299;&#30340;&#27010;&#24565;&#34701;&#20837;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#38544;&#24335;&#30340;&#22266;&#23450;&#28857;&#27714;&#35299;&#23618;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#32593;&#32476;&#20013;&#65292;&#23558;&#25193;&#25955;&#36807;&#31243;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#22266;&#23450;&#28857;&#38382;&#39064;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#20943;&#23567;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#24555;&#20102;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#33021;&#22815;&#24320;&#21457;&#20986;&#20004;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65306;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#37325;&#26032;&#20998;&#37197;&#35745;&#31639;&#21644;&#37325;&#29992;&#22266;&#23450;&#28857;&#35299;&#12290;&#25105;&#20204;&#23545;ImageNet&#12289;FFHQ&#12289;CelebA-HQ&#21644;LSUN-Church&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;DiT&#27169;&#22411;&#30456;&#27604;&#65292;FPDM&#21442;&#25968;&#20943;&#23569;&#20102;87%&#65292;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;60%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to image generation that integrates the concept of fixed point solving into the framework of diffusion-based generative modeling. Our approach embeds an implicit fixed point solving layer into the denoising network of a diffusion model, transforming the diffusion process into a sequence of closely-related fixed point problems. Combined with a new stochastic training method, this approach significantly reduces model size, reduces memory usage, and accelerates training. Moreover, it enables the development of two new techniques to improve sampling efficiency: reallocating computation across timesteps and reusing fixed point solutions between timesteps. We conduct extensive experiments with state-of-the-art models on ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in performance and efficiency. Compared to the state-of-the-art DiT model, FPDM contains 87% fewer parameters, consumes 60%
&lt;/p&gt;</description></item><item><title>SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.08740</link><description>&lt;p&gt;
SiT:&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#25506;&#32034;&#22522;&#20110;&#27969;&#21160;&#21644;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08740
&lt;/p&gt;
&lt;p&gt;
SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#21464;&#25442;&#22120;&#65288;DiT&#65289;&#39592;&#24178;&#30340;&#21487;&#25193;&#23637;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#22120;&#65288;SiT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#31995;&#21015;&#12290;&#25554;&#20540;&#26694;&#26550;&#20801;&#35768;&#20197;&#27604;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#24314;&#31435;&#22312;&#21160;&#24577;&#20256;&#36755;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#27169;&#22359;&#21270;&#30740;&#31350;&#65306;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#23398;&#20064;&#36824;&#26159;&#36830;&#32493;&#26102;&#38388;&#23398;&#20064;&#65292;&#20915;&#23450;&#27169;&#22411;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#36873;&#25321;&#36830;&#25509;&#20998;&#24067;&#30340;&#25554;&#20540;&#22120;&#65292;&#20197;&#21450;&#37096;&#32626;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#37319;&#26679;&#22120;&#12290;&#36890;&#36807;&#31934;&#24515;&#24341;&#20837;&#19978;&#36848;&#20803;&#32032;&#65292;SiT&#22312;&#20855;&#26377;&#30456;&#21516;&#39592;&#24178;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;GFLOPs&#30340;&#26465;&#20214;ImageNet 256x256&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#20840;&#38754;&#36229;&#36807;&#20102;DiT&#12290;&#36890;&#36807;&#25506;&#32034;&#21487;&#20197;&#19982;&#23398;&#20064;&#20998;&#24320;&#35843;&#25972;&#30340;&#21508;&#31181;&#25193;&#25955;&#31995;&#25968;&#65292;SiT&#22312;FID-50K&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2.06&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#21457;&#29616;IFI6&#21644;IFI27&#31561;&#22522;&#22240;&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#33021;&#26377;&#25928;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#12290;</title><link>http://arxiv.org/abs/2401.08738</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#20013;&#20234;&#27874;&#25289;&#30149;&#27602;&#23545;&#22522;&#22240;&#34920;&#36798;&#24433;&#21709;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Analysis of Ebola Virus' Impact on Gene Expression in Nonhuman Primates. (arXiv:2401.08738v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#65292;&#21457;&#29616;IFI6&#21644;IFI27&#31561;&#22522;&#22240;&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#33021;&#26377;&#25928;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21363;&#30417;&#30563;&#22411;&#24133;&#24230;-&#39640;&#24230;&#35780;&#20998; (SMAS) &#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602; (EBOV) &#30340;&#38750;&#20154;&#31867;&#28789;&#38271;&#31867;&#21160;&#29289; (NHPs) &#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#24863;&#26579;&#20234;&#27874;&#25289;&#30149;&#27602;&#30340; NHPs &#30340;NanoString&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;SMAS&#31995;&#32479;&#36827;&#34892;&#24494;&#22937;&#30340;&#23487;&#20027;-&#30149;&#21407;&#20307;&#30456;&#20114;&#20316;&#29992;&#20998;&#26512;&#12290;SMAS&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#32479;&#35745;&#23398;&#26174;&#33879;&#24615;&#21644;&#34920;&#36798;&#21464;&#21270;&#30340;&#22522;&#22240;&#36873;&#25321;&#65292;&#37319;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#22914;&#36923;&#36753;&#22238;&#24402;&#20197;&#20934;&#30830;&#21306;&#20998;RT-qPCR&#38451;&#24615;&#21644;&#38452;&#24615;NHP&#26679;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#39033;&#37325;&#35201;&#21457;&#29616;&#26159;&#37492;&#23450;&#20102;IFI6&#21644;IFI27&#20316;&#20026;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34920;&#29616;&#20986;100%&#30340;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215; (AUC) &#25351;&#26631;&#22312;&#20998;&#31867;&#19981;&#21516;&#38454;&#27573;&#30340;&#20234;&#27874;&#25289;&#24863;&#26579;&#20013;&#12290;&#38500;IFI6&#21644;IFI27&#22806;&#65292;&#22522;&#22240;&#22914;MX1&#65292;OAS1&#21644;ISG15&#26174;&#33879;&#19978;&#35843;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This study introduces the Supervised Magnitude-Altitude Scoring (SMAS) methodology, a machine learning-based approach, for analyzing gene expression data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV). We utilize a comprehensive dataset of NanoString gene expression profiles from Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen interaction analysis. SMAS effectively combines gene selection based on statistical significance and expression changes, employing linear classifiers such as logistic regression to accurately differentiate between RT-qPCR positive and negative NHP samples. A key finding of our research is the identification of IFI6 and IFI27 as critical biomarkers, demonstrating exceptional predictive performance with 100% accuracy and Area Under the Curve (AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6 and IFI27, genes, including MX1, OAS1, and ISG15, were significantly upregulated, highlighting the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#22635;&#20805;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;2018&#24180;&#33521;&#26684;&#20848;&#20840;&#38754;&#30340;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.08735</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Scalable Ambient Air Pollution Concentration Estimation. (arXiv:2401.08735v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08735
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#22635;&#20805;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;2018&#24180;&#33521;&#26684;&#20848;&#20840;&#38754;&#30340;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#20173;&#28982;&#26159;&#33521;&#22269;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#25968;&#25454;&#26159;&#25913;&#21892;&#31354;&#27668;&#36136;&#37327;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#33521;&#22269;&#24403;&#21069;&#30340;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#31449;&#32593;&#32476;&#23384;&#22312;&#31354;&#38388;&#31232;&#30095;&#12289;&#24322;&#36136;&#24067;&#23616;&#21644;&#39057;&#32321;&#30340;&#26102;&#38388;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#65292;&#36890;&#24120;&#26159;&#30001;&#20110;&#26029;&#30005;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22635;&#20805;&#32570;&#22833;&#30340;&#27979;&#37327;&#25968;&#25454;&#26469;&#35299;&#20915;&#26102;&#38388;&#21644;&#31354;&#38388;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;2018&#24180;&#33521;&#26684;&#20848;1kmx1km&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#20998;&#36776;&#29575;&#20026;&#27599;&#23567;&#26102;&#19968;&#27425;&#12290;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#26469;&#33258;&#20998;&#24067;&#19981;&#22343;&#21248;&#30340;&#30417;&#27979;&#31449;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#22312;&#30740;&#31350;&#21306;&#22495;&#20869;&#29983;&#25104;&#20102;355,827&#20010;&#21512;&#25104;&#30417;&#27979;&#31449;&#65292;&#20135;&#29983;&#20102;&#20215;&#20540;&#32422;700&#20159;&#33521;&#38225;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#20272;&#35745;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambient air pollution remains a critical issue in the United Kingdom, where data on air pollution concentrations form the foundation for interventions aimed at improving air quality. However, the current air pollution monitoring station network in the UK is characterized by spatial sparsity, heterogeneous placement, and frequent temporal data gaps, often due to issues such as power outages. We introduce a scalable data-driven supervised machine learning model framework designed to address temporal and spatial data gaps by filling missing measurements. This approach provides a comprehensive dataset for England throughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning techniques and real-world data from the sparsely distributed monitoring stations, we generate 355,827 synthetic monitoring stations across the study area, yielding data valued at approximately \pounds70 billion. Validation was conducted to assess the model's performance in forecasting, estimating missing l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08734</link><description>&lt;p&gt;
&#25552;&#39640;&#23545;&#25239;&#36716;&#31227;&#33021;&#21147;&#30340;&#19968;&#31995;&#21015;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks to Boost Adversarial Transferability. (arXiv:2401.08734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#30333;&#30418;&#35774;&#32622;&#19979;&#29983;&#25104;&#30340;&#32431;&#31929;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#20256;&#36882;&#33021;&#21147;&#36890;&#24120;&#36739;&#20302;&#12290;&#30001;&#20110;&#23545;&#25239;&#24615;&#36716;&#31227;&#23545;&#23454;&#38469;&#24212;&#29992;&#36896;&#25104;&#26356;&#20005;&#37325;&#30340;&#23041;&#32961;&#65292;&#22240;&#27492;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36755;&#20837;&#36716;&#25442;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25915;&#20987;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#30340;&#20960;&#20010;&#24494;&#23567;&#25913;&#21464;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25915;&#20987;&#24615;&#33021;&#65292;&#20363;&#22914;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#12290;&#22522;&#20110;&#23545;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20180;&#32454;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#21160;&#37327;&#21021;&#22987;&#21270;&#12289;&#23450;&#26399;&#35843;&#25972;&#27493;&#38271;&#12289;&#23545;&#25239;&#31034;&#20363;&#12289;&#22522;&#20110;&#35889;&#30340;&#36755;&#20837;&#36716;&#25442;&#20197;&#21450;&#20960;&#31181;&#38598;&#25104;&#31574;&#30053;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely known to be vulnerable to adversarial examples. However, vanilla adversarial examples generated under the white-box setting often exhibit low transferability across different models. Since adversarial transferability poses more severe threats to practical applications, various approaches have been proposed for better transferability, including gradient-based, input transformation-based, and model-related attacks, \etc. In this work, we find that several tiny changes in the existing adversarial attacks can significantly affect the attack performance, \eg, the number of iterations and step size. Based on careful studies of existing adversarial attacks, we propose a bag of tricks to enhance adversarial transferability, including momentum initialization, scheduled step size, dual example, spectral-based input transformation, and several ensemble strategies. Extensive experiments on the ImageNet dataset validate the high effectiveness of our proposed tricks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#25945;&#24072;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#32858;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08732</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#29992;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#25945;&#24072;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22270;&#20687;&#32858;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#65292;&#36890;&#24120;&#35748;&#20026;&#25945;&#24072;&#30340;&#35282;&#33394;&#26159;&#25552;&#20379;&#29992;&#20110;&#23398;&#29983;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26410;&#30693;&#36125;&#21494;&#26031;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#65288;BCPD&#65289;&#30340;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#65288;MLL&#65289;&#26041;&#27861;&#35757;&#32451;&#25945;&#24072;&#26469;&#33719;&#24471;&#27492;&#20272;&#35745;&#12290;&#20026;&#20102;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#20272;&#35745;&#65292;&#26412;&#25991;&#23558;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;BCPD&#30340;&#20272;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#26368;&#22823;CMI&#65288;MCMI&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;MCMI&#20272;&#35745;&#20013;&#65292;&#25945;&#24072;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;&#36890;&#36807;Eigen-CAM&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#25945;&#24072;&#30340;CMI&#20540;&#21487;&#20197;&#20351;&#25945;&#24072;&#22312;&#22270;&#20687;&#32858;&#31867;&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36890;&#36807;&#36827;&#34892;&#19968;&#31995;&#21015;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#36890;&#36807;MCMI&#20272;&#35745;&#35757;&#32451;&#30340;&#25945;&#24072;&#32780;&#19981;&#26159;&#36890;&#36807;MLL&#20272;&#35745;&#35757;&#32451;&#30340;&#25945;&#24072;&#65292;&#22312;&#21508;&#31181;&#29366;&#24577;&#19979;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.08727</link><description>&lt;p&gt;
MA2GCN: &#20351;&#29992;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#20132;&#36890;&#39044;&#27979;&#30340;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#65292;&#19981;&#20381;&#36182;&#20110;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#25552;&#21462;&#28789;&#27963;&#19988;&#20934;&#30830;&#30340;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#19981;&#20165;&#23548;&#33268;&#24040;&#22823;&#30340;&#32463;&#27982;&#25439;&#22833;&#65292;&#32780;&#19988;&#20005;&#37325;&#21361;&#23475;&#22478;&#24066;&#29615;&#22659;&#12290;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#19981;&#21516;&#36335;&#27573;&#19978;&#30340;&#20256;&#24863;&#22120;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#36895;&#24230;&#65292;&#20998;&#26512;&#26576;&#20010;&#36947;&#36335;&#27573;&#30340;&#20132;&#36890;&#25317;&#22581;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#30340;&#22266;&#23450;&#20301;&#32622;&#65292;&#24456;&#38590;&#25366;&#25496;&#26032;&#30340;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#25552;&#21462;&#20132;&#36890;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#27169;&#22411;&#8212;&#8212;&#22810;&#37051;&#25509;&#20851;&#31995;&#27880;&#24847;&#21147;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MA2GCN&#65289;&#12290;&#35813;&#27169;&#22411;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#36716;&#21270;&#20026;&#32593;&#26684;&#24418;&#24335;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#19981;&#21516;&#32593;&#26684;&#20043;&#38388;&#30340;&#27969;&#21160;&#24615;&#25552;&#20986;&#20102;&#36710;&#36742;&#36827;&#20986;&#30697;&#38453;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
The problem of traffic congestion not only causes a large amount of economic losses, but also seriously endangers the urban environment. Predicting traffic congestion has important practical significance. So far, most studies have been based on historical data from sensors placed on different roads to predict future traffic flow and speed, to analyze the traffic congestion conditions of a certain road segment. However, due to the fixed position of sensors, it is difficult to mine new information. On the other hand, vehicle trajectory data is more flexible and can extract traffic information as needed. Therefore, we proposed a new traffic congestion prediction model - Multi Adjacency relationship Attention Graph Convolutional Networks(MA2GCN). This model transformed vehicle trajectory data into graph structured data in grid form, and proposed a vehicle entry and exit matrix based on the mobility between different grids. At the same time, in order to improve the performance of the model,
&lt;/p&gt;</description></item><item><title>HierSFL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21106;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#27169;&#22411;&#24182;&#23450;&#24615;&#25351;&#23548;&#26368;&#20339;&#32858;&#21512;&#26102;&#38388;&#26694;&#26550;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08723</link><description>&lt;p&gt;
HierSFL&#65306;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21106;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08723
&lt;/p&gt;
&lt;p&gt;
HierSFL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#21106;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#27169;&#22411;&#24182;&#23450;&#24615;&#25351;&#23548;&#26368;&#20339;&#32858;&#21512;&#26102;&#38388;&#26694;&#26550;&#65292;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#30340;&#39640;&#35201;&#27714;&#20351;&#24471;&#20869;&#23384;&#25110;&#24102;&#23485;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#21442;&#19982;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#20998;&#21106;&#32852;&#21512;&#23398;&#20064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#23558;&#20013;&#38388;&#27169;&#22411;&#35757;&#32451;&#32467;&#26524;&#19978;&#20256;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#21327;&#21516;&#30340;&#26381;&#21153;&#22120;-&#23458;&#25143;&#31471;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#36164;&#28304;&#21463;&#38480;&#23458;&#25143;&#31471;&#30340;&#21442;&#19982;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Hierarchical Split Federated Learning&#65288;HierSFL&#65289;&#65292;&#23427;&#22312;&#36793;&#32536;&#21644;&#20113;&#38454;&#27573;&#21512;&#24182;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#24615;&#25351;&#23548;&#26469;&#30830;&#23450;&#26368;&#20339;&#32858;&#21512;&#26102;&#38388;&#26694;&#26550;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#32423;&#21035;&#23454;&#26045;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is a promising approach for learning from user data while preserving data privacy. However, the high requirements of the model training process make it difficult for clients with limited memory or bandwidth to participate. To tackle this problem, Split Federated Learning is utilized, where clients upload their intermediate model training outcomes to a cloud server for collaborative server-client model training. This methodology facilitates resource-constrained clients' participation in model training but also increases the training time and communication overhead. To overcome these limitations, we propose a novel algorithm, called Hierarchical Split Federated Learning (HierSFL), that amalgamates models at the edge and cloud phases, presenting qualitative directives for determining the best aggregation timeframes to reduce computation and communication expenses. By implementing local differential privacy at the client and edge server levels, we enhance privacy during 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#26399;&#26395;&#32602;&#29260; (xB) &#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#31639;&#36275;&#29699;&#29359;&#35268;&#23548;&#33268;&#40644;&#29260;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#26512;&#29699;&#38431;&#21644;&#29699;&#21592;&#29359;&#35268;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08718</link><description>&lt;p&gt;
&#20351;&#29992;&#26399;&#26395;&#32602;&#29260; (xB) &#27169;&#22411;&#30740;&#31350;&#36275;&#29699;&#29359;&#35268;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Investigating Fouling Efficiency in Football Using Expected Booking (xB) Model. (arXiv:2401.08718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#26399;&#26395;&#32602;&#29260; (xB) &#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#31639;&#36275;&#29699;&#29359;&#35268;&#23548;&#33268;&#40644;&#29260;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#26512;&#29699;&#38431;&#21644;&#29699;&#21592;&#29359;&#35268;&#31574;&#30053;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26399;&#26395;&#32602;&#29260; (xB) &#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20272;&#31639;&#36275;&#29699;&#29359;&#35268;&#23548;&#33268;&#40644;&#29260;&#30340;&#21487;&#33021;&#24615;&#30340;&#26032;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#23454;&#39564;&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#27169;&#22411;&#22312;&#22686;&#21152;&#29305;&#24449;&#21644;&#25193;&#23637;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#23545;2022&#24180;FIFA&#19990;&#30028;&#26479;&#25968;&#25454;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#22312;&#25552;&#20379;&#20851;&#20110;&#29699;&#38431;&#21644;&#29699;&#21592;&#29359;&#35268;&#31574;&#30053;&#30340;&#27934;&#23519;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#23454;&#38469;&#30340;&#38450;&#23432;&#34920;&#29616;&#30456;&#19968;&#33268;&#12290;xB&#27169;&#22411;&#22635;&#34917;&#20102;&#29359;&#35268;&#25928;&#29575;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#65292;&#24378;&#35843;&#20102;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38450;&#23432;&#31574;&#30053;&#12290;&#36890;&#36807;&#25972;&#21512;&#20840;&#38754;&#30340;&#25968;&#25454;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Expected Booking (xB) model, a novel metric designed to estimate the likelihood of a foul resulting in a yellow card in football. Through three iterative experiments, employing ensemble methods, the model demonstrates improved performance with additional features and an expanded dataset. Analysis of FIFA World Cup 2022 data validates the model's efficacy in providing insights into team and player fouling tactics, aligning with actual defensive performance. The xB model addresses a gap in fouling efficiency examination, emphasizing defensive strategies which often overlooked. Further enhancements are suggested through the incorporation of comprehensive data and spatial features.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36873;&#25321;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#36827;&#34892;&#36845;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.08715</link><description>&lt;p&gt;
&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Selecting Subsets of Source Data for Transfer Learning with Applications in Metal Additive Manufacturing. (arXiv:2401.08715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#29992;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#36873;&#25321;&#23376;&#38598;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#36827;&#34892;&#36845;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#23646;&#22686;&#26448;&#21046;&#36896;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20174;&#28304;&#39046;&#22495;&#65288;&#20363;&#22914;&#23436;&#25104;&#30340;&#25171;&#21360;&#65289;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#30446;&#26631;&#39046;&#22495;&#65288;&#20363;&#22914;&#26032;&#30340;&#25171;&#21360;&#65289;&#20013;&#30340;&#24314;&#27169;&#24615;&#33021;&#12290;&#24403;&#21069;&#30340;&#24212;&#29992;&#31243;&#24207;&#30452;&#25509;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#25152;&#26377;&#21487;&#35775;&#38382;&#30340;&#28304;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#23547;&#25214;&#36866;&#24403;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#38024;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#36890;&#36807;&#31354;&#38388;&#21644;&#27169;&#22411;&#36317;&#31163;&#24230;&#37327;&#26469;&#21051;&#30011;&#12290;&#22522;&#20110;Pareto frontier&#30340;&#28304;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#36890;&#36807;&#20004;&#20010;&#30456;&#20284;&#24230;&#36317;&#31163;&#24230;&#37327;&#23450;&#20041;&#30340;Pareto frontier&#19978;&#30340;&#28304;&#25968;&#25454;&#34987;&#36845;&#20195;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#34987;&#38598;&#25104;&#21040;&#22522;&#20110;&#23454;&#20363;&#30340;TL&#26041;&#27861;&#65288;&#20915;&#31574;&#26641;&#22238;&#24402;&#27169;&#22411;&#65289;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;TL&#26041;&#27861;&#65288;&#24494;&#35843;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering data insufficiency in metal additive manufacturing (AM), transfer learning (TL) has been adopted to extract knowledge from source domains (e.g., completed printings) to improve the modeling performance in target domains (e.g., new printings). Current applications use all accessible source data directly in TL with no regard to the similarity between source and target data. This paper proposes a systematic method to find appropriate subsets of source data based on similarities between the source and target datasets for a given set of limited target domain data. Such similarity is characterized by the spatial and model distance metrics. A Pareto frontier-based source data selection method is developed, where the source data located on the Pareto frontier defined by two similarity distance metrics are selected iteratively. The method is integrated into an instance-based TL method (decision tree regression model) and a model-based TL method (fine-tuned artificial neural network)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;SEER&#25968;&#25454;&#38598;&#30740;&#31350;&#20102;&#24180;&#36731;&#24180;&#40836;&#23545;&#19977;&#38452;&#24615;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24180;&#36731;&#24180;&#40836;&#22312;TNBC&#24739;&#32773;&#30340;&#23384;&#27963;&#33021;&#21147;&#20013;&#36215;&#30528;&#26174;&#33879;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.08712</link><description>&lt;p&gt;
&#24180;&#36731;&#19977;&#38452;&#24615;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Survival Analysis of Young Triple-Negative Breast Cancer Patients. (arXiv:2401.08712v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08712
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;SEER&#25968;&#25454;&#38598;&#30740;&#31350;&#20102;&#24180;&#36731;&#24180;&#40836;&#23545;&#19977;&#38452;&#24615;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#29983;&#23384;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24180;&#36731;&#24180;&#40836;&#22312;TNBC&#24739;&#32773;&#30340;&#23384;&#27963;&#33021;&#21147;&#20013;&#36215;&#30528;&#26174;&#33879;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#39044;&#21518;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#31181;&#30142;&#30149;&#22312;40&#23681;&#20197;&#19978;&#30340;&#22919;&#22899;&#20013;&#26356;&#20026;&#24120;&#35265;&#65292;&#20294;&#22312;40&#23681;&#20197;&#19979;&#30340;&#22919;&#22899;&#20013;&#32597;&#35265;&#65292;&#32654;&#22269;&#21482;&#26377;&#19981;&#21040;5&#65285;&#30340;&#30149;&#20363;&#20986;&#29616;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24180;&#36731;&#22919;&#22899;&#30340;&#39044;&#21518;&#36739;&#24046;&#65292;&#36825;&#31181;&#24046;&#24322;&#22240;&#31181;&#26063;&#32780;&#24322;&#12290;&#20083;&#33146;&#30284;&#26681;&#25454;&#38604;&#28608;&#32032;&#12289;&#23381;&#28608;&#32032;&#21644;HER2&#31561;&#21463;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#19977;&#38452;&#24615;&#20083;&#33146;&#30284;&#65288;TNBC&#65289;&#32570;&#20047;&#36825;&#20123;&#21463;&#20307;&#65292;&#32422;&#21344;&#30149;&#20363;&#30340;15&#65285;&#65292;&#22312;&#24180;&#36731;&#24739;&#32773;&#20013;&#26356;&#24120;&#35265;&#65292;&#36890;&#24120;&#23548;&#33268;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24180;&#40836;&#23545;TNBC&#39044;&#21518;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#32959;&#30244;&#20998;&#32423;&#12289;&#22823;&#23567;&#21644;&#28107;&#24052;&#32467;&#29366;&#24577;&#31561;&#22240;&#32032;&#34987;&#30740;&#31350;&#20854;&#23545;TNBC&#20020;&#24202;&#32467;&#26524;&#30340;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#23545;&#20110;&#24180;&#40836;&#30456;&#20851;&#24046;&#24322;&#30340;&#32467;&#35770;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;SEER&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#24180;&#36731;&#24180;&#40836;&#23545;TNBC&#24739;&#32773;&#30340;&#23384;&#27963;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#30830;&#23450;&#24180;&#40836;&#26159;&#21542;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#39044;&#21518;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24180;&#36731;&#24180;&#40836;&#22312;TNBC&#24739;&#32773;&#30340;&#23384;&#27963;&#33021;&#21147;&#20013;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer prognosis is crucial for effective treatment, with the disease more common in women over 40 years old but rare under 40 years old, where less than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in younger women, which varies by ethnicity. Breast cancers are classified based on receptors like estrogen, progesterone, and HER2. Triple-negative breast cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases and is more prevalent in younger patients, often resulting in poorer outcomes. Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like age, race, tumor grade, size, and lymph node status are studied for their role in TNBC's clinical outcomes, but current research is inconclusive about age-related differences. This study uses SEER data set to examine the influence of younger age on survivability in TNBC patients, aiming to determine if age is a significant prognostic factor. Our experimental results on S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoupled Prototype Learning (DPL)&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#20013;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08703</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08703
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decoupled Prototype Learning (DPL)&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#20013;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25345;&#32493;&#23558;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#22495;&#30340;&#20219;&#21153;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20272;&#35745;&#30340;&#20266;&#26631;&#31614;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#24494;&#35843;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#27599;&#20010;&#26679;&#26412;&#30340;&#20998;&#31867;&#38169;&#35823;&#26368;&#23567;&#21270;&#20250;&#20351;&#20132;&#21449;&#29109;&#25439;&#22833;&#23545;&#26631;&#31614;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32806;&#26679;&#26412;&#23398;&#20064;&#65288;DPL&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#26679;&#26412;&#21407;&#22411;&#20026;&#20013;&#24515;&#30340;&#25439;&#22833;&#35745;&#31639;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#31867;&#21407;&#22411;&#30340;&#20248;&#21270;&#12290;&#23545;&#20110;&#27599;&#20010;&#31867;&#21407;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#26041;&#24335;&#20943;&#23567;&#20854;&#19982;&#27491;&#26679;&#26412;&#30340;&#36317;&#31163;&#65292;&#24182;&#22686;&#21152;&#20854;&#19982;&#36127;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#23545;&#22122;&#22768;&#20266;&#26631;&#31614;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;DPL&#22312;TTA&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#23567;&#25209;&#37327;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#26356;&#26032;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#26102;&#20351;&#29992;&#35760;&#24518;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#30340;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#38416;&#36848;&#20102;&#19977;&#20010;&#21487;&#33021;&#30340;&#38169;&#35823;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.08702</link><description>&lt;p&gt;
&#25105;&#20204;&#30495;&#30340;&#38656;&#35201;&#25968;&#25454;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do We Really Even Need Data?. (arXiv:2401.08702v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#30340;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#38416;&#36848;&#20102;&#19977;&#20010;&#21487;&#33021;&#30340;&#38169;&#35823;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#30340;&#26222;&#21450;&#65292;&#31185;&#23398;&#23478;&#22312;&#25968;&#25454;&#25910;&#38598;&#26041;&#38754;&#38754;&#20020;&#30528;&#26032;&#30340;&#38556;&#30861;&#65288;&#20363;&#22914;&#25104;&#26412;&#19978;&#21319;&#12289;&#35843;&#26597;&#21709;&#24212;&#29575;&#19979;&#38477;&#65289;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#39044;&#35757;&#32451;&#31639;&#27861;&#30340;&#39044;&#27979;&#20316;&#20026;&#22240;&#21464;&#37327;&#12290;&#34429;&#28982;&#20174;&#36130;&#21153;&#21644;&#21518;&#21220;&#30340;&#35282;&#24230;&#26469;&#30475;&#36825;&#26679;&#20570;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#20351;&#29992;&#26631;&#20934;&#30340;&#25512;&#35770;&#24037;&#20855;&#21487;&#33021;&#20250;&#22312;&#26367;&#25442;&#30495;&#23454;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#20540;&#26102;&#35823;&#20195;&#34920;&#33258;&#21464;&#37327;&#19982;&#25152;&#20851;&#24515;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#25152;&#35859;&#8220;&#21518;&#39044;&#27979;&#25512;&#26029;&#8221;&#38382;&#39064;&#30340;&#32479;&#35745;&#25361;&#25112;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#38416;&#26126;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#19977;&#20010;&#38169;&#35823;&#26469;&#28304;&#65306;&#65288;i&#65289;&#39044;&#27979;&#32467;&#26524;&#19982;&#20854;&#30495;&#23454;&#26410;&#35266;&#23519;&#21040;&#30340;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#37325;&#26032;&#37319;&#26679;&#25110;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#36866;&#24403;&#22320;&#23558;&#39044;&#27979;&#32467;&#26524;&#30340;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#26368;&#32456;&#30340;&#25512;&#26029;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g. rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as outcome variables. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to this so-called ``post-prediction inference'' problem and elucidate three potential sources of error: (i) the relationship between predicted outcomes and their true, unobserved counterparts, (ii) robustness of the machine learning model to resampling or uncertainty about the training data, and (iii) appropriately propagating not just bias but also uncertainty from predictions into the ultimate inference
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24367;&#22836;&#22411;&#27844;&#21387;&#31649;&#30340;&#20248;&#21270;&#24037;&#20316;&#27969;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#36827;&#34892;&#35780;&#20272;&#21644;&#35774;&#35745;&#12290;&#20174;&#35780;&#20272;&#32467;&#26524;&#20013;&#25214;&#21040;&#20102;&#26368;&#20339;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21333;&#30446;&#26631;&#20248;&#21270;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30446;&#26631;&#30340;&#24433;&#21709;&#21644;&#32508;&#21512;&#24433;&#21709;&#23545;&#20110;&#27844;&#21387;&#31649;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.08700</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24367;&#22836;&#22411;&#27844;&#21387;&#31649;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Optimisation of Elbow-Type Draft Tube Using Neural Network Surrogates. (arXiv:2401.08700v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#24367;&#22836;&#22411;&#27844;&#21387;&#31649;&#30340;&#20248;&#21270;&#24037;&#20316;&#27969;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#36827;&#34892;&#35780;&#20272;&#21644;&#35774;&#35745;&#12290;&#20174;&#35780;&#20272;&#32467;&#26524;&#20013;&#25214;&#21040;&#20102;&#26368;&#20339;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21333;&#30446;&#26631;&#20248;&#21270;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30446;&#26631;&#30340;&#24433;&#21709;&#21644;&#32508;&#21512;&#24433;&#21709;&#23545;&#20110;&#27844;&#21387;&#31649;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32508;&#21512;&#35780;&#20272;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#35774;&#35745;&#24367;&#22836;&#22411;&#27844;&#21387;&#31649;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20248;&#21270;&#24037;&#20316;&#27969;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#21033;&#29992;&#20174;&#25968;&#20540;&#27169;&#25311;&#33719;&#24471;&#30340;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#21697;&#12290;&#20351;&#29992;&#26367;&#20195;&#21697;&#33021;&#22815;&#26356;&#28789;&#27963;&#12289;&#26356;&#24555;&#36895;&#22320;&#35780;&#20272;&#26032;&#39062;&#35774;&#35745;&#12290;&#25104;&#21151;&#30340;&#22522;&#20110;&#21382;&#21490;&#35760;&#24405;&#30340;&#33258;&#36866;&#24212;&#24046;&#20998;&#36827;&#21270;&#19982;&#32447;&#24615;&#20943;&#23569;&#21644;&#22522;&#20110;&#20998;&#35299;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#34987;&#30830;&#23450;&#20026;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#65292;&#24182;&#29992;&#20110;&#30830;&#23450;&#21333;&#30446;&#26631;&#20248;&#21270;&#20013;&#19981;&#21516;&#30446;&#26631;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#36825;&#20123;&#30446;&#26631;&#30340;&#32508;&#21512;&#24433;&#21709;&#23545;&#20110;&#27844;&#21387;&#31649;&#35774;&#35745;&#30340;&#24433;&#21709;&#12290;&#21333;&#30446;&#26631;&#31639;&#27861;&#30340;&#32467;&#26524;&#19982;&#22810;&#30446;&#26631;&#31639;&#27861;&#22312;&#20998;&#21035;&#32771;&#34385;&#30446;&#26631;&#26102;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to provide a comprehensive assessment of single-objective and multi-objective optimisation algorithms for the design of an elbow-type draft tube, as well as to introduce a computationally efficient optimisation workflow. The proposed workflow leverages deep neural network surrogates trained on data obtained from numerical simulations. The use of surrogates allows for a more flexible and faster evaluation of novel designs. The success history-based adaptive differential evolution with linear reduction and the multi-objective evolutionary algorithm based on decomposition were identified as the best-performing algorithms and used to determine the influence of different objectives in the single-objective optimisation and their combined impact on the draft tube design in the multi-objective optimisation. The results for the single-objective algorithm are consistent with those of the multi-objective algorithm when the objectives are considered separately. Multi-objective appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#27969;&#31243;&#12289;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;QoR&#25351;&#26631;&#65292;&#24182;&#22312;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08696</link><description>&lt;p&gt;
&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis with GNNs. (arXiv:2401.08696v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#23618;&#27425;&#32508;&#21512;&#20013;&#20351;&#29992;GNN&#36827;&#34892;&#28304;-&#21518;&#21521;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#20998;&#23618;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#27969;&#31243;&#12289;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;QoR&#25351;&#26631;&#65292;&#24182;&#22312;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23618;&#27425;&#32508;&#21512;&#65288;HLS&#65289;&#36890;&#36807;&#36991;&#20813;RTL&#32534;&#31243;&#26174;&#30528;&#21152;&#24555;&#20102;&#30828;&#20214;&#35774;&#35745;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32771;&#34385;&#21518;&#21521;&#36335;&#32467;&#26524;&#36136;&#37327;&#65288;QoR&#65289;&#26102;&#65292;HLS&#30340;&#21608;&#36716;&#26102;&#38388;&#26174;&#33879;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;FPGA HLS&#30340;&#20998;&#23618;&#21518;&#21521;&#36335;QoR&#39044;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#30452;&#25509;&#20174;C / C ++&#31243;&#24207;&#20272;&#35745;&#24310;&#36831;&#21644;&#21518;&#21521;&#36335;&#36164;&#28304;&#20351;&#29992;&#30340;&#24314;&#27169;&#27969;&#31243;&#65307;&#65288;2&#65289;&#26377;&#25928;&#34920;&#31034;&#28304;&#20195;&#30721;&#30340;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#22270;&#20197;&#21450;HLS&#20266;&#25351;&#20196;&#30340;&#24433;&#21709;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65307;&#65288;3&#65289;&#33021;&#22815;&#25429;&#33719;&#24490;&#29615;&#23618;&#27425;&#24433;&#21709;&#30340;&#20998;&#23618;GNN&#35757;&#32451;&#21644;&#39044;&#27979;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;QoR&#25351;&#26631;&#19979;&#30340;&#39044;&#27979;&#35823;&#24046;&#23567;&#20110;10%&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65292;&#22312;HLS&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-level synthesis (HLS) notably speeds up the hardware design process by avoiding RTL programming. However, the turnaround time of HLS increases significantly when post-route quality of results (QoR) are considered during optimization. To tackle this issue, we propose a hierarchical post-route QoR prediction approach for FPGA HLS, which features: (1) a modeling flow that directly estimates latency and post-route resource usage from C/C++ programs; (2) a graph construction method that effectively represents the control and data flow graph of source code and effects of HLS pragmas; and (3) a hierarchical GNN training and prediction method capable of capturing the impact of loop hierarchies. Experimental results show that our method presents a prediction error of less than 10% for different types of QoR metrics, which gains tremendous improvement compared with the state-of-the-art GNN methods. By adopting our proposed methodology, the runtime for design space exploration in HLS is shor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#38134;&#34892;&#19994;&#20013;&#35299;&#20915;&#20559;&#35265;&#20197;&#23454;&#29616;&#20844;&#24179;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26080;&#32541;&#25972;&#21512;&#20844;&#24179;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#30417;&#30563;&#65292;&#26500;&#24314;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#25991;&#21270;&#65292;&#20197;&#36981;&#23432;&#35268;&#23450;&#24182;&#31526;&#21512;&#20154;&#26435;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.08691</link><description>&lt;p&gt;
&#22312;&#38134;&#34892;&#19994;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#35299;&#20915;&#20559;&#35265;&#20197;&#23454;&#29616;&#20844;&#24179;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in Banking: Addressing Bias for Fair Decision-Making. (arXiv:2401.08691v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#38134;&#34892;&#19994;&#20013;&#35299;&#20915;&#20559;&#35265;&#20197;&#23454;&#29616;&#20844;&#24179;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26080;&#32541;&#25972;&#21512;&#20844;&#24179;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#30417;&#30563;&#65292;&#26500;&#24314;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#25991;&#21270;&#65292;&#20197;&#36981;&#23432;&#35268;&#23450;&#24182;&#31526;&#21512;&#20154;&#26435;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#26102;&#20195;&#65292;&#23545;&#20449;&#20219;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#24378;&#28872;&#12290;&#26412;&#35770;&#25991;&#23545;&#20559;&#35265;&#21644;&#20844;&#24179;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25506;&#35752;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;&#38134;&#34892;&#19994;&#20869;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#20197;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20915;&#31574;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#30417;&#30563;&#30340;&#26080;&#32541;&#25972;&#21512;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#26368;&#32456;&#24418;&#25104;&#20102;&#25152;&#35859;&#30340;&#8220;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#8221;&#12290;&#36825;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#31526;&#21512;&#20154;&#24037;&#26234;&#33021;&#35268;&#23450;&#21644;&#26222;&#19990;&#20154;&#26435;&#26631;&#20934;&#30340;&#20225;&#19994;&#25991;&#21270;&#26102;&#65292;&#35299;&#20915;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#39046;&#22495;&#12290;&#22914;&#20170;&#65292;&#23558;&#20262;&#29702;&#21407;&#21017;&#34701;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24320;&#21457;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#23545;&#20110;&#36981;&#23432;&#21363;&#23558;&#21040;&#26469;&#30340;&#35268;&#23450;&#38750;&#24120;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era characterized by the pervasive integration of artificial intelligence into decision-making processes across diverse industries, the demand for trust has never been more pronounced. This thesis embarks on a comprehensive exploration of bias and fairness, with a particular emphasis on their ramifications within the banking sector, where AI-driven decisions bear substantial societal consequences. In this context, the seamless integration of fairness, explainability, and human oversight is of utmost importance, culminating in the establishment of what is commonly referred to as "Responsible AI". This emphasizes the critical nature of addressing biases within the development of a corporate culture that aligns seamlessly with both AI regulations and universal human rights standards, particularly in the realm of automated decision-making systems. Nowadays, embedding ethical principles into the development, training, and deployment of AI models is crucial for compliance with forthcom
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#36127;&#26679;&#26412;&#37319;&#26679;&#20559;&#32622;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#26032;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08690</link><description>&lt;p&gt;
&#36127;&#37319;&#26679;&#30699;&#27491;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning with Negative Sampling Correction. (arXiv:2401.08690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08690
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30699;&#27491;&#36127;&#26679;&#26412;&#37319;&#26679;&#20559;&#32622;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PUCL&#30340;&#26032;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#22810;&#20010;&#36127;&#26679;&#26412;&#19982;&#27491;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#12290;&#22312;&#26631;&#20934;&#30340;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#27491;&#36127;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#25913;&#36827;&#27491;&#26679;&#26412;&#37319;&#26679;&#65292;&#32780;&#36127;&#26679;&#26412;&#37319;&#26679;&#21017;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#20107;&#23454;&#19978;&#65292;&#29983;&#25104;&#30340;&#36127;&#26679;&#26412;&#36890;&#24120;&#20250;&#21463;&#21040;&#27491;&#26679;&#26412;&#30340;&#27745;&#26579;&#65292;&#36825;&#23548;&#33268;&#20102;&#20559;&#32622;&#25439;&#22833;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#30699;&#27491;&#36127;&#26679;&#26412;&#37319;&#26679;&#20559;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27491;&#36127;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#65288;PUCL&#65289;&#30340;&#26032;&#22411;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;PUCL&#23558;&#29983;&#25104;&#30340;&#36127;&#26679;&#26412;&#35270;&#20026;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#27491;&#26679;&#26412;&#30340;&#20449;&#24687;&#26469;&#32416;&#27491;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;PUCL&#20013;&#20351;&#29992;&#30340;&#26657;&#27491;&#25439;&#22833;&#19982;&#26080;&#20559;&#24046;&#30340;&#23545;&#27604;&#25439;&#22833;&#30456;&#27604;&#65292;&#21482;&#24341;&#20837;&#20102;&#21487;&#24573;&#30053;&#30340;&#20559;&#24046;&#12290;PUCL&#21487;&#20197;&#24212;&#29992;&#20110;... &#65288;&#21407;&#25991;&#30465;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#65292;&#33719;&#24471;&#31283;&#23450;&#30340;&#22122;&#22768;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;OOD&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.08689</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#22122;&#22768;&#36827;&#34892;ODI&#65288;&#22806;&#20998;&#24067;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
NODI: Out-Of-Distribution Detection with Noise from Diffusion. (arXiv:2401.08689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#65292;&#33719;&#24471;&#31283;&#23450;&#30340;&#22122;&#22768;&#21521;&#37327;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;OOD&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26159;&#23433;&#20840;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#24182;&#24320;&#21457;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;OOD&#20998;&#25968;&#26102;&#23545;&#20869;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;OOD&#20998;&#25968;&#26159;&#26681;&#25454;&#20869;&#20998;&#24067;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#35745;&#31639;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#22270;&#20687;&#32534;&#30721;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#26816;&#26597;&#36825;&#20123;&#26041;&#27861;&#23545;&#19981;&#21516;&#35757;&#32451;&#26041;&#27861;&#21644;&#26550;&#26500;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#36807;&#31243;&#24341;&#20837;&#21040;ODD&#20219;&#21153;&#20013;&#12290;&#25193;&#25955;&#27169;&#22411;&#23558;&#25972;&#20010;&#35757;&#32451;&#38598;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#39044;&#27979;&#30340;&#22122;&#22768;&#21521;&#37327;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22122;&#22768;&#21521;&#37327;&#65288;&#31283;&#23450;&#28857;&#65289;&#30340;&#38381;&#24335;&#35299;&#12290;&#28982;&#21518;&#65292;&#23558;&#22122;&#22768;&#21521;&#37327;&#36716;&#21270;&#20026;&#25105;&#20204;&#30340;OOD&#20998;&#25968;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#28145;&#24230;&#27169;&#22411;&#39044;&#27979;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial part of deploying machine learning models safely. It has been extensively studied with a plethora of methods developed in the literature. This problem is tackled with an OOD score computation, however, previous methods compute the OOD scores with limited usage of the in-distribution dataset. For instance, the OOD scores are computed with information from a small portion of the in-distribution data. Furthermore, these methods encode images with a neural image encoder. The robustness of these methods is rarely checked with respect to image encoders of different training methods and architectures. In this work, we introduce the diffusion process into the OOD task. The diffusion model integrates information on the whole training set into the predicted noise vectors. What's more, we deduce a closed-form solution for the noise vector (stable point). Then the noise vector is converted into our OOD score, we test both the deep model predicted no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;Surfbeat&#21644;XBeach&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;cGAN&#23558;&#27874;&#28010;&#20914;&#19978;&#30340;&#22270;&#20687;&#20174;XBSB&#27169;&#24335;&#26144;&#23556;&#21040;XBNH&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#30340;&#26102;&#21464;&#27874;&#28010;&#20914;&#19978;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.08684</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26102;&#21464;&#27874;&#28010;&#20914;&#19978;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Physics-informed machine learning model for time-dependent wave runup prediction. (arXiv:2401.08684v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;Surfbeat&#21644;XBeach&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;cGAN&#23558;&#27874;&#28010;&#20914;&#19978;&#30340;&#22270;&#20687;&#20174;XBSB&#27169;&#24335;&#26144;&#23556;&#21040;XBNH&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#20934;&#30830;&#30340;&#26102;&#21464;&#27874;&#28010;&#20914;&#19978;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#28010;&#20914;&#19978;&#26159;&#24433;&#21709;&#28023;&#28393;&#27946;&#27700;&#12289;&#28023;&#23736;&#32447;&#21464;&#21270;&#21644;&#27839;&#28023;&#32467;&#26500;&#25439;&#22351;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27668;&#20505;&#21464;&#21270;&#36824;&#39044;&#35745;&#23558;&#22686;&#21152;&#27874;&#28010;&#20914;&#19978;&#23545;&#27839;&#28023;&#22320;&#21306;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#27874;&#28010;&#20914;&#19978;&#23545;&#20110;&#26377;&#25928;&#30340;&#27839;&#28023;&#24037;&#31243;&#35774;&#35745;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#31243;&#30340;&#22266;&#26377;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#39044;&#27979;&#26102;&#21464;&#30340;&#27874;&#28010;&#20914;&#19978;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#27169;&#25311;&#26102;&#24207;&#27874;&#28010;&#20914;&#19978;&#12290;&#35813;&#26041;&#27861;&#23558;Surfbeat (XBSB)&#27169;&#24335;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;XBeach&#27169;&#22411;&#30340;&#38750;&#38745;&#27700;&#21387;&#65288;XBNH&#65289;&#27169;&#24335;&#30340;&#20934;&#30830;&#24615;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGAN&#65289;&#23558;XBSB&#30340;&#27874;&#28010;&#20914;&#19978;&#22270;&#20687;&#34920;&#31034;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;XBNH&#22270;&#20687;&#34920;&#31034;&#12290;&#36825;&#20123;&#22270;&#20687;&#34920;&#31034;&#20351;&#29992;&#20809;&#27969;&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#21464;&#25442;&#24182;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20197;&#23454;&#29616;&#27874;&#28010;&#20914;&#19978;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wave runup is a critical factor affecting coastal flooding, shoreline changes, and damage to coastal structures. Climate change is also expected to amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave runup estimation is essential for effective coastal engineering design and management. However, predicting the time-dependent wave runup is challenging due to the intrinsic nonlinearities and non-stationarity of the process, even with the use of the most advanced machine learning techniques. In this study, a physics-informed machine learning-based approach is proposed to efficiently and accurately simulate time-series wave runup. The methodology combines the computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional generative adversarial network (cGAN) is used to map the image representation of wave runup from XBSB to the corresponding image from XBNH. These images ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30828;&#20214;&#35774;&#35745;&#20013;&#24555;&#36895;&#29983;&#25104;RTL&#20195;&#30721;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#22914;&#20309;&#25552;&#20379;&#21151;&#33021;&#12289;&#20248;&#21270;&#21644;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.08683</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;RTL&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30828;&#20214;&#35774;&#35745;&#20013;&#24555;&#36895;&#29983;&#25104;RTL&#20195;&#30721;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#22914;&#20309;&#25552;&#20379;&#21151;&#33021;&#12289;&#20248;&#21270;&#21644;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30828;&#20214;&#35774;&#35745;&#21644;&#20248;&#21270;&#38656;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#65292;&#38656;&#35201;&#30456;&#24403;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#20381;&#36182;&#20110;&#24050;&#24314;&#31435;&#30340;&#35774;&#35745;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#31616;&#21270;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25552;&#31034;&#25509;&#21463;&#39640;&#23618;&#35774;&#35745;&#35268;&#33539;&#65292;&#29983;&#25104;&#30456;&#24212;&#30340;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#20195;&#30721;&#12290;&#33021;&#22815;&#22312;RTL&#20195;&#30721;&#29983;&#25104;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21152;&#24555;&#20102;&#35774;&#35745;&#36845;&#20195;&#21608;&#26399;&#65292;&#36824;&#20415;&#20110;&#25506;&#32034;&#20256;&#32479;&#25216;&#26415;&#38590;&#20197;&#22788;&#29702;&#30340;&#35774;&#35745;&#31354;&#38388;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19981;&#36275;&#65292;&#24182;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26102;&#20135;&#29983;&#21151;&#33021;&#12289;&#20248;&#21270;&#19988;&#31526;&#21512;&#34892;&#19994;&#26631;&#20934;&#30340;RTL&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These fi
&lt;/p&gt;</description></item><item><title>&#22312;AI&#23545;&#40784;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27010;&#24565;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#22312;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#24517;&#39035;&#23545;&#20854;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08672</link><description>&lt;p&gt;
&#27010;&#24565;&#23545;&#40784;&#65288;arXiv&#65306;2401.08672v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Concept Alignment. (arXiv:2401.08672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08672
&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23545;&#40784;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27010;&#24565;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#35748;&#20026;&#22312;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#24517;&#39035;&#23545;&#20854;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#40784;&#65288;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#65289;&#30340;&#35752;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#20215;&#20540;&#23545;&#40784;&#19978;&#65292;&#24191;&#20041;&#19978;&#25351;&#30340;&#26159;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#21516;&#30340;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#25105;&#20204;&#23581;&#35797;&#23545;&#40784;&#20215;&#20540;&#35266;&#20043;&#21069;&#65292;AI&#31995;&#32479;&#21644;&#20154;&#31867;&#23545;&#20110;&#29702;&#35299;&#19990;&#30028;&#25152;&#20351;&#29992;&#30340;&#27010;&#24565;&#24517;&#39035;&#39318;&#20808;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#21746;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#35299;&#37322;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#38656;&#35201;&#27010;&#24565;&#23545;&#40784;&#32780;&#19981;&#20165;&#20165;&#26159;&#20215;&#20540;&#23545;&#40784;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#23454;&#29616;&#20849;&#20139;&#27010;&#24565;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#21033;&#29992;&#35748;&#30693;&#31185;&#23398;&#21644;AI&#30740;&#31350;&#20013;&#24050;&#32463;&#24320;&#21457;&#30340;&#24037;&#20855;&#21152;&#36895;&#27010;&#24565;&#23545;&#40784;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-FastGen&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#21160;&#24577;SplitFuse&#31574;&#30053;&#65292;&#36890;&#36807;MII&#21644;DeepSpeed-Inference&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#30340;LLMs&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;2.3&#20493;&#30340;&#26377;&#25928;&#21534;&#21520;&#37327;&#25552;&#21319;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;2&#20493;&#30340;&#24310;&#36831;&#65292;&#24182;&#38477;&#20302;&#20102;&#39640;&#36798;3.7&#20493;&#30340;&#23614;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2401.08671</link><description>&lt;p&gt;
DeepSpeed-FastGen: &#36890;&#36807;MII&#21644;DeepSpeed-Inference&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#30340;LLMs&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. (arXiv:2401.08671v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-FastGen&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#21160;&#24577;SplitFuse&#31574;&#30053;&#65292;&#36890;&#36807;MII&#21644;DeepSpeed-Inference&#23454;&#29616;&#20102;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#30340;LLMs&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;2.3&#20493;&#30340;&#26377;&#25928;&#21534;&#21520;&#37327;&#25552;&#21319;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;2&#20493;&#30340;&#24310;&#36831;&#65292;&#24182;&#38477;&#20302;&#20102;&#39640;&#36798;3.7&#20493;&#30340;&#23614;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#21644;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#28183;&#36879;&#65292;&#24182;&#35201;&#27714;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#30340;&#26381;&#21153;&#31995;&#32479;&#12290;&#29616;&#26377;&#26694;&#26550;&#23545;&#20110;&#20855;&#26377;&#38271;&#25552;&#31034;&#30340;&#24037;&#20316;&#36127;&#36733;&#24456;&#38590;&#24179;&#34913;&#36825;&#20123;&#35201;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSpeed-FastGen&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#21160;&#24577;SplitFuse&#30340;&#31995;&#32479;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#21644;&#29983;&#25104;&#32452;&#21512;&#31574;&#30053;&#65292;&#30456;&#27604;&#20110;vLLM&#31561;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;2.3&#20493;&#30340;&#26377;&#25928;&#21534;&#21520;&#37327;&#25552;&#21319;&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;2&#20493;&#30340;&#24310;&#36831;&#65292;&#24182;&#38477;&#20302;&#20102;&#39640;&#36798;3.7&#20493;&#65288;&#20196;&#29260;&#32423;&#65289;&#30340;&#23614;&#24310;&#36831;&#12290;&#25105;&#20204;&#21033;&#29992;DeepSpeed-MII&#21644;DeepSpeed-Inference&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#39640;&#25928;&#26131;&#29992;&#30340;&#26381;&#21153;&#31995;&#32479;&#12290;DeepSpeed-FastGen&#30340;&#20808;&#36827;&#23454;&#29616;&#25903;&#25345;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#38750;&#25345;&#20037;&#21644;&#25345;&#20037;&#37096;&#32626;&#36873;&#39033;&#65292;&#36866;&#29992;&#20110;&#20174;&#20132;&#20114;&#24335;&#20250;&#35805;&#21040;&#38271;&#26102;&#38388;&#36816;&#34892;&#24212;&#29992;&#30340;&#21508;&#31181;&#29992;&#25143;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.08669</link><description>&lt;p&gt;
&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#19968;&#20123;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#30340;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#21644;&#38750;&#24120;&#22797;&#26434;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#23578;&#26410;&#35777;&#26126;&#26377;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#31181;&#36825;&#26679;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#36742;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#38656;&#35201;&#27839;&#30528;&#33410;&#28857;&#24207;&#21015;&#31227;&#21160;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#36215;&#28857;&#21040;&#32456;&#28857;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#20026;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#35268;&#27169;&#30340;&#20379;&#24212;&#38142;&#29289;&#27969;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#22810;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#19979;&#36827;&#34892;&#65292;&#24182;&#33021;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08667</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65306;&#25968;&#23383;&#23402;&#29983;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08667
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;PINNs&#22312;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#20445;&#30495;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#25506;&#32034;&#20102;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;(DT)&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#30740;&#31350;&#20102;&#29992;&#20110;&#37197;&#28857;&#30340;&#21508;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23427;&#20204;&#22312;&#26080;&#32593;&#26684;&#26694;&#26550;&#30340;PINNs&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#26500;&#24314;&#34394;&#25311;&#34920;&#31034;&#65292;&#26080;&#38656;&#25163;&#21160;&#29983;&#25104;&#32593;&#26684;&#12290;&#28982;&#21518;&#65292;&#26816;&#39564;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;PINNs(DD-PINNs)&#26694;&#26550;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21033;&#29992;&#22312;DT&#22330;&#26223;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#23545;&#21442;&#25968;&#21270;&#30340;Navier-Stokes&#26041;&#31243;&#30340;&#26356;&#19968;&#33324;&#29289;&#29702;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#20854;&#20013;PINNs&#22312;&#38647;&#35834;&#25968;&#21464;&#21270;&#26102;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23454;&#38469;&#19978;&#25968;&#25454;&#38598;&#32463;&#24120;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20445;&#30495;&#24230;/&#31232;&#30095;&#24230;&#19979;&#25910;&#38598;&#65292;&#36824;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#20445;&#30495;&#24230;&#30340;DD-PINNs&#12290;&#23427;&#20204;&#22312;&#22806;&#25512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;42&#65285;&#21040;62&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#31435;&#36866;&#29992;&#20110;&#24191;&#27867;&#26465;&#20214;&#30340;&#25935;&#25463;&#39134;&#34892;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26500;&#24314;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#30495;&#23454;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#39134;&#34892;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.08663</link><description>&lt;p&gt;
&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#25935;&#25463;&#39134;&#34892;&#22120;&#25511;&#21046;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Integrated Imitation and Reinforcement Learning Methodology for Robust Agile Aircraft Control with Limited Pilot Demonstration Data. (arXiv:2401.08663v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#30340;&#39134;&#34892;&#21592;&#31034;&#33539;&#25968;&#25454;&#30340;&#38598;&#25104;&#27169;&#20223;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#31435;&#36866;&#29992;&#20110;&#24191;&#27867;&#26465;&#20214;&#30340;&#25935;&#25463;&#39134;&#34892;&#22120;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26500;&#24314;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#30495;&#23454;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#39134;&#34892;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#25935;&#25463;&#39134;&#34892;&#22120;&#22312;&#24191;&#27867;&#30340;&#24179;&#34913;&#26465;&#20214;&#21644;&#39134;&#34892;&#22120;&#27169;&#22411;&#21442;&#25968;&#19979;&#21487;&#20197;&#27867;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#12290;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#22312;&#39134;&#34892;&#22120;&#21407;&#22411;&#30340;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25552;&#20379;&#20851;&#20110;&#39134;&#34892;&#22120;&#30340;&#26426;&#21160;&#24615;&#21644;&#25935;&#25463;&#24615;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30495;&#23454;&#39134;&#34892;&#21592;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#26500;&#24314;&#30340;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#25152;&#21253;&#21547;&#30340;&#29305;&#23450;&#39134;&#34892;&#26465;&#20214;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#20010;&#31216;&#20026;&#28304;&#27169;&#22411;&#30340;&#20223;&#30495;&#27169;&#22411;&#12290;&#36825;&#20010;&#24320;&#28304;&#25935;&#25463;&#39134;&#34892;&#22120;&#27169;&#25311;&#22120;&#19982;&#30446;&#26631;&#39134;&#34892;&#22120;&#20855;&#26377;&#30456;&#20284;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26080;&#38480;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#20195;&#29702;&#26426;&#21160;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for constructing data-driven maneuver generation models for agile aircraft that can generalize across a wide range of trim conditions and aircraft model parameters. Maneuver generation models play a crucial role in the testing and evaluation of aircraft prototypes, providing insights into the maneuverability and agility of the aircraft. However, constructing the models typically requires extensive amounts of real pilot data, which can be time-consuming and costly to obtain. Moreover, models built with limited data often struggle to generalize beyond the specific flight conditions covered in the original dataset. To address these challenges, we propose a hybrid architecture that leverages a simulation model, referred to as the source model. This open-source agile aircraft simulator shares similar dynamics with the target aircraft and allows us to generate unlimited data for building a proxy maneuver generation model. We then fine-tune this model t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#65292;&#20197;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#20107;&#25925;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08661</link><description>&lt;p&gt;
&#32771;&#34385;&#36710;&#36742;&#37325;&#37327;&#30340;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-anticipatory autonomous driving strategies considering vehicles' weights, based on hierarchical deep reinforcement learning. (arXiv:2401.08661v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#39118;&#38505;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#65292;&#20197;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#20107;&#25925;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20855;&#26377;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#23548;&#33268;&#30340;&#20107;&#25925;&#21644;&#38477;&#20302;&#36947;&#36335;&#20132;&#36890;&#39118;&#38505;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#22411;&#36710;&#36742;&#30340;&#24615;&#36136;&#65292;&#20854;&#30896;&#25758;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#20107;&#25925;&#65292;&#22240;&#27492;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#32771;&#34385;&#36710;&#36742;&#30340;&#37325;&#37327;&#26469;&#21046;&#23450;&#38477;&#20302;&#28508;&#22312;&#39118;&#38505;&#21644;&#21518;&#26524;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#39118;&#38505;&#39044;&#27979;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32771;&#34385;&#21608;&#22260;&#36710;&#36742;&#30340;&#37325;&#37327;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#39118;&#38505;&#22330;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21608;&#22260;&#36710;&#36742;&#37325;&#37327;&#30340;&#39118;&#38505;&#25351;&#26631;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#20013;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#34892;&#21160;&#31354;&#38388;&#65292;&#20801;&#35768;&#24038;&#36710;&#36947;&#21464;&#36947;&#12289;&#21491;&#36710;&#36947;&#21464;&#36947;&#21644;&#36319;&#36710;&#34892;&#20026;&#65292;&#20351;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33021;&#22815;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#26356;&#33258;&#30001;&#12289;&#26356;&#30495;&#23454;&#22320;&#34892;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#28151;&#21512;&#20915;&#31574;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#20998;&#23618;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;HPPO&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AVs) have the potential to prevent accidents caused by drivers' error and reduce road traffic risks. Due to the nature of heavy vehicles, whose collisions cause more serious crashes, the weights of vehicles need to be considered when making driving strategies aimed at reducing the potential risks and their consequences in the context of autonomous driving. This study develops an autonomous driving strategy based on risk anticipation, considering the weights of surrounding vehicles and using hierarchical deep reinforcement learning. A risk indicator integrating surrounding vehicles' weights, based on the risk field theory, is proposed and incorporated into autonomous driving decisions. A hybrid action space is designed to allow for left lane changes, right lane changes and car-following, which enables AVs to act more freely and realistically whenever possible. To solve the above hybrid decision-making problem, a hierarchical proximal policy optimization (HPPO) algor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.08655</link><description>&lt;p&gt;
SAiD: &#20351;&#29992;&#25193;&#25955;&#26041;&#27861;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#34920;&#24773;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;Transformer-based U-Net&#27169;&#22411;&#21644;&#38899;&#39057;&#19982;&#35270;&#35273;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#21767;&#37096;&#21516;&#27493;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#65292;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#22238;&#24402;&#27169;&#22411;&#65292;&#20294;&#22312;&#20174;&#35821;&#38899;&#29983;&#25104;&#21508;&#31181;&#21767;&#37096;&#21160;&#20316;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#30340;&#36755;&#20986;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;SAiD&#65289;&#39537;&#21160;&#30340;&#35821;&#38899;&#39537;&#21160;&#30340;&#19977;&#32500;&#38754;&#37096;&#21160;&#30011;&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;Transformer&#30340;U-Net&#27169;&#22411;&#65292;&#20855;&#26377;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;&#20559;&#24046;&#65292;&#20197;&#22686;&#24378;&#21767;&#37096;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BlendVOCA&#65292;&#36825;&#26159;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#21644;&#28151;&#21512;&#24418;&#29366;&#38754;&#37096;&#27169;&#22411;&#21442;&#25968;&#23545;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#36164;&#28304;&#30340;&#32570;&#20047;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21767;&#37096;&#21516;&#27493;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30830;&#20445;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#21767;&#37096;&#36816;&#21160;&#65292;&#24182;&#31616;&#21270;&#20102;&#21160;&#30011;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-driven 3D facial animation is challenging due to the scarcity of large-scale visual-audio datasets despite extensive research. Most prior works, typically focused on learning regression models on a small dataset using the method of least squares, encounter difficulties generating diverse lip movements from speech and require substantial effort in refining the generated outputs. To address these issues, we propose a speech-driven 3D facial animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net with a cross-modality alignment bias between audio and visual to enhance lip synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs of speech audio and parameters of a blendshape facial model, to address the scarcity of public resources. Our experimental results demonstrate that the proposed approach achieves comparable or superior performance in lip synchronization to baselines, ensures more diverse lip movements, and streamlines the animati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;DPCNNs&#65289;&#26469;&#25552;&#39640;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35782;&#21035;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08649</link><description>&lt;p&gt;
&#28145;&#24230;&#33033;&#20914;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Pulse-Coupled Neural Networks. (arXiv:2401.08649v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;DPCNNs&#65289;&#26469;&#25552;&#39640;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35782;&#21035;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#65288;&#22914;&#27844;&#28431;&#31215;&#20998;&#31867;&#31070;&#32463;&#20803;&#65289;&#25429;&#25417;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#65292;&#35813;&#31070;&#32463;&#20803;&#21253;&#21547;&#26102;&#38388;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#31163;&#25955;&#21644;&#24322;&#27493;&#30340;&#33033;&#20914;&#20256;&#36882;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#27844;&#28431;&#31215;&#20998;&#31867;&#31070;&#32463;&#20803;&#30340;&#31616;&#21270;&#29983;&#29289;&#29305;&#24615;&#24573;&#30053;&#20102;&#30495;&#23454;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#20803;&#32806;&#21512;&#21644;&#26641;&#31361;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#20803;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#32467;&#26524;SNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26356;&#20855;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#21363;&#33033;&#20914;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;PCNN&#65289;&#65292;&#26469;&#25913;&#36827;SNNs&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35782;&#21035;&#24615;&#33021;&#12290;PCNN&#26159;&#19968;&#31181;&#33021;&#22815;&#27169;&#25311;&#20027;&#35201;&#35270;&#35273;&#30382;&#23618;&#20013;&#22797;&#26434;&#31070;&#32463;&#27963;&#21160;&#30340;&#30382;&#23618;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#29992;PCNN&#31070;&#32463;&#20803;&#21462;&#20195;SNNs&#20013;&#24120;&#29992;&#30340;&#27844;&#28431;&#31215;&#20998;&#31867;&#31070;&#32463;&#20803;&#26500;&#24314;&#20102;&#28145;&#24230;&#33033;&#20914;&#32806;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;DPCNNs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) capture the information processing mechanism of the brain by taking advantage of spiking neurons, such as the Leaky Integrate-and-Fire (LIF) model neuron, which incorporates temporal dynamics and transmits information via discrete and asynchronous spikes. However, the simplified biological properties of LIF ignore the neuronal coupling and dendritic structure of real neurons, which limits the spatio-temporal dynamics of neurons and thus reduce the expressive power of the resulting SNNs. In this work, we leverage a more biologically plausible neural model with complex dynamics, i.e., a pulse-coupled neural network (PCNN), to improve the expressiveness and recognition performance of SNNs for vision tasks. The PCNN is a type of cortical model capable of emulating the complex neuronal activities in the primary visual cortex. We construct deep pulse-coupled neural networks (DPCNNs) by replacing commonly used LIF neurons in SNNs with PCNN neurons. The intra-cou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#27493;&#25193;&#25955;&#33976;&#39311;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#33976;&#39311;&#21040;&#26356;&#24555;&#30340;&#32593;&#32476;&#20013;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#19968;&#27493;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08639</link><description>&lt;p&gt;
&#19968;&#27493;&#25193;&#25955;&#33976;&#39311;&#65306;&#36890;&#36807;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#27493;&#25193;&#25955;&#33976;&#39311;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#33976;&#39311;&#21040;&#26356;&#24555;&#30340;&#32593;&#32476;&#20013;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#19968;&#27493;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#36845;&#20195;&#65292;&#22240;&#27492;&#38656;&#35201;&#23581;&#35797;&#23558;&#29983;&#25104;&#36807;&#31243;&#33976;&#39311;&#21040;&#26356;&#24555;&#30340;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#25361;&#25112;&#65306;&#33976;&#39311;&#35757;&#32451;&#36807;&#31243;&#22797;&#26434;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#35757;&#32451;&#38454;&#27573;&#65292;&#32780;&#29983;&#25104;&#24212;&#29992;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30452;&#25509;&#20174;&#21021;&#22987;&#22122;&#22768;&#33976;&#39311;&#21040;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#38754;&#26159;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#20316;&#20026;&#33976;&#39311;&#32467;&#26500;&#65306;&#29983;&#25104;&#22411;&#24179;&#34913;&#21464;&#21387;&#22120;&#65288;GET&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#31163;&#32447;&#35757;&#32451;&#65292;&#21482;&#38656;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;/&#22270;&#20687;&#23545;&#65292;&#32780;&#19988;&#22312;&#30456;&#24403;&#30340;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#19968;&#27493;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DEQ&#26550;&#26500;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#26469;&#36827;&#34892;&#21327;&#20316;&#25512;&#26029;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#35774;&#22791;&#19978;AI&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#36890;&#36807;&#25552;&#20379;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#21644;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#32479;&#19968;&#34394;&#25311;&#21270;&#35270;&#22270;&#21644;&#36328;&#21160;&#24577;/&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#65292;&#20854;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.08637</link><description>&lt;p&gt;
&#36890;&#36807;MCU&#19978;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#21160;&#24577;&#32452;&#21512;&#23454;&#29616;&#21327;&#20316;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs. (arXiv:2401.08637v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#26469;&#36827;&#34892;&#21327;&#20316;&#25512;&#26029;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#35774;&#22791;&#19978;AI&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#36890;&#36807;&#25552;&#20379;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#21644;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#32479;&#19968;&#34394;&#25311;&#21270;&#35270;&#22270;&#21644;&#36328;&#21160;&#24577;/&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#65292;&#20854;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#20986;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26497;&#38480;&#36793;&#32536;&#19978;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#24310;&#36831;&#12289;&#36739;&#20302;&#30340;&#21151;&#32791;&#25104;&#26412;&#21644;&#25913;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#21152;&#36895;&#22120;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#20869;&#23384;&#21644;&#21333;&#35774;&#22791;&#28966;&#28857;&#65292;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#33021;&#22815;&#20026;&#22810;&#31199;&#25143;&#27169;&#22411;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23545;&#20110;&#35774;&#22791;&#19978;AI&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#20854;&#25552;&#20379;&#20102;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#65292;&#20026;&#36164;&#28304;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#34394;&#25311;&#21270;&#35270;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29289;&#29702;&#35774;&#22791;&#30340;&#39640;&#25928;&#20219;&#21153;&#26144;&#23556;&#12290;Synergy&#30340;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#30830;&#20445;&#20102;&#36328;&#21160;&#24577;&#21644;&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;Synergy&#30340;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of tiny AI accelerators opens opportunities for deep neural network deployment at the extreme edge, offering reduced latency, lower power cost, and improved privacy in on-device ML inference. Despite these advancements, challenges persist due to inherent limitations of these accelerators, such as restricted onboard memory and single-device focus. This paper introduces Synergy, a system that dynamically composes tiny AI accelerators for multi-tenant models, effectively addressing tinyML's critical challenges for the increasing demand for on-device AI. A key feature of Synergy is its virtual computing space, providing a unified, virtualized view of resources and enabling efficient task mapping to physical devices. Synergy's runtime orchestration module ensures optimal inference across dynamic and heterogeneous accelerators. Our evaluations with 7 baselines and 8 models demonstrate that Synergy improves throughput by an average of 8.0X compared to baselines.
&lt;/p&gt;</description></item><item><title>&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2401.08632</link><description>&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08632
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#22522;&#26412;&#29305;&#24449;&#20043;&#19968;&#26159;&#25214;&#21040;&#26032;&#39062;&#21644;&#26377;&#21019;&#36896;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;&#25361;&#25112;&#25110;&#36866;&#24212;&#26410;&#39044;&#26009;&#21040;&#30340;&#24773;&#20917;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26159;&#19968;&#31867;&#36827;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#20854;&#20013;&#65292;MAP-Elites&#26159;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#12290;&#28982;&#32780;&#65292;MAP-Elites&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#38543;&#26426;&#31361;&#21464;&#36827;&#34892;&#21457;&#25955;&#25628;&#32034;&#65292;&#22240;&#27492;&#20165;&#38480;&#20110;&#36827;&#21270;&#20302;&#32500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#12290;PGA-MAP-Elites&#36890;&#36807;&#21463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#21270;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#24615;&#33021;&#20248;&#31168;&#65292;&#20294;PGA-MAP-Elites&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#20854;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#30340;&#25910;&#25947;&#25628;&#32034;&#38459;&#30861;&#20102;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08627</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21644;&#35299;&#37322;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;
&lt;/p&gt;
&lt;p&gt;
Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks. (arXiv:2401.08627v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23646;&#29627;&#29827;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#24207;&#26448;&#26009;&#12290;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#26448;&#26009;&#31185;&#23398;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#23398;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#65292;&#24182;&#30740;&#31350;&#32467;&#26500;&#19982;&#30456;&#24212;&#30340;&#23616;&#37096;&#33021;&#22418;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25454;&#20449;&#33021;&#22815;&#25511;&#21046;&#37329;&#23646;&#29627;&#29827;&#30340;&#35768;&#22810;&#20851;&#38190;&#29289;&#29702;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#33021;&#22418;&#30340;&#26032;&#22411;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SymGNN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#21453;&#23556;&#12290;&#36825;&#31181;&#19981;&#21464;&#24615;&#26159;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#26080;&#27861;&#25429;&#25417;&#30340;&#24615;&#36136;&#12290;SymGNN&#36890;&#36807;&#32858;&#21512;&#22270;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#26469;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#25152;&#26377;&#19977;&#32500;&#27491;&#20132;&#21464;&#25442;&#36827;&#34892;&#26368;&#20248;&#20998;&#24067;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metallic Glasses (MGs) are widely used disordered materials. Understanding the relationship between the local structure and physical properties of MGs is one of the greatest challenges for both material science and condensed matter physics. In this work, we utilize Graph Neural Networks (GNNs) to model the atomic graph structure and study the connection between the structure and the corresponding local energy barrier, which is believed to govern many critical physical properties in MGs. One of our key contributions is to propose a novel Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is invariant under orthogonal transformations of the structure, e.g., rotations and reflections. Such invariance is a desired property that standard GNNs like Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by aggregating over orthogonal transformations of the graph structure for representation learning, and an optimal distribution over all 3D orthogonal 
&lt;/p&gt;</description></item><item><title>Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#26159;&#19968;&#31181;&#20511;&#37492;&#20154;&#33041;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#35273;&#37266;&#38454;&#27573;&#20013;&#27169;&#22411;&#36866;&#24212;&#24863;&#23448;&#36755;&#20837;&#24182;&#21033;&#29992;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#20445;&#25345;&#31283;&#23450;&#65292;&#30561;&#30496;&#38454;&#27573;&#26681;&#25454;NREM&#21644;REM&#38454;&#27573;&#23545;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#36827;&#34892;&#24041;&#22266;&#21644;&#35843;&#25972;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2401.08623</link><description>&lt;p&gt;
Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Wake-Sleep Consolidated Learning. (arXiv:2401.08623v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08623
&lt;/p&gt;
&lt;p&gt;
Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#26159;&#19968;&#31181;&#20511;&#37492;&#20154;&#33041;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#35273;&#37266;&#38454;&#27573;&#20013;&#27169;&#22411;&#36866;&#24212;&#24863;&#23448;&#36755;&#20837;&#24182;&#21033;&#29992;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#20445;&#25345;&#31283;&#23450;&#65292;&#30561;&#30496;&#38454;&#27573;&#26681;&#25454;NREM&#21644;REM&#38454;&#27573;&#23545;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#36827;&#34892;&#24041;&#22266;&#21644;&#35843;&#25972;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Wake-Sleep Consolidated Learning&#65288;WSCL&#65289;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#21033;&#29992;&#20114;&#34917;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#21644;&#20154;&#33041;&#30340;&#35273;&#37266;-&#30561;&#30496;&#38454;&#27573;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35273;&#37266;&#21644;&#30561;&#30496;&#38454;&#27573;&#20043;&#38388;&#30340;&#21516;&#27493;&#23398;&#20064;&#26469;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#22312;&#35273;&#37266;&#38454;&#27573;&#65292;&#27169;&#22411;&#26292;&#38706;&#20110;&#24863;&#23448;&#36755;&#20837;&#24182;&#35843;&#25972;&#20854;&#34920;&#31034;&#65292;&#36890;&#36807;&#21160;&#24577;&#21442;&#25968;&#20923;&#32467;&#26426;&#21046;&#30830;&#20445;&#31283;&#23450;&#24615;&#65292;&#24182;&#23558;&#24773;&#33410;&#35760;&#24518;&#23384;&#20648;&#22312;&#30701;&#26399;&#20020;&#26102;&#35760;&#24518;&#20013;&#65288;&#31867;&#20284;&#20110;&#28023;&#39532;&#20307;&#20013;&#30340;&#24773;&#20917;&#65289;&#12290;&#22312;&#30561;&#30496;&#38454;&#27573;&#65292;&#35757;&#32451;&#36807;&#31243;&#20998;&#20026;NREM&#21644;REM&#38454;&#27573;&#12290;&#22312;NREM&#38454;&#27573;&#65292;&#27169;&#22411;&#30340;&#31361;&#35302;&#26435;&#37325;&#21033;&#29992;&#26469;&#33258;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#30340;&#22238;&#25918;&#26679;&#26412;&#36827;&#34892;&#24041;&#22266;&#65292;&#24182;&#28608;&#27963;&#31361;&#35302;&#21487;&#22609;&#24615;&#26426;&#21046;&#65292;&#24378;&#21270;&#37325;&#35201;&#36830;&#25509;&#24182;&#21066;&#24369;&#19981;&#37325;&#35201;&#30340;&#36830;&#25509;&#12290;&#22312;REM&#38454;&#27573;&#65292;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy leveraging Complementary Learning System theory and the wake-sleep phases of the human brain to improve the performance of deep neural networks for visual classification tasks in continual learning settings. Our method learns continually via the synchronization between distinct wake and sleep phases. During the wake phase, the model is exposed to sensory input and adapts its representations, ensuring stability through a dynamic parameter freezing mechanism and storing episodic memories in a short-term temporary memory (similarly to what happens in the hippocampus). During the sleep phase, the training process is split into NREM and REM stages. In the NREM stage, the model's synaptic weights are consolidated using replayed samples from the short-term and long-term memory and the synaptic plasticity mechanism is activated, strengthening important connections and weakening unimportant ones. In the REM stage, the model
&lt;/p&gt;</description></item><item><title>MATE-Pred&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08619</link><description>&lt;p&gt;
MATE-Pred: &#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor. (arXiv:2401.08619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08619
&lt;/p&gt;
&lt;p&gt;
MATE-Pred&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;TCR-Epitope&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#29305;&#24449;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#39044;&#27979;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#23545;&#20110;&#24320;&#21457;&#25104;&#21151;&#30340;&#20813;&#30123;&#30103;&#27861;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#36890;&#36807;&#23558;&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#24207;&#21015;&#30340;&#27688;&#22522;&#37240;&#27531;&#22522;&#36716;&#21270;&#20026;&#25968;&#20540;&#26469;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#38598;&#25104;&#20102;&#36827;&#21270;&#29305;&#24449;&#65292;&#32780;&#21478;&#19968;&#20123;&#26041;&#27861;&#21017;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27688;&#22522;&#37240;&#27531;&#22522;&#23618;&#38754;&#19978;&#24635;&#32467;&#23884;&#20837;&#21521;&#37327;&#20197;&#33719;&#21462;&#24207;&#21015;&#32423;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#21487;&#38752;&#30340;&#26032;&#26041;&#27861;MATE-Pred&#65292;&#23427;&#36890;&#36807;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#36827;&#34892;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;&#12290;MATE-Pred&#19982;&#20854;&#20182;&#21033;&#29992;T&#32454;&#32990;&#21463;&#20307;&#21644;&#34920;&#20301;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#34507;&#30333;&#36136;&#30340;&#25991;&#26412;&#34920;&#31034;&#20197;&#39044;&#35757;&#32451;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#65292;&#24182;&#19982;&#20004;&#31181;&#38468;&#21152;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate binding affinity prediction between T-cell receptors and epitopes contributes decisively to develop successful immunotherapy strategies. Some state-of-the-art computational methods implement deep learning techniques by integrating evolutionary features to convert the amino acid residues of cell receptors and epitope sequences into numerical values, while some other methods employ pre-trained language models to summarize the embedding vectors at the amino acid residue level to obtain sequence-wise representations.  Here, we propose a highly reliable novel method, MATE-Pred, that performs multi-modal attention-based prediction of T-cell receptors and epitopes binding affinity. The MATE-Pred is compared and benchmarked with other deep learning models that leverage multi-modal representations of T-cell receptors and epitopes. In the proposed method, the textual representation of proteins is embedded with a pre-trained bi-directional encoder model and combined with two additiona
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.08603</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#32534;&#30721;&#22120;&#35774;&#35745;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning. (arXiv:2401.08603v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#21033;&#29992;&#20102;&#23545;&#26550;&#26500;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#35265;&#12289;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35201;&#27714;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#20248;&#21270;&#24037;&#20855;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#30340;&#26089;&#26399;&#23618;&#20013;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20197;&#20154;&#20026;&#25351;&#23450;&#30340;&#20934;&#19981;&#21464;&#28388;&#27874;&#22120;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65288;Hinge CLAPP Loss&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#22810;&#20010;&#24182;&#34892;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38590;&#24230;&#30340;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65288;GTSRB, STL10, CODEBR&#65289;
&lt;/p&gt;
&lt;p&gt;
Modern data-driven machine learning system designs exploit inductive biases on architectural structure, invariance and equivariance requirements, task specific loss functions, and computational optimization tools. Previous works have illustrated that inductive bias in the early layers of the encoder in the form of human specified quasi-invariant filters can serve as a powerful inductive bias to attain better robustness and transparency in learned classifiers. This paper explores this further in the context of representation learning with local plasticity rules i.e. bio-inspired Hebbian learning . We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel encoders each leveraging a different invariant visual descriptor as an inductive bias. We evaluate the representation learning capacity of our system in a classification scenario on image data of various difficulties (GTSRB, STL10, CODEBR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#20351;&#29992;&#21270;&#23398;&#21512;&#25104;&#19982;&#30005;&#21512;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08602</link><description>&lt;p&gt;
&#23398;&#20064;&#21270;&#23398;&#21512;&#25104;&#19982;&#30005;&#21512;&#25104;&#8212;&#8212;&#26377;&#21306;&#21035;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Learning with Chemical versus Electrical Synapses -- Does it Make a Difference?. (arXiv:2401.08602v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#20351;&#29992;&#21270;&#23398;&#21512;&#25104;&#19982;&#30005;&#21512;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26377;&#21161;&#20110;&#25512;&#21160;&#25105;&#20204;&#23545;&#31070;&#32463;&#35745;&#31639;&#30340;&#29702;&#35299;&#65292;&#24182;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29616;&#29366;&#12290;&#29983;&#29289;&#30005;&#21270;&#23398;&#21512;&#25104;&#30452;&#25509;&#36890;&#36807;&#20351;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30005;&#27969;&#24555;&#36895;&#27969;&#21160;&#26469;&#20256;&#36882;&#31070;&#32463;&#20449;&#21495;&#12290;&#30456;&#21453;&#65292;&#29983;&#29289;&#21270;&#23398;&#21512;&#25104;&#36890;&#36807;&#31070;&#32463;&#36882;&#36136;&#38388;&#25509;&#20256;&#36882;&#31070;&#32463;&#20449;&#21495;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31232;&#30095;&#30340;&#12289;&#29983;&#29289;&#21551;&#21457;&#30340;&#26550;&#26500;&#8212;&#8212;&#31070;&#32463;&#22238;&#36335;&#31574;&#30053;(NCPs)&#20013;&#20351;&#29992;&#21270;&#23398;&#21512;&#25104;&#21487;&#20197;&#23454;&#29616;&#22797;&#26434;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#21487;&#35299;&#37322;&#24615;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#27604;&#36739;&#36825;&#20004;&#31181;&#31361;&#35302;&#27169;&#22411;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#22312;&#31232;&#30095;&#21644;&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#20351;&#29992;&#21270;&#23398;&#21512;&#25104;&#19982;&#30005;&#21512;&#25104;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#36924;&#30495;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#36827;&#34892;&#33258;&#20027;&#36710;&#36947;&#20445;&#25345;&#30340;&#23454;&#39564;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#20197;&#21450;&#23384;&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bio-inspired neural networks have the potential to advance our understanding of neural computation and improve the state-of-the-art of AI systems. Bio-electrical synapses directly transmit neural signals, by enabling fast current flow between neurons. In contrast, bio-chemical synapses transmit neural signals indirectly, through neurotransmitters. Prior work showed that interpretable dynamics for complex robotic control, can be achieved by using chemical synapses, within a sparse, bio-inspired architecture, called Neural Circuit Policies (NCPs). However, a comparison of these two synaptic models, within the same architecture, remains an unexplored area. In this work we aim to determine the impact of using chemical synapses compared to electrical synapses, in both sparse and all-to-all connected networks. We conduct experiments with autonomous lane-keeping through a photorealistic autonomous driving simulator to evaluate their performance under diverse conditions and in the presence of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;</title><link>http://arxiv.org/abs/2401.08584</link><description>&lt;p&gt;
Nahid: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#23436;&#20840;&#33258;&#21160;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Nahid: AI-based Algorithm for operating fully-automatic surgery. (arXiv:2401.08584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#20214;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#20840;&#33258;&#21160;&#25163;&#26415;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23545;&#21307;&#30103;&#25163;&#26415;&#30340;&#35745;&#31639;&#26426;&#21270;&#20248;&#21183;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#23545;&#23396;&#31435;&#24615;&#21365;&#24034;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#30456;&#20851;&#30340;&#25163;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#22522;&#20110;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#35814;&#32454;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#33258;&#21160;&#35786;&#26029;&#21644;&#27835;&#30103;&#35813;&#30142;&#30149;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;U-net&#27169;&#22411;&#26469;&#26816;&#27979;&#25163;&#26415;&#36807;&#31243;&#20013;&#30340;&#23376;&#23467;&#20869;&#33180;&#24322;&#20301;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, for the first time, a method is presented that can provide a fully automated surgery based on software and computer vision techniques. Then, the advantages and challenges of computerization of medical surgery are examined. Finally, the surgery related to isolated ovarian endometriosis disease has been examined, and based on the presented method, a more detailed algorithm is presented that is capable of automatically diagnosing and treating this disease during surgery as proof of our proposed method where a U-net is trained to detect the endometriosis during surgery.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#30456;&#23545;&#24212;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#24182;&#21387;&#32553;&#20026;&#35821;&#20041;&#20998;&#21106;&#25152;&#38656;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#26102;&#38388;&#23884;&#20837;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.08581</link><description>&lt;p&gt;
&#26102;&#38388;&#23884;&#20837;&#65306;&#20174;&#26102;&#31354;&#25968;&#25454;&#20013;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#33258;&#30417;&#30563;&#26102;&#24207;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#35745;&#31639;&#26426;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
Temporal Embeddings: Scalable Self-Supervised Temporal Representation Learning from Spatiotemporal Data for Multimodal Computer Vision. (arXiv:2401.08581v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26102;&#38388;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#30456;&#23545;&#24212;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#24182;&#21387;&#32553;&#20026;&#35821;&#20041;&#20998;&#21106;&#25152;&#38656;&#30340;&#22270;&#20687;&#36890;&#36947;&#65292;&#26102;&#38388;&#23884;&#20837;&#21487;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#27963;&#21160;&#30340;&#26102;&#38388;&#27169;&#24335;&#19982;&#22303;&#22320;&#21033;&#29992;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#33258;&#32534;&#30721;&#22120;&#23558;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36716;&#25442;&#21040;&#39057;&#22495;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#22270;&#20687;&#36890;&#36947;&#65292;&#29992;&#20110;&#19979;&#28216;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#30340;&#28145;&#24230;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#38388;&#23884;&#20837;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#23545;&#20110;&#20998;&#31867;&#20303;&#23429;&#21306;&#21644;&#21830;&#19994;&#21306;&#31561;&#19981;&#21516;&#20219;&#21153;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exists a correlation between geospatial activity temporal patterns and type of land use. A novel self-supervised approach is proposed to stratify landscape based on mobility activity time series. First, the time series signal is transformed to the frequency domain and then compressed into task-agnostic temporal embeddings by a contractive autoencoder, which preserves cyclic temporal patterns observed in time series. The pixel-wise embeddings are converted to image-like channels that can be used for task-based, multimodal modeling of downstream geospatial tasks using deep semantic segmentation. Experiments show that temporal embeddings are semantically meaningful representations of time series data and are effective across different tasks such as classifying residential area and commercial areas. Temporal embeddings transform sequential, spatiotemporal motion trajectory data into semantically meaningful image-like tensor representations that can be combined (multimodal fusion) wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ExFlow&#30340;&#36731;&#37327;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#65292;&#22823;&#22823;&#21152;&#36895;&#20102;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.08383</link><description>&lt;p&gt;
&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#21152;&#36895;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference. (arXiv:2401.08383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ExFlow&#30340;&#36731;&#37327;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#65292;&#22823;&#22823;&#21152;&#36895;&#20102;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#28151;&#21512;&#19987;&#23478;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#21644;&#20934;&#30830;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23558;GPT MoE&#27169;&#22411;&#37096;&#32626;&#21040;&#20998;&#24067;&#24335;&#31995;&#32479;&#19978;&#36827;&#34892;&#24182;&#34892;&#25512;&#29702;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19987;&#23478;&#36335;&#30001;&#21644;&#32858;&#21512;&#25152;&#38656;&#30340;&#24191;&#27867;Alltoall&#36890;&#20449;&#12290;&#36825;&#31181;&#36890;&#20449;&#29942;&#39048;&#21152;&#21095;&#20102;&#24050;&#32463;&#22797;&#26434;&#30340;&#35745;&#31639;&#29615;&#22659;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20248;&#21270;&#25216;&#26415;ExFlow&#65292;&#20197;&#22823;&#22823;&#21152;&#36895;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#20174;&#21033;&#29992;&#23618;&#38388;&#19987;&#23478;&#20146;&#21644;&#24615;&#30340;&#26032;&#35270;&#35282;&#26469;&#20943;&#36731;&#36890;&#20449;&#24320;&#38144;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;MoE&#27169;&#22411;&#65292;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#25110;&#31934;&#24230;&#19979;&#38477;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#19978;&#19979;&#25991;&#36830;&#36143;&#30340;&#19987;&#23478;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
In large language models like the Generative Pre-trained Transformer, the Mixture of Experts paradigm has emerged as a powerful technique for enhancing model expressiveness and accuracy. However, deploying GPT MoE models for parallel inference on distributed systems presents significant challenges, primarily due to the extensive Alltoall communication required for expert routing and aggregation. This communication bottleneck exacerbates the already complex computational landscape, hindering the efficient utilization of high-performance computing resources. In this paper, we propose a lightweight optimization technique called ExFlow, to largely accelerate the inference of these MoE models. We take a new perspective on alleviating the communication overhead by exploiting the inter-layer expert affinity. Unlike previous methods, our solution can be directly applied to pre-trained MoE models without any fine-tuning or accuracy degradation. By proposing a context-coherent expert parallelism
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#23558;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#38899;&#27963;&#21160;&#12289;&#38899;&#20048;&#12289;&#22122;&#38899;&#21644;&#37325;&#21472;&#35821;&#38899;&#30340;&#21516;&#26102;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08268</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Explainable Proxy Model for Multiabel Audio Segmentation. (arXiv:2401.08268v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#38899;&#39057;&#20998;&#21106;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#23558;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#38899;&#27963;&#21160;&#12289;&#38899;&#20048;&#12289;&#22122;&#38899;&#21644;&#37325;&#21472;&#35821;&#38899;&#30340;&#21516;&#26102;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20449;&#21495;&#20998;&#21106;&#26159;&#33258;&#21160;&#38899;&#39057;&#32034;&#24341;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23427;&#21253;&#25324;&#22312;&#20449;&#21495;&#20013;&#26816;&#27979;&#31867;&#21516;&#36136;&#29255;&#27573;&#30340;&#36793;&#30028;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#36879;&#26126;&#20915;&#31574;&#30340;&#37325;&#35201;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#35821;&#38899;&#27963;&#21160;&#65288;SAD&#65289;&#12289;&#38899;&#20048;&#65288;MD&#65289;&#12289;&#22122;&#38899;&#65288;ND&#65289;&#21644;&#37325;&#21472;&#35821;&#38899;&#26816;&#27979;&#65288;OSD&#65289;&#12290;&#35813;&#20195;&#29702;&#27169;&#22411;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#23558;&#29992;&#20110;&#20998;&#21106;&#30340;&#23884;&#20837;&#26144;&#23556;&#21040;&#39057;&#22495;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#30456;&#20284;&#65292;&#21516;&#26102;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21151;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20915;&#31574;&#25152;&#20351;&#29992;&#30340;&#39057;&#29575;&#21306;&#38388;&#21487;&#20197;&#22312;&#29255;&#27573;&#32423;&#21035;&#65288;&#23616;&#37096;&#35299;&#37322;&#65289;&#21644;&#20840;&#23616;&#32423;&#21035;&#65288;&#31867;&#21407;&#22411;&#65289;&#19978;&#36731;&#26494;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio signal segmentation is a key task for automatic audio indexing. It consists of detecting the boundaries of class-homogeneous segments in the signal. In many applications, explainable AI is a vital process for transparency of decision-making with machine learning. In this paper, we propose an explainable multilabel segmentation model that solves speech activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD) simultaneously. This proxy uses the non-negative matrix factorization (NMF) to map the embedding used for the segmentation to the frequency domain. Experiments conducted on two datasets show similar performances as the pre-trained black box model while showing strong explainability features. Specifically, the frequency bins used for the decision can be easily identified at both the segment level (local explanations) and global level (class prototypes).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#37319;&#26679;&#30697;&#38453;&#26465;&#30446;&#12289;&#35266;&#23519;&#21040;&#30340;&#31038;&#20132;&#22270;&#21644;&#36229;&#22270;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#23574;&#38160;&#38408;&#20540;&#65292;&#21487;&#20197;&#31934;&#30830;&#34917;&#20840;&#35780;&#20998;&#30697;&#38453;&#12290;&#36890;&#36807;&#37327;&#21270;&#36229;&#22270;&#30340;&#8220;&#36136;&#37327;&#8221;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#36229;&#22270;&#21033;&#29992;&#23545;&#26679;&#26412;&#27010;&#29575;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24320;&#21457;&#39640;&#25928;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#39640;&#27010;&#29575;&#24773;&#20917;&#19979;&#25104;&#21151;&#22320;&#23436;&#25104;&#20102;&#30697;&#38453;&#34917;&#20840;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.08197</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#30697;&#38453;&#34917;&#20840;&#65306;&#23574;&#38160;&#38408;&#20540;&#21644;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient Algorithms. (arXiv:2401.08197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#37319;&#26679;&#30697;&#38453;&#26465;&#30446;&#12289;&#35266;&#23519;&#21040;&#30340;&#31038;&#20132;&#22270;&#21644;&#36229;&#22270;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#23574;&#38160;&#38408;&#20540;&#65292;&#21487;&#20197;&#31934;&#30830;&#34917;&#20840;&#35780;&#20998;&#30697;&#38453;&#12290;&#36890;&#36807;&#37327;&#21270;&#36229;&#22270;&#30340;&#8220;&#36136;&#37327;&#8221;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#36229;&#22270;&#21033;&#29992;&#23545;&#26679;&#26412;&#27010;&#29575;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24320;&#21457;&#39640;&#25928;&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#39640;&#27010;&#29575;&#24773;&#20917;&#19979;&#25104;&#21151;&#22320;&#23436;&#25104;&#20102;&#30697;&#38453;&#34917;&#20840;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23376;&#37319;&#26679;&#30697;&#38453;&#26465;&#30446;&#20197;&#21450;&#35266;&#23519;&#21040;&#30340;&#31038;&#20132;&#22270;&#21644;&#36229;&#22270;&#30340;&#34917;&#20840;&#35780;&#20998;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26679;&#26412;&#27010;&#29575;&#19978;&#23384;&#22312;&#19968;&#20010;&#23574;&#38160;&#38408;&#20540;&#65292;&#29992;&#20110;&#31934;&#30830;&#23436;&#25104;&#35780;&#20998;&#30697;&#38453;&#30340;&#20219;&#21153;&#65292;&#24403;&#26679;&#26412;&#27010;&#29575;&#39640;&#20110;&#38408;&#20540;&#26102;&#65292;&#20219;&#21153;&#21487;&#23436;&#25104;&#65292;&#21453;&#20043;&#21017;&#19981;&#21487;&#33021;&#65292;&#36825;&#23637;&#31034;&#20102;&#19968;&#20010;&#30456;&#21464;&#29616;&#35937;&#12290;&#38408;&#20540;&#21487;&#20197;&#20316;&#20026;&#36229;&#22270;&#30340;&#8220;&#36136;&#37327;&#8221;&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#30001;&#20110;&#36229;&#22270;&#21033;&#29992;&#32780;&#23548;&#33268;&#30340;&#26679;&#26412;&#27010;&#29575;&#20943;&#23569;&#37327;&#65292;&#36825;&#20063;&#31361;&#26174;&#20102;&#36229;&#22270;&#22312;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#22312;&#21457;&#29616;&#23574;&#38160;&#38408;&#20540;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#30697;&#38453;&#34917;&#20840;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#35266;&#23519;&#21040;&#30340;&#22270;&#21644;&#36229;&#22270;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#35201;&#26679;&#26412;&#27010;&#29575;&#39640;&#20110;&#26576;&#20010;&#38408;&#20540;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of completing a rating matrix based on sub-sampled matrix entries as well as observed social graphs and hypergraphs. We show that there exists a \emph{sharp threshold} on the sample probability for the task of exactly completing the rating matrix -- the task is achievable when the sample probability is above the threshold, and is impossible otherwise -- demonstrating a phase transition phenomenon. The threshold can be expressed as a function of the ``quality'' of hypergraphs, enabling us to \emph{quantify} the amount of reduction in sample probability due to the exploitation of hypergraphs. This also highlights the usefulness of hypergraphs in the matrix completion problem. En route to discovering the sharp threshold, we develop a computationally efficient matrix completion algorithm that effectively exploits the observed graphs and hypergraphs. Theoretical analyses show that our algorithm succeeds with high probability as long as the sample probability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#65292;&#31532;&#19968;&#23618;&#36127;&#36131;&#30456;&#21516;&#20301;&#32622;&#30340;&#25968;&#23383;&#30456;&#21152;&#65292;&#31532;&#20108;&#23618;&#26681;&#25454;&#27880;&#24847;&#21147;&#26426;&#21046;&#20915;&#23450;&#26159;&#21542;&#38656;&#35201;&#36827;&#20301;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#26426;&#26469;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.07993</link><description>&lt;p&gt;
&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Carrying over algorithm in transformers. (arXiv:2401.07993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#23454;&#29616;&#36827;&#20301;&#31639;&#27861;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#22312;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#20013;&#65292;&#31532;&#19968;&#23618;&#36127;&#36131;&#30456;&#21516;&#20301;&#32622;&#30340;&#25968;&#23383;&#30456;&#21152;&#65292;&#31532;&#20108;&#23618;&#26681;&#25454;&#27880;&#24847;&#21147;&#26426;&#21046;&#20915;&#23450;&#26159;&#21542;&#38656;&#35201;&#36827;&#20301;&#65292;&#24182;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#26426;&#26469;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#27861;&#21487;&#33021;&#26159;&#26368;&#31616;&#21333;&#30340;&#31639;&#26415;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#24120;&#20351;&#29992;&#36827;&#20301;&#31639;&#27861;&#36827;&#34892;&#35745;&#31639;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#36825;&#20010;&#31639;&#27861;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#20004;&#20010;&#20219;&#21153;&#20998;&#37197;&#32473;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#20004;&#23618;&#30340;&#20165;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21457;&#29616;&#36827;&#20301;&#31639;&#27861;&#20197;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#31532;&#19968;&#23618;&#20027;&#35201;&#36127;&#36131;&#22312;&#30456;&#21516;&#20301;&#32622;&#19978;&#28155;&#21152;&#25968;&#23383;&#12290;&#31532;&#20108;&#23618;&#39318;&#20808;&#22312;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#20915;&#23450;&#21738;&#20123;&#20301;&#32622;&#38656;&#35201;&#36827;&#20301;&#65292;&#28982;&#21518;&#22312;&#26368;&#32456;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#20013;&#25191;&#34892;&#36827;&#20301;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#31934;&#30830;&#23450;&#20301;&#36127;&#36131;&#36825;&#20010;&#20219;&#21153;&#30340;&#31070;&#32463;&#20803;&#12290;&#36827;&#20301;&#31639;&#27861;&#30340;&#36825;&#31181;&#23454;&#29616;&#22312;&#20004;&#23618;&#21644;&#19977;&#23618;&#27169;&#22411;&#30340;&#19968;&#31995;&#21015;&#36229;&#21442;&#25968;&#20013;&#37117;&#23384;&#22312;&#12290;&#23545;&#20110;&#23567;&#22411;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.07769</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#29992;&#20110;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#20013;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation. (arXiv:2401.07769v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#27969;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#22330;&#26223;&#65292;&#31216;&#20026;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35302;&#21457;&#39033;&#26126;&#30830;&#34920;&#36798;&#20182;&#20204;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22312;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65288;&#22914;&#38463;&#37324;&#24052;&#24052;&#21644;&#20122;&#39532;&#36874;&#65289;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#26126;&#30830;&#24314;&#27169;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22240;&#27492;&#22312;TIR&#20013;&#33719;&#24471;&#27425;&#20248;&#32467;&#26524;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#21516;&#26102;&#32771;&#34385;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20173;&#26410;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#29992;&#25143;&#21521;&#19979;&#28378;&#21160;&#26102;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;--&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#65292;&#29992;&#20110;TIR&#22330;&#26223;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation has been playing a key role in many industries, e.g., e-commerce, streaming media, social media, etc. Recently, a new recommendation scenario, called Trigger-Induced Recommendation (TIR), where users are able to explicitly express their instant interests via trigger items, is emerging as an essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon. Without explicitly modeling the user's instant interest, traditional recommendation methods usually obtain sub-optimal results in TIR. Even though there are a few methods considering the trigger and target items simultaneously to solve this problem, they still haven't taken into account temporal information of user behaviors, the dynamic change of user instant interest when the user scrolls down and the interactions between the trigger and target items. To tackle these problems, we propose a novel method -- Deep Evolutional Instant Interest Network (DEI2N), for click-through rate prediction in TIR scenarios
&lt;/p&gt;</description></item><item><title>CLSA-CIM&#26159;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#26550;&#26500;&#30340;&#36328;&#23618;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24179;&#38138;CIM&#26550;&#26500;&#30340;&#21033;&#29992;&#29575;&#65292;&#21152;&#36895;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.07671</link><description>&lt;p&gt;
CLSA-CIM: &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#26550;&#26500;&#30340;&#36328;&#23618;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory Architectures. (arXiv:2401.07671v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07671
&lt;/p&gt;
&lt;p&gt;
CLSA-CIM&#26159;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20869;&#23384;&#26550;&#26500;&#30340;&#36328;&#23618;&#35843;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#24179;&#38138;CIM&#26550;&#26500;&#30340;&#21033;&#29992;&#29575;&#65292;&#21152;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#22120;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#65292;&#25512;&#21160;&#20102;&#26032;&#22411;&#35745;&#31639;&#27010;&#24565;&#30340;&#21457;&#23637;&#65292;&#22914;&#22522;&#20110;&#38459;&#24615;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#65288;RRAM&#65289;&#30340;&#24179;&#38138;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#26550;&#26500;&#12290;CIM&#20801;&#35768;&#22312;&#20869;&#23384;&#21333;&#20803;&#20869;&#36827;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#21152;&#24555;&#25968;&#25454;&#22788;&#29702;&#36895;&#24230;&#65292;&#38477;&#20302;&#21151;&#32791;&#12290;&#39640;&#25928;&#30340;&#32534;&#35793;&#22120;&#31639;&#27861;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;&#24179;&#38138;CIM&#26550;&#26500;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#32534;&#35793;&#22120;&#19987;&#27880;&#20110;&#20026;CPU&#12289;GPU&#21644;&#20854;&#20182;&#20911;&#183;&#35834;&#20234;&#26364;&#26550;&#26500;&#29983;&#25104;&#20195;&#30721;&#65292;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#65292;&#20197;&#35206;&#30422;CIM&#26550;&#26500;&#12290;&#36328;&#23618;&#35843;&#24230;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#25552;&#39640;&#20102;CIM&#26680;&#24515;&#30340;&#21033;&#29992;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#35745;&#31639;&#12290;&#23613;&#31649;&#31867;&#20284;&#30340;&#27010;&#24565;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#38544;&#21547;&#22320;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#24179;&#38138;CIM&#26550;&#26500;&#30340;&#36328;&#23618;&#35843;&#24230;&#32570;&#20047;&#28165;&#26224;&#19988;&#21487;&#37327;&#21270;&#30340;&#31639;&#27861;&#23450;&#20041;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLSA-CIM&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#24179;&#38138;CIM&#26550;&#26500;&#30340;&#36328;&#23618;&#35843;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for efficient machine learning (ML) accelerators is growing rapidly, driving the development of novel computing concepts such as resistive random access memory (RRAM)-based tiled computing-in-memory (CIM) architectures. CIM allows to compute within the memory unit, resulting in faster data processing and reduced power consumption. Efficient compiler algorithms are essential to exploit the potential of tiled CIM architectures. While conventional ML compilers focus on code generation for CPUs, GPUs, and other von Neumann architectures, adaptations are needed to cover CIM architectures. Cross-layer scheduling is a promising approach, as it enhances the utilization of CIM cores, thereby accelerating computations. Although similar concepts are implicitly used in previous work, there is a lack of clear and quantifiable algorithmic definitions for cross-layer scheduling for tiled CIM architectures. To close this gap, we present CLSA-CIM, a cross-layer scheduling algorithm for tiled
&lt;/p&gt;</description></item><item><title>E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07595</link><description>&lt;p&gt;
E3x&#65306;&#31616;&#21270;&#30340;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07595
&lt;/p&gt;
&lt;p&gt;
E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E3x&#65292;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#36719;&#20214;&#21253;&#65292;&#35813;&#32593;&#32476;&#22312;&#19977;&#32500;&#31354;&#38388;&#30340;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#26041;&#38754;&#31561;&#21464;&#12290;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;$\mathrm{E}(3)$-&#31561;&#21464;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;/&#25110;&#36755;&#20986;&#25968;&#25454;&#26159;&#19982;&#19977;&#32500;&#23545;&#35937;&#30456;&#20851;&#30340;&#25968;&#37327;&#26102;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#26159;&#22240;&#20026;&#27492;&#31867;&#25968;&#37327;&#65288;&#20363;&#22914;&#20301;&#32622;&#65289;&#30340;&#25968;&#20540;&#36890;&#24120;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#22352;&#26631;&#31995;&#32479;&#12290;&#22312;&#21442;&#32771;&#31995;&#30340;&#21464;&#25442;&#19979;&#65292;&#36825;&#20123;&#20540;&#20250;&#21487;&#39044;&#27979;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#35268;&#21017;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#32622;&#30340;$\mathrm{E}(3)$-&#31561;&#21464;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20445;&#35777;&#23436;&#20840;&#28385;&#36275;&#30456;&#20851;&#30340;&#21464;&#25442;&#35268;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;E3x&#30340;&#20195;&#30721;&#21487;&#20174;https://github.com/google-research/e3x&#33719;&#24471;&#65292;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20351;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#20855;&#26377;&#23545;&#22240;&#26524;&#20851;&#31995;&#39034;&#24207;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.07231</link><description>&lt;p&gt;
&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21457;&#29616;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#21450;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Use of Prior Knowledge to Discover Causal Additive Models with Unobserved Variables and its Application to Time Series Data. (arXiv:2401.07231v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#20855;&#26377;&#23545;&#22240;&#26524;&#20851;&#31995;&#39034;&#24207;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;&#26410;&#35266;&#27979;&#21464;&#37327;&#30340;&#22240;&#26524;&#21487;&#21152;&#27169;&#22411;&#65288;CAM-UV&#65289;&#30340;&#26041;&#27861;&#12290;CAM-UV&#20551;&#35774;&#22240;&#26524;&#20989;&#25968;&#37319;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#24182;&#23384;&#22312;&#28508;&#22312;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#39640;&#25928;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#19982;&#20854;&#20182;&#29616;&#26377;&#30340;&#22240;&#26524;&#20989;&#25968;&#27169;&#22411;&#19981;&#21516;&#65292;&#21407;&#22987;&#30340;CAM-UV&#31639;&#27861;&#19981;&#23547;&#27714;&#35266;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#39034;&#24207;&#65292;&#32780;&#26159;&#26088;&#22312;&#30830;&#23450;&#27599;&#20010;&#35266;&#27979;&#21464;&#37327;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20013;&#25552;&#20986;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#65292;&#20363;&#22914;&#29702;&#35299;&#26576;&#20123;&#21464;&#37327;&#19981;&#33021;&#25104;&#20026;&#29305;&#23450;&#21464;&#37327;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#34701;&#20837;&#22240;&#26524;&#22312;&#26102;&#38388;&#19978;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#31639;&#27861;&#25193;&#23637;&#20026;&#31532;&#20108;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#31532;&#19968;&#20010;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two methods for causal additive models with unobserved variables (CAM-UV). CAM-UV assumes that the causal functions take the form of generalized additive models and that latent confounders are present. First, we propose a method that leverages prior knowledge for efficient causal discovery. Then, we propose an extension of this method for inferring causality in time series data. The original CAM-UV algorithm differs from other existing causal function models in that it does not seek the causal order between observed variables, but rather aims to identify the causes for each observed variable. Therefore, the first proposed method in this paper utilizes prior knowledge, such as understanding that certain variables cannot be causes of specific others. Moreover, by incorporating the prior knowledge that causes precedes their effects in time, we extend the first algorithm to the second method for causal discovery in time series data. We validate the first proposed method
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#19978;&#19979;&#30028;&#65292;&#36824;&#35299;&#20915;&#20102;&#22810;&#31867;&#23398;&#20064;&#20013;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#30340;&#20215;&#26684;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05794</link><description>&lt;p&gt;
&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounds on the price of feedback for mistake-bounded online learning. (arXiv:2401.05794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05794
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;&#38169;&#35823;&#26377;&#30028;&#22312;&#32447;&#23398;&#20064;&#20013;&#21453;&#39304;&#20215;&#26684;&#30340;&#19978;&#19979;&#30028;&#65292;&#36824;&#35299;&#20915;&#20102;&#22810;&#31867;&#23398;&#20064;&#20013;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#30340;&#20215;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25913;&#36827;&#20102;(Auer&#21644;Long&#65292;Machine Learning&#65292;1999)&#20013;&#21508;&#31181;&#22312;&#32447;&#23398;&#20064;&#22330;&#26223;&#30340;&#33509;&#24178;&#26368;&#22351;&#24773;&#20917;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#24310;&#36831;&#27169;&#26865;&#20004;&#21487;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;2&#20493;&#65292;&#23558;&#20989;&#25968;&#26063;&#32452;&#21512;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;2.41&#20493;&#65292;&#23558;&#19981;&#30693;&#24615;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;1.09&#20493;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#21516;&#19968;&#35770;&#25991;&#20013;&#20989;&#25968;&#26063;&#32452;&#21512;&#23398;&#20064;&#30340;&#19979;&#30028;&#65292;&#23558;&#20854;&#32553;&#23567;&#21040;&#920;(ln{k})&#30340;&#22240;&#23376;&#65292;&#19982;&#19978;&#30028;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;(Lon&#65292;Theoretical Computer Science&#65292;2020)&#20013;&#20851;&#20110;&#22810;&#31867;&#23398;&#20064;&#26631;&#20934;&#21453;&#39304;&#19982;&#36172;&#24466;&#21453;&#39304;&#20215;&#26684;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;(Feng&#31561;&#20154;&#65292;Theoretical Computer Science&#65292;2023)&#20013;$r$-&#36755;&#20837;&#24310;&#36831;&#27169;&#26865;&#20004;&#21487;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#30028;&#32553;&#23567;&#20102;$r$&#20493;&#65292;&#19982;&#21516;&#19968;&#35770;&#25991;&#20013;&#30340;&#19979;&#30028;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We improve several worst-case bounds for various online learning scenarios from (Auer and Long, Machine Learning, 1999). In particular, we sharpen an upper bound for delayed ambiguous reinforcement learning by a factor of 2, an upper bound for learning compositions of families of functions by a factor of 2.41, and an upper bound for agnostic learning by a factor of 1.09. We also improve a lower bound from the same paper for learning compositions of $k$ families of functions by a factor of $\Theta(\ln{k})$, matching the upper bound up to a constant factor. In addition, we solve a problem from (Long, Theoretical Computer Science, 2020) on the price of bandit feedback with respect to standard feedback for multiclass learning, and we improve an upper bound from (Feng et al., Theoretical Computer Science, 2023) on the price of $r$-input delayed ambiguous reinforcement learning by a factor of $r$, matching a lower bound from the same paper up to the leading term.
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03506</link><description>&lt;p&gt;
DiarizationLM: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35828;&#35805;&#20154;&#20998;&#31163;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20998;&#31163;&#38169;&#35823;&#29575;&#65292;&#23545;&#22810;&#31181;&#30446;&#26631;&#37117;&#26377;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiarizationLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#30446;&#26631;&#65292;&#22914;&#25913;&#21892;&#20998;&#31163;&#23545;&#35805;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#65292;&#25110;&#20943;&#23569;&#35789;&#32423;&#20998;&#31163;&#38169;&#35823;&#29575;&#65288;WDER&#65289;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#30340;&#36755;&#20986;&#34987;&#34920;&#31034;&#20026;&#19968;&#31181;&#32039;&#20945;&#30340;&#25991;&#26412;&#26684;&#24335;&#65292;&#20854;&#21253;&#21547;&#22312;&#19968;&#20010;&#21487;&#36873;&#25321;&#35843;&#25972;&#30340;LLM&#30340;&#25552;&#31034;&#20013;&#12290;LLM&#30340;&#36755;&#20986;&#21487;&#20197;&#20316;&#20026;&#25152;&#38656;&#25913;&#36827;&#30340;&#31934;&#32454;&#21270;&#20998;&#31163;&#32467;&#26524;&#12290;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;ASR&#21644;&#35828;&#35805;&#20154;&#20998;&#31163;&#31995;&#32479;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29616;&#26377;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;finetuned&#30340;PaLM 2-S&#27169;&#22411;&#21487;&#20197;&#22312;Fisher&#30005;&#35805;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23558;WDER&#38477;&#20302;55.5%&#65292;&#22312;Callhome&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#38477;&#20302;44.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.03006</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36215;
&lt;/p&gt;
&lt;p&gt;
The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#65292;&#35814;&#32454;&#20171;&#32461;&#20854;&#26465;&#20214;&#26041;&#27861;&#65292;&#24182;&#23457;&#26597;&#20102;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;11&#20010;&#20855;&#20307;&#30340;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#65292;&#23427;&#20204;&#30340;&#30452;&#35273;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20221;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#28165;&#26224;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01783</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#30340;&#36229;&#27874;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#29992;&#20110;&#25968;&#20540;&#35299;PDE&#65292;&#26368;&#36817;&#21457;&#23637;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22914;PINN&#21644;&#31070;&#32463;&#31639;&#23376;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#32570;&#28857;&#65292;&#26377;&#24456;&#22810;&#31181;&#31867;&#22411;&#30340;&#30740;&#31350;&#23558;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#65292;&#23558;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#25968;&#20540;&#36890;&#37327;&#26367;&#25442;&#20026;&#31070;&#32463;&#31639;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#21463;&#25968;&#20540;&#26041;&#26696;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;FNO&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19982;&#21407;&#22987;&#26041;&#27861;&#30340;&#27604;&#36739;&#20855;&#26377;&#25968;&#20540;&#26041;&#26696;&#21644;FNO&#30340;&#20248;&#21183;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21270;&#21512;&#29289;&#20445;&#30041;&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#21892;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#24211;&#30340;&#36136;&#37327;&#12290; (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)</title><link>http://arxiv.org/abs/2401.01506</link><description>&lt;p&gt;
AIRI: &#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#20445;&#30041;&#25351;&#25968;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence. (arXiv:2401.01506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01506
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21270;&#21512;&#29289;&#20445;&#30041;&#25351;&#25968;&#65292;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#21892;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#24211;&#30340;&#36136;&#37327;&#12290; (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kov&#225;t's&#20445;&#30041;&#25351;&#25968;&#65288;RI&#65289;&#26159;&#20351;&#29992;&#27668;&#30456;&#33394;&#35889;&#27979;&#37327;&#30340;&#21270;&#23398;&#32467;&#26500;&#37492;&#23450;&#20013;&#24120;&#29992;&#30340;&#25351;&#26631;&#12290;&#30001;&#20110;&#21019;&#24314;&#35266;&#23519;&#21040;&#30340;RI&#20540;&#30340;&#24211;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32467;&#26500;&#39044;&#27979;&#26631;&#20934;&#21322;&#26497;&#24615;&#26609;&#30340;RI&#20540;&#12290;&#35813;&#32593;&#32476;&#29983;&#25104;&#30340;&#39044;&#27979;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;15.1&#65292;&#22312;&#35823;&#24046;&#20998;&#24067;&#23614;&#37096;&#30340;&#37327;&#21270;&#20013;&#65292;95%&#30340;&#32477;&#23545;&#35823;&#24046;&#20026;46.5&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#20445;&#30041;&#25351;&#25968;&#65288;AIRI&#65289;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#23427;&#34987;&#29992;&#20110;&#39044;&#27979;NIST EI-MS&#20809;&#35889;&#24211;&#30340;RI&#20540;&#12290;&#36825;&#20123;RI&#20540;&#29992;&#20110;&#25913;&#36827;&#21270;&#23398;&#37492;&#23450;&#26041;&#27861;&#21644;&#25552;&#39640;&#24211;&#30340;&#36136;&#37327;&#12290;&#22312;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#20272;&#31639;&#19981;&#30830;&#23450;&#24615;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;&#20026;&#20102;&#37327;&#21270;&#25105;&#20204;&#32593;&#32476;&#23545;&#27599;&#20010;&#21333;&#29420;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;8&#20010;&#32593;&#32476;&#30340;&#36755;&#20986;&#26469;&#35745;&#31639;&#39044;&#27979;&#30340;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
The Kov\'ats Retention index (RI) is a quantity measured using gas chromatography and commonly used in the identification of chemical structures. Creating libraries of observed RI values is a laborious task, so we explore the use of a deep neural network for predicting RI values from structure for standard semipolar columns. This network generated predictions with a mean absolute error of 15.1 and, in a quantification of the tail of the error distribution, a 95th percentile absolute error of 46.5. Because of the Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was used to predict RI values for the NIST EI-MS spectral libraries. These RI values are used to improve chemical identification methods and the quality of the library. Estimating uncertainty is an important practical need when using prediction models. To quantify the uncertainty of our network for each individual prediction, we used the outputs of an ensemble of 8 networks to calculate a predicted standard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;</title><link>http://arxiv.org/abs/2401.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#37327;&#23376;&#22330;&#35770;&#22810;&#26684;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#37319;&#26679;&#31163;&#25955;&#22330;&#37197;&#32622;$\phi$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$S$&#26159;&#26576;&#20010;&#37327;&#23376;&#22330;&#35770;&#36830;&#32493;&#27431;&#20960;&#37324;&#24471;&#20316;&#29992;$\mathcal S$&#30340;&#26684;&#28857;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#23558;&#35813;&#23494;&#24230;&#36817;&#20284;&#35270;&#20026;&#24213;&#23618;&#20989;&#25968;&#23494;&#24230;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#30340;&#23398;&#20064;&#31639;&#23376;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;$\mathcal V_t$&#30340;&#26041;&#27861;&#65292;&#20854;&#26102;&#38388;&#31215;&#20998;&#25552;&#20379;&#20102;&#33258;&#30001;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$&#30340;&#20989;&#25968;&#20998;&#24067;&#19982;&#30446;&#26631;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#24403;&#36873;&#25321;&#29305;&#23450;&#30340;&#26684;&#28857;&#26102;&#65292;&#31639;&#23376;$\mathcal V_t$&#21487;&#20197;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#32500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30690;&#37327;&#22330;$V_t$&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#26684;&#28857;&#19978;&#23454;&#29616;&#20102;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00313</link><description>&lt;p&gt;
&#22312;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Matching of Users and Creators in Two-Sided Markets with Departures. (arXiv:2401.00313v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36793;&#24066;&#22330;&#20013;&#21305;&#37197;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#35768;&#22810;&#22312;&#32447;&#24179;&#21488;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#32593;&#31449;&#65292;&#37117;&#26159;&#26725;&#25509;&#20869;&#23481;&#21019;&#20316;&#32773;&#19982;&#29992;&#25143;&#30340;&#21452;&#36793;&#24066;&#22330;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#30340;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#25143;&#20559;&#22909;&#21644;&#20915;&#31574;&#19978;&#65292;&#24182;&#27809;&#26377;&#21516;&#26102;&#32771;&#34385;&#21040;&#21019;&#20316;&#32773;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;&#23481;&#25512;&#33616;&#27169;&#22411;&#65292;&#26126;&#30830;&#20851;&#27880;&#29992;&#25143;-&#20869;&#23481;&#21305;&#37197;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20854;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#22914;&#26524;&#29992;&#25143;&#21644;&#21019;&#20316;&#32773;&#27809;&#26377;&#36275;&#22815;&#30340;&#21442;&#19982;&#24863;&#65292;&#20182;&#20204;&#37117;&#21487;&#33021;&#27704;&#20037;&#31163;&#24320;&#24179;&#21488;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#26681;&#25454;&#24403;&#21069;&#21305;&#37197;&#30340;&#23454;&#29992;&#24615;&#20915;&#23450;&#26159;&#21542;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#21442;&#19982;&#65306;&#29992;&#25143;&#22522;&#20110;&#25512;&#33616;&#20869;&#23481;&#19982;&#20854;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#21019;&#20316;&#32773;&#21017;&#22522;&#20110;&#20854;&#21463;&#20247;&#35268;&#27169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36138;&#24515;&#31639;&#27861;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#21019;&#20316;&#32773;&#30340;&#31163;&#24320;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#21442;&#19982;&#24230;&#20219;&#24847;&#19979;&#38477;&#65292;&#30456;&#23545;&#20110;&#32771;&#34385;&#21040;&#21452;&#26041;&#21033;&#30410;&#26368;&#22823;&#21270;&#25972;&#20307;&#21442;&#19982;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2312.15965</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#25506;&#32034;&#19982;&#21033;&#29992;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcemen Learning with Decoupling Exploration and Utilization. (arXiv:2312.15965v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36807;&#20110;&#20445;&#23432;&#65292;&#23545;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#22788;&#29702;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#23548;&#33268;&#31639;&#27861;&#21482;&#36866;&#24212;&#26576;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#27425;&#20248;&#35299;&#12290;&#21516;&#26679;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20043;&#21069;&#30340;&#24809;&#32602;&#24615;&#24754;&#35266;&#20027;&#20041;&#20063;&#21093;&#22842;&#20102;&#27169;&#22411;&#30340;&#25506;&#32034;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20048;&#35266;&#19982;&#24754;&#35266;&#28436;&#21592;&#24378;&#21270;&#23398;&#20064;&#65288;OPARL&#65289;&#12290;OPARL&#37319;&#29992;&#29420;&#29305;&#30340;&#21452;&#28436;&#21592;&#26041;&#27861;&#65306;&#20048;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#25506;&#32034;&#65292;&#24754;&#35266;&#28436;&#21592;&#19987;&#27880;&#20110;&#21033;&#29992;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#25506;&#32034;&#21644;&#21033;&#29992;&#31574;&#30053;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#20419;&#36827;&#20102;&#26356;&#24179;&#34913;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24754;&#35266;&#30340;&#21033;&#29992;&#31574;&#30053;&#20248;&#21270;&#30528;&#37325;&#20110;&#20135;&#29983;&#39640;&#22870;&#21169;&#21160;&#20316;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network(DNN) generalization is limited by the over-reliance of current offline reinforcement learning techniques on conservative processing of existing datasets. This method frequently results in algorithms that settle for suboptimal solutions that only adjust to a certain dataset. Similarly, in online reinforcement learning, the previously imposed punitive pessimism also deprives the model of its exploratory potential. Our research proposes a novel framework, Optimistic and Pessimistic Actor Reinforcement Learning (OPARL). OPARL employs a unique dual-actor approach: an optimistic actor dedicated to exploration and a pessimistic actor focused on utilization, thereby effectively differentiating between exploration and utilization strategies. This unique combination in reinforcement learning methods fosters a more balanced and efficient approach. It enables the optimization of policies that focus on actions yielding high rewards through pessimistic utilization strategies, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#40479;&#40483;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#38656;&#26631;&#27880;&#30340;&#26041;&#24335;&#65292;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.15824</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#40479;&#40483;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Few-Shot Bird Sound Classification. (arXiv:2312.15824v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#40479;&#40483;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26080;&#38656;&#26631;&#27880;&#30340;&#26041;&#24335;&#65292;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#34920;&#31034;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#22312;&#29983;&#29289;&#22768;&#23398;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#25910;&#38598;&#22823;&#37327;&#30340;&#22768;&#38899;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#38656;&#26631;&#27880;&#23601;&#33021;&#22815;&#20174;&#38899;&#39057;&#24405;&#38899;&#20013;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#40479;&#40483;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#22330;&#26223;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#40479;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#39640;&#40479;&#27963;&#36291;&#30340;&#31383;&#21475;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#23398;&#20064;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) in audio holds significant potential across various domains, particularly in situations where abundant, unlabeled data is readily available at no cost. This is particularly pertinent in bioacoustics, where biologists routinely collect extensive sound datasets from the natural environment. In this study, we demonstrate that SSL is capable of acquiring meaningful representations of bird sounds from audio recordings without the need for annotations. Our experiments showcase that these learned representations exhibit the capacity to generalize to new bird species in few-shot learning (FSL) scenarios. Additionally, we show that selecting windows with high bird activation for self-supervised learning, using a pretrained audio neural network, significantly enhances the quality of the learned representations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#31383;&#21475;&#21270;&#27880;&#24847;&#32593;&#32476;&#65288;SWAN&#65289;&#65292;&#36890;&#36807;&#31383;&#21475;&#25552;&#31034;&#21644;&#25513;&#30721;&#27880;&#24847;&#36716;&#25442;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#38271;&#24230;&#30340;&#20851;&#27880;&#21306;&#38388;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#20449;&#20219;&#39044;&#27979;&#20013;&#31383;&#21475;&#22823;&#23567;&#30340;&#25361;&#36873;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.10209</link><description>&lt;p&gt;
&#36229;&#36234;&#32463;&#39564;&#31383;&#21475;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20449;&#20219;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles. (arXiv:2312.10209v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10209
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#31383;&#21475;&#21270;&#27880;&#24847;&#32593;&#32476;&#65288;SWAN&#65289;&#65292;&#36890;&#36807;&#31383;&#21475;&#25552;&#31034;&#21644;&#25513;&#30721;&#27880;&#24847;&#36716;&#25442;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#38271;&#24230;&#30340;&#20851;&#27880;&#21306;&#38388;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#35299;&#20915;&#20449;&#20219;&#39044;&#27979;&#20013;&#31383;&#21475;&#22823;&#23567;&#30340;&#25361;&#36873;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#20869;&#37096;&#29366;&#24577;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24341;&#21457;&#20102;&#20154;&#31867;&#29366;&#24577;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#30340;&#20852;&#36215;&#12290;&#30456;&#27604;&#20110;&#35832;&#22914;&#24778;&#35766;&#21644;&#28902;&#24700;&#31561;&#24555;&#36895;&#29366;&#24577;&#21464;&#21270;&#65292;&#24314;&#27169;&#20687;&#20449;&#20219;&#21644;&#28385;&#24847;&#24230;&#36825;&#26679;&#28176;&#36827;&#29366;&#24577;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#26631;&#31614;&#31232;&#30095;&#24615;&#65306;&#38271;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36890;&#24120;&#21482;&#19982;&#19968;&#20010;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#23548;&#33268;&#24456;&#38590;&#30830;&#23450;&#29366;&#24577;&#21464;&#21270;&#30340;&#20851;&#38190;&#26102;&#21051;&#12290;&#31383;&#21475;&#21270;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23616;&#37096;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#23545;&#31383;&#21475;&#22823;&#23567;&#25935;&#24863;&#65292;&#30830;&#23450;&#26368;&#20339;&#31383;&#21475;&#22823;&#23567;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#25628;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#31383;&#21475;&#21270;&#27880;&#24847;&#32593;&#32476;&#65288;SWAN&#65289;&#65292;&#23427;&#20351;&#29992;&#31383;&#21475;&#25552;&#31034;&#21644;&#25513;&#30721;&#27880;&#24847;&#36716;&#25442;&#65292;&#20197;&#23454;&#29616;&#36873;&#25321;&#20855;&#26377;&#28789;&#27963;&#38271;&#24230;&#30340;&#20851;&#27880;&#21306;&#38388;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#20449;&#20219;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;SWAN&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans' internal states play a key role in human-machine interaction, leading to the rise of human state estimation as a prominent field. Compared to swift state changes such as surprise and irritation, modeling gradual states like trust and satisfaction are further challenged by label sparsity: long time-series signals are usually associated with a single label, making it difficult to identify the critical span of state shifts. Windowing has been one widely-used technique to enable localized analysis of long time-series data. However, the performance of downstream models can be sensitive to the window size, and determining the optimal window size demands domain expertise and extensive search. To address this challenge, we propose a Selective Windowing Attention Network (SWAN), which employs window prompts and masked attention transformation to enable the selection of attended intervals with flexible lengths. We evaluate SWAN on the task of trust prediction on a new multimodal driving 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09084</link><description>&lt;p&gt;
&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#33021;&#21147;&#20063;&#22312;&#22686;&#21152;&#12290;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#30340;&#20107;&#20214;&#39537;&#21160;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26174;&#33879;&#38477;&#20302;&#25512;&#29702;&#33021;&#32791;&#30340;&#28508;&#22312;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21487;&#20197;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#20219;&#21153;&#24615;&#33021;&#29978;&#33267;&#19981;&#33021;&#19982;LSTM&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#20284;&#20046;&#26159;&#19968;&#20010;&#36965;&#36828;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411; - &#20855;&#20307;&#26469;&#35828;&#26159;&#22522;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#21517;&#20026;EGRU&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#30340;SpiNNaker 2&#33455;&#29255;&#12290;SpiNNaker 2&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#20247;&#26680;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#65292;&#32780;EGRU&#26159;&#20026;&#20102;&#22312;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36825;&#31181;&#30828;&#20214;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#20010;&#23454;&#29616;&#26631;&#24535;&#30528;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#30340;&#31532;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the firs
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#25506;&#31350;&#20102;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#23545;&#24188;&#20799;&#35270;&#35273;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#35821;&#35328;&#36755;&#20837;&#26377;&#38480;&#30340;&#24773;&#22659;&#19979;&#65292;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#20173;&#28982;&#33021;&#22815;&#25552;&#21319;&#24188;&#20799;&#30340;&#35270;&#35273;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.04118</link><description>&lt;p&gt;
&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#22609;&#36896;&#24188;&#20799;&#35270;&#35273;&#65306;&#19968;&#39033;&#20851;&#20110;&#21452;&#21442;&#19982;&#28216;&#25103;&#30340;&#35745;&#31639;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play. (arXiv:2312.04118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#25506;&#31350;&#20102;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#23545;&#24188;&#20799;&#35270;&#35273;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#35821;&#35328;&#36755;&#20837;&#26377;&#38480;&#30340;&#24773;&#22659;&#19979;&#65292;&#25252;&#29702;&#32773;&#30340;&#35848;&#35805;&#20173;&#28982;&#33021;&#22815;&#25552;&#21319;&#24188;&#20799;&#30340;&#35270;&#35273;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23156;&#20799;&#35782;&#21035;&#21644;&#20998;&#31867;&#29289;&#20307;&#30340;&#33021;&#21147;&#36880;&#28176;&#21457;&#23637;&#12290;&#29983;&#21629;&#30340;&#31532;&#20108;&#24180;&#26631;&#24535;&#30528;&#26356;&#22810;&#35821;&#20041;&#35270;&#35273;&#34920;&#24449;&#30340;&#20986;&#29616;&#21644;&#23545;&#35789;&#27719;&#21547;&#20041;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#36825;&#34920;&#26126;&#35821;&#35328;&#36755;&#20837;&#21487;&#33021;&#22312;&#22609;&#36896;&#35270;&#35273;&#34920;&#24449;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#36866;&#21512;&#23398;&#20064;&#21333;&#35789;&#30340;&#24773;&#22659;&#19979;&#65292;&#22914;&#21452;&#21442;&#19982;&#28216;&#25103;&#20250;&#35805;&#20013;&#65292;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#20063;&#26159;&#31232;&#23569;&#21644;&#19981;&#26126;&#30830;&#30340;&#65292;&#24120;&#24120;&#25351;&#30340;&#26159;&#19982;&#20799;&#31461;&#27880;&#24847;&#30340;&#29289;&#20307;&#19981;&#21516;&#30340;&#29289;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#21040;&#24213;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#22686;&#24378;&#35270;&#35273;&#34920;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21452;&#21442;&#19982;&#28216;&#25103;&#36807;&#31243;&#20013;&#23398;&#20064;&#35270;&#35273;&#34920;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30001;&#24188;&#20799;&#20195;&#29702;&#20154;&#24863;&#30693;&#21040;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#65292;&#22312;&#19981;&#21516;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#31227;&#21160;&#21644;&#26059;&#36716;&#29609;&#20855;&#29289;&#20307;&#65292;&#24182;&#21516;&#26102;&#21548;&#21040;&#34987;&#24314;&#27169;&#20026;&#23383;&#24149;&#30340;&#25252;&#29702;&#32773;&#30340;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18672</link><description>&lt;p&gt;
&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29702;&#35299;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;(LHC)&#19978;&#20135;&#29983;&#30340;&#22823;&#37327;&#39640;&#33021;&#31890;&#23376;&#30896;&#25758;&#25968;&#25454;&#26102;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#30896;&#25758;&#20107;&#20214;&#30340;&#25968;&#25454;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#22270;&#32467;&#26500;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#28145;&#24230;&#20960;&#20309;&#26041;&#27861;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#65292;&#24050;&#32463;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20219;&#21153;&#26159;&#21943;&#27880;&#26631;&#35760;&#65292;&#20854;&#20013;&#21943;&#27880;&#34987;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#21644;&#20854;&#32452;&#25104;&#31890;&#23376;&#20043;&#38388;&#30340;&#36793;&#36830;&#25509;&#30340;&#28857;&#20113;&#12290;LHC&#31890;&#23376;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#20197;&#21450;&#29992;&#20110;&#20854;&#20998;&#26512;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22823;&#22823;&#20419;&#36827;&#20102;&#24320;&#21457;&#26367;&#20195;&#24555;&#36895;&#19988;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#22914;&#37327;&#23376;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#26469;&#21033;&#29992;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.12023</link><description>&lt;p&gt;
LQ-LoRA: &#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12023
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#31639;&#27861;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#37327;&#21270;&#37096;&#20998;&#20445;&#25345;&#22266;&#23450;&#65292;&#21482;&#26377;&#20302;&#31209;&#37096;&#20998;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#37096;&#20998;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#34920;&#36798;&#65292;&#21487;&#20197;&#26681;&#25454;&#24635;&#20307;&#20869;&#23384;&#39044;&#31639;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#65288;&#20363;&#22914;&#27604;&#29305;&#23485;&#24230;&#12289;&#22359;&#22823;&#23567;&#65289;&#32473;&#23450;&#27599;&#20010;&#30697;&#38453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#36817;&#20284;&#26469;&#21152;&#26435;&#30697;&#38453;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#37325;&#26500;&#30446;&#26631;&#12290;&#22312;RoBERTa&#21644;LLaMA-2&#65288;7B&#21644;70B&#65289;&#30340;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;LQ-LoRA&#65289;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;QLoRA&#21644;GPTQ-LoRA&#65292;&#24182;&#23454;&#29616;&#20102;&#28608;&#36827;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
&lt;/p&gt;</description></item><item><title>FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.10359</link><description>&lt;p&gt;
FIKIT&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#19982;&#20869;&#26680;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification. (arXiv:2311.10359v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.10359
&lt;/p&gt;
&lt;p&gt;
FIKIT&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#23454;&#26102;GPU&#22810;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#20855;&#26377;&#20869;&#26680;&#35782;&#21035;&#21151;&#33021;&#65292;&#33021;&#22815;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#22635;&#20805;&#31354;&#38386;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24182;&#34892;&#24037;&#20316;&#36127;&#36733;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12289;&#25512;&#26029;&#21644;&#19968;&#33324;HPC&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;GPU&#35774;&#22791;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#21152;&#36895;&#12290;&#22312;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#20849;&#20139;&#26469;&#25552;&#20379;GPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#65292;&#22240;&#20026;&#24635;&#26159;&#26377;&#26356;&#22810;&#30340;&#20219;&#21153;&#35831;&#27714;&#32780;&#19981;&#26159;&#21487;&#29992;&#30340;GPU&#25968;&#37327;&#12290;&#29616;&#26377;&#30340;GPU&#20849;&#20139;&#35299;&#20915;&#26041;&#26696;&#30528;&#37325;&#20110;&#20943;&#23569;&#22810;&#20010;&#20316;&#19994;&#20105;&#22842;&#21333;&#20010;GPU&#26102;&#30340;&#20219;&#21153;&#32423;&#31561;&#24453;&#26102;&#38388;&#25110;&#20219;&#21153;&#32423;&#20999;&#25442;&#25104;&#26412;&#12290;&#36830;&#32493;&#35745;&#31639;&#35831;&#27714;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#20808;&#32423;&#65292;&#23545;&#20110;&#20849;&#20139;GPU&#35774;&#22791;&#65292;&#23545;QoS&#20135;&#29983;&#20102;&#38750;&#23545;&#31216;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#24037;&#20316;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#24102;&#26469;&#30340;&#20869;&#26680;&#32423;&#20248;&#21270;&#26426;&#20250;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#26680;&#32423;&#35843;&#24230;&#31574;&#30053;FIKIT&#65306;&#22635;&#20805;&#20869;&#26680;&#38388;&#31354;&#38386;&#26102;&#38388;&#12290;FIKIT&#21253;&#21547;&#20219;&#21153;&#32423;&#20248;&#20808;&#32423;&#20449;&#24687;&#12289;&#32454;&#31890;&#24230;&#20869;&#26680;&#35782;&#21035;&#21644;&#20869;&#26680;&#27979;&#37327;&#65292;&#20801;&#35768;&#20302;&#20248;&#20808;&#32423;&#20219;&#21153;&#22312;&#39640;&#20248;&#20808;&#32423;&#20219;&#21153;&#30340;&#20869;&#26680;&#38388;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2311.01771</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generalized Low-Rank Tensor Contextual Bandits. (arXiv:2311.01771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;G-LowTESTR&#31639;&#27861;&#26469;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#19968;&#31181;&#26032;&#39062;&#30340;&#36172;&#21338;&#31639;&#27861;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22810;&#32500;&#25968;&#25454;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#22266;&#26377;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#39640;&#21487;&#29992;&#21644;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#26381;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#24773;&#22659;&#36172;&#21338;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#20316;&#30001;&#19977;&#20010;&#29305;&#24449;&#21521;&#37327;&#32452;&#25104;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#24352;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#22870;&#21169;&#26159;&#36890;&#36807;&#23558;&#21160;&#20316;&#30340;&#29305;&#24449;&#24352;&#37327;&#19982;&#19968;&#20010;&#22266;&#23450;&#20294;&#26410;&#30693;&#30340;&#21442;&#25968;&#24352;&#37327;&#30340;&#20869;&#31215;&#24212;&#29992;&#20110;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#26469;&#30830;&#23450;&#30340;&#65292;&#32780;&#36825;&#20010;&#21442;&#25968;&#24352;&#37327;&#20855;&#26377;&#36739;&#20302;&#30340;&#31649;&#29366;&#31209;&#12290;&#20026;&#20102;&#23454;&#29616;&#25506;&#32034;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24191;&#20041;&#20302;&#31209;&#24352;&#37327;&#25506;&#32034;&#23376;&#31354;&#38388;&#28982;&#21518;&#32454;&#21270;&#8221;&#30340;&#26032;&#31639;&#27861;&#65288;G-LowTESTR&#65289;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#65292;&#20197;&#25506;&#32034;&#23884;&#20837;&#22312;&#20915;&#31574;&#24773;&#22659;&#20013;&#30340;&#26412;&#36136;&#20302;&#31209;&#24352;&#37327;&#23376;&#31354;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#23558;&#21407;&#22987;&#27010;&#29575;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#21270;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to build a novel bandits algorithm that is capable of fully harnessing the power of multi-dimensional data and the inherent non-linearity of reward functions to provide high-usable and accountable decision-making services. To this end, we introduce a generalized low-rank tensor contextual bandits model in which an action is formed from three feature vectors, and thus can be represented by a tensor. In this formulation, the reward is determined through a generalized linear function applied to the inner product of the action's feature tensor and a fixed but unknown parameter tensor with a low tubal rank. To effectively achieve the trade-off between exploration and exploitation, we introduce a novel algorithm called "Generalized Low-Rank Tensor Exploration Subspace then Refine" (G-LowTESTR). This algorithm first collects raw data to explore the intrinsic low-rank tensor subspace information embedded in the decision-making scenario, and then converts the original prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#21644;&#32463;&#20856;&#30340;&#22270;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#22270;&#21106;&#31639;&#27861;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01475</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21106;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#28145;&#24230;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#21644;&#32463;&#20856;&#30340;&#22270;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#22270;&#21106;&#31639;&#27861;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#22312;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#23558;&#22270;&#20687;&#20013;&#30340;&#19981;&#21516;&#35821;&#20041;&#27169;&#24335;&#20998;&#32452;&#12290;&#31867;&#20284;&#22320;&#65292;&#22270;&#20687;&#32858;&#31867;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#20041;&#20869;&#23481;&#25628;&#32034;&#22270;&#20687;&#30340;&#20998;&#32452;&#65292;&#20063;&#19981;&#38656;&#35201;&#30417;&#30563;&#12290;&#32463;&#20856;&#22320;&#65292;&#36825;&#20004;&#20010;&#38382;&#39064;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#20174;&#22362;&#23454;&#30340;&#25968;&#23398;&#27010;&#24565;&#20013;&#20135;&#29983;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#31185;&#23398;&#30028;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#20102;&#22522;&#20110;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24456;&#23569;&#21033;&#29992;&#32463;&#20856;&#26041;&#27861;&#21462;&#24471;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#30340;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;&#31574;&#30053;&#65292;&#23558;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#36827;&#23637;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#21106;&#36827;&#34892;&#36845;&#20195;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised image segmentation aims at grouping different semantic patterns in an image without the use of human annotation. Similarly, image clustering searches for groupings of images based on their semantic content without supervision. Classically, both problems have captivated researchers as they drew from sound mathematical concepts to produce concrete applications. With the emergence of deep learning, the scientific community turned its attention to complex neural network-based solvers that achieved impressive results in those domains but rarely leveraged the advances made by classical methods. In this work, we propose a patch-based unsupervised image segmentation strategy that bridges advances in unsupervised feature extraction from deep clustering methods with the algorithmic help of classical graph-based methods. We show that a simple convolutional neural network, trained to classify image patches and iteratively regularized using graph cuts, naturally leads to a state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.19802</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#38543;&#26426;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21442;&#25968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;PPM&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20174;&#26412;&#36136;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#28909;&#21147;&#23398;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#65288;&#35760;&#20026;$\Theta$&#65289;&#19982;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#65288;&#35760;&#20026;$X$&#65289;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#22120;&#30340;&#20316;&#29992;&#26159;&#39537;&#21160;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#33021;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26679;&#26412;$X$&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;$\Theta$&#30340;&#29109;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#20102;&#19968;&#20010;&#28909;&#24211;&#65292;&#26377;&#25928;&#22320;&#23384;&#20648;&#20102;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#28909;&#24211;&#30340;&#35282;&#33394;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#19988;&#19968;&#33268;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28909;&#21147;&#23398;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13121</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21152;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Addition in Transformers. (arXiv:2310.13121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20687;Transformer&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#23545;&#20110;&#20854;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#32463;&#36807;&#35757;&#32451;&#36827;&#34892;&#25972;&#25968;&#21152;&#27861;&#30340;&#21333;&#23618;Transformer&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#23558;&#20219;&#21153;&#20998;&#20026;&#24182;&#34892;&#30340;&#12289;&#29305;&#23450;&#20110;&#25968;&#23383;&#30340;&#27969;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#30340;&#25968;&#23383;&#20301;&#32622;&#37319;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#35813;&#27169;&#22411;&#24320;&#22987;&#35745;&#31639;&#36739;&#26202;&#65292;&#20294;&#25191;&#34892;&#36895;&#24230;&#38750;&#24120;&#24555;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#31181;&#32597;&#35265;&#30340;&#39640;&#25439;&#22833;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#20104;&#20197;&#35299;&#37322;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35814;&#32454;&#35299;&#37322;&#20102;&#35813;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#21644;&#25968;&#23398;&#24314;&#27169;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#23545;&#20110;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#12289;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#31561;&#24191;&#27867;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20998;&#26512;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#22810;&#23618;Transformer&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10224</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#25512;&#24191;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26085;&#30410;&#22686;&#21152;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#24403;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#24191;&#27867;&#65292;&#22240;&#20026;&#32570;&#20047;&#26041;&#27861;&#35770;&#26631;&#20934;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#20013;&#24515;&#25110;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#36741;&#21161;&#22240;&#32032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23384;&#22312;&#36739;&#22823;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26222;&#36866;&#30340;&#12289;&#25968;&#25454;-&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65288;QUAVE&#65289;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#32467;&#21512;&#23454;&#38469;&#12289;&#22235;&#20803;&#25968;&#25110;&#36229;&#22797;&#20540;&#27169;&#22411;&#65292;&#25512;&#24191;&#23427;&#20204;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;QUAVE&#39318;&#20808;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#19981;&#21516;&#30340;&#23376;&#24102;&#65292;&#24471;&#21040;&#20302;&#39057;/&#36817;&#20284;&#39057;&#24102;&#21644;&#39640;&#39057;/&#32454;&#31890;&#24230;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#23545;&#26368;&#26377;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#19981;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08056</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65306;&#36890;&#36807;&#20449;&#24565;&#20256;&#25773;&#23545;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#65288;&#31216;&#20026;&#21253;&#65289;&#30340;&#32858;&#21512;&#32423;&#21035;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#30340;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#20363;&#32423;&#21035;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#24191;&#21578;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30001;&#20110;&#38544;&#31169;&#32771;&#34385;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#31532;&#19968;&#27493;&#65288;&#20266;&#26631;&#31614;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#20108;&#36827;&#21046;&#23454;&#20363;&#26631;&#31614;&#30340;&#21513;&#24067;&#26031;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#36890;&#36807;&#20197;&#19979;&#32422;&#26463;&#23558;covariate&#20449;&#24687;&#65288;&#21327;&#21464;&#37327;&#20449;&#24687;&#65289;&#21512;&#24182;&#36827;&#21435;&#65306;&#20855;&#26377;&#30456;&#20284;covariates&#30340;&#23454;&#20363;&#24212;&#35813;&#20855;&#26377;&#30456;&#20284;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#36890;&#36807;&#21253;&#32423;&#21035;&#30340;&#32858;&#21512;&#26631;&#31614;&#26469;&#32508;&#21512;covariate&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#26469;&#36793;&#32536;&#21270;&#21513;&#24067;&#26031;&#20998;&#24067;&#20197;&#33719;&#24471;&#20266;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#27493;&#65288;&#23884;&#20837;&#32454;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#31614;&#20026;&#23398;&#20064;&#22120;&#25552;&#20379;&#30417;&#30563;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#24341;&#20837;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#65292;&#21033;&#29992;&#23545;&#31216;&#30340;&#25237;&#24433;&#25805;&#20316;&#26469;&#26500;&#24314;&#20849;&#20139;&#30340;&#29942;&#39048;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#26469;&#26377;&#25928;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.06234</link><description>&lt;p&gt;
&#22823;&#22411;Vision Transformer&#30340;&#39640;&#25928;&#36866;&#24212;&#24615;&#36890;&#36807;Adapter&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing. (arXiv:2310.06234v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#30340;&#31574;&#30053;&#65292;&#26088;&#22312;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#21644;&#24341;&#20837;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#65292;&#21033;&#29992;&#23545;&#31216;&#30340;&#25237;&#24433;&#25805;&#20316;&#26469;&#26500;&#24314;&#20849;&#20139;&#30340;&#29942;&#39048;&#25805;&#20316;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#26469;&#26377;&#25928;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23481;&#37327;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#24335;&#65292;&#23558;&#28966;&#28857;&#20174;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#36716;&#21521;&#20102;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#36866;&#24212;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#36731;&#37327;&#32423;&#30340;&#36866;&#37197;&#22120;&#21450;&#20854;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#38656;&#35201;&#26356;&#26032;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Adapter&#37325;&#32452;&#65288;ARC&#65289;&#31574;&#30053;&#65292;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#36866;&#24212;&#21442;&#25968;&#30340;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#21442;&#25968;&#20849;&#20139;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#31216;&#30340;&#21521;&#19979;/&#21521;&#19978;&#25237;&#24433;&#26469;&#26500;&#24314;&#29942;&#39048;&#25805;&#20316;&#65292;&#36825;&#20123;&#25805;&#20316;&#22312;&#19981;&#21516;&#23618;&#20043;&#38388;&#20849;&#20139;&#12290;&#36890;&#36807;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#37325;&#26032;&#32553;&#25918;&#31995;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#26032;&#32452;&#21512;&#23618;&#36866;&#24212;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of high-capacity pre-trained models has revolutionized problem-solving in computer vision, shifting the focus from training task-specific models to adapting pre-trained models. Consequently, effectively adapting large pre-trained models to downstream tasks in an efficient manner has become a prominent research area. Existing solutions primarily concentrate on designing lightweight adapters and their interaction with pre-trained models, with the goal of minimizing the number of parameters requiring updates. In this study, we propose a novel Adapter Re-Composing (ARC) strategy that addresses efficient pre-trained model adaptation from a fresh perspective. Our approach considers the reusability of adaptation parameters and introduces a parameter-sharing scheme. Specifically, we leverage symmetric down-/up-projections to construct bottleneck operations, which are shared across layers. By learning low-dimensional re-scaling coefficients, we can effectively re-compose layer-adapti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05725</link><description>&lt;p&gt;
&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#23545;&#20844;&#24179;&#20998;&#31867;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Post-hoc Bias Scoring Is Optimal For Fair Classification. (arXiv:2310.05725v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#39564;&#20559;&#24046;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22312;&#28385;&#36275;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#22522;&#20110;&#20559;&#24046;&#20998;&#25968;&#30340;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#26159;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#24615;&#65288;DP&#65289;&#65292;&#26426;&#20250;&#22343;&#31561;&#65288;EOp&#65289;&#25110;&#31561;&#27010;&#29575;&#65288;EO&#65289;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#26126;&#30830;&#29305;&#24449;&#21270;&#65292;&#32467;&#26524;&#26159;&#19981;&#21463;&#32422;&#26463;&#20998;&#31867;&#22120;&#30340;&#31616;&#21333;&#20462;&#25913;&#35268;&#21017;&#12290;&#21363;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#21035;&#30340;&#20559;&#24046;&#24230;&#37327;&#65292;&#31216;&#20026;&#20559;&#24046;&#20998;&#25968;&#65292;&#32780;&#20462;&#25913;&#35268;&#21017;&#21017;&#26159;&#22312;&#26377;&#38480;&#37327;&#30340;&#20559;&#24046;&#20998;&#25968;&#20043;&#19978;&#30340;&#31616;&#21333;&#32447;&#24615;&#35268;&#21017;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21518;&#39564;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#20844;&#24179;&#24615;&#32422;&#26463;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;DP&#21644;EOp&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#25913;&#35268;&#21017;&#26159;&#22522;&#20110;&#21333;&#20010;&#20559;&#24046;&#20998;&#25968;&#30340;&#38408;&#20540;&#36873;&#25321;&#65292;&#32780;&#22312;EO&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#35843;&#25972;&#20855;&#26377;2&#20010;&#21442;&#25968;&#30340;&#32447;&#24615;&#20462;&#25913;&#35268;&#21017;&#12290;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#29992;&#20110;&#21253;&#21547;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#30340;&#22797;&#21512;&#32676;&#20307;&#20844;&#24179;&#24615;&#26631;&#20934;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a binary classification problem under group fairness constraints, which can be one of Demographic Parity (DP), Equalized Opportunity (EOp), or Equalized Odds (EO). We propose an explicit characterization of Bayes optimal classifier under the fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Namely, we introduce a novel instance-level measure of bias, which we call bias score, and the modification rule is a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows us to adapt to fairness constraints while maintaining high accuracy. In the case of DP and EOp constraints, the modification rule is thresholding a single bias score, while in the case of EO constraints we are required to fit a linear modification rule with 2 parameters. The method can also be applied for composite group-fairness criteria, such as ones involving several sensitive att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04741</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65306;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#36825;&#31181;&#24179;&#34913;&#36827;&#34892;&#20102;&#35299;&#21078;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#39318;&#20808;&#35299;&#20915;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#22256;&#22659;&#21450;&#20854;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#31995;&#12290;&#23427;&#23558;&#23398;&#20064;&#35825;&#23548;&#30340;&#28608;&#27963;&#21464;&#21270;&#19982;&#20808;&#21069;&#35835;&#20986;&#33539;&#22260;&#20869;&#30340;&#31283;&#23450;&#24615;&#31243;&#24230;&#21644;&#38646;&#31354;&#38388;&#30340;&#21464;&#21270;&#19982;&#21487;&#22609;&#24615;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;&#22312;&#22788;&#29702;&#20998;&#35010;CIFAR-110&#20219;&#21153;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#35813;&#26694;&#26550;&#38416;&#26126;&#20102;&#24120;&#29992;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;SI&#12289;EWC&#21644;LwF&#65289;&#20197;&#21450;&#37325;&#25918;&#31639;&#27861;&#65288;GEM&#21644;&#25968;&#25454;&#37325;&#25918;&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
&lt;/p&gt;</description></item><item><title>Chameleon&#26159;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#31614;&#21807;&#19968;&#35774;&#32622;&#20013;&#25552;&#39640;&#25104;&#21592;&#27844;&#38706;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03838</link><description>&lt;p&gt;
Chameleon: &#20351;&#29992;&#33258;&#36866;&#24212;&#27745;&#26579;&#22686;&#24378;&#26631;&#31614;&#21807;&#19968;&#25104;&#21592;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning. (arXiv:2310.03838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03838
&lt;/p&gt;
&lt;p&gt;
Chameleon&#26159;&#19968;&#31181;&#26032;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#22312;&#26631;&#31614;&#21807;&#19968;&#35774;&#32622;&#20013;&#25552;&#39640;&#25104;&#21592;&#27844;&#38706;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#20851;&#38190;&#24212;&#29992;&#20013;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;(ML)&#21518;&#65292;&#20010;&#20154;&#25968;&#25454;&#25552;&#20379;&#32773;&#38754;&#20020;&#22810;&#31181;&#38544;&#31169;&#38382;&#39064;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#25104;&#21592;&#25512;&#29702;(MI)&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#12290;&#30446;&#21069;&#30340;MI&#25915;&#20987;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#27745;&#26579;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#19988;&#26356;&#29616;&#23454;&#30340;&#20165;&#26631;&#31614;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#20165;&#22312;&#26597;&#35810;&#26679;&#26412;&#19978;&#25552;&#20379;&#39044;&#27979;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#30340;&#20165;&#26631;&#31614;MI&#25915;&#20987;&#22312;&#20302;&#35823;&#25253;&#29575;(FPR)&#24773;&#20917;&#19979;&#26080;&#27861;&#26377;&#25928;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#27861;Chameleon&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#25968;&#25454;&#27745;&#26579;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#26597;&#35810;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for model training. One such privacy risk is Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. Current state-of-the-art MI attacks capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label on a queried sample. We show that existing label-only MI attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel adaptive data poisoning strategy and an efficient query selection meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniPredict&#65292;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#22791;&#29702;&#35299;&#22810;&#26679;&#21270;&#34920;&#26684;&#36755;&#20837;&#21644;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniPredict&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.03266</link><description>&lt;p&gt;
UniPredict&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#36890;&#29992;&#30340;&#34920;&#26684;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniPredict&#65292;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#22791;&#29702;&#35299;&#22810;&#26679;&#21270;&#34920;&#26684;&#36755;&#20837;&#21644;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniPredict&#27169;&#22411;&#22312;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#26102;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#30784;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21028;&#21035;&#24314;&#27169;&#65292;&#24182;&#22312;&#20551;&#35774;&#22266;&#23450;&#30446;&#26631;&#21015;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36816;&#31639;&#65292;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#30340;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#36890;&#29992;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#22120;UniPredict&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;LLM&#25193;&#23637;&#21040;&#24222;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29702;&#35299;&#19981;&#21516;&#30340;&#34920;&#26684;&#36755;&#20837;&#24182;&#26681;&#25454;&#36755;&#20837;&#25351;&#20196;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;169&#20010;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20010;&#22810;&#21151;&#33021;&#30340;UniPredict&#27169;&#22411;&#22312;&#19982;&#26368;&#20339;&#30340;&#26641;&#25552;&#21319;&#27169;&#22411;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;5.4%&#21040;13.4%&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets with the capability of comprehending diverse tabular inputs and predicting for target variables following the input instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting b
&lt;/p&gt;</description></item><item><title>Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00229</link><description>&lt;p&gt;
&#22312;&#35268;&#21010;&#20013;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Spatial and Temporal Abstraction in Planning for Better Generalization. (arXiv:2310.00229v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00229
&lt;/p&gt;
&lt;p&gt;
Skipper&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#22312;&#26032;&#24773;&#22659;&#20013;&#25512;&#24191;&#23398;&#21040;&#30340;&#25216;&#33021;&#12290;&#23427;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#26377;&#24847;&#35782;&#35268;&#21010;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Skipper&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26102;&#31354;&#25277;&#35937;&#26469;&#25512;&#24191;&#22312;&#26032;&#24773;&#22659;&#20013;&#23398;&#21040;&#30340;&#25216;&#33021;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23427;&#33258;&#21160;&#23558;&#32473;&#23450;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#21487;&#31649;&#29702;&#30340;&#23376;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#31232;&#30095;&#20915;&#31574;&#21644;&#23545;&#29615;&#22659;&#30456;&#20851;&#37096;&#20998;&#30340;&#19987;&#27880;&#35745;&#31639;&#12290;&#36825;&#20381;&#36182;&#20110;&#20174;&#22238;&#28335;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#20026;&#26377;&#21521;&#22270;&#30340;&#25277;&#35937;&#20195;&#29702;&#38382;&#39064;&#30340;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#26377;&#26395;&#25552;&#20379;&#24110;&#21161;&#12290;&#38024;&#23545;&#27867;&#21270;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Skipper&#22312;&#38646;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human conscious planning, we propose Skipper, a model-based reinforcement learning agent utilizing spatio-temporal abstractions to generalize learned skills in novel situations. It automatically decomposes the given task into smaller, more manageable subtasks, and hence enables sparse decision-making and focused computation on the relevant parts of the environment. This relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices and edges are learned end-to-end from hindsight. Our theoretical analyses provide performance guarantees under appropriate assumptions and establish where our approach is expected to be helpful. Generalization-focused experiments validate Skipper's significant advantage in zero-shot generalization, compared to existing state-of-the-art hierarchical planning methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16746</link><description>&lt;p&gt;
&#38544;&#24615;&#39640;&#26031;&#36807;&#31243;&#34920;&#31034;&#20219;&#24847;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#22330;
&lt;/p&gt;
&lt;p&gt;
Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16746
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;RVGP&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#36924;&#36817;&#26041;&#27861;&#23545;&#28508;&#22312;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26159;&#29992;&#20110;&#23398;&#20064;&#26410;&#30693;&#20989;&#25968;&#21644;&#37327;&#21270;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#34892;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;GPs&#65292;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#22495;&#19978;&#30340;&#26631;&#37327;&#21644;&#21521;&#37327;&#25968;&#25454;&#65292;&#21253;&#25324;&#20986;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#21160;&#21147;&#31995;&#32479;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#20247;&#22810;&#39046;&#22495;&#20013;&#30340;&#24179;&#28369;&#27969;&#24418;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#25968;&#25454;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#24050;&#30693;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;RVGP&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#28508;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#21521;&#37327;&#20449;&#21495;&#30340;GP&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19982;&#20999;&#21521;&#19995;&#20851;&#32852;&#30340;&#36830;&#25509;Laplacian&#30340;&#29305;&#24449;&#20989;&#25968;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#36825;&#20123;&#29305;&#24449;&#20989;&#25968;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#30340;&#24120;&#35265;&#25968;&#25454;&#36817;&#20284;&#20013;&#36731;&#26494;&#25512;&#23548;&#20986;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RVGP&#22312;&#27969;&#24418;&#19978;&#20855;&#26377;&#20840;&#23616;&#35268;&#24459;&#24615;&#65292;&#20351;&#24471;&#20854;&#33021;&#22815;&#22312;&#20445;&#30041;&#22855;&#24322;&#24615;&#30340;&#21516;&#26102;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#21521;&#37327;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;RVGP&#26469;&#37325;&#26500;&#39640;&#23494;&#24230;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;</title><link>http://arxiv.org/abs/2309.12697</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#20248;&#20110;&#20854;&#20182;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#23376;&#24207;&#21015;&#30340;&#37325;&#21472;&#65288;&#20363;&#22914;BLEU&#65289;&#25110;&#20351;&#29992;&#23884;&#20837;&#65288;&#20363;&#22914;BERTScore&#65292;S-BERT&#65289;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#25105;&#20204;&#20165;&#23545;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#30452;&#25509;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20284;&#24615;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#24494;&#35843;&#30340;STS-B&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;STSScore&#26041;&#27861;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#19982;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#65292;&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#65292;&#20351;F-score&#25552;&#39640;&#33267;68.19%&#65288;0.75&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.08971</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#29992;&#20110;&#23567;&#26679;&#26412;&#29983;&#29289;&#22768;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound Detection. (arXiv:2309.08971v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27491;&#21017;&#21270;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#65292;&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#33021;&#65292;&#20351;F-score&#25552;&#39640;&#33267;68.19%&#65288;0.75&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21160;&#29289;&#34892;&#20026;&#65292;&#21033;&#29992;&#22768;&#38899;&#36827;&#34892;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#28982;&#32780;&#24456;&#38590;&#33719;&#24471;&#36275;&#22815;&#30340;&#26377;&#26631;&#27880;&#25968;&#25454;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36825;&#20123;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;Detection and Classification of Acoustic Scenes and Events (DCASE)&#31038;&#21306;&#23558;&#38382;&#39064;&#37325;&#26032;&#26694;&#26550;&#20026;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#23450;&#26399;&#20030;&#21150;&#19968;&#20010;&#20851;&#20110;&#20174;&#20165;&#26377;&#20116;&#20010;&#24102;&#26631;&#27880;&#31034;&#20363;&#20013;&#23398;&#20064;&#26816;&#27979;&#21160;&#29289;&#22768;&#38899;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#21487;&#20197;&#22312;&#26410;&#35265;&#36807;&#30340;&#26032;&#30446;&#26631;&#20219;&#21153;&#20013;&#36827;&#34892;&#33391;&#22909;&#36801;&#31227;&#30340;&#29305;&#24449;&#65292;&#22312;&#27809;&#26377;&#24212;&#29992;&#29305;&#24449;&#36866;&#24212;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;61.52%&#65288;0.48&#65289;&#30340;&#39640;F-score&#65292;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#27599;&#20010;&#26032;&#30446;&#26631;&#20219;&#21153;&#36866;&#24212;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26102;&#65292;F-score&#25552;&#39640;&#33267;68.19%&#65288;0.75&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#38477;&#20302;&#23567;&#26679;&#26412;&#29983;&#29289;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#20837;&#38376;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bioacoustic sound event detection allows for better understanding of animal behavior and for better monitoring biodiversity using audio. Deep learning systems can help achieve this goal, however it is difficult to acquire sufficient annotated data to train these systems from scratch. To address this limitation, the Detection and Classification of Acoustic Scenes and Events (DCASE) community has recasted the problem within the framework of few-shot learning and organize an annual challenge for learning to detect animal sounds from only five annotated examples. In this work, we regularize supervised contrastive pre-training to learn features that can transfer well on new target tasks with animal sounds unseen during training, achieving a high F-score of 61.52%(0.48) when no feature adaptation is applied, and an F-score of 68.19%(0.75) when we further adapt the learned features for each new target task. This work aims to lower the entry bar to few-shot bioacoustic sound event detection by
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08748</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#20445;&#35777;&#30340;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#22312;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08748
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#19982;&#29615;&#22659;&#30452;&#25509;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#25910;&#38598;&#30340;&#29615;&#22659;&#36890;&#24120;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#24212;&#29992;&#30340;&#29615;&#22659;&#19981;&#21516;&#12290;&#20026;&#20102;&#22312;&#23398;&#20064;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#32771;&#34385;&#19981;&#21516;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;(DRO)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20551;&#35774;&#26032;&#29615;&#22659;&#30340;&#20998;&#24067;&#20301;&#20110;&#19981;&#30830;&#23450;&#38598;&#21512;&#20869;&#26102;&#65292;&#35745;&#31639;&#31574;&#30053;&#20540;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30028;&#12290;&#20856;&#22411;&#22320;&#65292;&#36825;&#20010;&#19981;&#30830;&#23450;&#38598;&#21512;&#26159;&#22522;&#20110;&#20174;&#26085;&#24535;&#25968;&#25454;&#38598;&#20013;&#35745;&#31639;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;KL&#19981;&#30830;&#23450;&#38598;&#21512;&#26080;&#27861;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#25903;&#25345;&#30340;&#20998;&#24067;&#65292;&#20063;&#32570;&#20047;&#23545;&#20998;&#24067;&#25903;&#25345;&#30340;&#20960;&#20309;&#24863;&#30693;&#12290;&#32467;&#26524;&#65292;KL&#26041;&#27861;&#22312;&#35299;&#20915;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#26368;&#22351;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;DRO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20102;&#26102;&#31354;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07690</link><description>&lt;p&gt;
&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20102;&#26102;&#31354;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#65288;ASAD&#65289;&#26088;&#22312;&#21033;&#29992;EEG&#22312;&#22810;&#20154;&#28436;&#35762;&#32773;&#29615;&#22659;&#20013;&#35299;&#30721;&#34987;&#27880;&#24847;&#30340;&#31354;&#38388;&#20301;&#32622;&#12290;ASAD&#26041;&#27861;&#21463;&#21040;&#22823;&#33041;&#30382;&#23618;&#31070;&#32463;&#20803;&#22312;&#22788;&#29702;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26102;&#30340;&#20391;&#21270;&#21551;&#21457;&#65292;&#24182;&#22312;&#31070;&#32463;&#35760;&#24405;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65288;AAD&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#22312;&#20197;&#24448;&#30340;ASAD&#26041;&#27861;&#20013;&#65292;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;EEG&#20449;&#36947;&#36716;&#25442;&#20026;&#20108;&#32500;&#31354;&#38388;&#25299;&#25169;&#22270;&#65292;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#21253;&#21547;&#26102;&#31354;&#20449;&#24687;&#30340;&#19977;&#32500;&#25490;&#21015;&#12290;&#28982;&#21518;&#20351;&#29992;&#19977;&#32500;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet-3D&#65289;&#26469;&#25552;&#21462;&#25152;&#20851;&#27880;&#20301;&#32622;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditory spatial attention detection (ASAD) aims to decode the attended spatial location with EEG in a multiple-speaker setting. ASAD methods are inspired by the brain lateralization of cortical neural responses during the processing of auditory spatial attention, and show promising performance for the task of auditory attention decoding (AAD) with neural recordings. In the previous ASAD methods, the spatial distribution of EEG electrodes is not fully exploited, which may limit the performance of these methods. In the present work, by transforming the original EEG channels into a two-dimensional (2D) spatial topological map, the EEG data is transformed into a three-dimensional (3D) arrangement containing spatial-temporal information. And then a 3D deep convolutional neural network (DenseNet-3D) is used to extract temporal and spatial features of the neural representation for the attended locations. The results show that the proposed method achieves higher decoding accuracy than the sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.04001</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#29992;&#20110;&#26448;&#26009;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#27169;&#24577;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65306;RGB&#12289;&#32447;&#24615;&#20559;&#25391;&#35282;&#65288;AoLP&#65289;&#12289;&#32447;&#24615;&#20559;&#25391;&#24230;&#65288;DoLP&#65289;&#21644;&#36817;&#32418;&#22806;&#65288;NIR&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20998;&#21106;&#21464;&#25442;&#22120;&#65288;MMSFormer&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#12290;MMSFormer&#22312;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#65288;MCubeS&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;52.05&#65285;&#30340;mIoU&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#30782;&#30707;&#65288;+10.4&#65285;&#65289;&#21644;&#20154;&#31867;&#65288;+9.1&#65285;&#65289;&#31867;&#19978;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#34701;&#21512;&#22359;&#20013;&#30340;&#19981;&#21516;&#27169;&#22359;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12252</link><description>&lt;p&gt;
&#25105;&#30475;&#21040;&#30340;&#19996;&#35199;&#26377;&#22810;&#23433;&#20840;&#65311;&#22522;&#20110;&#22270;&#20687;&#25511;&#21046;&#30340;&#33258;&#27835;&#23433;&#20840;&#24615;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#24320;&#21457;&#33258;&#27835;&#31995;&#32479;&#30340;&#20027;&#35201;&#33539; paradigm&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#20854;&#24615;&#33021;&#21644;&#20415;&#21033;&#24615;&#65292;&#23433;&#20840;&#20445;&#35777;&#38754;&#20020;&#30528;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#25361;&#25112;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#32570;&#20047;&#20302;&#32500;&#21487;&#35299;&#37322;&#21160;&#24577;&#29366;&#24577;&#30340;&#27010;&#24565;&#65292;&#20256;&#32479;&#30340;&#20445;&#35777;&#26041;&#27861;&#37117;&#22260;&#32469;&#36825;&#19968;&#27010;&#24565;&#23637;&#24320;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#21487;&#37197;&#32622;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#19981;&#38656;&#35201;&#20302;&#32500;&#29366;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#28508;&#22312;&#34920;&#31034;&#21644;&#39044;&#27979;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#23545;&#20854;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#25552;&#20379;&#20102;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#22312;&#20004;&#20010;&#22270;&#20687;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65306;&#36187;&#36710;&#21644;&#27773;&#36710;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11863</link><description>&lt;p&gt;
KinSPEAK: &#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11863
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12289;&#35838;&#31243;&#36827;&#24230;&#24494;&#35843;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#20855;&#22791;&#20102;&#22823;&#35268;&#27169;&#35760;&#24405;&#30340;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#35821;&#38899;&#25968;&#25454;&#65292;&#20294;&#26159;&#23454;&#29616;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#24378;&#22823;&#35821;&#38899;&#35782;&#21035;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36981;&#24490;&#31616;&#21333;&#30340;&#35838;&#31243;&#36827;&#24230;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26469;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35821;&#38899;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#23612;&#20122;&#20848;&#36798;&#35821;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20851;&#27880;&#20351;&#29992;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#12290;&#25105;&#20204;&#20174;&#20844;&#20849;&#32593;&#31449;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#21046;&#20316;&#23460;&#32423;&#21035;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#24178;&#20928;&#30340;&#22522;&#20934;&#27169;&#22411;&#23545;&#26469;&#33258;&#26356;&#22810;&#22810;&#26679;&#21644;&#22024;&#26434;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#65292;&#23450;&#20041;&#19968;&#20010;&#31616;&#21333;&#30340;&#35838;&#31243;&#35757;&#32451;&#36827;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;&#36830;&#32493;&#22235;&#20195;&#23545;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#21644;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;3.2&#65285;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#22312;Mozilla Common Voice&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;15.9&#65285;&#30340;WER&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#36827;&#34892;&#32852;&#37030;&#20998;&#31867;&#30340;&#39318;&#20010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#20984;SVM&#20998;&#31867;&#22120;&#21644;&#20984;&#21253;&#30340;&#23433;&#20840;&#32858;&#21512;&#26469;&#35299;&#20915;&#22312;&#38544;&#31169;&#20445;&#25252;&#29615;&#22659;&#20013;&#22788;&#29702;&#26641;&#29366;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.06895</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#32852;&#37030;&#20998;&#31867;&#36890;&#36807;&#20984;&#21253;&#30340;&#23433;&#20840;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls. (arXiv:2308.06895v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#36827;&#34892;&#32852;&#37030;&#20998;&#31867;&#30340;&#39318;&#20010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#20984;SVM&#20998;&#31867;&#22120;&#21644;&#20984;&#21253;&#30340;&#23433;&#20840;&#32858;&#21512;&#26469;&#35299;&#20915;&#22312;&#38544;&#31169;&#20445;&#25252;&#29615;&#22659;&#20013;&#22788;&#29702;&#26641;&#29366;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#21644;&#26641;&#29366;&#25968;&#25454;&#38598;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#21253;&#25324;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#25968;&#25454;&#25366;&#25496;&#12289;&#31995;&#32479;&#21457;&#32946;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#24050;&#30693;&#26641;&#29366;&#25968;&#25454;&#26080;&#27861;&#20197;&#23567;&#22833;&#30495;&#23884;&#20837;&#21040;&#26377;&#38480;&#32500;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24403;&#36825;&#26679;&#30340;&#25968;&#25454;&#36824;&#24517;&#39035;&#22312;&#20998;&#24067;&#24335;&#21644;&#31169;&#26377;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#22788;&#29702;&#26102;&#65292;&#38656;&#35201;&#20351;&#29992;&#38024;&#23545;&#36229;&#20960;&#20309;&#31354;&#38388;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#20026;&#21457;&#23637;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24050;&#30693;&#30340;&#31532;&#19968;&#31181;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#36827;&#34892;&#32852;&#37030;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22914;&#19979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;Poincar\'e&#22270;&#20013;&#24320;&#21457;&#20102;&#20984;SVM&#20998;&#31867;&#22120;&#30340;&#20998;&#24067;&#24335;&#29256;&#26412;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20174;&#23458;&#25143;&#31471;&#21521;&#20840;&#23616;&#20998;&#31867;&#22120;&#20256;&#36882;&#30340;&#20449;&#24687;&#26159;&#21508;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#32858;&#31867;&#30340;&#20984;&#21253;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#36991;&#20813;&#26631;&#31614;&#20999;&#25442;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#20197;&#32422;&#26463;&#26631;&#31614;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we intro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02040</link><description>&lt;p&gt;
VertiBench: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#20013;&#25512;&#36827;&#29305;&#24449;&#20998;&#24067;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#22312;&#29305;&#24449;&#21010;&#20998;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#24456;&#23569;&#26377;&#20844;&#24320;&#30340;&#30495;&#23454;&#19990;&#30028;VFL&#25968;&#25454;&#38598;&#29992;&#20110;&#31639;&#27861;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#20195;&#34920;&#20102;&#26377;&#38480;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#36890;&#24120;&#37319;&#29992;&#20174;&#20840;&#23616;&#38598;&#21512;&#20013;&#30340;&#20219;&#24847;&#29305;&#24449;&#21010;&#20998;&#23548;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#21482;&#25429;&#25417;&#21040;&#20102;&#19968;&#37096;&#20998;&#29305;&#24449;&#20998;&#24067;&#65292;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#19981;&#36275;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#8212;&#8212;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#26469;&#24357;&#34917;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#23574;&#31471;VFL&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25968;&#23383;&#36890;&#20449;&#20449;&#21495;&#65292;&#36890;&#36807;&#20998;&#31163;&#21472;&#21152;&#28304;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65292;&#23454;&#29616;&#23545;&#31163;&#25955;&#28304;&#30340;&#24674;&#22797;&#21644;&#32534;&#30721;&#27604;&#29305;&#30340;&#20934;&#30830;&#24615;&#25913;&#21892;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#21644;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27604;&#29305;&#38169;&#35823;&#29575;&#19978;&#25552;&#20379;&#20102;95&#65285;&#30340;&#20943;&#23567;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#28176;&#36817;&#22320;&#25509;&#36817;&#22522;&#30784;&#31163;&#25955;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#24182;&#20316;&#20026;&#35780;&#20998;&#25552;&#21462;&#37319;&#26679;&#26041;&#26696;&#30340;&#22810;&#28304;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.14411</link><description>&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#21450;&#20854;&#22312;&#25968;&#23383;&#36890;&#20449;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Score-based Source Separation with Applications to Digital Communication Signals. (arXiv:2306.14411v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14411
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#20998;&#30340;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25968;&#23383;&#36890;&#20449;&#20449;&#21495;&#65292;&#36890;&#36807;&#20998;&#31163;&#21472;&#21152;&#28304;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#21644;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65292;&#23454;&#29616;&#23545;&#31163;&#25955;&#28304;&#30340;&#24674;&#22797;&#21644;&#32534;&#30721;&#27604;&#29305;&#30340;&#20934;&#30830;&#24615;&#25913;&#21892;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#21644;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27604;&#29305;&#38169;&#35823;&#29575;&#19978;&#25552;&#20379;&#20102;95&#65285;&#30340;&#20943;&#23567;&#65292;&#21516;&#26102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#28176;&#36817;&#22320;&#25509;&#36817;&#22522;&#30784;&#31163;&#25955;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#24182;&#20316;&#20026;&#35780;&#20998;&#25552;&#21462;&#37319;&#26679;&#26041;&#26696;&#30340;&#22810;&#28304;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#31163;&#21472;&#21152;&#28304;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#29420;&#31435;&#28304;&#30340;&#32479;&#35745;&#20808;&#39564;&#65292;&#36890;&#36807;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#24341;&#23548;&#19968;&#20010;&#30001;&#22810;&#23618;&#39640;&#26031;&#24179;&#28369;&#24341;&#23548;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;&#12290;&#21463;&#23556;&#39057;&#65288;RF&#65289;&#31995;&#32479;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#22522;&#30784;&#31163;&#25955;&#24615;&#36136;&#30340;&#28304;&#20197;&#21450;&#20174;&#24863;&#20852;&#36259;&#20449;&#21495;&#20013;&#24674;&#22797;&#32534;&#30721;&#27604;&#29305;&#30340;&#33021;&#21147;&#24863;&#20852;&#36259;&#65292;&#22914;&#27604;&#29305;&#38169;&#35823;&#29575;&#65288;BER&#65289;&#25152;&#34913;&#37327;&#12290;&#36890;&#36807;RF&#28151;&#21512;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BER&#19978;&#27604;&#32463;&#20856;&#30340;&#21644;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#20943;&#23567;&#20102;95&#65285;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#28176;&#36817;&#22320;&#25509;&#36817;&#22522;&#30784;&#31163;&#25955;&#20998;&#24067;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#35780;&#20998;&#25552;&#21462;&#37319;&#26679;&#26041;&#26696;&#30340;&#22810;&#28304;&#25193;&#23637;&#65292;&#25581;&#31034;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#24615;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method for separating superimposed sources using diffusion-based generative models. Our method relies only on separately trained statistical priors of independent sources to establish a new objective function guided by maximum a posteriori estimation with an $\alpha$-posterior, across multiple levels of Gaussian smoothing. Motivated by applications in radio-frequency (RF) systems, we are interested in sources with underlying discrete nature and the recovery of encoded bits from a signal of interest, as measured by the bit error rate (BER). Experimental results with RF mixtures demonstrate that our method results in a BER reduction of 95% over classical and existing learning-based methods. Our analysis demonstrates that our proposed method yields solutions that asymptotically approach the modes of an underlying discrete distribution. Furthermore, our method can be viewed as a multi-source extension to the recently proposed score distillation sampling scheme, shedding ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;MDP&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#36845;&#20195;&#30340;&#38750;&#28176;&#36827;&#25910;&#25947;&#21040;&#26368;&#20248;&#32422;&#26463;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.11700</link><description>&lt;p&gt;
&#20026;&#32422;&#26463;MDP&#38382;&#39064;&#35774;&#35745;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs. (arXiv:2306.11700v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;MDP&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#31574;&#30053;&#36845;&#20195;&#30340;&#38750;&#28176;&#36827;&#25910;&#25947;&#21040;&#26368;&#20248;&#32422;&#26463;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#32422;&#26463;MDP&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#35745;&#31639;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#22522;&#20110;Lagrangian&#30340;&#25919;&#31574;&#25628;&#32034;&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20013;&#31574;&#30053;&#36845;&#20195;&#30340;&#25391;&#33633;&#23578;&#26410;&#23436;&#20840;&#35299;&#37322;&#28165;&#26970;&#65292;&#20174;&#32780;&#24341;&#20986;&#20102;&#35832;&#22914;&#32422;&#26463;&#36829;&#35268;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;Lagrangian&#26041;&#27861;&#23558;&#32422;&#26463;MDP&#36716;&#21270;&#20026;&#19968;&#20010;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#65292;&#20854;&#20013;&#26368;&#22823;/&#26368;&#23567;&#29609;&#23478;&#20998;&#21035;&#23545;&#24212;&#21407;&#22987;/&#23545;&#20598;&#21464;&#37327;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#21333;&#26102;&#38388;&#23610;&#24230;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#31574;&#30053;&#36845;&#20195;&#38750;&#28176;&#36827;&#25910;&#25947;&#21040;&#26368;&#20248;&#32422;&#26463;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#65288;RPG-PD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#31574;&#30053;&#65292;&#21516;&#26102;&#20351;&#29992;&#20108;&#27425;&#27491;&#21017;&#21270;&#30340;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; ...
&lt;/p&gt;
&lt;p&gt;
We study the problem of computing an optimal policy of an infinite-horizon discounted constrained Markov decision process (constrained MDP). Despite the popularity of Lagrangian-based policy search methods used in practice, the oscillation of policy iterates in these methods has not been fully understood, bringing out issues such as violation of constraints and sensitivity to hyper-parameters. To fill this gap, we employ the Lagrangian method to cast a constrained MDP into a constrained saddle-point problem in which max/min players correspond to primal/dual variables, respectively, and develop two single-time-scale policy-based primal-dual algorithms with non-asymptotic convergence of their policy iterates to an optimal constrained policy. Specifically, we first propose a regularized policy gradient primal-dual (RPG-PD) method that updates the policy using an entropy-regularized policy gradient, and the dual variable via a quadratic-regularized gradient ascent, simultaneously. We prove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09980</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21019;&#24314;&#22810;&#32423;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20160;&#20040;&#26679;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#33258;&#20027;&#20195;&#29702;&#20154;&#26159;&#26377;&#29992;&#30340;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#26469;&#25581;&#31034;&#22270;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#25216;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06155</link><description>&lt;p&gt;
&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#65306;&#29992;&#20110;&#21160;&#24577;&#32593;&#32476;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks. (arXiv:2306.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#28085;&#30422;&#26680;&#24179;&#28369;&#30340;&#24378;&#24230;&#20989;&#25968;&#20272;&#35745;&#12289;&#26368;&#23567;&#21270;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#23398;&#20064;&#21644;&#24402;&#32435;&#26500;&#36896;&#33410;&#28857;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#32467;&#26500;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24378;&#24230;&#36718;&#24275;&#25237;&#24433;&#8221;&#30340;&#26032;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#33410;&#28857;&#30340;&#36830;&#32493;&#26102;&#38388;&#34920;&#31034;&#65292;&#35813;&#21160;&#24577;&#32593;&#32476;&#30001;&#33410;&#28857;&#38598;&#21644;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#21457;&#29983;&#30340;&#30636;&#26102;&#20132;&#20114;&#20107;&#20214;&#30340;&#38598;&#21512;&#25152;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#36890;&#36807;&#26680;&#24179;&#28369;&#31561;&#26041;&#27861;&#20272;&#35745;&#33410;&#28857;&#23545;&#20043;&#38388;&#20132;&#20114;&#30340;&#24378;&#24230;&#20989;&#25968;&#65307;&#23398;&#20064;&#19968;&#20010;&#26368;&#23567;&#21270;&#26576;&#31181;&#24378;&#24230;&#37325;&#26500;&#35823;&#24046;&#30340;&#25237;&#24433;&#65307;&#36890;&#36807;&#23398;&#20064;&#30340;&#25237;&#24433;&#24402;&#32435;&#26500;&#36896;&#20986;&#19981;&#26029;&#21457;&#23637;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#20445;&#30041;&#20102;&#32593;&#32476;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#24182;&#20855;&#26377;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#28857;&#19978;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#26500;&#24314;&#20102;&#20272;&#35745;&#29702;&#35770;&#26469;&#38416;&#26126;&#24179;&#28369;&#20316;&#20026;&#20559;&#24046;&#26041;&#24046;&#25240;&#34935;&#30340;&#20316;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#38543;&#30528;&#20449;&#22122;&#27604;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#24179;&#28369;&#31243;&#24230;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithmic framework, Intensity Profile Projection, for learning continuous-time representations of the nodes of a dynamic network, characterised by a node set and a collection of instantaneous interaction events which occur in continuous time. Our framework consists of three stages: estimating the intensity functions underlying the interactions between pairs of nodes, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and inductively constructing evolving node representations via the learned projection. We show that our representations preserve the underlying structure of the network, and are temporally coherent, meaning that node representations can be meaningfully compared at different points in time. We develop estimation theory which elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce smoothing as the signal-to-noise ratio increases on account of the algorithm `borrow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18171</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Probabilistic Image-Text Representations. (arXiv:2305.18171v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#21644;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#36807;&#37325;&#21644;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65292;&#30001;&#20110;&#22810;&#26679;&#24615;&#21644;&#19981;&#23436;&#32654;&#27880;&#37322;&#23548;&#33268;&#30340;&#22266;&#26377;&#27495;&#20041;&#20351;&#20854;&#21463;&#21040;&#22256;&#25200;&#12290;&#30830;&#23450;&#24615;&#20989;&#25968;&#26080;&#27861;&#36275;&#22815;&#24378;&#22823;&#22320;&#25429;&#25417;&#21040;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#27010;&#29575;&#23884;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27010;&#29575;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#32570;&#28857;&#65306;&#33945;&#29305;&#21345;&#27931;&#36924;&#36817;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#36739;&#37325;&#65292;&#19988;&#22312;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#27010;&#29575;&#36328;&#27169;&#24577;&#23884;&#20837;&#26041;&#27861;&#65288;PCME++&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#38381;&#21512;&#24418;&#24335;&#35299;&#30340;&#26032;&#30340;&#27010;&#29575;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#25216;&#26415;&#36827;&#19968;&#27493;&#22686;&#24378;PCME++&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20266;&#27491;&#26679;&#26412;&#20197;&#38450;&#27490;&#22823;&#37327;&#35823;&#26816;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#39281;&#21644;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#37319;&#29992;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#27010;&#29575;&#21305;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PCME++&#22312;ITM&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching (ITM) task, a fundamental vision-language (VL) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic ITM approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance PCME++ further; first, the incorporation of pseudo-positives to prevent the loss saturation problem under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.16102</link><description>&lt;p&gt;
&#25581;&#31034;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24182;&#19981;&#33021;&#35299;&#20915;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#25351;&#30340;&#26159;&#22686;&#21152;&#32593;&#32476;&#28145;&#24230;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#21464;&#24471;&#30456;&#21516;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#20250;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26159;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#36824;&#23384;&#22312;&#20105;&#35758;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#38750;&#32447;&#24615;&#26102;&#21464;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#38750;&#40784;&#27425;&#30697;&#38453;&#20056;&#31215;&#21644;&#32852;&#21512;&#35889;&#21322;&#24452;&#29702;&#35770;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#27969;&#34892;&#35266;&#28857;&#30456;&#21453;&#65292;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#19981;&#33021;&#38450;&#27490;&#24179;&#28369;&#36807;&#24230;&#29616;&#35937;&#65292;&#24182;&#19988;&#21576;&#25351;&#25968;&#32423;&#22833;&#21435;&#34920;&#36798;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#23545;&#31216;GCN&#30340;&#24179;&#28369;&#36807;&#24230;&#38382;&#39064;&#25193;&#23637;&#21040;&#20102;&#26356;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#31867;&#21035;&#20013;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#12289;&#29366;&#24577;&#30456;&#20851;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12162</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;DSIC&#20223;&#23556;&#26497;&#22823;&#20215;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
A Scalable Neural Network for DSIC Affine Maximizer Auction Design. (arXiv:2305.12162v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;AMenuNet&#26469;&#26500;&#36896;AMAs&#21442;&#25968;&#21644;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20854;&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25293;&#21334;&#35774;&#35745;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23547;&#25214;&#32463;&#39564;&#19978;&#39640;&#25910;&#20837;&#30340;&#26426;&#21046;&#12290;&#29616;&#26377;&#30340;&#22810;&#29289;&#21697;&#25293;&#21334;&#24773;&#26223;&#30340;&#24037;&#20316;&#21487;&#20197;&#31895;&#30053;&#22320;&#20998;&#20026;RegretNet&#31867;&#21644;&#20223;&#23556;&#26497;&#22823;&#20215;&#65288;AMAs&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#33021;&#20005;&#26684;&#20445;&#35777;&#21344;&#20248;&#31574;&#30053;&#28608;&#21169;&#20860;&#23481;&#24615;&#65288;DSIC&#65289;&#65292;&#32780;&#21518;&#32773;&#22240;&#20026;&#20998;&#37197;&#20505;&#36873;&#20154;&#25968;&#36807;&#22810;&#32780;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMenuNet&#65292;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20174;&#20986;&#20215;&#20154;&#21644;&#29289;&#21697;&#34920;&#31034;&#20013;&#26500;&#36896;AMA&#21442;&#25968;&#65288;&#29978;&#33267;&#21253;&#25324;&#20998;&#37197;&#33756;&#21333;&#65289;&#12290;&#30001;&#20110;AMA&#30340;&#23646;&#24615;&#65292;AMenuNet&#22987;&#32456;&#26159;DSIC&#21644;&#20010;&#20154;&#29702;&#24615;&#65288;IR&#65289;&#30340;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20505;&#36873;&#20998;&#37197;&#26469;&#22686;&#24378;&#21487;&#20280;&#32553;&#24615;&#12290;&#27492;&#22806;&#65292;AMenuNet&#26159;&#32622;&#25442;&#31561;&#21464;&#30340;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#19981;&#21463;&#25293;&#21334;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;AMenuNet&#22312;&#21327;&#21830;&#19968;&#33268;&#30340;&#20215;&#20540;&#21644;&#31038;&#20250;&#27531;&#20313;&#20215;&#20540;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated auction design aims to find empirically high-revenue mechanisms through machine learning. Existing works on multi item auction scenarios can be roughly divided into RegretNet-like and affine maximizer auctions (AMAs) approaches. However, the former cannot strictly ensure dominant strategy incentive compatibility (DSIC), while the latter faces scalability issue due to the large number of allocation candidates. To address these limitations, we propose AMenuNet, a scalable neural network that constructs the AMA parameters (even including the allocation menu) from bidder and item representations. AMenuNet is always DSIC and individually rational (IR) due to the properties of AMAs, and it enhances scalability by generating candidate allocations through a neural network. Additionally, AMenuNet is permutation equivariant, and its number of parameters is independent of auction scale. We conduct extensive experiments to demonstrate that AMenuNet outperforms strong baselines in both co
&lt;/p&gt;</description></item><item><title>Flame &#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#32423;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65292;&#35299;&#32806; ML &#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#37096;&#32626;&#65292;&#20943;&#23569;&#24320;&#21457;&#24037;&#20316;&#65292;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.05118</link><description>&lt;p&gt;
Flame&#65306;&#31616;&#21270;&#32852;&#37030;&#23398;&#20064;&#25805;&#20316;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Operations Made Simple with Flame. (arXiv:2305.05118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05118
&lt;/p&gt;
&lt;p&gt;
Flame &#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#32423;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65292;&#35299;&#32806; ML &#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#37096;&#32626;&#65292;&#20943;&#23569;&#24320;&#21457;&#24037;&#20316;&#65292;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#20998;&#24067;&#30340;&#22522;&#30784;&#26550;&#26500;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#26102;&#24102;&#26469;&#20102;&#35768;&#22810;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39044;&#26399;&#30340;&#25928;&#30410;&#65292;&#38656;&#35201;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#21644;&#37197;&#32622;&#32423;&#21035;&#30340;&#26356;&#25913;&#65292;&#36825;&#28041;&#21450;&#37096;&#32626;&#29305;&#23450;&#30340;&#32454;&#33410;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#39640;&#32423;&#21035;&#30340;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65288;TAG&#65289;&#12290;TAG&#23558;ML&#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#35299;&#32806;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#65292;&#20174;&#32780;&#38477;&#20302;&#24320;&#21457;&#24037;&#20316;&#37327;&#65292;&#24182;&#20026;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#25512;&#20986;&#20102;Flame&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#36825;&#20123;&#25277;&#35937;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#23427;&#23545;&#22810;&#20010;&#29992;&#20363;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed machine learning approaches, including a broad class of federated learning techniques, present a number of benefits when deploying machine learning applications over widely distributed infrastructures. To realize the expected benefits, however, introduces substantial operational challenges due to required application and configuration-level changes related to deployment-specific details. Such complexities can be greatly reduced by introducing higher-level abstractions -- role and channel -- using which federated learning applications are described as Topology Abstraction Graphs (TAGs). TAGs decouple the ML application logic from the underlying deployment details, making it possible to specialize the application deployment, thus reducing development effort and paving the way for improved automation and tuning. We present Flame, the first system that supports these abstractions, and demonstrate its benefits for several use cases.
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;</title><link>http://arxiv.org/abs/2304.10045</link><description>&lt;p&gt;
ID-MixGCL: &#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ID-MixGCL: Identity Mixup for Graph Contrastive Learning. (arXiv:2304.10045v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20221;&#28151;&#21512;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#24471;&#21040;&#30340;&#19981;&#21516;&#20294;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#26631;&#31614;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#25429;&#33719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#26159;&#27604;&#36739;&#21516;&#19968;&#20010;&#22270;&#24418;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#8220;&#35270;&#22270;&#8221;&#20197;&#23398;&#20064;&#33410;&#28857;/&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#36890;&#36807;&#22270;&#24418;&#22686;&#24378;&#65292;&#21487;&#20197;&#29983;&#25104;&#20960;&#20010;&#32467;&#26500;&#19981;&#21516;&#20294;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#22240;&#27492;&#21407;&#22987;&#21644;&#22686;&#24378;&#30340;&#22270;&#24418;/&#33410;&#28857;&#30340;&#36523;&#20221;&#26631;&#31614;&#24212;&#35813;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20363;&#22914;&#20998;&#23376;&#22270;&#20013;&#23545;&#33410;&#28857;&#25110;&#36793;&#30340;&#20219;&#20309;&#25200;&#21160;&#37117;&#20250;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25913;&#21464;&#22270;&#24418;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22686;&#24378;&#22270;&#24418;&#32467;&#26500;&#24212;&#35813;&#20276;&#38543;&#30528;&#23545;&#23545;&#27604;&#25439;&#22833;&#20351;&#29992;&#30340;&#26631;&#31614;&#30340;&#36866;&#24212;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ID-MixGCL&#65292;&#23427;&#20801;&#35768;&#21516;&#26102;&#35843;&#33410;&#36755;&#20837;&#22270;&#24418;&#21644;&#30456;&#24212;&#30340;&#36523;&#20221;&#26631;&#31614;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#20174;&#32780;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed graph contrastive learning (GCL) approaches compare two different "views" of the same graph in order to learn node/graph representations. The core assumption of these approaches is that by graph augmentation, it is possible to generate several structurally different but semantically similar graph structures, and therefore, the identity labels of the original and augmented graph/nodes should be identical. However, in this paper, we observe that this assumption does not always hold, for example, any perturbation to nodes or edges in a molecular graph will change the graph labels to some degree. Therefore, we believe that augmenting the graph structure should be accompanied by an adaptation of the labels used for the contrastive loss. Based on this idea, we propose ID-MixGCL, which allows for simultaneous modulation of both the input graph and the corresponding identity labels, with a controllable degree of change, leading to the capture of fine-grained representations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.02811</link><description>&lt;p&gt;
HomPINNs&#65306;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
HomPINNs: homotopy physics-informed neural networks for solving the inverse problems of nonlinear differential equations with multiple solutions. (arXiv:2304.02811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22522;&#20110;&#21516;&#20262;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65292;&#26469;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#30340;&#21453;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#24182;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#36890;&#36807;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35299;&#31354;&#38388;&#20013;&#30340;&#38750;&#21807;&#19968;&#24615;&#12289;&#23545;&#31216;&#24615;&#21644;&#20998;&#23700;&#31561;&#22797;&#26434;&#34892;&#20026;&#65292;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#65288;DEs&#65289;&#30340;&#21453;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21516;&#20262;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;HomPINNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;NNs&#21516;&#26102;&#36924;&#36817;&#24050;&#30693;&#35266;&#27979;&#32467;&#26524;&#21644;&#31526;&#21512;DEs&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#21516;&#20262;&#36830;&#32493;&#26041;&#27861;&#65292;&#36924;&#36817;&#21487;&#36861;&#36394;&#35266;&#23519;&#32467;&#26524;&#20197;&#30830;&#23450;&#22810;&#20010;&#35299;&#24182;&#35299;&#20915;&#21453;&#38382;&#39064;&#12290;&#23454;&#39564;&#28085;&#30422;&#22312;&#19968;&#32500;DEs&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#24212;&#29992;&#23427;&#26469;&#35299;&#20915;&#20108;&#32500;Gray-Scott&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#20280;&#32553;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#65292;&#20026;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#35299;&#30340;DEs&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the complex behavior arising from non-uniqueness, symmetry, and bifurcations in the solution space, solving inverse problems of nonlinear differential equations (DEs) with multiple solutions is a challenging task. To address this issue, we propose homotopy physics-informed neural networks (HomPINNs), a novel framework that leverages homotopy continuation and neural networks (NNs) to solve inverse problems. The proposed framework begins with the use of a NN to simultaneously approximate known observations and conform to the constraints of DEs. By utilizing the homotopy continuation method, the approximation traces the observations to identify multiple solutions and solve the inverse problem. The experiments involve testing the performance of the proposed method on one-dimensional DEs and applying it to solve a two-dimensional Gray-Scott simulation. Our findings demonstrate that the proposed method is scalable and adaptable, providing an effective solution for solving DEs with mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19981;&#21516;&#30340;&#20998;&#26512;&#31649;&#36947;&#23545;DNN&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#31649;&#36947;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#24182;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#19968;&#20010;&#31751;&#32676;&#12290;</title><link>http://arxiv.org/abs/2301.13506</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;DNN&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches. (arXiv:2301.13506v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19981;&#21516;&#30340;&#20998;&#26512;&#31649;&#36947;&#23545;DNN&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#31649;&#36947;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#24182;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#19968;&#20010;&#31751;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25163;&#27573;&#35299;&#37322;&#20854;&#32467;&#26524;&#65288;&#23588;&#20854;&#26159;&#24403;&#32467;&#26524;&#38169;&#35823;&#26102;&#65289;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24448;&#24448;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30333;&#30418;&#26041;&#27861;&#65288;HUDD&#65289;&#21644;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#65288;SAFE&#65289;&#26469;&#33258;&#21160;&#21270;&#34920;&#24449;DNN&#30340;&#22833;&#25928;&#24773;&#20917;&#12290;&#23427;&#20204;&#37117;&#21487;&#20197;&#20174;&#19968;&#32452;&#28508;&#22312;&#24222;&#22823;&#30340;&#23548;&#33268;DNN&#22833;&#25928;&#30340;&#22270;&#20687;&#20013;&#35782;&#21035;&#30456;&#20284;&#30340;&#22270;&#20687;&#38598;&#32676;&#12290;&#28982;&#32780;&#65292;HUDD&#21644;SAFE&#30340;&#20998;&#26512;&#31649;&#36947;&#26159;&#25353;&#29031;&#24120;&#35268;&#23454;&#36341;&#23454;&#20363;&#21270;&#30340;&#65292;&#23558;&#20854;&#20182;&#31649;&#36947;&#30340;&#20998;&#26512;&#25512;&#36831;&#21040;&#23558;&#26469;&#30340;&#24037;&#20316;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;99&#31181;&#19981;&#21516;&#30340;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;DNN&#22833;&#25928;&#30340;&#31649;&#36947;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#12290;&#23427;&#20204;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#31070;&#32463;&#20803;&#30456;&#20851;&#24615;&#30340;&#28909;&#22270;&#65292;&#38477;&#32500;&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#30340;&#31649;&#36947;&#26159;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#31751;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.13247</link><description>&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Loss Function Learning. (arXiv:2301.13247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13247
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#32463;&#24120;&#25913;&#21892;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#32456;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#26159;&#25439;&#22833;&#20989;&#25968;&#20197;&#32447;&#19979;&#26041;&#24335;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#20803;&#30446;&#26631;&#20165;&#32771;&#34385;&#35757;&#32451;&#30340;&#21069;&#20960;&#20010;&#27493;&#39588;&#65292;&#36825;&#19982;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#30456;&#27604;&#26174;&#33879;&#36739;&#30701;&#12290;&#36825;&#23548;&#33268;&#23545;&#20110;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#34920;&#29616;&#19981;&#20339;&#30340;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#26126;&#26174;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently out
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.07996</link><description>&lt;p&gt;
&#36864;&#21270;&#26159;&#21487;&#20197;&#25509;&#21463;&#30340;&#65306;&#24102;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#20013;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#21644;T&#27425;&#29420;&#31435;&#21516;&#20998;&#24067;&#21040;&#36798;&#30340;&#32463;&#20856;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#65288;NRM&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20998;&#24067;&#24418;&#24335;&#65292;&#27599;&#20010;&#21040;&#36798;&#24517;&#39035;&#23646;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#33021;&#31867;&#21035;&#20043;&#19968;&#65292;&#27599;&#20010;&#31867;&#21035;&#20855;&#26377;&#30830;&#23450;&#30340;&#36164;&#28304;&#28040;&#32791;&#21521;&#37327;&#65292;&#20294;&#26159;&#19968;&#20010;&#22312;&#21306;&#38388;&#19978;&#36830;&#32493;&#20998;&#24067;&#30340;&#38543;&#26426;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;O(log^2 T)&#30340;&#36951;&#25022;&#65292;&#21807;&#19968;&#65288;&#24517;&#35201;&#65289;&#30340;&#20551;&#35774;&#26159;&#27010;&#29575;&#23494;&#24230;&#36828;&#31163;0&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#20108;&#20010;&#32467;&#26524;&#65292;&#22312;&#20108;&#38454;&#22686;&#38271;&#30340;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#20102;O(log T)&#30340;&#36951;&#25022;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#36807;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#36793;&#30028;mypopic regret&#65292;&#31163;&#32447;&#20998;&#37197;&#30340;&#8220;&#21322;&#27969;&#20307;&#8221;&#25918;&#26494;&#20197;&#21450;&#25913;&#36827;&#36793;&#30028;&#30340;&#26032;&#25216;&#26415;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#24212;&#29992;&#19978;&#19979;&#25991;&#34920;&#36798;&#21644;&#22810;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#31354;&#38388;&#29615;&#22659;&#34920;&#31034;&#65292;&#21487;&#20197;&#22686;&#24378;&#33258;&#21160;&#36710;&#36742;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#21644;&#23041;&#32961;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25668;&#20687;&#22836;&#30340;&#20449;&#24687;&#26469;&#23454;&#29616;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2210.06758</link><description>&lt;p&gt;
&#25506;&#32034;&#19978;&#19979;&#25991;&#34920;&#36798;&#21644;&#22810;&#27169;&#24577;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving. (arXiv:2210.06758v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#24212;&#29992;&#19978;&#19979;&#25991;&#34920;&#36798;&#21644;&#22810;&#27169;&#24577;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#31354;&#38388;&#29615;&#22659;&#34920;&#31034;&#65292;&#21487;&#20197;&#22686;&#24378;&#33258;&#21160;&#36710;&#36742;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#21644;&#23041;&#32961;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#25668;&#20687;&#22836;&#30340;&#20449;&#24687;&#26469;&#23454;&#29616;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#31354;&#38388;&#29615;&#22659;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#33258;&#21160;&#36710;&#36742;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#23041;&#32961;&#30340;&#39044;&#27979;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#24863;&#30693;&#31995;&#32479;&#36890;&#36807;&#20256;&#24863;&#22120;&#34701;&#21512;&#22686;&#24378;&#20102;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#23436;&#25972;&#30340;&#29615;&#22659;&#19978;&#19979;&#25991;&#12290;&#20154;&#31867;&#22312;&#39550;&#39542;&#26102;&#33258;&#28982;&#22320;&#20351;&#29992;&#31070;&#32463;&#22320;&#22270;&#65292;&#23558;&#21382;&#21490;&#25968;&#25454;&#12289;&#24773;&#22659;&#32454;&#33410;&#21644;&#20854;&#20182;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#34892;&#20026;&#39044;&#27979;&#31561;&#21508;&#31181;&#22240;&#32032;&#25972;&#21512;&#36215;&#26469;&#65292;&#24418;&#25104;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#36825;&#31181;&#22522;&#20110;&#31070;&#32463;&#22320;&#22270;&#30340;&#29702;&#35299;&#23545;&#20110;&#22312;&#36947;&#36335;&#19978;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23613;&#31649;&#33258;&#21160;&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#27809;&#26377;&#23436;&#20840;&#21033;&#29992;&#20154;&#31867;&#33324;&#28145;&#24230;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20511;&#37492;&#20154;&#31867;&#39550;&#39542;&#27169;&#24335;&#65292;&#26088;&#22312;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#20013;&#24418;&#24335;&#21270;&#20256;&#24863;&#22120;&#34701;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19977;&#20010;&#25668;&#20687;&#22836;&#65288;&#24038;&#12289;&#21491;&#12289;&#21518;&#65289;&#30340;&#20449;&#24687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning contextual and spatial environmental representations enhances autonomous vehicle's hazard anticipation and decision-making in complex scenarios. Recent perception systems enhance spatial understanding with sensor fusion but often lack full environmental context. Humans, when driving, naturally employ neural maps that integrate various factors such as historical data, situational subtleties, and behavioral predictions of other road users to form a rich contextual understanding of their surroundings. This neural map-based comprehension is integral to making informed decisions on the road. In contrast, even with their significant advancements, autonomous systems have yet to fully harness this depth of human-like contextual understanding. Motivated by this, our work draws inspiration from human driving patterns and seeks to formalize the sensor fusion approach within an end-to-end autonomous driving framework. We introduce a framework that integrates three cameras (left, right, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32972;&#21387;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#36890;&#32593;&#32476;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.02612</link><description>&lt;p&gt;
&#20855;&#26377;&#32972;&#21387;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;Lyapunov&#20989;&#25968;&#19968;&#33268;&#33258;&#36866;&#24212;&#32593;&#32476;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Lyapunov Function Consistent Adaptive Network Signal Control with Back Pressure and Reinforcement Learning. (arXiv:2210.02612v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02612
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32972;&#21387;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#20132;&#36890;&#32593;&#32476;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20013;&#65292;&#22522;&#20110;&#27969;&#37327;&#21644;&#22522;&#20110;&#21387;&#21147;&#30340;&#26041;&#27861;&#36890;&#24120;&#20998;&#21035;&#20351;&#29992;&#65292;&#20294;&#24448;&#24448;&#34987;&#21333;&#29420;&#32771;&#34385;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;Lyapunov&#25511;&#21046;&#29702;&#35770;&#65292;&#20998;&#21035;&#20026;&#36825;&#20123;&#26041;&#27861;&#23450;&#20041;&#20102;&#29305;&#23450;&#30340;Lyapunov&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#33879;&#21517;&#30340;&#32972;&#21387;&#26041;&#27861;&#31561;&#20110;&#20132;&#21449;&#21475;&#36710;&#36947;&#39281;&#21644;&#27969;&#37327;&#21152;&#26435;&#30340;&#24046;&#20998;&#38431;&#21015;&#38271;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#28155;&#21152;&#22522;&#26412;&#30340;&#20132;&#36890;&#27969;&#29702;&#35770;&#26469;&#25913;&#36827;&#23427;&#12290;&#25511;&#21046;&#31995;&#32479;&#19981;&#20165;&#24212;&#35813;&#30830;&#20445;&#31283;&#23450;&#65292;&#36824;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#12290;&#22522;&#20110;Lyapunov&#29702;&#35770;&#30340;&#21551;&#31034;&#65292;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#32593;&#32476;&#20449;&#21495;&#25511;&#21046;&#35774;&#35745;&#20102;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#20351;&#29992;Double Deep Q-Network (DDQN)&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#20960;&#31181;&#24120;&#29992;&#30340;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traffic signal control, flow-based (optimizing the overall flow) and pressure-based methods (equalizing and alleviating congestion) are commonly used but often considered separately. This study introduces a unified framework using Lyapunov control theory, defining specific Lyapunov functions respectively for these methods. We have found interesting results. For example, the well-recognized back-pressure method is equal to differential queue lengths weighted by intersection lane saturation flows. We further improve it by adding basic traffic flow theory. Rather than ensuring that the control system be stable, the system should be also capable of adaptive to various performance metrics. Building on insights from Lyapunov theory, this study designs a reward function for the Reinforcement Learning (RL)-based network signal control, whose agent is trained with Double Deep Q-Network (DDQN) for effective control over complex traffic networks. The proposed algorithm is compared with several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#27169;&#22411;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (MI-GAN) &#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#23545;&#20248;&#21270;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.01864</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (MI-GAN) &#29992;&#20110;&#23398;&#20064;&#26368;&#20248;&#21151;&#29575;&#27969;
&lt;/p&gt;
&lt;p&gt;
Model-Informed Generative Adversarial Network (MI-GAN) for Learning Optimal Power Flow. (arXiv:2206.01864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#27169;&#22411;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (MI-GAN) &#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#23545;&#20248;&#21270;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#21151;&#29575;&#27969; (OPF) &#38382;&#39064;&#20316;&#20026;&#30005;&#21147;&#31995;&#32479;&#36816;&#33829;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#30001;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#21464;&#24322;&#24615;&#12289;&#38388;&#27463;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#32473;&#35299;&#20915;&#24102;&#26469;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;&#38543;&#26426;&#21270;&#21644;&#40065;&#26834;&#24615;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35299;&#20915;OPF&#38382;&#39064;&#65292;&#20294;&#38754;&#23545;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21363;&#20248;&#21270;&#27169;&#22411;&#20013;&#30340;&#21160;&#24577;&#31995;&#25968;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#21033;&#29992;&#25968;&#25454;&#35299;&#20915;OPF&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#21644;&#26368;&#20248;&#24615;&#21487;&#33021;&#26080;&#27861;&#24471;&#21040;&#20445;&#35777;&#65292;&#31995;&#32479;&#21160;&#24577;&#20063;&#26080;&#27861;&#24471;&#21040;&#36866;&#24403;&#30340;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#27169;&#22411;&#20449;&#24687;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (MI-GAN) &#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal power flow (OPF) problem, as a critical component of power system operations, becomes increasingly difficult to solve due to the variability, intermittency, and unpredictability of renewable energy brought to the power system. Although traditional optimization techniques, such as stochastic and robust optimization approaches, could be leveraged to address the OPF problem, in the face of renewable energy uncertainty, i.e., the dynamic coefficients in the optimization model, their effectiveness in dealing with large-scale problems remains limited. As a result, deep learning techniques, such as neural networks, have recently been developed to improve computational efficiency in solving OPF problems with the utilization of data. However, the feasibility and optimality of the solution may not be guaranteed, and the system dynamics cannot be properly addressed as well. In this paper, we propose an optimization model-informed generative adversarial network (MI-GAN) framework to so
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#21644;&#23454;&#29992;&#31995;&#32479;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#32416;&#27491;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22833;&#36133;&#65292;&#22686;&#24378;&#20102;&#20854;&#21487;&#20449;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.02779</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#32974;&#20799;&#33041;MRI&#20998;&#21106;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Dempster-Shafer approach to trustworthy AI with application to fetal brain MRI segmentation. (arXiv:2204.02779v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#26041;&#27861;&#30340;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#21644;&#23454;&#29992;&#31995;&#32479;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#36890;&#36807;&#26816;&#27979;&#21644;&#32416;&#27491;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22833;&#36133;&#65292;&#22686;&#24378;&#20102;&#20854;&#21487;&#20449;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30149;&#29702;&#24773;&#20917;&#21644;&#19982;&#35757;&#32451;&#22270;&#20687;&#25293;&#25668;&#22312;&#19981;&#21516;&#20013;&#24515;&#30340;&#22270;&#20687;&#20013;&#21487;&#33021;&#20986;&#29616;&#24847;&#22806;&#21644;&#26174;&#33879;&#30340;&#22833;&#36133;&#65292;&#20854;&#26631;&#31614;&#38169;&#35823;&#36829;&#21453;&#20102;&#19987;&#23478;&#30693;&#35782;&#12290;&#36825;&#20123;&#38169;&#35823;&#21066;&#24369;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#20449;&#24615;&#12290;&#26816;&#27979;&#21644;&#32416;&#27491;&#36825;&#20123;&#22833;&#36133;&#30340;&#26426;&#21046;&#23545;&#20110;&#23433;&#20840;&#22320;&#23558;&#36825;&#39033;&#25216;&#26415;&#24212;&#29992;&#20110;&#20020;&#24202;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27861;&#35268;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#29702;&#35770;&#26694;&#26550;&#21644;&#19968;&#20010;&#23454;&#29992;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;Dempster-Shafer&#29702;&#35770;&#30340;&#22791;&#29992;&#26041;&#27861;&#21644;&#25925;&#38556;&#23433;&#20840;&#26426;&#21046;&#26469;&#22686;&#24378;&#20219;&#20309;&#39592;&#24178;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#25805;&#20316;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#20002;&#24323;&#39592;&#24178;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#30340;&#20307;&#32032;&#32423;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#36829;&#21453;&#20102;&#19987;&#23478;&#30693;&#35782;&#65292;&#24182;&#23545;&#36825;&#20123;&#20307;&#32032;&#20351;&#29992;&#22791;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models for medical image segmentation can fail unexpectedly and spectacularly for pathological cases and images acquired at different centers than training images, with labeling errors that violate expert knowledge. Such errors undermine the trustworthiness of deep learning models for medical image segmentation. Mechanisms for detecting and correcting such failures are essential for safely translating this technology into clinics and are likely to be a requirement of future regulations on artificial intelligence (AI). In this work, we propose a trustworthy AI theoretical framework and a practical system that can augment any backbone AI system using a fallback method and a fail-safe mechanism based on Dempster-Shafer theory. Our approach relies on an actionable definition of trustworthy AI. Our method automatically discards the voxel-level labeling predicted by the backbone AI that violate expert knowledge and relies on a fallback for those voxels. We demonstrate the effec
&lt;/p&gt;</description></item></channel></rss>