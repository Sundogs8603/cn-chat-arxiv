<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.16157</link><description>&lt;p&gt;
&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#21019;&#36896;&#20102;&#26032;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#27010;&#24565;&#65292;&#24182;&#21051;&#30011;&#20102;&#20854;&#29305;&#24449;&#12290;&#36825;&#20123;&#39063;&#31890;&#39318;&#20808;&#30830;&#23450;&#33258;&#24049;&#65292;&#28982;&#21518;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#39063;&#31890;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#24212;&#29992;&#21644;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#21457;&#26126;&#20102;&#30830;&#23450;&#33258;&#24049;&#30340;&#23384;&#22312;&#24615;&#39063;&#31890;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#20174;&#20195;&#25968;&#12289;&#25299;&#25169;&#21644;&#32454;&#37096;&#23398;&#30340;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20102;&#21051;&#30011;&#12290;&#23384;&#22312;&#24615;&#39063;&#31890;&#26159;&#37027;&#20123;&#26368;&#21021;&#30830;&#23450;&#33258;&#24049;&#65292;&#24182;&#38543;&#21518;&#19982;&#20854;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#39063;&#31890;&#12290;&#36825;&#20010;&#27010;&#24565;&#30340;&#31034;&#20363;&#65292;&#27604;&#22914;&#39063;&#31890;&#29699;&#65292;&#22312;&#20043;&#21069;&#20854;&#20182;&#20154;&#30340;&#20316;&#21697;&#20013;&#34429;&#28982;&#23450;&#20041;&#19981;&#23436;&#22791;&#12289;&#31639;&#27861;&#24314;&#31435;&#19981;&#20805;&#20998;&#12289;&#29702;&#35770;&#21270;&#19981;&#36275;&#65292;&#20294;&#24050;&#32463;&#22312;&#31895;&#31961;&#38598;&#21644;&#36719;&#35745;&#31639;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#36866;&#21512;&#20110;&#39063;&#31890;&#35745;&#31639;&#30340;&#22810;&#20010;&#29702;&#35770;&#26694;&#26550;&#65288;&#20844;&#29702;&#21270;&#12289;&#36866;&#24212;&#24615;&#31561;&#65289;&#12290;&#36825;&#31181;&#21051;&#30011;&#26088;&#22312;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#12289;&#20998;&#31867;&#38382;&#39064;&#30340;&#24212;&#29992;&#20197;&#21450;&#21487;&#33021;&#30340;&#26041;&#27861;&#25512;&#24191;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#24182;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.16150</link><description>&lt;p&gt;
&#20351;&#29992;&#36974;&#34109;&#26465;&#20214;&#25193;&#25955;&#30340;&#27169;&#24577;&#24490;&#29615;&#36827;&#34892;MRI&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#22810;&#27169;&#24577;MRI&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#20998;&#21106;&#12290;&#26041;&#27861;&#22522;&#20110;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#21644;&#26465;&#20214;&#25193;&#25955;&#30340;&#24605;&#24819;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#20013;&#26410;&#36935;&#21040;&#30340;&#24322;&#24120;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#20998;&#21106;&#26088;&#22312;&#26816;&#27979;&#19982;&#35757;&#32451;&#36807;&#31243;&#20013;&#22788;&#29702;&#30340;&#20219;&#20309;&#27169;&#24335;&#19981;&#21516;&#30340;&#27169;&#24335;&#65292;&#36890;&#24120;&#31216;&#20026;&#24322;&#24120;&#25110;&#36229;&#20986;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#32780;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#32852;&#30340;&#25163;&#21160;&#20998;&#21106;&#12290;&#30001;&#20110;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#24322;&#24120;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#25928;&#65292;&#26816;&#27979;&#24322;&#24120;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#36974;&#34109;&#27169;&#24577;&#24490;&#29615;&#19982;&#26465;&#20214;&#25193;&#25955;&#65288;MMCCD&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#27169;&#24577;MRI&#20013;&#20998;&#21106;&#21508;&#31181;&#27169;&#24335;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#22522;&#26412;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24490;&#29615;&#27169;&#24577;&#36716;&#25442;&#20316;&#20026;&#21551;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#26426;&#21046;&#12290;&#22270;&#20687;&#36716;&#25442;&#27169;&#22411;&#23398;&#20064;&#32452;&#32455;&#29305;&#24322;&#30340;&#27169;&#24577;&#26144;&#23556;&#65292;&#36825;&#26159;&#32452;&#32455;&#29983;&#29702;&#23398;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#26144;&#23556;&#26080;&#27861;&#23558;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#26410;&#36935;&#21040;&#30340;&#32452;&#32455;&#25110;&#22270;&#20687;&#27169;&#24335;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#65292;&#20351;&#24471;&#24322;&#24120;&#33021;&#22815;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
&lt;/p&gt;</description></item><item><title>Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.16149</link><description>&lt;p&gt;
Jais&#21644;Jais-chat&#65306;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16149
&lt;/p&gt;
&lt;p&gt;
Jais&#21644;Jais-chat&#26159;&#26032;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;13&#20159;&#21442;&#25968;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#33521;&#35821;&#26041;&#38754;&#20063;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#24067;&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Jais&#21644;Jais-chat&#65292;&#36825;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#20197;&#38463;&#25289;&#20271;&#35821;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;GPT-3&#30340;&#20165;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#25991;&#26412;&#30340;&#28151;&#21512;&#29289;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#28304;&#20195;&#30721;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;130&#20159;&#20010;&#21442;&#25968;&#65292;&#26681;&#25454;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#33521;&#35821;&#25968;&#25454;&#35201;&#23569;&#24471;&#22810;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#33521;&#35821;&#26041;&#38754;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#12289;&#35843;&#20248;&#12289;&#23433;&#20840;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24320;&#25918;&#29256;&#26412;--&#22522;&#30784;Jais&#27169;&#22411;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;Jais-chat&#21464;&#31181;--&#26088;&#22312;&#20419;&#36827;&#38463;&#25289;&#20271;&#35821;LLMs&#30340;&#30740;&#31350;&#12290;&#35814;&#35265;https://hugging
&lt;/p&gt;
&lt;p&gt;
We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
&lt;/p&gt;</description></item><item><title>MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.16139</link><description>&lt;p&gt;
MedShapeNet - &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16139
&lt;/p&gt;
&lt;p&gt;
MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MedShapeNet&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#35299;&#21078;&#24418;&#29366;&#65288;&#22914;&#39592;&#39612;&#12289;&#22120;&#23448;&#12289;&#34880;&#31649;&#65289;&#21644;&#19977;&#32500;&#25163;&#26415;&#22120;&#26800;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#20043;&#21069;&#65292;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#35777;&#26126;&#20102;&#24418;&#29366;&#24120;&#34987;&#29992;&#26469;&#25551;&#36848;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#20307;&#32032;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#24418;&#29366;&#65288;&#21253;&#25324;&#20307;&#32032;&#21344;&#25454;&#32593;&#26684;&#12289;&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#38544;&#24335;&#34920;&#38754;&#27169;&#22411;&#65289;&#26159;&#19977;&#32500;&#25968;&#25454;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#22823;&#37327;&#20851;&#20110;&#24418;&#29366;&#30340;&#25991;&#31456;&#21450;&#22312;&#39030;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#65288;&#22914;IEEE/CVF&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;&#65288;CVPR&#65289;&#65289;&#20013;&#35265;&#21040;&#65292;&#21516;&#26102;ShapeNet&#65288;&#32422;51300&#20010;&#27169;&#22411;&#65289;&#21644;&#26222;&#26519;&#26031;&#39039;ModelNet&#65288;127,915&#20010;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#24230;&#20063;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;MedShapeNet&#30340;&#21019;&#24314;&#26159;&#20026;&#20102;&#20316;&#20026;&#36825;&#20123;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20262;&#25958;&#30340;&#22825;&#27668;&#21644;&#24037;&#20316;&#26085;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#29305;&#24449;&#21644;&#22320;&#29702;&#25509;&#36817;&#24615;&#30340;&#22270;&#31895;&#21270;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16122</link><description>&lt;p&gt;
&#31354;&#38388;&#22270;&#31895;&#21270;&#65306;&#20351;&#29992;GNN&#36827;&#34892;&#20262;&#25958;&#33258;&#34892;&#36710;&#20849;&#20139;&#26381;&#21153;&#30340;&#22825;&#27668;&#21644;&#24037;&#20316;&#26085;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial Graph Coarsening: Weather and Weekday Prediction with London's Bike-Sharing Service using GNN. (arXiv:2308.16122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20262;&#25958;&#30340;&#22825;&#27668;&#21644;&#24037;&#20316;&#26085;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#29305;&#24449;&#21644;&#22320;&#29702;&#25509;&#36817;&#24615;&#30340;&#22270;&#31895;&#21270;&#25805;&#20316;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#39044;&#27979;&#20262;&#25958;&#19968;&#22825;&#30340;&#22825;&#27668;&#21644;&#24037;&#20316;&#26085;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26705;&#22374;&#24503;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#22270;&#20998;&#31867;&#20219;&#21153;&#12290;&#25552;&#20986;&#30340;GNN&#27169;&#22411;&#24341;&#20837;&#20102;&#65288;i&#65289;&#22270;&#29305;&#24449;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#33410;&#28857;&#23884;&#20837;&#30340;&#25340;&#25509;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#22320;&#29702;&#25509;&#36817;&#24615;&#30340;&#22270;&#31895;&#21270;&#25805;&#20316;&#65292;&#21363;&#8220;&#31354;&#38388;&#22270;&#31895;&#21270;&#8221;&#12290;&#21033;&#29992;&#22303;&#22320;&#21033;&#29992;&#29305;&#24449;&#21644;&#33258;&#34892;&#36710;&#31449;&#21608;&#22260;&#30340;&#23478;&#24237;&#25968;&#37327;&#30340;&#33410;&#28857;&#29305;&#24449;&#20197;&#21450;&#22478;&#24066;&#28201;&#24230;&#30340;&#22270;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduced the use of Graph Neural Network (GNN) for predicting the weather and weekday of a day in London, from the dataset of Santander Cycles bike-sharing system as a graph classification task. The proposed GNN models newly introduced (i) a concatenation operator of graph features with trained node embeddings and (ii) a graph coarsening operator based on geographical contiguity, namely "Spatial Graph Coarsening". With the node features of land-use characteristics and number of households around the bike stations and graph features of temperatures in the city, our proposed models outperformed the baseline model in cross-entropy loss and accuracy of the validation dataset.
&lt;/p&gt;</description></item><item><title>survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;</title><link>http://arxiv.org/abs/2308.16113</link><description>&lt;p&gt;
survex&#65306;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29983;&#23384;&#27169;&#22411;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16113
&lt;/p&gt;
&lt;p&gt;
survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#20986;&#33394;&#24615;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#34917;&#20805;&#21644;&#36229;&#36234;&#20256;&#32479;&#30340;&#32479;&#35745;&#29983;&#23384;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#25805;&#20316;&#21644;&#39044;&#27979;&#21407;&#29702;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;survex R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#12290;&#25152;&#25552;&#36719;&#20214;&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#35786;&#26029;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#36827;&#23427;&#20204;&#12290;&#36890;&#36807;&#25581;&#31034;&#21464;&#37327;&#25928;&#24212;&#21644;&#37325;&#35201;&#24615;&#31561;&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#65292;survex&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24182;&#26816;&#27979;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#21307;&#30103;&#24212;&#29992;&#31561;&#25935;&#24863;&#39046;&#22495;&#21487;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#24207;&#21015;&#21367;&#31215;&#21644;LSTM&#21333;&#20803;&#30340;&#39640;&#32423;&#25968;&#25454;&#39537;&#21160;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30707;&#27833;&#20135;&#37327;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#21382;&#21490;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16105</link><description>&lt;p&gt;
&#39640;&#32423;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30707;&#27833;&#20135;&#37327;
&lt;/p&gt;
&lt;p&gt;
Advanced Deep Regression Models for Forecasting Time Series Oil Production. (arXiv:2308.16105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#24207;&#21015;&#21367;&#31215;&#21644;LSTM&#21333;&#20803;&#30340;&#39640;&#32423;&#25968;&#25454;&#39537;&#21160;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30707;&#27833;&#20135;&#37327;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#21382;&#21490;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#30707;&#27833;&#38656;&#27714;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#65292;&#39044;&#35745;&#21040;2040&#24180;&#23558;&#36798;&#21040;&#27599;&#22825;106.3&#19975;&#26742;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#27833;&#27668;&#24320;&#37319;&#34892;&#19994;&#26469;&#35828;&#65292;&#39044;&#27979;&#20854;&#20135;&#37327;&#20197;&#20248;&#21270;&#36816;&#33829;&#24182;&#36991;&#20813;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#20844;&#21496;&#24050;&#32463;&#24847;&#35782;&#21040;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;(DL)&#30340;&#21147;&#37327;&#21644;&#26469;&#33258;&#21508;&#31181;&#27833;&#20117;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#30340;&#36816;&#33829;&#25104;&#26412;&#24182;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;(ML)&#25216;&#26415;&#36827;&#34892;&#30707;&#27833;&#20135;&#37327;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#21382;&#21490;&#27169;&#24335;&#65292;&#23548;&#33268;&#39044;&#27979;&#19981;&#20934;&#30830;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21367;&#31215;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#21333;&#20803;&#24320;&#21457;&#20808;&#36827;&#30340;&#25968;&#25454;&#39537;&#21160;&#22238;&#24402;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#20998;&#26512;&#20197;&#36873;&#25321;&#26368;&#20248;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global oil demand is rapidly increasing and is expected to reach 106.3 million barrels per day by 2040. Thus, it is vital for hydrocarbon extraction industries to forecast their production to optimize their operations and avoid losses. Big companies have realized that exploiting the power of deep learning (DL) and the massive amount of data from various oil wells for this purpose can save a lot of operational costs and reduce unwanted environmental impacts. In this direction, researchers have proposed models using conventional machine learning (ML) techniques for oil production forecasting. However, these techniques are inappropriate for this problem as they can not capture historical patterns found in time series data, resulting in inaccurate predictions. This research aims to overcome these issues by developing advanced data-driven regression models using sequential convolutions and long short-term memory (LSTM) units. Exhaustive analyses are conducted to select the optimal sequence 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.16089</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#26041;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#22312;&#21152;&#28909;&#28809;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Zone Method based Machine Learning and Physics-Informed Neural Networks in Reheating Furnaces. (arXiv:2308.16089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#21152;&#28909;&#28809;&#25511;&#21046;&#31995;&#32479;&#30340;&#35757;&#32451;&#65292;&#20026;&#22522;&#30784;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21046;&#36896;&#21644;&#33021;&#32791;&#38477;&#20302;&#30446;&#26631;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#30784;&#20135;&#19994;&#30340;&#32463;&#27982;&#37325;&#35201;&#24615;&#24456;&#39640;&#65292;&#20294;&#20854;&#29983;&#20135;&#38142;&#20013;&#30340;&#19968;&#20123;&#32452;&#20214;&#65292;&#22914;&#21152;&#28909;&#28809;&#65292;&#33021;&#32791;&#36739;&#39640;&#12290;&#36890;&#36807;&#20943;&#23569;&#21152;&#28909;&#28809;&#20013;&#30340;&#25972;&#20307;&#21152;&#28909;&#26102;&#38388;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#12290;&#22312;&#22522;&#30784;&#20135;&#19994;&#21487;&#25345;&#32493;&#21046;&#36896;&#20013;&#65292;&#35745;&#31639;&#26426;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25511;&#21046;&#31995;&#32479;&#21487;&#33021;&#26159;&#23454;&#29616;&#8220;&#38646;&#20928;&#25490;&#25918;&#8221;&#30446;&#26631;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#20013;&#65292;&#30001;&#20110;&#22312;&#21152;&#28909;&#28809;&#31561;&#22330;&#26223;&#20013;&#26080;&#27861;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#32463;&#20856;&#30340;Hottel&#21306;&#22495;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;ML&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#30340;&#22238;&#24402;&#35757;&#32451;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21306;&#22495;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#36752;&#23556;&#20256;&#28909;&#65288;RHT&#65289;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#36825;&#26159;&#21152;&#28909;&#28809;&#20869;&#39640;&#28201;&#36807;&#31243;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20256;&#28909;&#26426;&#21046;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the high economic relevance of Foundation Industries, certain components like Reheating furnaces within their manufacturing chain are energy-intensive. Notable energy consumption reduction could be obtained by reducing the overall heating time in furnaces. Computer-integrated Machine Learning (ML) and Artificial Intelligence (AI) powered control systems in furnaces could be enablers in achieving the Net-Zero goals in Foundation Industries for sustainable manufacturing.  In this work, due to the infeasibility of achieving good quality data in scenarios like reheating furnaces, classical Hottel's zone method based computational model has been used to generate data for ML and Deep Learning (DL) based model training via regression. It should be noted that the zone method provides an elegant way to model the physical phenomenon of Radiative Heat Transfer (RHT), the dominating heat transfer mechanism in high-temperature processes inside heating furnaces. Using this data, an extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20849;&#35782;&#20102;&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#32467;&#21512;&#21307;&#30103;&#21382;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16067</link><description>&lt;p&gt;
&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#30340;&#20849;&#35782;&#65306;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Consensus of state of the art mortality prediction models: From all-cause mortality to sudden death prediction. (arXiv:2308.16067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20849;&#35782;&#20102;&#26368;&#26032;&#30340;&#27515;&#20129;&#39044;&#27979;&#27169;&#22411;&#65292;&#20174;&#20840;&#22240;&#27515;&#20129;&#21040;&#31361;&#21457;&#27515;&#20129;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#32467;&#21512;&#21307;&#30103;&#21382;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#27599;&#24180;&#26377;&#25968;&#30334;&#19975;&#20154;&#31361;&#28982;&#21644;&#24847;&#22806;&#27515;&#20129;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20808;&#21069;&#21382;&#21490;&#12290;&#36825;&#31181;&#20107;&#20214;&#24456;&#23569;&#35265;&#65292;&#35768;&#22810;&#21463;&#23475;&#32773;&#22312;&#24515;&#33039;&#30142;&#30149;&#20043;&#21069;&#27809;&#26377;&#25509;&#21463;&#36807;&#35843;&#26597;&#65292;&#32780;&#31361;&#21457;&#27515;&#20129;&#30340;&#23450;&#20041;&#20063;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#31361;&#21457;&#27515;&#20129;&#24456;&#38590;&#39044;&#27979;&#12290;&#26412;&#20998;&#26512;&#20351;&#29992;&#20102;2010&#24180;&#22823;&#26684;&#25289;&#26031;&#21733;&#21644;&#20811;&#33713;&#24503;&#65288;GG&#65286;C&#65289;&#22320;&#21306;50&#23681;&#21450;&#20197;&#19978;&#30340;&#20154;&#32676;&#30340;&#33521;&#22269;&#22269;&#27665;&#20445;&#20581;&#26381;&#21153;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#65288;n = 380,000&#65289;&#26469;&#23581;&#35797;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#21307;&#30103;&#21490;&#12289;&#34880;&#28082;&#26816;&#26597;&#12289;&#33647;&#29289;&#22788;&#26041;&#21644;&#20303;&#38498;&#27835;&#30103;&#26159;&#21542;&#30456;&#32467;&#21512;&#21487;&#20197;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#30340;&#39118;&#38505;&#22686;&#21152;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#31361;&#21457;&#27515;&#20129;&#25110;&#20840;&#22240;&#27515;&#20129;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26500;&#24314;&#20102;&#20845;&#20010;&#27169;&#22411;&#65306;&#19977;&#20010;&#21462;&#33258;&#26368;&#26032;&#30740;&#31350;&#65288;BEHRT&#65292;Deepr&#21644;Deep Patient&#65289;&#65292;&#19977;&#20010;&#20026;&#25105;&#20204;&#33258;&#24049;&#21019;&#24314;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Worldwide, many millions of people die suddenly and unexpectedly each year, either with or without a prior history of cardiovascular disease. Such events are sparse (once in a lifetime), many victims will not have had prior investigations for cardiac disease and many different definitions of sudden death exist. Accordingly, sudden death is hard to predict.  This analysis used NHS Electronic Health Records (EHRs) for people aged $\geq$50 years living in the Greater Glasgow and Clyde (GG\&amp;C) region in 2010 (n = 380,000) to try to overcome these challenges. We investigated whether medical history, blood tests, prescription of medicines, and hospitalisations might, in combination, predict a heightened risk of sudden death.  We compared the performance of models trained to predict either sudden death or all-cause mortality. We built six models for each outcome of interest: three taken from state-of-the-art research (BEHRT, Deepr and Deep Patient), and three of our own creation. We trained t
&lt;/p&gt;</description></item><item><title>Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16061</link><description>&lt;p&gt;
Conti&#20844;&#21496;&#65306;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20102;&#35299;&#19968;&#20010;&#22823;&#22411;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#30340;&#20869;&#37096;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning. (arXiv:2308.16061v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16061
&lt;/p&gt;
&lt;p&gt;
Conti&#20844;&#21496;&#30340;&#32842;&#22825;&#35760;&#24405;&#27844;&#38706;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#20102;&#35299;&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#36816;&#33829;&#21830;&#20869;&#37096;&#36816;&#20316;&#30340;&#26426;&#20250;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21644;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#30740;&#31350;&#21457;&#29616;&#19994;&#21153;&#12289;&#25216;&#26415;&#12289;&#20869;&#37096;&#20219;&#21153;&#31649;&#29702;&#12289;&#24694;&#24847;&#36719;&#20214;&#21644;&#23458;&#25143;&#26381;&#21153;&#26159;Conti&#25104;&#21592;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21202;&#32034;&#36719;&#20214;&#26381;&#21153;&#65288;RaaS&#65289;&#27491;&#22312;&#22686;&#21152;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#12290;&#20102;&#35299;RaaS&#32972;&#21518;&#30340;&#20869;&#37096;&#36816;&#20316;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#27492;&#31867;&#27963;&#21160;&#26159;&#38750;&#27861;&#30340;&#12290;&#26368;&#36817;Conti&#20844;&#21496;&#27844;&#38706;&#30340;&#32842;&#22825;&#35760;&#24405;&#32473;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20102;&#35299;&#36825;&#31867;&#32452;&#32455;&#20869;&#37096;&#36816;&#20316;&#30340;&#33391;&#26426;&#12290;&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;Conti&#20844;&#21496;&#32842;&#22825;&#35760;&#24405;&#20013;&#30340;&#20027;&#35201;&#20027;&#39064;&#35752;&#35770;&#12290;&#21457;&#29616;&#20102;&#20116;&#20010;&#35752;&#35770;&#20027;&#39064;&#65306;1&#65289;&#19994;&#21153;&#65292;2&#65289;&#25216;&#26415;&#65292;3&#65289;&#20869;&#37096;&#20219;&#21153;/&#31649;&#29702;&#65292;4&#65289;&#24694;&#24847;&#36719;&#20214;&#65292;5&#65289;&#23458;&#25143;&#26381;&#21153;/&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;Conti&#25104;&#21592;&#30340;&#20027;&#39064;&#20998;&#24067;&#26174;&#31034;&#65292;&#21482;&#26377;4%&#30340;&#20154;&#36827;&#34892;&#20102;&#19987;&#38376;&#30340;&#35752;&#35770;&#65292;&#32780;&#20960;&#20046;&#25152;&#26377;&#20154;&#65288;96%&#65289;&#37117;&#26159;&#20840;&#33021;&#22411;&#65292;&#24847;&#21619;&#30528;&#20182;&#20204;&#30340;&#35752;&#35770;&#37117;&#22260;&#32469;&#30528;&#36825;&#20116;&#20010;&#20027;&#39064;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ransomware-as-a-service (RaaS) is increasing the scale and complexity of ransomware attacks. Understanding the internal operations behind RaaS has been a challenge due to the illegality of such activities. The recent chat leak of the Conti RaaS operator, one of the most infamous ransomware operators on the international scene, offers a key opportunity to better understand the inner workings of such organizations. This paper analyzes the main topic discussions in the Conti chat leak using machine learning techniques such as Natural Language Processing (NLP) and Latent Dirichlet Allocation (LDA), as well as visualization strategies. Five discussion topics are found: 1) Business, 2) Technical, 3) Internal tasking/Management, 4) Malware, and 5) Customer Service/Problem Solving. Moreover, the distribution of topics among Conti members shows that only 4% of individuals have specialized discussions while almost all individuals (96%) are all-rounders, meaning that their discussions revolve aro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#35299;&#20915;&#20102;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#23545;&#35282;&#32447;&#20027;&#23548;&#24773;&#20917;&#19979;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#20197;&#21450;&#20381;&#36182;&#26410;&#30693;&#21442;&#25968;&#30340;&#25238;&#21160;&#23610;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16059</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#25913;&#36827;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate. (arXiv:2308.16059v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#35299;&#20915;&#20102;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#23545;&#35282;&#32447;&#20027;&#23548;&#24773;&#20917;&#19979;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#20197;&#21450;&#20381;&#36182;&#26410;&#30693;&#21442;&#25968;&#30340;&#25238;&#21160;&#23610;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Dirksen, Maly and Rauhut&#22312;&#12298;Annals of Statistics&#12299;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#27599;&#20010;&#26465;&#30446;&#20004;&#20301;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#22312;&#19968;&#33324;&#20122;&#39640;&#26031;&#20998;&#24067;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26497;&#23567;&#21270;&#36895;&#29575;&#65292;&#20294;&#20063;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#29702;&#35770;&#19978;&#65292;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#30001;&#23569;&#25968;&#26465;&#30446;&#20027;&#23548;&#26102;&#65292;&#20854;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#23384;&#22312;&#26412;&#36136;&#19978;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#65307;&#23454;&#38469;&#19978;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#38656;&#35201;&#26681;&#25454;&#19968;&#20123;&#26410;&#30693;&#21442;&#25968;&#36827;&#34892;&#35843;&#25972;&#30340;&#25238;&#21160;&#23610;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#26032;&#22411;&#20108;&#20301;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#19982;Dirksen&#31561;&#20154;&#37319;&#29992;&#30340;&#22343;&#21248;&#25238;&#21160;&#30456;&#20851;&#30340;&#31526;&#21495;&#37327;&#21270;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#22810;&#20301;&#22343;&#21248;&#37327;&#21270;&#22120;&#21551;&#21457;&#30340;&#19977;&#35282;&#25238;&#21160;&#22120;&#20043;&#21518;&#20877;&#36827;&#34892;&#20108;&#20301;&#37327;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#20010;&#26465;&#30446;&#20043;&#38388;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#29575;&#65292;&#35813;&#35823;&#24046;&#29575;&#21462;&#20915;&#20110;...
&lt;/p&gt;
&lt;p&gt;
A covariance matrix estimator using two bits per entry was recently developed by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The estimator achieves near minimax rate for general sub-Gaussian distributions, but also suffers from two downsides: theoretically, there is an essential gap on operator norm error between their estimator and sample covariance when the diagonal of the covariance matrix is dominated by only a few entries; practically, its performance heavily relies on the dithering scale, which needs to be tuned according to some unknown parameters. In this work, we propose a new 2-bit covariance matrix estimator that simultaneously addresses both issues. Unlike the sign quantizer associated with uniform dither in Dirksen et al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the multi-bit uniform quantizer. By employing dithering scales varying across entries, our estimator enjoys an improved operator norm error rate that depends o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20302;&#31209;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#38454;&#24352;&#37327;&#34920;&#31034;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#20132;&#26367;&#20248;&#21270;&#21644;Lagrangian&#20989;&#25968;&#35299;&#20915;&#30456;&#20851;&#30340;&#20984;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16056</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20302;&#31209;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Multitask Learning based on Tensorized SVMs and LSSVMs. (arXiv:2308.16056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#20302;&#31209;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39640;&#38454;&#24352;&#37327;&#34920;&#31034;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#20132;&#26367;&#20248;&#21270;&#21644;Lagrangian&#20989;&#25968;&#35299;&#20915;&#30456;&#20851;&#30340;&#20984;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21033;&#29992;&#20219;&#21153;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#38543;&#30528;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#20219;&#21153;&#29616;&#22312;&#21487;&#20197;&#30001;&#22810;&#20010;&#32034;&#24341;&#24341;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#24352;&#37327;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#24335;&#23545;&#24212;&#19968;&#20010;&#20219;&#21153;&#32034;&#24341;&#65292;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#30001;&#22810;&#20010;&#32034;&#24341;&#24341;&#29992;&#30340;&#20219;&#21153;&#24182;&#20445;&#30041;&#23427;&#20204;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24352;&#37327;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;LSSVM&#65289;&#30340;&#20302;&#31209;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;CP&#20998;&#35299;&#37096;&#32626;&#22312;&#31995;&#25968;&#24352;&#37327;&#19978;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#22240;&#23376;&#21152;&#26435;&#30340;&#20849;&#20139;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#24314;&#27169;&#20219;&#21153;&#20851;&#31995;&#65292;&#24182;&#25512;&#24191;&#21040;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#26041;&#26696;&#21644;Lagrangian&#20989;&#25968;&#65292;&#27599;&#20010;&#23376;&#38382;&#39064;&#37117;&#34987;&#36716;&#21270;&#20026;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#24418;&#24335;&#21270;&#20026;&#23545;&#20598;&#24418;&#24335;&#30340;&#20108;&#27425;&#35268;&#21010;&#25110;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) leverages task-relatedness to enhance performance. With the emergence of multimodal data, tasks can now be referenced by multiple indices. In this paper, we employ high-order tensors, with each mode corresponding to a task index, to naturally represent tasks referenced by multiple indices and preserve their structural relations. Based on this representation, we propose a general framework of low-rank MTL methods with tensorized support vector machines (SVMs) and least square support vector machines (LSSVMs), where the CP factorization is deployed over the coefficient tensor. Our approach allows to model the task relation through a linear combination of shared factors weighted by task-specific factors and is generalized to both classification and regression problems. Through the alternating optimization scheme and the Lagrangian function, each subproblem is transformed into a convex problem, formulated as a quadratic programming or linear system in the dual form
&lt;/p&gt;</description></item><item><title>PAVI&#26159;&#19968;&#31181;&#26495;&#22359;&#21270;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#20154;&#21475;&#30740;&#31350;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21270;&#21644;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#21464;&#20998;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#20998;&#23618;&#38382;&#39064;&#19978;&#30340;&#34920;&#36798;&#21147;&#24378;&#21644;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16022</link><description>&lt;p&gt;
PAVI&#65306;&#26495;&#22359;&#21270;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PAVI: Plate-Amortized Variational Inference. (arXiv:2308.16022v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16022
&lt;/p&gt;
&lt;p&gt;
PAVI&#26159;&#19968;&#31181;&#26495;&#22359;&#21270;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#35268;&#27169;&#20154;&#21475;&#30740;&#31350;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21270;&#21644;&#23398;&#20064;&#21152;&#36895;&#35757;&#32451;&#21464;&#20998;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#20998;&#23618;&#38382;&#39064;&#19978;&#30340;&#34920;&#36798;&#21147;&#24378;&#21644;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#21644;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#23547;&#25214;&#21487;&#33021;&#20135;&#29983;&#25968;&#25454;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#21475;&#30740;&#31350;&#20013;&#65292;&#25512;&#26029;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#25104;&#30334;&#19978;&#21315;&#30340;&#21463;&#35797;&#32773;&#32676;&#20307;&#19978;&#36827;&#34892;&#20102;&#25968;&#30334;&#19975;&#27425;&#27979;&#37327;&#65292;&#23548;&#33268;&#21442;&#25968;&#31354;&#38388;&#24040;&#22823;&#12290;&#36825;&#31181;&#22823;&#22522;&#25968;&#20351;&#24471;&#29616;&#25104;&#30340;&#21464;&#20998;&#25512;&#26029;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#20154;&#21475;&#30740;&#31350;&#30340;&#32467;&#26500;&#21270;&#21464;&#20998;&#25512;&#26029;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20849;&#20139;&#21442;&#25968;&#21270;&#21644;&#23398;&#20064;&#65292;&#36328;&#36234;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;i.i.d.&#21464;&#37327;&#65292;&#30001;&#27169;&#22411;&#30340;&#8220;&#26495;&#22359;&#8221;&#26469;&#35937;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27010;&#24565;&#21629;&#21517;&#20026;&#8220;&#26495;&#22359;&#21270;&#8221;&#12290;&#19982;&#20943;&#32531;&#25512;&#26029;&#30340;&#29616;&#25104;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#30456;&#21453;&#65292;&#26495;&#22359;&#21270;&#23548;&#33268;&#35757;&#32451;&#21464;&#20998;&#20998;&#24067;&#30340;&#36895;&#24230;&#25552;&#39640;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#20998;&#23618;&#38382;&#39064;&#65292;PAVI&#20135;&#29983;&#20102;&#34920;&#36798;&#21147;&#24378;&#12289;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given observed data and a probabilistic generative model, Bayesian inference searches for the distribution of the model's parameters that could have yielded the data. Inference is challenging for large population studies where millions of measurements are performed over a cohort of hundreds of subjects, resulting in a massive parameter space. This large cardinality renders off-the-shelf Variational Inference (VI) computationally impractical.  In this work, we design structured VI families that efficiently tackle large population studies. Our main idea is to share the parameterization and learning across the different i.i.d. variables in a generative model, symbolized by the model's \textit{plates}. We name this concept \textit{plate amortization}. Contrary to off-the-shelf stochastic VI, which slows down inference, plate amortization results in orders of magnitude faster to train variational distributions.  Applied to large-scale hierarchical problems, PAVI yields expressive, parsimoni
&lt;/p&gt;</description></item><item><title>EnsembleFollower&#26159;&#19968;&#20010;&#37319;&#29992;&#20998;&#23618;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;</title><link>http://arxiv.org/abs/2308.16008</link><description>&lt;p&gt;
EnsembleFollower: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#20998;&#23618;&#35268;&#21010;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EnsembleFollower: A Hybrid Car-Following Framework Based On Reinforcement Learning and Hierarchical Planning. (arXiv:2308.16008v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16008
&lt;/p&gt;
&lt;p&gt;
EnsembleFollower&#26159;&#19968;&#20010;&#37319;&#29992;&#20998;&#23618;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#36710;&#36742;&#36319;&#39536;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#23545;&#20110;&#25105;&#20204;&#23545;&#32437;&#21521;&#34892;&#39542;&#34892;&#20026;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#36319;&#39536;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#30001;&#20110;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21463;&#38480;&#39550;&#39542;&#25216;&#33021;&#32780;&#22312;&#26410;&#30693;&#24773;&#20917;&#19979;&#20986;&#29616;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#31181;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#22312;&#29305;&#23450;&#39550;&#39542;&#22330;&#26223;&#20013;&#20855;&#26377;&#33258;&#24049;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EnsembleFollower&#30340;&#20998;&#23618;&#35268;&#21010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20808;&#36827;&#30340;&#31867;&#20154;&#36710;&#36742;&#36319;&#39536;&#12290;EnsembleFollower&#26694;&#26550;&#28041;&#21450;&#19968;&#20010;&#39640;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20351;&#29992;&#65292;&#36127;&#36131;&#26681;&#25454;&#24403;&#21069;&#29366;&#24577;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#20302;&#23618;&#27169;&#22411;&#25191;&#34892;&#21160;&#20316;&#25110;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#26469;&#31649;&#29702;&#22810;&#20010;&#20302;&#23618;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car-following models have made significant contributions to our understanding of longitudinal driving behavior. However, they often exhibit limited accuracy and flexibility, as they cannot fully capture the complexity inherent in car-following processes, or may falter in unseen scenarios due to their reliance on confined driving skills present in training data. It is worth noting that each car-following model possesses its own strengths and weaknesses depending on specific driving scenarios. Therefore, we propose EnsembleFollower, a hierarchical planning framework for achieving advanced human-like car-following. The EnsembleFollower framework involves a high-level Reinforcement Learning-based agent responsible for judiciously managing multiple low-level car-following models according to the current state, either by selecting an appropriate low-level model to perform an action or by allocating different weights across all low-level components. Moreover, we propose a jerk-constrained kin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15987</link><description>&lt;p&gt;
FPTQ&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FPTQ: Fine-grained Post-Training Quantization for Large Language Models. (arXiv:2308.15987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;W4A8&#21644;W8A8&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#21644;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#26469;&#35299;&#20915;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#20013;&#65292;&#24222;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#32473;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#37327;&#21270;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#27969;&#23454;&#36341;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20004;&#31181;&#26041;&#26696;W8A8&#21644;W4A16&#65288;&#21363;&#36825;&#20004;&#31181;&#20301;&#23485;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;W4A8&#35757;&#32451;&#21518;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#28304;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;4&#20301;&#26435;&#37325;&#37327;&#21270;&#30340;I/O&#21033;&#29992;&#29575;&#20248;&#21183;&#21644;8&#20301;&#30697;&#38453;&#35745;&#31639;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;W4A8&#38754;&#20020;&#30528;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36880;&#23618;&#28608;&#27963;&#37327;&#21270;&#31574;&#30053;&#65292;&#23545;&#20110;&#26368;&#26840;&#25163;&#30340;&#23618;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25968;&#22343;&#34913;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#32454;&#31890;&#24230;&#26435;&#37325;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large-scale language models, the substantial parameter size poses significant challenges for deployment. Being a prevalent compression technique, quantization has emerged as the mainstream practice to tackle this issue, which is mainly centered on two recipes W8A8 and W4A16 (i.e. weights and activations in such bit widths). In this study, we propose a novel W4A8 post-training quantization method for the available open-sourced LLMs, which combines the advantages of both two recipes. Therefore, we can leverage the benefit in the I/O utilization of 4-bit weight quantization and the acceleration due to 8-bit matrix computation. Nevertheless, the W4A8 faces notorious performance degradation. As a remedy, we involve layerwise activation quantization strategies which feature a novel logarithmic equalization for most intractable layers, and we combine them with fine-grained weight quantization. Without whistles and bells, we eliminate the necessity for further fine-tuning and obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15984</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#20256;&#32479;&#30340;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#38382;&#39064;&#20013;&#30340;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#24207;&#21015;&#30340;&#24555;&#36895;&#25512;&#26029;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#23398;&#20064;&#32467;&#26500;&#36816;&#21160;&#65288;SfM&#65289;&#30340;&#38382;&#39064;&#12290;SfM&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#36890;&#36807;&#36845;&#20195;&#26368;&#23567;&#21270;&#37325;&#25237;&#24433;&#35823;&#24046;&#65288;&#31216;&#20026;&#26463;&#35843;&#25972;&#65289;&#35299;&#20915;&#65292;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#22909;&#30340;&#21021;&#22987;&#21270;&#32467;&#26524;&#65292;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#23376;&#38382;&#39064;&#65288;&#22914;&#25104;&#23545;&#23039;&#24577;&#20272;&#35745;&#12289;&#23039;&#24577;&#24179;&#22343;&#21270;&#25110;&#19977;&#35282;&#27979;&#37327;&#65289;&#65292;&#36825;&#20123;&#23376;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#28982;&#21518;&#20351;&#29992;&#26463;&#35843;&#25972;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#26367;&#25442;&#20026;&#20197;&#22810;&#20010;&#35270;&#22270;&#19978;&#26816;&#27979;&#21040;&#30340;2D&#20851;&#38190;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#30456;&#24212;&#30340;&#30456;&#26426;&#23039;&#24577;&#21644;3D&#20851;&#38190;&#28857;&#22352;&#26631;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;SfM&#29305;&#23450;&#30340;&#21407;&#35821;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#23427;&#21487;&#20197;&#29992;&#20110;&#24555;&#36895;&#25512;&#26029;&#26032;&#30340;&#21644;&#26410;&#35265;&#36807;&#30340;&#24207;&#21015;&#30340;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26500;&#36816;&#21160;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;5G&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#21151;&#33021;&#65292;&#20351;&#24471;5G&#31995;&#32479;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#29992;&#25143;&#36830;&#25509;&#24322;&#24120;&#24182;&#20027;&#21160;&#36827;&#34892;&#36164;&#28304;&#25511;&#21046;&#21644;&#36830;&#25509;&#24674;&#22797;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.15973</link><description>&lt;p&gt;
&#28436;&#31034;&#65306;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;5G&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Demo: A Digital Twin of the 5G Radio Access Network for Anomaly Detection Functionality. (arXiv:2308.15973v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;5G&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#21151;&#33021;&#65292;&#20351;&#24471;5G&#31995;&#32479;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#29992;&#25143;&#36830;&#25509;&#24322;&#24120;&#24182;&#20027;&#21160;&#36827;&#34892;&#36164;&#28304;&#25511;&#21046;&#21644;&#36830;&#25509;&#24674;&#22797;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;5G/6G&#39046;&#22495;&#20013;&#65292;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#30340;&#27010;&#24565;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#28436;&#31034;&#23637;&#31034;&#20102;&#19968;&#31181;&#38024;&#23545;5G&#22522;&#30784;&#35774;&#26045;&#38598;&#25104;&#32780;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#21019;&#26032;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#23383;&#23402;&#29983;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#29992;&#25143;&#36830;&#25509;&#24322;&#24120;&#65292;&#36171;&#20104;5G&#31995;&#32479;&#20027;&#21160;&#25191;&#34892;&#36164;&#28304;&#25511;&#21046;&#21644;&#36830;&#25509;&#24674;&#22797;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the concept of digital twins (DTs) has received significant attention within the realm of 5G/6G. This demonstration shows an innovative DT design and implementation framework tailored toward integration within the 5G infrastructure. The proposed DT enables near real-time anomaly detection capability pertaining to user connectivity. It empowers the 5G system to proactively execute decisions for resource control and connection restoration.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Jaccard&#32422;&#26463;&#19979;&#30340;&#23494;&#38598;&#23376;&#22270;&#21457;&#29616;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15936</link><description>&lt;p&gt;
Jaccard&#32422;&#26463;&#19979;&#30340;&#23494;&#38598;&#23376;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Jaccard-constrained dense subgraph discovery. (arXiv:2308.15936v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Jaccard&#32422;&#26463;&#19979;&#30340;&#23494;&#38598;&#23376;&#22270;&#21457;&#29616;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#25366;&#25496;&#20013;&#65292;&#21457;&#29616;&#23494;&#38598;&#23376;&#22270;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#21516;&#26102;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21363;&#25968;&#25454;&#38598;&#21487;&#20197;&#34920;&#31034;&#20026;&#22270;&#24555;&#29031;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#32771;&#34385;&#21040;&#22312;&#20801;&#35768;&#19968;&#23450;&#31243;&#24230;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#23494;&#38598;&#23376;&#22270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25628;&#32034;&#20855;&#26377;&#36739;&#22823;Jaccard&#30456;&#20284;&#24615;&#31995;&#25968;&#30340;&#23494;&#38598;&#23376;&#22270;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#22270;&#24555;&#29031;&#21644;&#26435;&#37325;&#955;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#32452;&#23494;&#38598;&#23376;&#22270;&#65292;&#20351;&#24471;&#24863;&#24212;&#23376;&#22270;&#30340;&#23494;&#24230;&#20043;&#21644;&#21152;&#19978;&#20197;&#955;&#21152;&#26435;&#30340;Jaccard&#25351;&#25968;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#21457;&#29616;&#20855;&#26377;&#33391;&#22909;&#30446;&#26631;&#20540;&#30340;&#23494;&#38598;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#65292;&#27599;&#27425;&#36845;&#20195;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(n^2k^2 + m \log n + k^3 n)$&#65292;&#21644;&#19968;&#20010;&#36138;&#23146;&#31639;&#27861;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(n^2k^2 + m \log n)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding dense subgraphs is a core problem in graph mining with many applications in diverse domains. At the same time many real-world networks vary over time, that is, the dataset can be represented as a sequence of graph snapshots. Hence, it is natural to consider the question of finding dense subgraphs in a temporal network that are allowed to vary over time to a certain degree. In this paper, we search for dense subgraphs that have large pairwise Jaccard similarity coefficients. More formally, given a set of graph snapshots and a weight $\lambda$, we find a collection of dense subgraphs such that the sum of densities of the induced subgraphs plus the sum of Jaccard indices, weighted by $\lambda$, is maximized. We prove that this problem is NP-hard. To discover dense subgraphs with good objective value, we present an iterative algorithm which runs in $\mathcal{O}(n^2k^2 + m \log n + k^3 n)$ time per single iteration, and a greedy algorithm which runs in $\mathcal{O}(n^2k^2 + m \log n
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#32780;&#19981;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15911</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;&#25490;&#26021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cyclophobic Reinforcement Learning. (arXiv:2308.15911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#32780;&#19981;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#65292;&#23547;&#25214;&#33391;&#22909;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#20419;&#36827;&#25506;&#32034;&#23545;&#20110;&#26234;&#33021;&#20307;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#30528;&#20004;&#20010;&#31454;&#20105;&#30340;&#30446;&#26631;&#65306;&#26032;&#39062;&#24615;&#25628;&#32034;&#21644;&#31995;&#32479;&#24615;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#25506;&#32034;&#65292;&#21487;&#20197;&#25214;&#21040;&#26032;&#39062;&#24615;&#65292;&#20294;&#26377;&#26102;&#19981;&#20250;&#23545;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#25506;&#32034;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#19982;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#21363;&#21608;&#26399;&#24615;&#25490;&#26021;&#22411;&#65292;&#23427;&#19981;&#20250;&#22870;&#21169;&#26032;&#39062;&#24615;&#65292;&#32780;&#26159;&#36890;&#36807;&#36991;&#20813;&#24490;&#29615;&#26469;&#24809;&#32602;&#20887;&#20313;&#12290;&#36890;&#36807;&#23558;&#21608;&#26399;&#24615;&#25490;&#26021;&#30340;&#20869;&#22312;&#22870;&#21169;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#35009;&#21098;&#35266;&#27979;&#30340;&#20998;&#23618;&#34920;&#31034;&#24207;&#21015;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;MiniGrid&#21644;MiniHack&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20004;&#20010;&#29615;&#22659;&#37117;&#29305;&#21035;&#38590;&#20197;&#35299;&#20915;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19982;&#19981;&#21516;&#23545;&#35937;&#36827;&#34892;&#22797;&#26434;&#30340;&#20132;&#20114;&#25165;&#33021;&#35299;&#20915;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#30340;&#35814;&#32454;&#27604;&#36739;&#21644;&#24443;&#24213;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#26032;&#25552;&#20986;&#30340;&#21608;&#26399;&#24615;&#25490;&#26021;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In environments with sparse rewards, finding a good inductive bias for exploration is crucial to the agent's success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiosity-driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e., it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent's cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#20102;&#28909;&#21147;&#23398;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#65292;&#21487;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#31070;&#32463;&#20803;&#32593;&#32476;&#25191;&#34892;&#20219;&#20309; needed&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15905</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#28909;&#21147;&#23398;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Thermodynamic Computing via Autonomous Quantum Thermal Machines. (arXiv:2308.15905v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15905
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#23454;&#29616;&#20102;&#28909;&#21147;&#23398;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#65292;&#21487;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#31070;&#32463;&#20803;&#32593;&#32476;&#25191;&#34892;&#20219;&#20309; needed&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;&#33258;&#20027;&#37327;&#23376;&#28909;&#26426;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20123;&#26426;&#22120;&#30001;&#23569;&#25968;&#30456;&#20114;&#20316;&#29992;&#30340;&#37327;&#23376;&#20301;&#65288;qubits&#65289;&#19982;&#19981;&#21516;&#28201;&#24230;&#30340;&#20960;&#20010;&#29615;&#22659;&#30456;&#36830;&#12290;&#36890;&#36807;&#26426;&#22120;&#30340;&#28909;&#27969;&#36827;&#34892;&#35745;&#31639;&#12290;&#36807;&#31243;&#20174;&#26681;&#25454;&#36923;&#36753;&#36755;&#20837;&#35774;&#23450;&#29615;&#22659;&#28201;&#24230;&#24320;&#22987;&#12290;&#26426;&#22120;&#28436;&#21270;&#65292;&#26368;&#32456;&#36798;&#21040;&#38750;&#24179;&#34913;&#31283;&#24577;&#65292;&#36890;&#36807;&#36741;&#21161;&#26377;&#38480;&#23610;&#23544;&#20648;&#23384;&#27744;&#30340;&#28201;&#24230;&#21487;&#20197;&#30830;&#23450;&#35745;&#31639;&#32467;&#26524;&#12290;&#36825;&#26679;&#30340;&#26426;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#28909;&#21147;&#23398;&#31070;&#32463;&#20803;&#8221;&#65292;&#21487;&#20197;&#23454;&#29616;&#20219;&#20309;&#32447;&#24615;&#21487;&#20998;&#31163;&#20989;&#25968;&#65292;&#25105;&#20204;&#26126;&#30830;&#35752;&#35770;&#20102;NOT&#65292;3-majority&#21644;NOR&#38376;&#30340;&#24773;&#20917;&#12290;&#21453;&#36807;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28909;&#21147;&#23398;&#31070;&#32463;&#20803;&#32593;&#32476;&#21487;&#20197;&#25191;&#34892;&#20219;&#20309;&#38656;&#35201;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20154;&#24037;&#31070;&#32463;&#20803;&#65288;&#24863;&#30693;&#22120;&#65289;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a physics-based model for classical computation based on autonomous quantum thermal machines. These machines consist of few interacting quantum bits (qubits) connected to several environments at different temperatures. Heat flows through the machine are here exploited for computing. The process starts by setting the temperatures of the environments according to the logical input. The machine evolves, eventually reaching a non-equilibrium steady state, from which the output of the computation can be determined via the temperature of an auxilliary finite-size reservoir. Such a machine, which we term a "thermodynamic neuron", can implement any linearly-separable function, and we discuss explicitly the cases of NOT, 3-majority and NOR gates. In turn, we show that a network of thermodynamic neurons can perform any desired function. We discuss the close connection between our model and artificial neurons (perceptrons), and argue that our model provides an alternative physics-based
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#12289;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.15899</link><description>&lt;p&gt;
&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#22686;&#21152;&#25512;&#29702;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Traditional Neural Networks: Toward adding Reasoning and Learning Capabilities through Computational Logic Techniques. (arXiv:2308.15899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#36923;&#36753;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#12289;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12289;&#32570;&#20047;&#36879;&#26126;&#24615;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;&#31526;&#21495;&#30693;&#35782;&#27880;&#20837;&#65288;SKI&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#30693;&#35782;&#34701;&#20837;&#23376;&#31526;&#21495;&#31995;&#32479;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#27880;&#20837;&#36807;&#31243;&#24182;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#36923;&#36753;&#20803;&#32032;&#34701;&#20837;&#22810;&#20027;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) models have become popular for solving complex problems, but they have limitations such as the need for high-quality training data, lack of transparency, and robustness issues. Neuro-Symbolic AI has emerged as a promising approach combining the strengths of neural networks and symbolic reasoning. Symbolic knowledge injection (SKI) techniques are a popular method to incorporate symbolic knowledge into sub-symbolic systems. This work proposes solutions to improve the knowledge injection process and integrate elements of ML and logic into multi-agent systems (MAS).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#36827;&#34892;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#26080;&#27861;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.15887</link><description>&lt;p&gt;
&#23545;&#20110;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340; CLIP &#28508;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Potential of CLIP for Compositional Logical Reasoning. (arXiv:2308.15887v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#36827;&#34892;&#32452;&#21512;&#36923;&#36753;&#25512;&#29702;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#26080;&#27861;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;OpenAI&#30340;CLIP&#36827;&#34892;&#36923;&#36753;&#36830;&#36143;&#30340;&#35270;&#35273;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#25105;&#20204;&#30340;&#26415;&#35821;&#65292;&#24182;&#23545;CLIP&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#20102;&#20960;&#20309;&#20998;&#26512;&#65292;&#20197;&#20415;&#31995;&#32479;&#22312;&#36923;&#36753;&#19978;&#36830;&#36143;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#35770;&#26159;&#65292;&#36890;&#24120;&#37197;&#32622;&#30340;CLIP&#19981;&#33021;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we explore the possibility of using OpenAI's CLIP to perform logically coherent grounded visual reasoning. To that end, we formalize our terms and give a geometric analysis of how embeddings in CLIP's latent space would need to be configured in order for the system to be logically coherent. Our main conclusion is that, as usually configured, CLIP cannot perform such reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#21644;&#20803;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#20998;&#31867;&#35268;&#21017;&#65292;&#19988;&#22797;&#26434;&#24230;&#26356;&#39640;&#30340;&#26679;&#26412;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15885</link><description>&lt;p&gt;
&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards One-Shot Learning for Text Classification using Inductive Logic Programming. (arXiv:2308.15885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#27425;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#21644;&#20803;&#35299;&#37322;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#20998;&#31867;&#35268;&#21017;&#65292;&#19988;&#22797;&#26434;&#24230;&#26356;&#39640;&#30340;&#26679;&#26412;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#25191;&#34892;&#20010;&#24615;&#21270;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#24320;&#21457;&#25968;&#25454;&#39640;&#25928;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20803;&#35299;&#37322;&#23398;&#20064;&#65288;MIL&#65289;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#20174;ConceptNet&#20013;&#25552;&#21462;&#30340;&#24120;&#35782;&#32972;&#26223;&#30693;&#35782;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;MIL&#21487;&#20197;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#23398;&#20064;&#25991;&#26412;&#20998;&#31867;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#36873;&#25321;&#30340;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#36234;&#39640;&#65292;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#20063;&#36234;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-increasing potential of AI to perform personalised tasks, it is becoming essential to develop new machine learning techniques which are data-efficient and do not require hundreds or thousands of training data. In this paper, we explore an Inductive Logic Programming approach for one-shot text classification. In particular, we explore the framework of Meta-Interpretive Learning (MIL), along with using common-sense background knowledge extracted from ConceptNet. Results indicate that MIL can learn text classification rules from a small number of training examples. Moreover, the higher complexity of chosen examples, the higher accuracy of the outcome.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#26102;&#32771;&#34385;&#20102;&#22240;&#26524;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.15883</link><description>&lt;p&gt;
&#22914;&#26524;&#25105;&#22788;&#20110;AI&#20013;&#65292;&#29983;&#27963;&#20250;&#26356;&#26377;&#36259;&#21527;&#65311;&#22522;&#20110;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
"Would life be more interesting if I were in AI?" Answering Counterfactuals based on Probabilistic Inductive Logic Programming. (arXiv:2308.15883v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#26102;&#32771;&#34385;&#20102;&#22240;&#26524;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#26159;&#19968;&#31181;&#26576;&#20123;&#20107;&#23454;&#20197;&#29305;&#23450;&#27010;&#29575;&#25104;&#31435;&#30340;&#36923;&#36753;&#31243;&#24207;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20123;&#31243;&#24207;&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;&#36890;&#24120;&#36890;&#36807;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#32479;&#35745;&#27979;&#35797;&#26469;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#31243;&#24207;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32479;&#35745;&#27979;&#35797;&#32570;&#20047;&#20851;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#20351;&#29992;&#29983;&#25104;&#30340;&#31243;&#24207;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#29255;&#27573;&#65292;&#20801;&#35768;&#20174;&#20854;&#24341;&#23548;&#20998;&#24067;&#20013;&#37325;&#24314;&#31243;&#24207;&#12290;&#36825;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#25903;&#25345;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic logic programs are logic programs where some facts hold with a specified probability. Here, we investigate these programs with a causal framework that allows counterfactual queries. Learning the program structure from observational data is usually done through heuristic search relying on statistical tests. However, these statistical tests lack information about the causal mechanism generating the data, which makes it unfeasible to use the resulting programs for counterfactual reasoning. To address this, we propose a language fragment that allows reconstructing a program from its induced distribution. This further enables us to learn programs supporting counterfactual queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#35777;&#26126;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.15873</link><description>&lt;p&gt;
&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26368;&#23567;&#23485;&#24230;&#38382;&#39064;&#65306;&#19968;&#31181;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Width for Deep, Narrow MLP: A Diffeomorphism and the Whitney Embedding Theorem Approach. (arXiv:2308.15873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#21644;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#30340;&#35777;&#26126;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20110;&#30830;&#23450;&#28145;&#24230;&#12289;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#30340;&#26368;&#23567;&#23485;&#24230;&#38382;&#39064;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#65292;&#20197;&#19968;&#33268;&#33539;&#25968;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#19979;&#30028;&#21644;&#19978;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#24456;&#38590;&#32553;&#23567;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#23567;&#23485;&#24230;&#30340;&#19978;&#30028;&#65292;&#21363;$\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$&#65292;&#20197;&#23454;&#29616;&#22312;&#28145;&#24230;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#20013;&#30340;&#19968;&#33268;&#36924;&#36817;&#65292;&#20854;&#20013;$0\leq \alpha(\sigma)\leq 2$&#20195;&#34920;&#20102;&#19982;&#28608;&#27963;&#20989;&#25968;&#30456;&#20851;&#30340;&#24120;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#35777;&#26126;&#26469;&#35777;&#26126;&#36825;&#20010;&#19978;&#30028;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#36739;&#23567;&#30340;&#39069;&#22806;&#23485;&#24230;&#30340;&#28145;&#24230;&#31364;&#22411;&#22810;&#23618;&#24863;&#30693;&#26426;&#21487;&#20197;&#36924;&#36817;&#24494;&#20998;&#21516;&#32986;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#24800;&#29305;&#23612;&#23884;&#20837;&#23450;&#29702;&#34920;&#26126;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#37117;&#21487;&#20197;&#36890;&#36807;&#23884;&#20837;&#36924;&#36817;&#65292;&#36827;&#19968;&#27493;&#20998;&#35299;&#20026;&#32447;&#24615;&#21464;&#25442;&#21644;&#24494;&#20998;&#21516;&#32986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant attention on determining the minimum width for the universal approximation property of deep, narrow MLPs. Among these challenges, approximating a continuous function under the uniform norm is important and challenging, with the gap between its lower and upper bound being hard to narrow. In this regard, we propose a novel upper bound for the minimum width, given by $\operatorname{max}(2d_x+1, d_y) + \alpha(\sigma)$, to achieve uniform approximation in deep narrow MLPs, where $0\leq \alpha(\sigma)\leq 2$ represents the constant depending on the activation function. We demonstrate this bound through two key proofs. First, we establish that deep, narrow MLPs with little additional width can approximate diffeomorphisms. Secondly, we utilize the Whitney embedding theorem to show that any continuous function can be approximated by embeddings, further decomposed into linear transformations and diffeomorphisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22495;&#27867;&#21270;&#20013;&#36807;&#37327;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20445;&#35777;&#32463;&#39564;&#39118;&#38505;&#26368;&#20248;&#30340;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24809;&#32602;&#65292;&#36991;&#20813;&#20102;&#24809;&#32602;&#23545;&#32463;&#39564;&#39118;&#38505;&#20248;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15856</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization without Excess Empirical Risk. (arXiv:2308.15856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22495;&#27867;&#21270;&#20013;&#36807;&#37327;&#39118;&#38505;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20445;&#35777;&#32463;&#39564;&#39118;&#38505;&#26368;&#20248;&#30340;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24809;&#32602;&#65292;&#36991;&#20813;&#20102;&#24809;&#32602;&#23545;&#32463;&#39564;&#39118;&#38505;&#20248;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19981;&#21516;&#20998;&#24067;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22495;&#27867;&#21270;&#26088;&#22312;&#23398;&#20064;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#20998;&#24067;&#30340;&#27169;&#22411;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#24809;&#32602;&#26469;&#25429;&#25417;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19982;&#24809;&#32602;&#19968;&#36215;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#22833;&#36133;&#27169;&#24335;&#26159;&#30001;&#20110;&#38169;&#35823;&#30340;&#24809;&#32602;&#25110;&#32852;&#21512;&#20248;&#21270;&#30340;&#22256;&#38590;&#32780;&#23548;&#33268;&#30340;&#36807;&#37327;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#26159;&#23558;&#32463;&#39564;&#39118;&#38505;&#21644;&#24809;&#32602;&#32852;&#21512;&#26368;&#23567;&#21270;&#65292;&#32780;&#26159;&#22312;&#20445;&#35777;&#32463;&#39564;&#39118;&#38505;&#26368;&#20248;&#30340;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24809;&#32602;&#12290;&#36825;&#31181;&#25913;&#21464;&#20445;&#35777;&#20102;&#22495;&#27867;&#21270;&#24809;&#32602;&#19981;&#20250;&#24433;&#21709;&#23545;&#32463;&#39564;&#39118;&#38505;&#30340;&#20248;&#21270;&#65292;&#21363;&#22312;&#20998;&#24067;&#20869;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29575;&#22833;&#30495;&#29702;&#35770;&#30340;&#20196;&#20154;&#20852;&#22859;&#30340;&#32852;&#31995;&#65292;&#24182;&#21033;&#29992;&#20854;&#24037;&#20855;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#24809;&#32602;&#30340;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given data from diverse sets of distinct distributions, domain generalization aims to learn models that generalize to unseen distributions. A common approach is designing a data-driven surrogate penalty to capture generalization and minimize the empirical risk jointly with the penalty. We argue that a significant failure mode of this recipe is an excess risk due to an erroneous penalty or hardness in joint optimization. We present an approach that eliminates this problem. Instead of jointly minimizing empirical risk with the penalty, we minimize the penalty under the constraint of optimality of the empirical risk. This change guarantees that the domain generalization penalty cannot impair optimization of the empirical risk, i.e., in-distribution performance. To solve the proposed optimization problem, we demonstrate an exciting connection to rate-distortion theory and utilize its tools to design an efficient method. Our approach can be applied to any penalty-based domain generalization
&lt;/p&gt;</description></item><item><title>MSGNN&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#21644;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;GNN&#27169;&#22411;&#22312;&#20445;&#30041;&#36828;&#36317;&#31163;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#22312;&#20256;&#26579;&#30149;&#39044;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.15840</link><description>&lt;p&gt;
MSGNN: &#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27969;&#34892;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSGNN: Multi-scale Spatio-temporal Graph Neural Network for Epidemic Forecasting. (arXiv:2308.15840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15840
&lt;/p&gt;
&lt;p&gt;
MSGNN&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#21644;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;GNN&#27169;&#22411;&#22312;&#20445;&#30041;&#36828;&#36317;&#31163;&#36830;&#25509;&#21644;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#19978;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#22312;&#20256;&#26579;&#30149;&#39044;&#27979;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#30149;&#39044;&#27979;&#26159;&#38450;&#25511;&#27969;&#34892;&#30149;&#30340;&#20851;&#38190;&#65292;&#24182;&#34987;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#24403;&#21069;&#27169;&#22411;&#36890;&#36807;&#32553;&#25918;GNN&#30340;&#28145;&#24230;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#65292;&#20294;&#36825;&#23545;&#20110;&#20445;&#30041;&#36828;&#36317;&#31163;&#20294;&#19982;&#27969;&#34892;&#30149;&#30456;&#20851;&#30340;&#22320;&#21306;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#36275;&#22815;&#12290;&#65288;2&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#27169;&#25311;&#21333;&#19968;&#31354;&#38388;&#23610;&#24230;&#20869;&#30340;&#27969;&#34892;&#30149;&#65292;&#32780;&#24573;&#35270;&#20102;&#19981;&#21516;&#23610;&#24230;&#19978;&#23548;&#20986;&#30340;&#22810;&#23610;&#24230;&#27969;&#34892;&#30149;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#21019;&#26032;&#30340;&#22810;&#23610;&#24230;&#35270;&#22270;&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MSGNN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25552;&#20986;&#30340;MSGNN&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#23427;&#30452;&#25509;&#25429;&#25417;&#36328;&#21306;&#22495;&#27969;&#34892;&#30149;&#20449;&#21495;&#30340;&#36828;&#36317;&#31163;&#36830;&#25509;&#65292;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#19968;&#20010;&#22810;&#23610;&#24230;&#22270;&#20013;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#22270;&#65292;&#25105;&#20204;&#20351;&#29992;&#31354;&#38388;-&#26102;&#38388;GCN&#36827;&#34892;&#27969;&#34892;&#30149;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious disease forecasting has been a key focus and proved to be crucial in controlling epidemic. A recent trend is to develop forecast-ing models based on graph neural networks (GNNs). However, existing GNN-based methods suffer from two key limitations: (1) Current models broaden receptive fields by scaling the depth of GNNs, which is insuffi-cient to preserve the semantics of long-range connectivity between distant but epidemic related areas. (2) Previous approaches model epidemics within single spatial scale, while ignoring the multi-scale epidemic pat-terns derived from different scales. To address these deficiencies, we devise the Multi-scale Spatio-temporal Graph Neural Network (MSGNN) based on an innovative multi-scale view. To be specific, in the proposed MSGNN model, we first devise a novel graph learning module, which directly captures long-range connectivity from trans-regional epidemic signals and integrates them into a multi-scale graph. Based on the learned multi-scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;Lasso&#21644;&#36716;&#31227;Lasso&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36716;&#31227;Lasso&#30340;&#28176;&#36827;&#24615;&#36136;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#23427;&#19982;&#33258;&#36866;&#24212;Lasso&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#34701;&#21512;&#24182;&#34917;&#20607;&#20102;&#20182;&#20204;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.15838</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;Lasso&#12289;&#36716;&#31227;Lasso&#21450;&#20854;&#25299;&#23637;&#65306;&#28176;&#36827;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Lasso, Transfer Lasso, and Beyond: An Asymptotic Perspective. (arXiv:2308.15838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;Lasso&#21644;&#36716;&#31227;Lasso&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#36890;&#36807;&#23545;&#36716;&#31227;Lasso&#30340;&#28176;&#36827;&#24615;&#36136;&#36827;&#34892;&#29702;&#35770;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#23427;&#19982;&#33258;&#36866;&#24212;Lasso&#30340;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#34701;&#21512;&#24182;&#34917;&#20607;&#20102;&#20182;&#20204;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#36866;&#24212;Lasso&#21644;&#36716;&#31227;Lasso&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#33258;&#36866;&#24212;Lasso&#26159;&#19968;&#31181;&#25104;&#29087;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26681;&#25454;&#21021;&#22987;&#20272;&#35745;&#20540;&#36827;&#34892;&#30340;&#27491;&#21017;&#21270;&#65292;&#20855;&#26377;&#28176;&#36827;&#27491;&#24577;&#24615;&#21644;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#30340;&#29305;&#28857;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#36716;&#31227;Lasso&#37319;&#29992;&#26681;&#25454;&#21021;&#22987;&#20272;&#35745;&#20540;&#36827;&#34892;&#30340;&#27491;&#21017;&#21270;&#20943;&#27861;&#65292;&#20855;&#26377;&#20943;&#23569;&#38750;&#28176;&#36827;&#20272;&#35745;&#35823;&#24046;&#30340;&#33021;&#21147;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#22240;&#27492;&#20986;&#29616;&#65306;&#37492;&#20110;&#33258;&#36866;&#24212;Lasso&#21644;&#36716;&#31227;Lasso&#22312;&#20351;&#29992;&#21021;&#22987;&#20272;&#35745;&#20540;&#26041;&#38754;&#23384;&#22312;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#36825;&#31181;&#24046;&#24322;&#32473;&#27599;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#20160;&#20040;&#22909;&#22788;&#25110;&#24330;&#31471;&#65311;&#26412;&#25991;&#23545;&#36716;&#31227;Lasso&#30340;&#28176;&#36827;&#24615;&#36136;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23427;&#19982;&#33258;&#36866;&#24212;Lasso&#30340;&#21306;&#21035;&#12290;&#26681;&#25454;&#36825;&#20010;&#20998;&#26512;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21508;&#33258;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#34701;&#21512;&#24182;&#34917;&#20607;&#20102;&#20182;&#20204;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of the theoretical properties inherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a well-established method, employs regularization divided by initial estimators and is characterized by asymptotic normality and variable selection consistency. In contrast, the recently proposed Transfer Lasso employs regularization subtracted by initial estimators with the demonstrated capacity to curtail non-asymptotic estimation errors. A pivotal question thus emerges: Given the distinct ways the Adaptive Lasso and the Transfer Lasso employ initial estimators, what benefits or drawbacks does this disparity confer upon each method? This paper conducts a theoretical examination of the asymptotic properties of the Transfer Lasso, thereby elucidating its differentiation from the Adaptive Lasso. Informed by the findings of this analysis, we introduce a novel method, one that amalgamates the strengths and compensates for the weaknesses o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15821</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#23398;&#20064;&#38477;&#32423;&#21644;&#24930;&#25910;&#25947;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#22320;&#37319;&#29992;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#27010;&#24565;&#65292;&#21482;&#20801;&#35768;&#22312;&#27599;&#20010;&#32452;&#20869;&#32858;&#21512;&#27169;&#22411;&#26435;&#37325;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#26799;&#24230;&#25110;&#25512;&#29702;&#36755;&#20986;&#20316;&#20026;&#23458;&#25143;&#31471;&#20998;&#21306;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#30446;&#30340;&#26159;&#23558;&#30456;&#20284;&#35774;&#22791;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20294;&#27599;&#20010;&#32858;&#31867;&#20869;&#37096;&#20173;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30740;&#31350;&#25506;&#32034;&#30830;&#23450;&#32858;&#31867;&#30340;&#36866;&#24403;&#26102;&#38388;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#23548;&#33268;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21040;&#20854;&#33258;&#24049;&#30340;&#29420;&#31435;&#32858;&#31867;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38750;ind&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#22312;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#20102;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;</title><link>http://arxiv.org/abs/2308.15793</link><description>&lt;p&gt;
HAlf-MAsked&#27169;&#22411;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
HAlf-MAsked Model for Named Entity Sentiment analysis. (arXiv:2308.15793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15793
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#20102;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#20998;&#26512;&#65288;NESA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#26368;&#27963;&#36291;&#30340;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#12290;&#31038;&#20132;&#23186;&#20307;NESA&#26159;&#24847;&#35265;&#20998;&#26512;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#22240;&#20026;&#26816;&#27979;&#21644;&#36319;&#36394;&#26032;&#38395;&#27969;&#20013;&#30340;&#24773;&#24863;&#36235;&#21183;&#23545;&#20110;&#26500;&#24314;&#21508;&#31181;&#20998;&#26512;&#31995;&#32479;&#21644;&#30417;&#27979;&#29305;&#23450;&#20154;&#29289;&#25110;&#20844;&#21496;&#30340;&#23186;&#20307;&#24418;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;transformer&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#22312;RuSentNE-23&#35780;&#20272;&#20013;&#30340;NESA&#12290;&#23613;&#31649;BERT&#31561;&#27169;&#22411;&#30340;&#25928;&#26524;&#24456;&#22909;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21487;&#33021;&#22312;&#26576;&#20123;&#25361;&#25112;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#26159;&#22312;RuSentNE-23&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#22312;&#26368;&#32456;&#39044;&#27979;&#20043;&#21069;&#65292;&#23545;&#32473;&#23450;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#30340;&#25513;&#30721;&#23454;&#20307;&#20256;&#36882;&#65292;&#20197;&#20415;&#22312;&#27169;&#22411;&#30693;&#36947;&#39044;&#27979;&#24773;&#24863;&#30340;&#30830;&#20999;&#23454;&#20307;&#21644;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21512;&#24182;&#26469;&#33258;&#27169;&#22411;&#30340;&#36923;&#36753;&#12290;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Named Entity Sentiment analysis (NESA) is one of the most actively developing application domains in Natural Language Processing (NLP). Social media NESA is a significant field of opinion analysis since detecting and tracking sentiment trends in the news flow is crucial for building various analytical systems and monitoring the media image of specific people or companies. In this paper, we study different transformers-based solutions NESA in RuSentNE-23 evaluation. Despite the effectiveness of the BERT-like models, they can still struggle with certain challenges, such as overfitting, which appeared to be the main obstacle in achieving high accuracy on the RuSentNE-23 data. We present several approaches to overcome this problem, among which there is a novel technique of additional pass over given data with masked entity before making the final prediction so that we can combine logits from the model when it knows the exact entity it predicts sentiment for and when it does not. Utilizing 
&lt;/p&gt;</description></item><item><title>FedCiR&#26159;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#26469;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.15786</link><description>&lt;p&gt;
FedCiR: &#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#32852;&#37030;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
FedCiR: Client-Invariant Representation Learning for Federated Non-IID Features. (arXiv:2308.15786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15786
&lt;/p&gt;
&lt;p&gt;
FedCiR&#26159;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#36890;&#36807;&#25913;&#36827;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#26469;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#30340;&#25968;&#25454;&#24448;&#24448;&#26159;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#30340;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#35774;&#22791;&#20043;&#38388;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;&#36890;&#24120;&#31216;&#20026;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#23545;&#20840;&#23616;&#27169;&#22411;&#30340;&#35757;&#32451;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#20998;&#26512;&#29305;&#24449;&#20559;&#31227;&#38382;&#39064;&#30340;&#20869;&#22312;&#21407;&#22240;&#65292;&#25105;&#20204;&#22312;FL&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#24191;&#20041;&#35823;&#24046;&#36793;&#30028;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#25552;&#20986;&#20102;FedCiR&#65292;&#19968;&#31181;&#23458;&#25143;&#31471;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#25552;&#21462;&#20449;&#24687;&#19988;&#19982;&#23458;&#25143;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#34920;&#31034;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#65292;&#20197;&#40723;&#21169;&#34920;&#31034;&#25658;&#24102;&#22522;&#26412;&#30340;&#20998;&#31867;&#30693;&#35782;&#65292;&#24182;&#20943;&#23567;&#20102;&#23458;&#25143;&#31471;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that maximizes the potential of data-driven models for edge devices without sharing their raw data. However, devices often have non-independent and identically distributed (non-IID) data, meaning their local data distributions can vary significantly. The heterogeneity in input data distributions across devices, commonly referred to as the feature shift problem, can adversely impact the training convergence and accuracy of the global model. To analyze the intrinsic causes of the feature shift problem, we develop a generalization error bound in FL, which motivates us to propose FedCiR, a client-invariant representation learning framework that enables clients to extract informative and client-invariant features. Specifically, we improve the mutual information term between representations and labels to encourage representations to carry essential classification knowledge, and diminish the mutual information term between the client 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;Split Learning&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Split Learning&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#38544;&#31169;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2308.15783</link><description>&lt;p&gt;
&#26080;&#27844;&#38706;&#30340;Split&#65306;&#20943;&#23569;Split Learning&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Split Without a Leak: Reducing Privacy Leakage in Split Learning. (arXiv:2308.15783v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;Split Learning&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Split Learning&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#27492;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#38544;&#31169;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26222;&#21450;&#20351;&#24471;&#25935;&#24863;&#25968;&#25454;&#30340;&#38544;&#31169;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#21508;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22312;&#21508;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#20013;&#65292;&#21327;&#20316;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;Split Learning&#65289;&#24050;&#34987;&#29992;&#20110;&#21152;&#36895;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#12290;&#26368;&#21021;&#65292;Split Learning&#34987;&#35748;&#20026;&#26159;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38543;&#21518;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Split Learning&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25915;&#20987;&#65292;&#22240;&#27492;&#19981;&#33021;&#20316;&#20026;&#20445;&#25252;&#38544;&#31169;&#30340;&#25216;&#26415;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20063;&#24341;&#20837;&#20102;&#20351;&#29992;Split Learning&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#20445;&#25252;&#38544;&#31169;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Split Learning&#21644;&#21516;&#24577;&#21152;&#23494;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#20854;&#32972;&#21518;&#30340;&#24605;&#24819;&#26159;&#22312;&#23558;&#28608;&#27963;&#22270;&#65288;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20998;&#21106;&#23618;&#30340;&#36755;&#20986;&#65289;&#21457;&#36865;&#32473;&#26381;&#21153;&#22120;&#20043;&#21069;&#65292;&#23458;&#25143;&#31471;&#23545;&#20854;&#36827;&#34892;&#21152;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of Deep Learning (DL) makes the privacy of sensitive data more imperative than ever. As a result, various privacy-preserving techniques have been implemented to preserve user data privacy in DL. Among various privacy-preserving techniques, collaborative learning techniques, such as Split Learning (SL) have been utilized to accelerate the learning and prediction process. Initially, SL was considered a promising approach to data privacy. However, subsequent research has demonstrated that SL is susceptible to many types of attacks and, therefore, it cannot serve as a privacy-preserving technique. Meanwhile, countermeasures using a combination of SL and encryption have also been introduced to achieve privacy-preserving deep learning. In this work, we propose a hybrid approach using SL and Homomorphic Encryption (HE). The idea behind it is that the client encrypts the activation map (the output of the split layer between the client and the server) before sending it to the ser
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.15734</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;GNNs&#65292;&#20294;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#35774;&#35745;/&#36873;&#25321;&#26368;&#20339;GNN&#26550;&#26500;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33410;&#30465;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;Graph NAS&#65289;&#26469;&#25628;&#32034;&#32467;&#21512;&#29616;&#26377;&#32452;&#20214;&#30340;&#27425;&#20248;GNN&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;Graph NAS&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#22810;&#26679;&#21270;&#22270;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;Graph NAS&#26041;&#27861;&#65292;&#31216;&#20026;ExGNAS&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#65288;ii&#65289;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#25628;&#32034;&#31354;&#38388;&#20165;&#21253;&#21547;&#21487;&#20197;&#22788;&#29702;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22522;&#26412;&#20989;&#25968;&#12290;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#20302;&#32500;&#24230;&#37319;&#26679;&#31354;&#38388;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#30830;&#20445;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2308.15730</link><description>&lt;p&gt;
&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#20302;&#32500;&#24230;&#37319;&#26679;&#31354;&#38388;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#30830;&#20445;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#20135;&#29983;&#19982;&#25152;&#24314;&#27169;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#30456;&#31526;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#38024;&#23545;&#23454;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#21516;&#26102;&#25429;&#33719;&#25968;&#25454;&#30340;&#38745;&#24577;&#20998;&#24067;&#20197;&#21450;&#20219;&#20309;&#28508;&#22312;&#26102;&#38388;&#33539;&#22260;&#30340;&#23436;&#25972;&#26102;&#38388;&#20998;&#24067;&#12290;&#36825;&#20010;&#26102;&#38388;&#35201;&#32032;&#20135;&#29983;&#20102;&#19968;&#20010;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#38480;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#26131;&#20110;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#22312;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#20013;&#65292;&#25972;&#20010;&#24207;&#21015;&#36890;&#36807;&#19968;&#20010;seq2seq&#39118;&#26684;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#30452;&#25509;&#36716;&#25442;&#20026;&#29983;&#25104;&#22120;&#30340;&#37319;&#26679;&#31354;&#38388;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#29992;&#20110;&#22312;&#29305;&#24449;&#31354;&#38388;&#21644;&#36739;&#20302;&#32500;&#24230;&#30340;&#37319;&#26679;&#31354;&#38388;&#21305;&#37197;&#35757;&#32451;&#20998;&#24067;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#32422;&#26463;&#26465;&#20214;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#30340;&#26494;&#25955;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#36229;&#36807;&#38408;&#20540;&#65288;FAT&#65289;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) should produce synthetic data that fits the underlying distribution of the data being modeled. For real valued time-series data, this implies the need to simultaneously capture the static distribution of the data, but also the full temporal distribution of the data for any potential time horizon. This temporal element produces a more complex problem that can potentially leave current solutions under-constrained, unstable during training, or prone to varying degrees of mode collapse. In FETSGAN, entire sequences are translated directly to the generator's sampling space using a seq2seq style adversarial auto encoder (AAE), where adversarial training is used to match the training distribution in both the feature space and the lower dimensional sampling space. This additional constraint provides a loose assurance that the temporal distribution of the synthetic samples will not collapse. In addition, the First Above Threshold (FAT) operator is introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15720</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems. (arXiv:2308.15720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;(RandNLA)&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35745;&#31639;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32463;&#39564;&#24615;&#33021;&#20197;&#21450;&#24378;&#22823;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#19968;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#25152;&#38480;&#21046;&#65292;&#21363;&#29992;&#25143;&#38656;&#35201;&#35774;&#32622;&#21508;&#31181;&#19981;&#21516;&#20110;&#20256;&#32479;NLA&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#29305;&#23450;&#35843;&#21442;&#21442;&#25968;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#26469;&#35299;&#20915;RandNLA&#31639;&#27861;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#33609;&#22270;&#21644;&#39044;&#22788;&#29702;(SAP)&#30340;&#38543;&#26426;&#21270;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#36827;&#34892;&#20102;&#26367;&#20195;&#27169;&#22411;&#33258;&#21160;&#35843;&#20248;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#22312;&#29616;&#20195;RandNLA&#20013;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20197;&#27604;&#38543;&#26426;&#25628;&#32034;&#23569;&#32422;4&#20493;&#30340;&#35797;&#39564;&#25104;&#26412;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be effective in handling high-dimensional computational problems, providing high-quality empirical performance as well as strong probabilistic guarantees. However, their practical application is complicated by the fact that the user needs to set various algorithm-specific tuning parameters which are different than those used in traditional NLA. This paper demonstrates how a surrogate-based autotuning approach can be used to address fundamental problems of parameter selection in RandNLA algorithms. In particular, we provide a detailed investigation of surrogate-based autotuning for sketch-and-precondition (SAP) based randomized least squares methods, which have been one of the great success stories in modern RandNLA. Empirical results show that our surrogate-based autotuning approach can achieve near-optimal performance with much less tuning cost than a random search (up to about 4x fewer trials of different para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#30340;Grad-CAM&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#30424;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#20998;&#26512;&#21457;&#29616;&#20102;&#27963;&#21160;&#21306;&#29305;&#24449;&#19982;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.15712</link><description>&lt;p&gt;
&#29992;&#25351;&#23548;&#30340;Grad-CAM&#35299;&#37322;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#29699;&#30424;&#38754;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Deep Learning for Full-disk Solar Flare Prediction with Empirical Insights from Guided Grad-CAM Explanations. (arXiv:2308.15712v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#25351;&#23548;&#30340;Grad-CAM&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#30424;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#35780;&#20272;&#20998;&#26512;&#21457;&#29616;&#20102;&#27963;&#21160;&#21306;&#29305;&#24449;&#19982;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23637;&#31034;&#19968;&#20010;&#20840;&#30424;&#38754;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25512;&#36827;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#30740;&#31350;&#65292;&#24182;&#22312;&#20013;&#24515;&#65288;&#22312;&#177;70&#176;&#33539;&#22260;&#20869;&#65289;&#21644;&#36817;&#26049;&#65288;&#36229;&#36807;&#177;70&#176;&#33539;&#22260;&#22806;&#65289;&#20107;&#20214;&#19978;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#20107;&#21518;&#35299;&#37322;&#30340;&#23450;&#24615;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#20154;&#20026;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#32463;&#39564;&#24615;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#27599;&#23567;&#26102;&#30340;&#20840;&#30424;&#38754;&#30913;&#32447;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#22312;&#25509;&#19979;&#26469;&#30340;24&#23567;&#26102;&#39044;&#27979;&#31383;&#21475;&#20869;&#30340;M&#32423;&#22826;&#38451;&#32768;&#26001;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25351;&#23548;&#30340;&#26799;&#24230;&#26435;&#37325;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;Guided Grad-CAM&#65289;&#24402;&#22240;&#26041;&#27861;&#26469;&#35299;&#37322;&#25105;&#20204;&#27169;&#22411;&#30340;&#39044;&#27979;&#24182;&#35780;&#20272;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20840;&#30424;&#38754;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#19982;&#27963;&#21160;&#21306;&#29305;&#24449;&#30340;&#30456;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study progresses solar flare prediction research by presenting a full-disk deep-learning model to forecast $\geq$M-class solar flares and evaluating its efficacy on both central (within $\pm$70$^\circ$) and near-limb (beyond $\pm$70$^\circ$) events, showcasing qualitative assessment of post hoc explanations for the model's predictions, and providing empirical findings from human-centered quantitative assessments of these explanations. Our model is trained using hourly full-disk line-of-sight magnetogram images to predict $\geq$M-class solar flares within the subsequent 24-hour prediction window. Additionally, we apply the Guided Gradient-weighted Class Activation Mapping (Guided Grad-CAM) attribution method to interpret our model's predictions and evaluate the explanations. Our analysis unveils that full-disk solar flare predictions correspond with active region characteristics. The following points represent the most important findings of our study: (1) Our deep learning models a
&lt;/p&gt;</description></item><item><title>Speech Wikimedia&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;77&#31181;&#35821;&#35328;&#30340;&#22823;&#37327;&#38899;&#39057;&#21644;&#36716;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.15710</link><description>&lt;p&gt;
Speech Wikimedia&#65306;&#19968;&#31181;&#21253;&#25324;77&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia: A 77 Language Multilingual Speech Dataset. (arXiv:2308.15710v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15710
&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;77&#31181;&#35821;&#35328;&#30340;&#22823;&#37327;&#38899;&#39057;&#21644;&#36716;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Speech Wikimedia&#25968;&#25454;&#38598;&#26159;&#20174;&#32500;&#22522;&#23186;&#20307;&#20849;&#20139;&#36164;&#28304;&#20013;&#25552;&#21462;&#30340;&#24102;&#26377;&#36716;&#24405;&#30340;&#38899;&#39057;&#30340;&#20844;&#24320;&#21487;&#29992;&#32534;&#35793;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#22330;&#26223;&#21644;&#35828;&#35805;&#32773;&#30340;1780&#23567;&#26102;&#65288;195GB&#65289;&#30340;CC-BY-SA&#35768;&#21487;&#30340;&#36716;&#24405;&#35821;&#38899;&#65292;&#28085;&#30422;&#20102;77&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#27599;&#20010;&#38899;&#39057;&#25991;&#20214;&#37117;&#26377;&#19968;&#20010;&#25110;&#22810;&#20010;&#19981;&#21516;&#35821;&#35328;&#30340;&#36716;&#24405;&#65292;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#36866;&#29992;&#20110;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Speech Wikimedia Dataset is a publicly available compilation of audio with transcriptions extracted from Wikimedia Commons. It includes 1780 hours (195 GB) of CC-BY-SA licensed transcribed speech from a diverse set of scenarios and speakers, in 77 different languages. Each audio file has one or more transcriptions in different languages, making this dataset suitable for training speech recognition, speech translation, and machine translation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#25913;&#36827;&#26041;&#27861;TKNN-Shapley&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.15709</link><description>&lt;p&gt;
&#38408;&#20540;KNN-Shapley&#65306;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#21644;&#38544;&#31169;&#21451;&#22909;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation. (arXiv:2308.15709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#21451;&#22909;&#30340;&#25913;&#36827;&#26041;&#27861;TKNN-Shapley&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#33021;&#22815;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26159;&#25968;&#25454;&#20013;&#24515;&#21270;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#26088;&#22312;&#37327;&#21270;&#21333;&#20010;&#25968;&#25454;&#28304;&#22312;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38754;&#20020;&#30528;&#24456;&#22810;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#38544;&#31169;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#30446;&#21069;&#26368;&#23454;&#29992;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#20043;&#19968;KNN-Shapley&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;KNN-Shapley&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;KNN-Shapley&#25913;&#36827;&#20197;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;(DP)&#30340;&#26174;&#33879;&#25216;&#26415;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TKNN-Shapley&#65292;KNN-Shapley&#30340;&#19968;&#31181;&#25913;&#36827;&#21464;&#20307;&#65292;&#20855;&#26377;&#38544;&#31169;&#21451;&#22909;&#24615;&#65292;&#21487;&#20197;&#36827;&#34892;&#31616;&#21333;&#30340;&#20462;&#27491;&#20197;&#21253;&#21547;DP&#20445;&#35777;&#65288;DP-TKNN-Shapley&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;DP-TKNN-Shapley&#22312;&#36776;&#21035;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#24182;&#22312;&#38544;&#31169;-&#23454;&#29992;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#26420;&#32032;&#21270;&#30340;KNN-Shapley&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#38750;&#38544;&#31169;&#30340;TKNN-Shapley&#20063;&#33021;&#20197;&#32447;&#24615;&#26102;&#38388;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation, a critical aspect of data-centric ML research, aims to quantify the usefulness of individual data sources in training machine learning (ML) models. However, data valuation faces significant yet frequently overlooked privacy challenges despite its importance. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical difficulties in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. Moreover, even non-private TKNN-Shapley ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#30456;&#20851;&#23450;&#29702;&#65292;&#25552;&#21319;&#20102;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#19982;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15704</link><description>&lt;p&gt;
&#23545;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards a Rigorous Analysis of Mutual Information in Contrastive Learning. (arXiv:2308.15704v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#20114;&#20449;&#24687;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#30456;&#20851;&#23450;&#29702;&#65292;&#25552;&#21319;&#20102;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#19982;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;&#20854;&#20027;&#35201;&#33539;&#24335;&#28041;&#21450;&#19968;&#20010;&#20114;&#20449;&#24687;&#25439;&#22833;&#30340;&#23454;&#20363;&#21306;&#20998;&#20219;&#21153;&#12290;&#36825;&#31181;&#25439;&#22833;&#34987;&#31216;&#20026;InfoNCE&#65292;&#36890;&#36807;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#20114;&#20449;&#24687;&#30340;&#20272;&#35745;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23548;&#33268;&#20854;&#25968;&#23398;&#22522;&#30784;&#30340;&#20248;&#38597;&#19982;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#20174;&#20114;&#20449;&#24687;&#20998;&#26512;&#20013;&#24471;&#20986;&#20005;&#26684;&#30340;&#35265;&#35299;&#25110;&#32467;&#35770;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#26041;&#27861;&#21644;&#19968;&#20123;&#30456;&#20851;&#23450;&#29702;&#65292;&#26088;&#22312;&#22686;&#24378;&#20114;&#20449;&#24687;&#20998;&#26512;&#30340;&#20005;&#35880;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#31616;&#21333;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#19977;&#20010;&#23545;&#27604;&#23398;&#20064;&#20998;&#26512;&#23454;&#20363;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20419;&#36827;&#26356;&#28145;&#20837;&#29702;&#35299;&#25110;&#32416;&#27491;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has emerged as a cornerstone in recent achievements of unsupervised representation learning. Its primary paradigm involves an instance discrimination task with a mutual information loss. The loss is known as InfoNCE and it has yielded vital insights into contrastive learning through the lens of mutual information analysis. However, the estimation of mutual information can prove challenging, creating a gap between the elegance of its mathematical foundation and the complexity of its estimation. As a result, drawing rigorous insights or conclusions from mutual information analysis becomes intricate. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating their capacity to facilitate deeper comprehension or to rectif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fragment and Integrate Network (FIN)&#30340;&#26032;&#22411;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#39135;&#21697;&#35746;&#36141;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#39034;&#24207;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#20010;&#23376;&#24207;&#21015;&#65292;&#20998;&#21035;&#23545;&#27599;&#20010;&#23376;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25429;&#25417;&#29992;&#25143;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.15703</link><description>&lt;p&gt;
&#22522;&#20110;&#38271;&#26399;&#39034;&#24207;&#34892;&#20026;&#30340;&#22312;&#32447;&#39135;&#21697;&#35746;&#36141;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26032;&#22411;&#26102;&#31354;&#24314;&#27169;&#26041;&#27861;-Fragment and Integrate Network (FIN)
&lt;/p&gt;
&lt;p&gt;
Fragment and Integrate Network (FIN): A Novel Spatial-Temporal Modeling Based on Long Sequential Behavior for Online Food Ordering Click-Through Rate Prediction. (arXiv:2308.15703v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fragment and Integrate Network (FIN)&#30340;&#26032;&#22411;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#39135;&#21697;&#35746;&#36141;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#39034;&#24207;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#20010;&#23376;&#24207;&#21015;&#65292;&#20998;&#21035;&#23545;&#27599;&#20010;&#23376;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25429;&#25417;&#29992;&#25143;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#24050;&#34987;&#35777;&#26126;&#22312;&#22312;&#32447;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#65288;LBS&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#20027;&#27969;&#30340;&#39135;&#21697;&#35746;&#36141;&#24179;&#21488;&#19978;&#65292;&#22914;DoorDash&#12289;Uber Eats&#12289;&#32654;&#22242;&#21644;&#39295;&#20102;&#20040;&#12290;&#36890;&#36807;&#39034;&#24207;&#34892;&#20026;&#25968;&#25454;&#24314;&#27169;&#29992;&#25143;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20559;&#22909;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#32570;&#20047;&#23545;&#20016;&#23500;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;&#25110;&#32773;&#20165;&#22788;&#29702;&#38271;&#24230;&#26377;&#38480;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20363;&#22914;100&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Fragment and Integrate Network (FIN)&#30340;&#26032;&#22411;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;FIN&#21253;&#25324;&#20004;&#20010;&#32593;&#32476;&#65306;&#29255;&#27573;&#32593;&#32476;&#65288;FN&#65289;&#20174;&#32456;&#36523;&#39034;&#24207;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#22810;&#20010;&#23376;&#24207;&#21015;&#65288;MSS&#65289;&#65292;&#36890;&#36807;&#20998;&#21035;&#23545;&#27599;&#20010;MSS&#36827;&#34892;&#24314;&#27169;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#31354;&#38388;-&#26102;&#38388;&#34920;&#31034;&#12290;&#36825;&#37324;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22797;&#26434;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal information has been proven to be of great significance for click-through rate prediction tasks in online Location-Based Services (LBS), especially in mainstream food ordering platforms such as DoorDash, Uber Eats, Meituan, and Ele.me. Modeling user spatial-temporal preferences with sequential behavior data has become a hot topic in recommendation systems and online advertising. However, most of existing methods either lack the representation of rich spatial-temporal information or only handle user behaviors with limited length, e.g. 100. In this paper, we tackle these problems by designing a new spatial-temporal modeling paradigm named Fragment and Integrate Network (FIN). FIN consists of two networks: (i) Fragment Network (FN) extracts Multiple Sub-Sequences (MSS) from lifelong sequential behavior data, and captures the specific spatial-temporal representation by modeling each MSS respectively. Here both a simplified attention and a complicated attention are adopted 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#23454;&#39564;&#65292;&#21457;&#29616;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;&#22521;&#35757;&#21487;&#20197;&#25903;&#25345;&#20154;&#20204;&#30340;&#25209;&#21028;&#20351;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15700</link><description>&lt;p&gt;
&#35757;&#32451;&#26397;&#21521;&#25209;&#21028;&#20351;&#29992;&#65306;&#23398;&#20064;&#23558;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#32622;&#20110;&#20154;&#31867;&#30693;&#35782;&#20043;&#20013;
&lt;/p&gt;
&lt;p&gt;
Training Towards Critical Use: Learning to Situate AI Predictions Relative to Human Knowledge. (arXiv:2308.15700v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#30740;&#31350;&#36890;&#36807;&#22312;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#23454;&#39564;&#65292;&#21457;&#29616;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;&#22521;&#35757;&#21487;&#20197;&#25903;&#25345;&#20154;&#20204;&#30340;&#25209;&#21028;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#25903;&#25345;&#20154;&#20204;&#26356;&#22909;&#22320;&#21033;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21253;&#25324;&#36890;&#36807;&#22521;&#35757;&#21644;&#20837;&#38376;&#25351;&#23548;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20915;&#31574;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#27604;&#36739;&#27599;&#20010;&#20915;&#31574;&#19982;&#29616;&#23454;&#26631;&#31614;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#8220;&#36866;&#24403;&#30340;&#20381;&#36182;&#8221;&#65292;&#36825;&#20123;&#26631;&#31614;&#28165;&#26224;&#22320;&#26144;&#23556;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#39044;&#27979;&#30446;&#26631;&#21644;&#20154;&#31867;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#34987;&#37096;&#32626;&#22312;&#31038;&#20250;&#24037;&#20316;&#12289;&#21009;&#20107;&#21496;&#27861;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36807;&#31243;&#23548;&#21521;&#30340;&#36866;&#24403;&#20381;&#36182;&#27010;&#24565;&#65292;&#31216;&#20026;&#25209;&#21028;&#20351;&#29992;&#65292;&#20854;&#23558;&#20154;&#31867;&#33021;&#22815;&#23558;&#20154;&#24037;&#26234;&#33021;&#39044;&#27979;&#32622;&#20110;&#20182;&#20204;&#29420;&#29305;&#30340;&#30693;&#35782;&#20043;&#20013;&#65292;&#32780;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#33719;&#24471;&#30340;&#12290;&#20026;&#20102;&#25506;&#32034;&#35757;&#32451;&#22914;&#20309;&#25903;&#25345;&#25209;&#21028;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22797;&#26434;&#30340;&#31038;&#20250;&#20915;&#31574;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#38543;&#26426;&#22312;&#32447;&#23454;&#39564;&#65306;&#20799;&#31461;&#34384;&#24453;&#31579;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
A growing body of research has explored how to support humans in making better use of AI-based decision support, including via training and onboarding. Existing research has focused on decision-making tasks where it is possible to evaluate "appropriate reliance" by comparing each decision against a ground truth label that cleanly maps to both the AI's predictive target and the human decision-maker's goals. However, this assumption does not hold in many real-world settings where AI tools are deployed today (e.g., social work, criminal justice, and healthcare). In this paper, we introduce a process-oriented notion of appropriate reliance called critical use that centers the human's ability to situate AI predictions against knowledge that is uniquely available to them but unavailable to the AI model. To explore how training can support critical use, we conduct a randomized online experiment in a complex social decision-making setting: child maltreatment screening. We find that, by providi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36870;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#21106;&#20855;&#26377;&#26426;&#26800;&#24322;&#36136;&#24615;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.15697</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#23545;&#26426;&#26800;&#24322;&#36136;&#39046;&#22495;&#36827;&#34892;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segmenting mechanically heterogeneous domains via unsupervised learning. (arXiv:2308.15697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36870;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#21106;&#20855;&#26377;&#26426;&#26800;&#24322;&#36136;&#24615;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#22120;&#23448;&#21040;&#26580;&#24615;&#26426;&#22120;&#20154;&#65292;&#39640;&#21464;&#24418;&#26448;&#26009;&#26159;&#33258;&#28982;&#21644;&#24037;&#31243;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#39640;&#21464;&#24418;&#26448;&#26009;&#21487;&#20197;&#20855;&#26377;&#24322;&#36136;&#30340;&#26448;&#26009;&#29305;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#25110;&#26080;&#28508;&#22312;&#26448;&#26009;&#24322;&#36136;&#24615;&#30340;&#24773;&#20917;&#19979;&#32463;&#21382;&#24322;&#36136;&#30340;&#21464;&#24418;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#29702;&#35299;&#21644;&#39044;&#27979;&#26448;&#26009;&#24322;&#36136;&#24615;&#30340;&#21518;&#26524;&#65292;&#24182;&#35299;&#37322;&#35266;&#27979;&#21040;&#30340;&#24322;&#36136;&#24212;&#21464;&#22330;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#24320;&#21457;&#36870;&#20998;&#26512;&#26041;&#27861;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#21487;&#20197;&#23558;&#35266;&#27979;&#21040;&#30340;&#36816;&#21160;&#23398;&#37327;&#65288;&#20363;&#22914;&#20301;&#31227;&#12289;&#24212;&#21464;&#65289;&#36716;&#21270;&#20026;&#26448;&#26009;&#29305;&#24615;&#21644;&#21147;&#23398;&#29366;&#24577;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#65292;&#24448;&#24448;&#20381;&#36182;&#20110;&#23545;&#36793;&#30028;&#26465;&#20214;&#30340;&#20005;&#26684;&#25511;&#21046;&#21644;&#20102;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26222;&#21450;&#24615;&#65292;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
From biological organs to soft robotics, highly deformable materials are essential components of natural and engineered systems. These highly deformable materials can have heterogeneous material properties, and can experience heterogeneous deformations with or without underlying material heterogeneity. Many recent works have established that computational modeling approaches are well suited for understanding and predicting the consequences of material heterogeneity and for interpreting observed heterogeneous strain fields. In particular, there has been significant work towards developing inverse analysis approaches that can convert observed kinematic quantities (e.g., displacement, strain) to material properties and mechanical state. Despite the success of these approaches, they are not necessarily generalizable and often rely on tight control and knowledge of boundary conditions. Here, we will build on the recent advances (and ubiquity) of machine learning approaches to explore altern
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15690</link><description>&lt;p&gt;
CongNaMul: &#19968;&#31181;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CongNaMul&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22823;&#35910;&#33469;&#22270;&#20687;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;CongNaMul&#25968;&#25454;&#38598;&#26088;&#22312;&#20419;&#36827;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#20197;&#21450;&#38271;&#24230;&#21644;&#37325;&#37327;&#30340;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22235;&#20010;&#31867;&#21035;&#26469;&#30830;&#23450;&#22823;&#35910;&#33469;&#30340;&#36136;&#37327;&#65306;&#27491;&#24120;&#12289;&#26029;&#35010;&#12289;&#26001;&#28857;&#21644;&#26029;&#35010;&#21644;&#26001;&#28857;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#33258;&#21160;&#36136;&#37327;&#26816;&#27979;&#25216;&#26415;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#22270;&#20687;&#65292;&#20174;&#21333;&#20010;&#33469;&#22270;&#20687;&#21040;&#20855;&#26377;&#22810;&#20010;&#33469;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#35760;&#30340;&#25513;&#33180;&#22270;&#20687;&#12290;&#26631;&#31614;&#21253;&#25324;4&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32972;&#26223;&#12289;&#22836;&#37096;&#12289;&#36523;&#20307;&#21644;&#23614;&#37096;&#12290;&#25968;&#25454;&#38598;&#36824;&#20026;&#22270;&#20687;&#20998;&#35299;&#20219;&#21153;&#25552;&#20379;&#20102;&#22270;&#20687;&#21644;&#25513;&#33180;&#65292;&#21253;&#25324;&#20004;&#20010;&#20998;&#31163;&#30340;&#33469;&#22270;&#20687;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#33469;&#30340;5&#20010;&#29289;&#29702;&#29305;&#24449;&#65288;&#22836;&#37096;&#38271;&#24230;&#12289;&#36523;&#20307;&#38271;&#24230;&#12289;&#36523;&#20307;&#21402;&#24230;&#12289;&#23614;&#37096;&#38271;&#24230;&#12289;&#37325;&#37327;&#65289;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
&lt;/p&gt;</description></item><item><title>MDTD&#26159;&#19968;&#31181;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#35302;&#21457;&#22120;&#23884;&#20837;&#31574;&#30053;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#12290;&#23427;&#21033;&#29992;&#20102;&#36755;&#20837;&#26679;&#26412;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26469;&#26816;&#27979;&#26408;&#39532;&#35302;&#21457;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.15673</link><description>&lt;p&gt;
MDTD: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
MDTD: A Multi Domain Trojan Detector for Deep Neural Networks. (arXiv:2308.15673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15673
&lt;/p&gt;
&lt;p&gt;
MDTD&#26159;&#19968;&#31181;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#35302;&#21457;&#22120;&#23884;&#20837;&#31574;&#30053;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#12290;&#23427;&#21033;&#29992;&#20102;&#36755;&#20837;&#26679;&#26412;&#19982;&#20915;&#31574;&#36793;&#30028;&#30340;&#36317;&#31163;&#26469;&#26816;&#27979;&#26408;&#39532;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#22312;&#19968;&#23567;&#37096;&#20998;&#36755;&#20837;&#26679;&#26412;&#20013;&#23884;&#20837;&#19968;&#20010;&#39044;&#23450;&#20041;&#30340;&#24178;&#25200;&#29289;&#65292;&#31216;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#35757;&#32451;DNN&#65292;&#20351;&#24471;&#36755;&#20837;&#20013;&#23384;&#22312;&#35302;&#21457;&#22120;&#23548;&#33268;&#36755;&#20986;&#20026;&#25915;&#20987;&#32773;&#25152;&#26399;&#26395;&#30340;&#31867;&#21035;&#12290;&#27492;&#31867;&#23545;&#25239;&#24615;&#37325;&#35757;&#32451;&#38656;&#35201;&#30830;&#20445;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#36755;&#20986;&#19981;&#21463;&#24433;&#21709;&#65292;&#24182;&#23545;&#24178;&#20928;&#26679;&#26412;&#25552;&#20379;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MDTD&#65292;&#19968;&#31181;&#38024;&#23545;DNN&#30340;&#22810;&#39046;&#22495;&#26408;&#39532;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#12290;MDTD&#19981;&#38656;&#35201;&#30693;&#36947;&#25915;&#20987;&#32773;&#23884;&#20837;&#35302;&#21457;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#22522;&#20110;&#22270;&#30340;&#36755;&#20837;&#12290;MDTD&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#21363;&#21253;&#21547;&#26408;&#39532;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#26679;&#26412;&#30456;&#23545;&#20110;&#24178;&#20928;&#26679;&#26412;&#20301;&#20110;&#20915;&#31574;&#36793;&#30028;&#20043;&#22806;&#36739;&#36828;&#30340;&#20301;&#32622;&#12290;MDTD&#20272;&#35745;&#20102;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models that use deep neural networks (DNNs) are vulnerable to backdoor attacks. An adversary carrying out a backdoor attack embeds a predefined perturbation called a trigger into a small subset of input samples and trains the DNN such that the presence of the trigger in the input results in an adversary-desired output class. Such adversarial retraining however needs to ensure that outputs for inputs without the trigger remain unaffected and provide high classification accuracy on clean samples. In this paper, we propose MDTD, a Multi-Domain Trojan Detector for DNNs, which detects inputs containing a Trojan trigger at testing time. MDTD does not require knowledge of trigger-embedding strategy of the attacker and can be applied to a pre-trained DNN model with image, audio, or graph-based inputs. MDTD leverages an insight that input samples containing a Trojan trigger are located relatively farther away from a decision boundary than clean samples. MDTD estimates the dista
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#32534;&#30721;&#22270;&#20687;&#21040;&#39640;&#32500;&#28508;&#31354;&#38388;&#24182;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#20998;&#24067;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#39640;&#32500;&#31354;&#38388;&#32473;&#32858;&#31867;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15667</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#23558;&#20998;&#24067;&#23398;&#20064;&#19982;&#22270;&#20687;&#32858;&#31867;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bridging Distribution Learning and Image Clustering in High-dimensional Space. (arXiv:2308.15667v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#32534;&#30721;&#22270;&#20687;&#21040;&#39640;&#32500;&#28508;&#31354;&#38388;&#24182;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#20998;&#24067;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#39640;&#32500;&#31354;&#38388;&#32473;&#32858;&#31867;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#23398;&#20064;&#20851;&#27880;&#20110;&#20174;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#32858;&#31867;&#26088;&#22312;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#30456;&#20284;&#30340;&#23545;&#35937;&#20998;&#32452;&#22312;&#19968;&#36215;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#34987;&#35748;&#20026;&#26159;&#26080;&#20851;&#30340;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#23384;&#22312;&#38388;&#25509;&#30340;&#30456;&#20851;&#24615;&#65292;&#20854;&#20013;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#36215;&#21040;&#20102;&#26725;&#26753;&#20316;&#29992;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#25506;&#32034;&#20998;&#24067;&#23398;&#20064;&#19982;&#32858;&#31867;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#23558;&#22270;&#20687;&#32534;&#30721;&#25104;&#39640;&#32500;&#28508;&#31354;&#38388;&#65292;&#20197;&#22635;&#34917;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#36793;&#32536;&#21270;&#65288;MCMarg&#65289;&#21644;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#25439;&#22833;&#26469;&#25311;&#21512;GMM&#30340;&#39640;&#26031;&#20998;&#37327;&#21644;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;GMM&#30340;&#27599;&#20010;&#39640;&#26031;&#20998;&#37327;&#23454;&#29616;&#22270;&#20687;&#32858;&#31867;&#12290;&#28982;&#32780;&#65292;&#8220;&#32500;&#25968;&#28798;&#38590;&#8221;&#32473;&#22823;&#22810;&#25968;&#32858;&#31867;&#31639;&#27861;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#19982;&#32463;&#20856;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#22270;&#20687;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution learning focuses on learning the probability density function from a set of data samples. In contrast, clustering aims to group similar objects together in an unsupervised manner. Usually, these two tasks are considered unrelated. However, the relationship between the two may be indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge. In this paper, we focus on exploring the correlation between distribution learning and clustering, with the motivation to fill the gap between these two fields, utilizing an autoencoder (AE) to encode images into a high-dimensional latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler (KL) divergence loss are used to fit the Gaussian components of the GMM and learn the data distribution. Finally, image clustering is achieved through each Gaussian component of GMM. Yet, the "curse of dimensionality" poses severe challenges for most clustering algorithms. Compared with the classic Expectation-Maximiz
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#26399;&#38388;&#36710;&#36742;&#25490;&#38431;&#23548;&#33268;&#30340;&#34892;&#36710;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.15656</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Based Framework for Mobile Energy Disseminator Dispatching to Charge On-the-Road Electric Vehicles. (arXiv:2308.15656v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#19978;&#20026;&#30005;&#21160;&#36710;&#20805;&#30005;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#26399;&#38388;&#36710;&#36742;&#25490;&#38431;&#23548;&#33268;&#30340;&#34892;&#36710;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#36710;&#30340;&#24555;&#36895;&#22686;&#38271;&#32473;&#20445;&#25252;&#30005;&#27744;&#20581;&#24247;&#21644;&#35299;&#20915;&#36710;&#36742;&#32493;&#33322;&#28966;&#34385;&#38382;&#39064;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#31227;&#21160;&#33021;&#37327;&#20256;&#25773;&#22120;&#65288;MEDs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;MED&#23433;&#35013;&#22312;&#22823;&#22411;&#36710;&#36742;&#21518;&#26041;&#65292;&#24182;&#22312;&#23427;&#30340;&#19978;&#28216;&#21322;&#24452;&#33539;&#22260;&#20869;&#20026;&#25152;&#26377;&#21442;&#19982;&#30340;&#30005;&#21160;&#36710;&#20805;&#30005;&#12290;&#28982;&#32780;&#65292;V2V&#20805;&#30005;&#26399;&#38388;&#65292;MED&#21644;&#30005;&#21160;&#36710;&#24847;&#22806;&#22320;&#24418;&#25104;&#36710;&#38431;&#65292;&#20174;&#32780;&#21344;&#25454;&#20102;&#22810;&#26465;&#36710;&#36947;&#65292;&#24182;&#20005;&#37325;&#24433;&#21709;&#20102;&#25972;&#20010;&#36890;&#36947;&#30340;&#34892;&#36710;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26377;&#38480;&#30340;MED&#37197;&#32622;&#39044;&#31639;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#30830;&#23450;&#22312;&#20132;&#36890;&#20013;&#24341;&#20837;MED&#30340;&#26368;&#20339;&#26102;&#38388;&#21644;&#20301;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#36710;&#36742;&#35843;&#24230;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of electric vehicles (EVs) presents novel challenges in preserving battery health and in addressing the persistent problem of vehicle range anxiety. To address these concerns, wireless charging, particularly, Mobile Energy Disseminators (MEDs) have emerged as a promising solution. The MED is mounted behind a large vehicle and charges all participating EVs within a radius upstream of it. Unfortuantely, during such V2V charging, the MED and EVs inadvertently form platoons, thereby occupying multiple lanes and impairing overall corridor travel efficiency. In addition, constrained budgets for MED deployment necessitate the development of an effective dispatching strategy to determine optimal timing and locations for introducing the MEDs into traffic. This paper proposes a deep reinforcement learning (DRL) based methodology to develop a vehicle dispatching framework. In the first component of the framework, we develop a realistic reinforcement learning environment ter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.15651</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#25512;&#33616;&#31995;&#32479;&#20013;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ensuring User-side Fairness in Dynamic Recommender Systems. (arXiv:2308.15651v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#20391;&#32676;&#20307;&#20844;&#24179;&#24615;&#23545;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#26088;&#22312;&#20943;&#36731;&#30001;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#24180;&#40836;&#65289;&#23450;&#20041;&#30340;&#29992;&#25143;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#24046;&#24322;&#24448;&#24448;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25345;&#32493;&#23384;&#22312;&#29978;&#33267;&#22686;&#21152;&#12290;&#36825;&#38656;&#35201;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#26377;&#25928;&#35299;&#20915;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#25506;&#35752;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#30830;&#20445;&#29992;&#25143;&#20391;&#20844;&#24179;&#24615;&#65288;&#21363;&#20943;&#23569;&#24615;&#33021;&#24046;&#24322;&#65289;&#30340;&#20856;&#22411;&#26041;&#27861;&#8212;&#8212;&#20844;&#24179;&#32422;&#26463;&#37325;&#26032;&#25490;&#21517;&#65292;&#22312;&#21160;&#24577;&#35774;&#23450;&#20013;&#38754;&#20020;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22522;&#20110;&#25490;&#21517;&#30340;&#20844;&#24179;&#32422;&#26463;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#33539;&#24335;&#65307;&#65288;2&#65289;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#65292;&#38459;&#30861;&#20102;&#23545;&#29992;&#25143;&#20559;&#22909;&#21464;&#21270;&#30340;&#24555;&#36895;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FADE&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#21160;&#24577;&#20943;&#36731;&#24615;&#33021;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;FADE&#25552;&#20986;&#20102;&#19968;&#31181; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-side group fairness is crucial for modern recommender systems, as it aims to alleviate performance disparity between groups of users defined by sensitive attributes such as gender, race, or age. We find that the disparity tends to persist or even increase over time. This calls for effective ways to address user-side fairness in a dynamic environment, which has been infrequently explored in the literature. However, fairness-constrained re-ranking, a typical method to ensure user-side fairness (i.e., reducing performance disparity), faces two fundamental challenges in the dynamic setting: (1) non-differentiability of the ranking-based fairness constraint, which hinders the end-to-end training paradigm, and (2) time-inefficiency, which impedes quick adaptation to changes in user preferences. In this paper, we propose FAir Dynamic rEcommender (FADE), an end-to-end framework with fine-tuning strategy to dynamically alleviate performance disparity. To tackle the above challenges, FADE u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#24615;&#22238;&#39038;&#20027;&#35201;&#26041;&#27861;&#65292;&#23558;&#22522;&#26412;&#27010;&#24565;&#25552;&#28860;&#20986;&#26469;&#20197;&#25903;&#25345;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15647</link><description>&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Recipe for Automated Machine Learning in Practice. (arXiv:2308.15647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#24615;&#22238;&#39038;&#20027;&#35201;&#26041;&#27861;&#65292;&#23558;&#22522;&#26412;&#27010;&#24565;&#25552;&#28860;&#20986;&#26469;&#20197;&#25903;&#25345;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26159;&#19968;&#39033;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#33021;&#22815;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#22320;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24819;&#27861;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#23454;&#36341;&#25552;&#20379;&#20102;&#24040;&#22823;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#22914;&#20309;&#35774;&#35745;AutoML&#31995;&#32479;&#30340;&#20449;&#24687;&#38750;&#24120;&#26377;&#38480;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#20248;&#21270;&#31639;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#32780;&#24573;&#30053;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#32454;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;&#36890;&#29992;AutoML&#31995;&#32479;&#30340;&#21442;&#32771;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#35813;&#39046;&#22495;&#20027;&#35201;&#26041;&#27861;&#30340;&#21465;&#20107;&#24615;&#22238;&#39038;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#25552;&#28860;&#22522;&#26412;&#27010;&#24565;&#65292;&#20197;&#25903;&#25345;&#23427;&#20204;&#22312;&#21333;&#19968;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#19982;AutoML&#24212;&#29992;&#30456;&#20851;&#30340;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning (AutoML) is an area of research that focuses on developing methods to generate machine learning models automatically. The idea of being able to build machine learning models with very little human intervention represents a great opportunity for the practice of applied machine learning. However, there is very little information on how to design an AutoML system in practice. Most of the research focuses on the problems facing optimization algorithms and leaves out the details of how that would be done in practice. In this paper, we propose a frame of reference for building general AutoML systems. Through a narrative review of the main approaches in the area, our main idea is to distill the fundamental concepts in order to support them in a single design. Finally, we discuss some open problems related to the application of AutoML for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#36827;&#34892;&#22270;&#32858;&#31867;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#26080;&#35770;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15642</link><description>&lt;p&gt;
&#26080;&#38656;&#29305;&#24449;&#38388;&#38548;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Clustering Without an Eigengap. (arXiv:2308.15642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#36827;&#34892;&#22270;&#32858;&#31867;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#26080;&#35770;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#65292;&#24182;&#19988;&#23545;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#25552;&#20986;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#20013;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#32858;&#31867;&#21644;&#23567;&#19981;&#21487;&#24674;&#22797;&#32858;&#31867;&#30340;&#22270;&#32858;&#31867;&#38382;&#39064;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#19981;&#20801;&#35768;&#23567;&#20110;$ o&#65288;\sqrt {n}&#65289;$&#22823;&#23567;&#30340;&#23567;&#32858;&#31867;&#65292;&#35201;&#20040;&#35201;&#27714;&#26368;&#23567;&#21487;&#24674;&#22797;&#32858;&#31867;&#21644;&#26368;&#22823;&#19981;&#21487;&#24674;&#22797;&#32858;&#31867;&#20043;&#38388;&#23384;&#22312;&#22823;&#23567;&#38388;&#38548;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#30340;&#31639;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#36825;&#20123;&#35201;&#27714;&#65292;&#24182;&#21487;&#20197;&#30830;&#23450;&#22320;&#24674;&#22797;&#22823;&#32858;&#31867;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#20182;&#32858;&#31867;&#30340;&#22823;&#23567;&#12290;&#20013;&#31561;&#22823;&#23567;&#30340;&#32858;&#31867;&#23545;&#20998;&#26512;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#25509;&#36817;&#24674;&#22797;&#38408;&#20540;&#65292;&#38750;&#24120;&#25935;&#24863;&#20110;&#23567;&#30340;&#22122;&#22768;&#25200;&#21160;&#65292;&#19981;&#20801;&#35768;&#38381;&#21512;&#24418;&#24335;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;leave-one-out&#39118;&#26684;&#30340;&#35770;&#35777;&#65292;&#21363;&#20351;&#21435;&#25481;&#19968;&#34892;&#22122;&#22768;&#20063;&#21487;&#33021;&#22823;&#24133;&#25913;&#21464;SDP&#35299;&#20915;&#26041;&#26696;&#65292;&#20173;&#28982;&#21487;&#20197;&#25511;&#21046;SDP&#35299;&#20915;&#26041;&#26696;&#19982;&#22122;&#22768;&#21521;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.15640</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22797;&#26434;&#36229;&#24377;&#24615;&#22266;&#20307;&#30340;&#32452;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#29983;&#29289;&#26448;&#26009;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#21644;&#26426;&#26800;&#34892;&#20026;&#30340;&#26448;&#26009;&#20013;&#65292;&#35782;&#21035;&#32452;&#20998;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#36890;&#24120;&#20165;&#38480;&#20110;&#22522;&#26412;&#30340;&#32452;&#20998;&#23450;&#24459;&#65292;&#24182;&#22312;&#19982;&#23454;&#39564;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#36935;&#21040;&#23454;&#38469;&#32422;&#26463;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PINN&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#36719;&#26448;&#26009;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#21576;&#29616;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#12290;&#35813;&#27169;&#22411;&#24378;&#35843;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;PINN&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#22330;&#21464;&#24418;&#21644;&#21152;&#36733;&#21382;&#21490;&#65292;&#20197;&#30830;&#20445;&#31639;&#27861;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#20173;&#28982;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
&lt;/p&gt;</description></item><item><title>&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#29305;&#24615;&#12290;&#27973;&#23618;&#23884;&#20837;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.15639</link><description>&lt;p&gt;
&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Convolutional Neural Networks. (arXiv:2308.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15639
&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#29305;&#24615;&#12290;&#27973;&#23618;&#23884;&#20837;&#26500;&#24314;&#20102;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23613;&#31649;&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#23884;&#20837;&#31354;&#38388;&#19978;&#27809;&#26377;&#35774;&#23450;&#24402;&#32435;&#20559;&#32622;&#65292;&#21487;&#20197;&#35828;&#30456;&#24403;&#24188;&#31258;&#12290;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#21367;&#31215;&#32593;&#32476; - &#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#26469;&#23884;&#20837;&#25968;&#25454;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#24378;&#22823;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;&#21452;&#26354;&#31354;&#38388;&#12290;&#30001;&#20110;&#33021;&#22815;&#23558;&#26356;&#22810;&#30340;&#25968;&#25454;&#36866;&#24212;&#20110;&#20302;&#32500;&#31354;&#38388;&#24182;&#20855;&#26377;&#26641;&#29366;&#29305;&#24615;&#65292;&#21452;&#26354;&#31354;&#38388;&#23588;&#20854;&#26377;&#29992;&#12290;&#20808;&#21069;&#30340;&#22810;&#31687;&#35770;&#25991;&#24050;&#32463;&#34920;&#26126;&#65292;&#36825;&#20123;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#26377;&#21161;&#20110;&#20351;&#29992;&#27973;&#23618;&#23884;&#20837;&#26469;&#26500;&#24314;&#23618;&#27425;&#21270;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning is mostly responsible for the surge of interest in Artificial Intelligence in the last decade. So far, deep learning researchers have been particularly successful in the domain of image processing, where Convolutional Neural Networks are used. Although excelling at image classification, Convolutional Neural Networks are quite naive in that no inductive bias is set on the embedding space for images. Similar flaws are also exhibited by another type of Convolutional Networks - Graph Convolutional Neural Networks. However, using non-Euclidean space for embedding data might result in more robust and explainable models. One example of such a non-Euclidean space is hyperbolic space. Hyperbolic spaces are particularly useful due to their ability to fit more data in a low-dimensional space and tree-likeliness properties. These attractive properties have been previously used in multiple papers which indicated that they are beneficial for building hierarchical embeddings using shall
&lt;/p&gt;</description></item><item><title>RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2308.15618</link><description>&lt;p&gt;
RACR-MIL&#65306;&#20351;&#29992;&#22522;&#20110;&#25490;&#21517;&#24863;&#30693;&#32972;&#26223;&#25512;&#29702;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware Contextual Reasoning on Whole Slide Images. (arXiv:2308.15618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15618
&lt;/p&gt;
&lt;p&gt;
RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#24494;&#20998;&#22270;&#25915;&#20987;&#65288;DGA&#65289;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#36830;&#32493;&#26494;&#24347;&#21644;&#21442;&#25968;&#21270;&#22270;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#26377;&#25928;&#29983;&#25104;&#25915;&#20987;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20046;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#25104;&#26412;&#20943;&#23569;6&#20493;&#12290;</title><link>http://arxiv.org/abs/2308.15614</link><description>&lt;p&gt;
&#25152;&#26377;&#19996;&#35199;&#21516;&#26102;&#21463;&#25200;&#65306;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#22270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Everything Perturbed All at Once: Enabling Differentiable Graph Attacks. (arXiv:2308.15614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#24494;&#20998;&#22270;&#25915;&#20987;&#65288;DGA&#65289;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#36830;&#32493;&#26494;&#24347;&#21644;&#21442;&#25968;&#21270;&#22270;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#26377;&#25928;&#29983;&#25104;&#25915;&#20987;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20046;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#65292;&#20294;&#35757;&#32451;&#25104;&#26412;&#20943;&#23569;6&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#32593;&#32476;&#26381;&#21153;&#31561;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;GNN&#23545;&#20110;&#23545;&#25239;&#24615;&#25915;&#20987;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#36825;&#20250;&#26174;&#33879;&#38477;&#20302;&#20854;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#20511;&#21161;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#36873;&#21462;&#25915;&#20987;&#24471;&#20998;&#26368;&#39640;&#30340;&#21333;&#20010;&#36793;&#36827;&#34892;&#24178;&#25200;&#65292;&#30452;&#21040;&#36798;&#21040;&#39044;&#31639;&#38480;&#21046;&#12290;&#34429;&#28982;&#33021;&#22815;&#26377;&#25928;&#25214;&#20986;&#26131;&#21463;&#25915;&#20987;&#30340;&#36830;&#25509;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#36830;&#32493;&#26494;&#24347;&#21644;&#21442;&#25968;&#21270;&#22270;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21487;&#24494;&#20998;&#22270;&#25915;&#20987;&#65288;DGA&#65289;&#30340;&#26032;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#25915;&#20987;&#65292;&#24182;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;DGA&#22312;&#35757;&#32451;&#25104;&#26412;&#20943;&#23569;&#20102;6&#20493;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20960;&#20046;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As powerful tools for representation learning on graphs, graph neural networks (GNNs) have played an important role in applications including social networks, recommendation systems, and online web services. However, GNNs have been shown to be vulnerable to adversarial attacks, which can significantly degrade their effectiveness. Recent state-of-the-art approaches in adversarial attacks rely on gradient-based meta-learning to selectively perturb a single edge with the highest attack score until they reach the budget constraint. While effective in identifying vulnerable links, these methods are plagued by high computational costs. By leveraging continuous relaxation and parameterization of the graph structure, we propose a novel attack method called Differentiable Graph Attack (DGA) to efficiently generate effective attacks and meanwhile eliminate the need for costly retraining. Compared to the state-of-the-art, DGA achieves nearly equivalent attack performance with 6 times less trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2308.15613</link><description>&lt;p&gt;
&#28151;&#21512;&#26041;&#24046;&#27969;&#29992;&#20110;&#31163;&#25955;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Mixed Variational Flows for Discrete Variables. (arXiv:2308.15613v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#24046;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#31163;&#25955;&#19988;&#20445;&#25345;&#24230;&#37327;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#38656;&#35201;&#36830;&#32493;&#23884;&#20837;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#36830;&#32493;&#23884;&#20837;&#27969;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27969;&#20801;&#35768;&#20174;&#20107;&#32773;&#23398;&#20064;&#22797;&#26434;&#30340;&#36830;&#32493;&#20998;&#24067;&#65292;&#20294;&#26159;&#36817;&#20284;&#31163;&#25955;&#20998;&#24067;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#31163;&#25955;&#30446;&#26631;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#20013;-&#36890;&#24120;&#26159;&#36890;&#36807;&#36830;&#32493;&#26494;&#24347;&#25110;&#21435;&#37327;&#21270;-&#28982;&#21518;&#24212;&#29992;&#36830;&#32493;&#27969;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#19968;&#20010;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#21407;&#22987;&#31163;&#25955;&#30446;&#26631;&#30340;&#26367;&#20195;&#30446;&#26631;&#65292;&#21487;&#33021;&#20855;&#26377;&#20559;&#20506;&#25110;&#19981;&#31283;&#23450;&#30340;&#26799;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#21019;&#24314;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#20998;&#24067;&#30340;&#21464;&#20998;&#27969;&#26063;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#36830;&#32493;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20445;&#25345;&#24230;&#37327;&#30340;&#31163;&#25955;&#21487;&#36870;&#26144;&#23556;&#65292;&#20351;&#31163;&#25955;&#30446;&#26631;&#20445;&#25345;&#19981;&#21464;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#26144;&#23556;&#21019;&#24314;&#20102;&#19968;&#20010;&#28151;&#21512;&#21464;&#20998;&#27969;(MAD Mix)&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#32852;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MAD Mix&#20135;&#29983;&#20102;&#27604;&#36830;&#32493;&#23884;&#20837;&#27969;&#26356;&#21487;&#38752;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational flows allow practitioners to learn complex continuous distributions, but approximating discrete distributions remains a challenge. Current methodologies typically embed the discrete target in a continuous space - usually via continuous relaxation or dequantization - and then apply a continuous flow. These approaches involve a surrogate target that may not capture the original discrete target, might have biased or unstable gradients, and can create a difficult optimization problem. In this work, we develop a variational flow family for discrete distributions without any continuous embedding. First, we develop a measure-preserving and discrete (MAD) invertible map that leaves the discrete target invariant, and then create a mixed variational flow (MAD Mix) based on that map. We also develop an extension to MAD Mix that handles joint discrete and continuous models. Our experiments suggest that MAD Mix produces more reliable approximations than continuous-embedding flows while 
&lt;/p&gt;</description></item><item><title>InstaTune&#26159;&#19968;&#31181;&#22312;&#24494;&#35843;&#38454;&#27573;&#21363;&#26102;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.15609</link><description>&lt;p&gt;
InstaTune: &#22312;&#24494;&#35843;&#26399;&#38388;&#21363;&#26102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning. (arXiv:2308.15609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15609
&lt;/p&gt;
&lt;p&gt;
InstaTune&#26159;&#19968;&#31181;&#22312;&#24494;&#35843;&#38454;&#27573;&#21363;&#26102;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20026;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#35757;&#32451;&#30828;&#20214;&#26080;&#20851;&#30340;&#36229;&#32423;&#32593;&#32476;&#12290;&#28982;&#21518;&#20174;&#35757;&#32451;&#22909;&#30340;&#36229;&#32423;&#32593;&#32476;&#20013;&#25552;&#21462;&#20986;&#36866;&#29992;&#20110;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#30340;&#26368;&#20339;&#23376;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36229;&#32423;&#32593;&#32476;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#23610;&#23544;&#36739;&#22823;&#65292;&#26497;&#22823;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;InstaTune&#65292;&#19968;&#31181;&#21033;&#29992;&#21363;&#26102;&#24494;&#35843;&#38454;&#27573;&#29983;&#25104;&#36229;&#32423;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;InstaTune&#20855;&#26377;&#22810;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#30001;&#20110;&#35813;&#36807;&#31243;&#21457;&#29983;&#22312;&#24494;&#35843;&#26399;&#38388;&#65292;&#23427;&#21487;&#20197;&#26368;&#23567;&#21270;&#36827;&#34892;NAS&#25152;&#38656;&#30340;&#24635;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20854;&#27425;&#65292;&#25552;&#21462;&#20986;&#30340;&#23376;&#32593;&#32476;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-Shot Neural Architecture Search (NAS) algorithms often rely on training a hardware agnostic super-network for a domain specific task. Optimal sub-networks are then extracted from the trained super-network for different hardware platforms. However, training super-networks from scratch can be extremely time consuming and compute intensive especially for large models that rely on a two-stage training process of pre-training and fine-tuning. State of the art pre-trained models are available for a wide range of tasks, but their large sizes significantly limits their applicability on various hardware platforms. We propose InstaTune, a method that leverages off-the-shelf pre-trained weights for large models and generates a super-network during the fine-tuning stage. InstaTune has multiple benefits. Firstly, since the process happens during fine-tuning, it minimizes the overall time and compute resources required for NAS. Secondly, the sub-networks extracted are optimized for the target ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.15605</link><description>&lt;p&gt;
&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22235;&#20010;&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#27979;&#37327;&#32467;&#26524;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#23637;&#31034;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#23545;&#20248;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#35757;&#32451;&#20449;&#21495;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#27979;&#37327;&#31713;&#25913;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25805;&#32437;&#22810;&#20010;&#27979;&#37327;&#32467;&#26524;&#65292;&#20197;&#33829;&#36896;&#33391;&#22909;&#32467;&#26524;&#30340;&#20551;&#35937;&#65292;&#32780;&#19981;&#26159;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#27979;&#37327;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#19968;&#32452;&#25991;&#26412;&#36755;&#20837;&#21644;&#27979;&#37327;&#32467;&#26524;&#65292;&#26088;&#22312;&#30830;&#23450;&#26576;&#20010;&#32467;&#26524;&#26159;&#21542;&#21457;&#29983;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#27979;&#37327;&#32467;&#26524;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#25152;&#26377;&#27979;&#37327;&#32467;&#26524;&#37117;&#34920;&#26126;&#32467;&#26524;&#21457;&#29983;&#30340;&#31034;&#20363;&#26159;&#21542;&#30830;&#23454;&#21457;&#29983;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#36825;&#26159;&#30001;&#20110;&#27979;&#37327;&#31713;&#25913;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31616;&#21333;&#22522;&#20934;&#30340;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#22312;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#25105;&#20204;&#24863;&#21040;&#20852;&#22859;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20998;&#21306;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15602</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20998;&#21306;&#31574;&#30053;&#30340;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
An Experimental Comparison of Partitioning Strategies for Distributed Graph Neural Network Training. (arXiv:2308.15602v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#20998;&#21306;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20316;&#20026;&#19968;&#31181;&#33021;&#22815;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;GNN&#35757;&#32451;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#21487;&#33021;&#36229;&#36807;&#21333;&#21488;&#26426;&#22120;&#25110;GPU&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#25104;&#20026;&#22823;&#35268;&#27169;GNN&#35757;&#32451;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#20808;&#20915;&#26465;&#20214;&#26159;&#23558;&#36755;&#20837;&#22270;&#20998;&#21106;&#25104;&#36739;&#23567;&#30340;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20998;&#24067;&#22312;&#35745;&#31639;&#38598;&#32676;&#30340;&#22810;&#21488;&#26426;&#22120;&#38388;&#12290;&#34429;&#28982;&#22270;&#20998;&#21306;&#22312;&#22270;&#20998;&#26512;&#21644;&#22270;&#25968;&#25454;&#24211;&#26041;&#38754;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#23545;GNN&#35757;&#32451;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#21306;&#23545;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#19981;&#21516;&#22240;&#32032;&#65288;&#22914;GNN&#21442;&#25968;&#12289;&#23567;&#25209;&#37327;&#22823;&#23567;&#12289;&#22270;&#31867;&#22411;&#12289;&#29305;&#24449;&#22823;&#23567;&#21644;&#25193;&#23637;&#22240;&#23376;&#65289;&#23545;&#20998;&#21306;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs) have gained much attention as a growing area of deep learning capable of learning on graph-structured data. However, the computational and memory requirements for training GNNs on large-scale graphs can exceed the capabilities of single machines or GPUs, making distributed GNN training a promising direction for large-scale GNN training. A prerequisite for distributed GNN training is to partition the input graph into smaller parts that are distributed among multiple machines of a compute cluster. Although graph partitioning has been extensively studied with regard to graph analytics and graph databases, its effect on GNN training performance is largely unexplored.  In this paper, we study the effectiveness of graph partitioning for distributed GNN training. Our study aims to understand how different factors such as GNN parameters, mini-batch size, graph type, features size, and scale-out factor influence the effectiveness of graph partitioning. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15594</link><description>&lt;p&gt;
&#21464;&#24418;&#37329;&#21018;&#26159;&#21542;&#33021;&#23398;&#20250;&#26368;&#22823;&#20844;&#32422;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can transformers learn the greatest common divisor?. (arXiv:2308.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#26368;&#22823;&#20844;&#32422;&#25968;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26126;&#30830;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23567;&#22411;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#35745;&#31639;&#20004;&#20010;&#27491;&#25972;&#25968;&#30340;&#26368;&#22823;&#20844;&#32422;&#25968;&#65288;GCD&#65289;&#30340;&#33021;&#21147;&#12290;&#24403;&#35757;&#32451;&#20998;&#24067;&#21644;&#34920;&#31034;&#22522;&#20934;&#20180;&#32454;&#36873;&#25321;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;98%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#19988;&#27491;&#30830;&#39044;&#27979;&#21069;100&#20010;GCD&#20013;&#30340;91&#20010;&#12290;&#27169;&#22411;&#30340;&#39044;&#27979;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#23398;&#20250;&#23558;&#20855;&#26377;&#30456;&#21516;GCD&#30340;&#36755;&#20837;&#23545;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#20854;&#38500;&#25968;&#36827;&#34892;&#20998;&#31867;&#12290;&#22522;&#26412;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#22522;&#25968;&#32534;&#30721;&#30340;&#22343;&#21248;&#25805;&#20316;&#25968;&#20165;&#35745;&#31639;&#23569;&#25968;GCD&#65288;&#26368;&#22810;100&#20010;&#20013;&#30340;38&#20010;&#65289;&#65306;&#22522;&#25968;&#30340;&#38500;&#25968;&#20056;&#31215;&#12290;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#26356;&#22823;&#30340;&#22522;&#25968;&#20801;&#35768;&#19968;&#20123;&#27169;&#22411;&#8220;&#20102;&#35299;&#8221;&#23567;&#30340;&#32032;&#25968;GCD&#12290;&#20351;&#29992;&#23545;&#25968;&#22343;&#21248;&#25805;&#20316;&#25968;&#36827;&#34892;&#35757;&#32451;&#23558;&#24615;&#33021;&#25552;&#21319;&#21040;&#27491;&#30830;&#30340;73&#20010;GCD&#65292;&#24182;&#36890;&#36807;&#20174;&#20498;&#25968;&#24179;&#26041;&#21040;&#23545;&#25968;&#22343;&#21248;&#30340;GCD&#35757;&#32451;&#20998;&#24067;&#30340;&#24179;&#34913;&#65292;&#20351;&#24615;&#33021;&#36798;&#21040;91&#20010;GCD&#12290;&#20174;GCD&#30340;&#22343;&#21248;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#30772;&#22351;&#20102;&#30830;&#23450;&#24615;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21407;&#22411;&#20998;&#35010;&#65288;PF&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32454;&#31890;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#25366;&#25496;&#23558;&#31867;&#21035;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#21106;&#20026;&#32039;&#20945;&#30340;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26356;&#22909;&#22320;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.15575</link><description>&lt;p&gt;
&#21407;&#22411;&#20998;&#35010;&#65306;&#29992;&#20110;&#31283;&#20581;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23553;&#38381;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Prototype Fission: Closing Set for Robust Open-set Semi-supervised Learning. (arXiv:2308.15575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21407;&#22411;&#20998;&#35010;&#65288;PF&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#32454;&#31890;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#25366;&#25496;&#23558;&#31867;&#21035;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#21106;&#20026;&#32039;&#20945;&#30340;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26356;&#22909;&#22320;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#29616;&#23454;&#30340;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#25968;&#25454;&#38598;&#20013;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#22240;&#20026;&#36807;&#20110;&#33258;&#20449;&#22320;&#23558;OOD&#26679;&#26412;&#35823;&#21028;&#20026;&#30446;&#26631;&#20998;&#24067;&#65288;ID&#65289;&#26679;&#26412;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#30001;&#20110;SSL&#30340;&#33258;&#25105;&#35757;&#32451;&#24490;&#29615;&#23548;&#33268;&#20102;&#26469;&#33258;&#24050;&#30693;&#20998;&#24067;&#31354;&#38388;&#30340;&#28508;&#22312;&#31354;&#38388;&#25193;&#25955;&#21040;&#26410;&#30693;&#20998;&#24067;&#31354;&#38388;&#65292;&#19988;&#36825;&#31181;&#20559;&#24046;&#22312;SSL&#20013;&#20250;&#34987;&#25918;&#22823;&#12290;&#20026;&#20102;&#23553;&#38381;ID&#20998;&#24067;&#38598;&#21512;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#25298;&#32477;OOD&#26679;&#26412;&#20197;&#23454;&#29616;&#23433;&#20840;&#30340;SSL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#35010;&#65288;PF&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#32454;&#31890;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#25366;&#25496;&#23558;&#31867;&#21035;&#30340;&#28508;&#22312;&#31354;&#38388;&#20998;&#21106;&#20026;&#32039;&#20945;&#30340;&#23376;&#31354;&#38388;&#65292;&#20165;&#20381;&#36182;&#20110;&#31895;&#31890;&#24230;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#31867;&#21035;&#24418;&#25104;&#22810;&#20010;&#29420;&#29305;&#30340;&#21487;&#23398;&#20064;&#23376;&#31867;&#21035;&#21407;&#22411;&#65292;&#20248;&#21270;&#20854;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22810;&#26679;&#24615;&#24314;&#27169;&#39033;&#40723;&#21169;&#26679;&#26412;&#34987;&#32858;&#38598;&#21040;&#22810;&#20010;&#23376;&#31867;&#21035;&#21407;&#22411;&#20043;&#19968;&#65292;&#32780;&#19968;&#33268;&#24615;&#24314;&#27169;&#39033;&#23558;&#21516;&#19968;&#31867;&#21035;&#30340;&#25152;&#26377;&#26679;&#26412;&#32858;&#38598;&#21040;&#19968;&#20010;&#20840;&#23616;&#21407;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Learning (SSL) has been proven vulnerable to out-of-distribution (OOD) samples in realistic large-scale unsupervised datasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A key underlying problem is class-wise latent space spreading from closed seen space to open unseen space, and the bias is further magnified in SSL's self-training loops. To close the ID distribution set so that OODs are better rejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise latent spaces into compact sub-spaces by automatic fine-grained latent space mining, driven by coarse-grained labels only. Specifically, we form multiple unique learnable sub-class prototypes for each class, optimized towards both diversity and consistency. The Diversity Modeling term encourages samples to be clustered by one of the multiple sub-class prototypes, while the Consistency Modeling term clusters all samples of the same class to a global prototype. Instead of "opening s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;fMRI&#24207;&#21015;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36866;&#24212;&#945;-GAN&#32467;&#26500;&#21644;&#32858;&#21512;&#26102;&#24207;&#20449;&#24687;&#65292;&#21512;&#25104;&#30340;&#20219;&#21153;&#39537;&#21160;fMRI&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#25968;&#25454;&#22686;&#30410;&#65292;&#24182;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15564</link><description>&lt;p&gt;
&#22312;&#20219;&#21153;&#39537;&#21160;&#30340;fMRI&#20013;&#23398;&#20064;&#24207;&#21015;&#20449;&#24687;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
Learning Sequential Information in Task-based fMRI for Synthetic Data Augmentation. (arXiv:2308.15564v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;fMRI&#24207;&#21015;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36866;&#24212;&#945;-GAN&#32467;&#26500;&#21644;&#32858;&#21512;&#26102;&#24207;&#20449;&#24687;&#65292;&#21512;&#25104;&#30340;&#20219;&#21153;&#39537;&#21160;fMRI&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#20379;&#25968;&#25454;&#22686;&#30410;&#65292;&#24182;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26159;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20351;&#29992;&#29305;&#23450;&#35748;&#30693;&#20219;&#21153;&#33719;&#21462;&#30340;&#26102;&#31354;&#25104;&#20687;&#25968;&#25454;&#30340;&#20219;&#21153;&#39537;&#21160;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#22270;&#20687;(fMRI)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;fMRI&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#21019;&#24314;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#30340;&#20219;&#21153;&#29305;&#23450;fMRI&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#945;-GAN&#32467;&#26500;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32858;&#21512;&#26102;&#24207;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#21516;&#36873;&#25321;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20998;&#31867;&#20219;&#21153;&#23545;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#25104;&#30340;&#20219;&#21153;&#39537;&#21160;fMRI&#21487;&#20197;&#22312;&#23398;&#20064;ASD&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#20379;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insufficiency of training data is a persistent issue in medical image analysis, especially for task-based functional magnetic resonance images (fMRI) with spatio-temporal imaging data acquired using specific cognitive tasks. In this paper, we propose an approach for generating synthetic fMRI sequences that can then be used to create augmented training datasets in downstream learning tasks. To synthesize high-resolution task-specific fMRI, we adapt the $\alpha$-GAN structure, leveraging advantages of both GAN and variational autoencoder models, and propose different alternatives in aggregating temporal information. The synthetic images are evaluated from multiple perspectives including visualizations and an autism spectrum disorder (ASD) classification task. The results show that the synthetic task-based fMRI can provide effective data augmentation in learning the ASD classification task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#65288;&#20171;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#20043;&#38388;&#30340;&#35299;&#37322;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#29256;&#26412;&#30340;SHAP&#20540;&#21644;&#37096;&#20998;&#20381;&#36182;&#20989;&#25968;&#65292;&#21487;&#20197;&#23545;&#22242;&#38431;&#21644;&#29699;&#21592;&#27700;&#24179;&#36827;&#34892;&#32489;&#25928;&#20998;&#26512;&#21644;&#30693;&#35782;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2308.15559</link><description>&lt;p&gt;
&#36275;&#29699;&#20013;&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Glocal Explanations of Expected Goal Models in Soccer. (arXiv:2308.15559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#65288;&#20171;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#20043;&#38388;&#30340;&#35299;&#37322;&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#29256;&#26412;&#30340;SHAP&#20540;&#21644;&#37096;&#20998;&#20381;&#36182;&#20989;&#25968;&#65292;&#21487;&#20197;&#23545;&#22242;&#38431;&#21644;&#29699;&#21592;&#27700;&#24179;&#36827;&#34892;&#32489;&#25928;&#20998;&#26512;&#21644;&#30693;&#35782;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#36890;&#24120;&#26377;&#38480;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#40657;&#30418;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#12290;&#38543;&#30528;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#20986;&#29616;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#20174;&#21333;&#20010;&#35266;&#23519;&#25110;&#25152;&#26377;&#35266;&#23519;&#20013;&#25552;&#21462;&#25551;&#36848;&#24615;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#35299;&#37322;&#29305;&#23450;&#32676;&#20307;&#35266;&#23519;&#30340;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#26356;&#26377;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#20351;&#29992;SHAP&#20540;&#21644;&#37096;&#20998;&#20381;&#36182;&#20989;&#25968;&#30340;&#32858;&#21512;&#29256;&#26412;&#65292;&#24341;&#20837;&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#30340;&#20840;&#23616;&#35299;&#37322;&#65288;&#20171;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#20043;&#38388;&#30340;&#35299;&#37322;&#65289;&#26469;&#23454;&#29616;&#23545;&#22242;&#38431;&#21644;&#29699;&#21592;&#27700;&#24179;&#30340;&#32489;&#25928;&#20998;&#26512;&#12290;&#36825;&#26679;&#21487;&#20197;&#20174;&#39044;&#26399;&#36827;&#29699;&#27169;&#22411;&#20013;&#25552;&#21462;&#19982;&#29699;&#21592;&#25110;&#29699;&#38431;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#23556;&#38376;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#26469;&#35828;&#26126;&#32858;&#21512;SHAP&#20540;&#21644;&#32858;&#21512;&#20989;&#25968;&#30340;&#26377;&#29992;&#24615;&#12290;&#26412;&#25991;&#26368;&#21518;&#23545;&#36825;&#20123;&#35299;&#37322;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected goal models have gained popularity, but their interpretability is often limited, especially when trained using black-box methods. Explainable artificial intelligence tools have emerged to enhance model transparency and extract descriptive knowledge for a single observation or for all observations. However, explaining black-box models for a specific group of observations may be more useful in some domains. This paper introduces the glocal explanations (between local and global levels) of the expected goal models to enable performance analysis at the team and player levels by proposing the use of aggregated versions of the SHAP values and partial dependence profiles. This allows knowledge to be extracted from the expected goal model for a player or team rather than just a single shot. In addition, we conducted real-data applications to illustrate the usefulness of aggregated SHAP and aggregated profiles. The paper concludes with remarks on the potential of these explanations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#20266;&#24067;&#23572;&#22810;&#39033;&#24335;&#30340;&#24809;&#32602;&#24615;&#34920;&#36798;&#24335;&#20316;&#20026;&#31751;&#20998;&#26512;&#20013;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#28165;&#26224;&#35299;&#37322;&#24615;&#25552;&#21462;&#31751;&#12290;</title><link>http://arxiv.org/abs/2308.15553</link><description>&lt;p&gt;
&#20351;&#29992;&#20266;&#24067;&#23572;&#22810;&#39033;&#24335;&#36827;&#34892;&#31751;&#20998;&#26512;&#30340;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction Using pseudo-Boolean polynomials For Cluster Analysis. (arXiv:2308.15553v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#20266;&#24067;&#23572;&#22810;&#39033;&#24335;&#30340;&#24809;&#32602;&#24615;&#34920;&#36798;&#24335;&#20316;&#20026;&#31751;&#20998;&#26512;&#20013;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#28165;&#26224;&#35299;&#37322;&#24615;&#25552;&#21462;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20266;&#24067;&#23572;&#22810;&#39033;&#24335;&#30340;&#24809;&#32602;&#24615;&#34920;&#36798;&#24335;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#20316;&#20026;&#31751;&#20998;&#26512;&#36807;&#31243;&#20013;&#19981;&#21464;&#38477;&#32500;&#30340;&#26426;&#21046;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#32500;&#25968;&#25454;&#65292;&#22914;4&#32500;&#40482;&#23614;&#33457;&#25968;&#25454;&#38598;&#21487;&#20197;&#38477;&#33267;2&#32500;&#31354;&#38388;&#65292;&#32780;30&#32500;&#23041;&#26031;&#24247;&#26143;&#24030;&#20083;&#33146;&#30284;&#65288;WDBC&#65289;&#25968;&#25454;&#38598;&#21487;&#20197;&#38477;&#33267;3&#32500;&#31354;&#38388;&#65292;&#36890;&#36807;&#25628;&#32034;&#20301;&#20110;&#38477;&#32500;&#26679;&#26412;&#20043;&#38388;&#30340;&#30452;&#32447;&#25110;&#24179;&#38754;&#65292;&#25105;&#20204;&#21487;&#20197;&#20197;&#32447;&#24615;&#21644;&#26080;&#20559;&#30340;&#26041;&#24335;&#25552;&#21462;&#31751;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24230;&#65292;&#21487;&#37325;&#22797;&#24615;&#21644;&#28165;&#26224;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce usage of a reduction property of penalty-based formulation of pseudo-Boolean polynomials as a mechanism for invariant dimensionality reduction in cluster analysis processes. In our experiments, we show that multidimensional data, like 4-dimensional Iris Flower dataset can be reduced to 2-dimensional space while the 30-dimensional Wisconsin Diagnostic Breast Cancer (WDBC) dataset can be reduced to 3-dimensional space, and by searching lines or planes that lie between reduced samples we can extract clusters in a linear and unbiased manner with competitive accuracies, reproducibility and clear interpretation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#20171;&#32773;&#26469;&#27169;&#25311;&#19968;&#20123;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#31163;&#32447;&#23398;&#20064;&#12289;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2308.15552</link><description>&lt;p&gt;
&#32431;&#25506;&#32034;&#19979;&#30340;&#20013;&#20171;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration under Mediators' Feedback. (arXiv:2308.15552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#20171;&#32773;&#26469;&#27169;&#25311;&#19968;&#20123;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#31163;&#32447;&#23398;&#20064;&#12289;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#26159;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;&#65292;&#27599;&#19968;&#27493;&#20132;&#20114;&#20013;&#23398;&#20064;&#32773;&#36873;&#25321;&#19968;&#20010;&#33218;&#24182;&#35266;&#23519;&#19968;&#20010;&#38543;&#26426;&#22238;&#25253;&#12290;&#22312;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#33218;&#65292;&#21363;&#20855;&#26377;&#26368;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#33218;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;BAI&#38382;&#39064;&#30340;&#39034;&#24207;&#20132;&#20114;&#21327;&#35758;&#65292;&#21363;&#23398;&#20064;&#32773;&#22312;&#27599;&#19968;&#36718;&#20013;&#23545;&#36873;&#25321;&#30340;&#33218;&#20855;&#26377;&#23436;&#20840;&#25511;&#21046;&#26435;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#19968;&#20123;&#20540;&#24471;&#20851;&#27880;&#30340;&#20915;&#31574;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#31163;&#32447;&#23398;&#20064;&#65292;&#37096;&#20998;&#21487;&#25511;&#29615;&#22659;&#21644;&#20154;&#31867;&#21453;&#39304;&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#25512;&#24191;&#30340;&#20256;&#32479;BAI&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#20013;&#20171;&#21453;&#39304;&#19979;&#30340;&#26368;&#20248;&#33218;&#35782;&#21035;&#65288;BAI-MF&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#32773;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;&#20013;&#20171;&#32773;&#30340;&#24773;&#20917;&#65292;&#27599;&#20010;&#20013;&#20171;&#32773;&#37117;&#36873;&#25321;&#35201;&#25289;&#21160;&#30340;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. Within the context of best-arm identification (BAI) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. Nevertheless, the sequential interaction protocol of classical BAI problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). For this reason, in this work, we propose a novel strict generalization of the classical BAI problem that we refer to as best-arm identification under mediators' feedback (BAI-MF). More specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#20102;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15550</link><description>&lt;p&gt;
&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Style Transfer for Robust Policy Optimization in Deep Reinforcement Learning. (arXiv:2308.15550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24335;&#39118;&#26684;&#36716;&#31227;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#20102;&#29983;&#25104;&#22120;&#21644;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#30340;&#26041;&#24335;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31639;&#27861;&#30456;&#27604;&#20110;&#20854;&#20182;&#22522;&#20934;&#31639;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#28040;&#38500;&#23545;&#28151;&#28102;&#29305;&#24449;&#36807;&#25311;&#21512;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20102;&#19968;&#20010;&#26368;&#22823;-&#26368;&#23567;&#21338;&#24328;&#29702;&#35770;&#30340;&#30446;&#26631;&#12290;&#29983;&#25104;&#22120;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#36716;&#31227;&#35266;&#23519;&#26679;&#24335;&#12290;&#29983;&#25104;&#22120;&#30340;&#21478;&#19968;&#20010;&#30446;&#26631;&#26159;&#25200;&#21160;&#35266;&#23519;&#65292;&#20197;&#26368;&#22823;&#21270;&#26234;&#33021;&#20307;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#30340;&#27010;&#29575;&#12290;&#30456;&#21453;&#65292;&#31574;&#30053;&#32593;&#32476;&#26356;&#26032;&#20854;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;&#36825;&#31181;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#22312;&#26368;&#22823;&#21270;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#30340;&#21516;&#26102;&#20445;&#25345;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;Adversarial Robust Policy Optimization (ARPO)&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#21487;&#20197;&#27867;&#21270;&#21040;&#26410;&#35265;&#29615;&#22659;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Distracting Control Suite&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#27867;&#21270;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20960;&#31181;&#22522;&#20934;&#31639;&#27861;&#65288;&#21253;&#25324;&#25968;&#25454;&#22686;&#24191;&#65289;&#30456;&#27604;&#65292;ARPO&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm that aims to improve generalization for reinforcement learning agents by removing overfitting to confounding features. Our approach consists of a max-min game theoretic objective. A generator transfers the style of observation during reinforcement learning. An additional goal of the generator is to perturb the observation, which maximizes the agent's probability of taking a different action. In contrast, a policy network updates its parameters to minimize the effect of such perturbations, thus staying robust while maximizing the expected future reward. Based on this setup, we propose a practical deep reinforcement learning algorithm, Adversarial Robust Policy Optimization (ARPO), to find a robust policy that generalizes to unseen environments. We evaluate our approach on Procgen and Distracting Control Suite for generalization and sample efficiency. Empirically, ARPO shows improved performance compared to a few baseline algorithms, including data augmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15513</link><description>&lt;p&gt;
&#35843;&#25972;&#22256;&#24785;&#24230;&#24182;&#35745;&#31639;&#22522;&#20110;&#37319;&#26679;&#30340;t-SNE&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#24120;&#29992;&#30340;&#31649;&#36947;&#21033;&#29992;&#20108;&#32500;&#21487;&#35270;&#21270;&#65292;&#20363;&#22914;&#36890;&#36807;t&#20998;&#24067;&#37051;&#36817;&#38543;&#26426;&#23884;&#20837;&#65288;t-SNE&#65289;&#12290;&#20294;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#20250;&#29983;&#25104;&#27425;&#20248;&#30340;&#23884;&#20837;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#19981;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#12290;&#23558;&#36825;&#20123;&#21442;&#25968;&#22686;&#21152;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35745;&#31639;&#23545;&#20110;&#23454;&#38469;&#24037;&#20316;&#27969;&#31243;&#26469;&#35828;&#22826;&#26114;&#36149;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#37319;&#26679;&#30340;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24517;&#39035;&#35880;&#24910;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#21462;&#20915;&#20110;&#37319;&#26679;&#29575;&#21644;&#39044;&#26399;&#30340;&#26368;&#32456;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
&lt;/p&gt;</description></item><item><title>unORANIC&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#21270;&#35299;&#21078;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25439;&#22351;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15507</link><description>&lt;p&gt;
unORANIC: &#26080;&#30417;&#30563;&#35299;&#21078;&#19982;&#22270;&#20687;&#29305;&#24449;&#30340;&#27491;&#20132;&#21270;
&lt;/p&gt;
&lt;p&gt;
unORANIC: Unsupervised Orthogonalization of Anatomy and Image-Characteristic Features. (arXiv:2308.15507v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15507
&lt;/p&gt;
&lt;p&gt;
unORANIC&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#20132;&#21270;&#35299;&#21078;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25439;&#22351;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;unORANIC&#65292;&#19968;&#31181;&#20351;&#29992;&#35843;&#25972;&#21518;&#30340;&#25439;&#22833;&#20989;&#25968;&#39537;&#21160;&#35299;&#21078;&#21644;&#22270;&#20687;&#29305;&#24449;&#30340;&#27491;&#20132;&#21270;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#27169;&#24577;&#21644;&#20219;&#21153;&#65292;&#19981;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#12289;&#37197;&#23545;&#25968;&#25454;&#26679;&#26412;&#25110;&#26631;&#31614;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;unORANIC&#34987;&#24212;&#29992;&#20110;&#21487;&#33021;&#26377;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#35299;&#21078;&#23398;&#21644;&#29305;&#24449;&#20998;&#37327;&#27491;&#20132;&#21270;&#65292;&#28982;&#21518;&#37325;&#24314;&#26080;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#20165;&#26174;&#31034;&#20854;&#39046;&#22495;&#19981;&#21464;&#30340;&#35299;&#21078;&#23398;&#29305;&#24449;&#12290;&#36825;&#31181;&#29305;&#24449;&#30340;&#27491;&#20132;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25439;&#22351;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;unORANIC&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12289;&#25439;&#22351;&#26816;&#27979;&#21644;&#20462;&#35746;&#33021;&#21147;&#65292;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978; qualitatively &#21644; quantitatively &#30830;&#35748;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#30340;&#28508;&#21147;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/sdoerrich97/unORANIC &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce unORANIC, an unsupervised approach that uses an adapted loss function to drive the orthogonalization of anatomy and image-characteristic features. The method is versatile for diverse modalities and tasks, as it does not require domain knowledge, paired data samples, or labels. During test time unORANIC is applied to potentially corrupted images, orthogonalizing their anatomy and characteristic components, to subsequently reconstruct corruption-free images, showing their domain-invariant anatomy only. This feature orthogonalization further improves generalization and robustness against corruptions. We confirm this qualitatively and quantitatively on 5 distinct datasets by assessing unORANIC's classification accuracy, corruption detection and revision capabilities. Our approach shows promise for enhancing the generalizability and robustness of practical applications in medical image analysis. The source code is available at https://github.com/sdoerrich97/unORANIC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#23481;&#37327;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#21487;&#20197;&#35206;&#30422;&#30340;&#24050;&#35757;&#32451;&#21442;&#25968;&#30340;&#20302;&#38454;&#20301;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38543;&#20302;&#38454;&#20301;&#25968;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36873;&#23450;&#27169;&#22411;&#21508;&#23618;&#30340;&#38544;&#20889;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.15502</link><description>&lt;p&gt;
&#20851;&#20110;&#36873;&#23450;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Steganographic Capacity of Selected Learning Models. (arXiv:2308.15502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#23481;&#37327;&#38382;&#39064;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21069;&#25552;&#19979;&#21487;&#20197;&#35206;&#30422;&#30340;&#24050;&#35757;&#32451;&#21442;&#25968;&#30340;&#20302;&#38454;&#20301;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38543;&#20302;&#38454;&#20301;&#25968;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36873;&#23450;&#27169;&#22411;&#21508;&#23618;&#30340;&#38544;&#20889;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#28508;&#22312;&#36733;&#20307;&#12290;&#20363;&#22914;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24694;&#24847;&#36719;&#20214;&#21487;&#20197;&#38544;&#34255;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#23558;&#20449;&#24687;&#38544;&#34255;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#35270;&#20026;&#19968;&#31181;&#38544;&#20889;&#26415;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#20889;&#23481;&#37327;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#20197;&#35206;&#30422;&#32780;&#19981;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#30340;&#24050;&#35757;&#32451;&#21442;&#25968;&#30340;&#20302;&#38454;&#20301;&#25968;&#12290;&#23545;&#20110;&#27599;&#20010;&#32771;&#34385;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20197;&#24050;&#35206;&#30422;&#30340;&#20302;&#38454;&#20301;&#25968;&#20316;&#20026;&#33258;&#21464;&#37327;&#32472;&#21046;&#20934;&#30830;&#24615;&#22270;&#65292;&#24182;&#38024;&#23545;&#36873;&#23450;&#30340;&#27169;&#22411;&#20998;&#26512;&#20102;&#27599;&#20010;&#23618;&#30340;&#38544;&#20889;&#23481;&#37327;&#12290;&#25105;&#20204;&#27979;&#35797;&#30340;&#27169;&#22411;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32447;&#24615;&#22238;&#24402;&#65288;LR&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65307;&#27969;&#34892;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning models are potential vectors for various attack scenarios. For example, previous research has shown that malware can be hidden in deep learning models. Hiding information in a learning model can be viewed as a form of steganography. In this research, we consider the general question of the steganographic capacity of learning models. Specifically, for a wide range of models, we determine the number of low-order bits of the trained parameters that can be overwritten, without adversely affecting model performance. For each model considered, we graph the accuracy as a function of the number of low-order bits that have been overwritten, and for selected models, we also analyze the steganographic capacity of individual layers. The models that we test include the classic machine learning techniques of Linear Regression (LR) and Support Vector Machine (SVM); the popular general deep learning models of Multilayer Perceptron (MLP) and Convolutional Neural Netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#65292;&#20182;&#20204;&#22312;&#21644;&#24179;&#26102;&#26399;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22909;&#25429;&#25417;&#32593;&#32476;&#25112;&#22763;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15491</link><description>&lt;p&gt;
&#20174;&#22312;&#32447;&#35770;&#22363;&#20013;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;
&lt;/p&gt;
&lt;p&gt;
Detecting Inactive Cyberwarriors from Online Forums. (arXiv:2308.15491v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#65292;&#20182;&#20204;&#22312;&#21644;&#24179;&#26102;&#26399;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22909;&#25429;&#25417;&#32593;&#32476;&#25112;&#22763;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#25104;&#20026;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#25112;&#20105;&#12290;&#36825;&#31181;&#25112;&#20105;&#28041;&#21450;&#21040;&#32593;&#32476;&#25112;&#22763;&#65292;&#20182;&#20204;&#26377;&#24847;&#20256;&#25773;&#26088;&#22312;&#35837;&#35876;&#23545;&#25163;&#25110;&#22242;&#32467;&#30431;&#21451;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#22312;&#32447;&#35770;&#22363;&#20013;&#32593;&#32476;&#25112;&#22763;&#30340;&#27963;&#21160;&#27700;&#24179;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#23569;&#25968;&#32593;&#32476;&#25112;&#22763;&#26159;&#27963;&#36291;&#29992;&#25143;&#12290;&#20196;&#20154;&#24847;&#22806;&#30340;&#26159;&#65292;&#23613;&#31649;&#20182;&#20204;&#34987;&#26399;&#26395;&#31215;&#26497;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#22312;&#21644;&#24179;&#26102;&#26399;&#32593;&#32476;&#25112;&#22763;&#21364;&#20445;&#25345;&#27785;&#40664;&#65292;&#21482;&#22312;&#24517;&#35201;&#26102;&#25165;&#34892;&#21160;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#35782;&#21035;&#32593;&#32476;&#25112;&#22763;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#26816;&#27979;&#19981;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#27604;&#35782;&#21035;&#27963;&#36291;&#30340;&#32593;&#32476;&#25112;&#22763;&#35201;&#38590;&#24471;&#22810;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#22320;&#22312;&#32593;&#32476;&#25112;&#22763;&#19981;&#27963;&#36291;&#30340;&#38454;&#27573;&#35782;&#21035;&#20182;&#20204;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#20026;&#26356;&#22909;&#22320;&#25429;&#25417;&#20182;&#20204;&#30340;&#34892;&#21160;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of misinformation has emerged as a new form of warfare in the information age. This type of warfare involves cyberwarriors, who deliberately propagate messages aimed at defaming opponents or fostering unity among allies. In this study, we investigate the level of activity exhibited by cyberwarriors within a large online forum, and remarkably, we discover that only a minute fraction of cyberwarriors are active users. Surprisingly, despite their expected role of actively disseminating misinformation, cyberwarriors remain predominantly silent during peacetime and only spring into action when necessary. Moreover, we analyze the challenges associated with identifying cyberwarriors and provide evidence that detecting inactive cyberwarriors is considerably more challenging than identifying their active counterparts. Finally, we discuss potential methodologies to more effectively identify cyberwarriors during their inactive phases, offering insights into better capturing thei
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#24403;&#21069;&#38459;&#28382;&#32531;&#35299;&#25216;&#26415;&#22312;&#19981;&#21516;&#36845;&#20195;&#25910;&#25947;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#25968;&#26381;&#21153;&#22120;&#31574;&#30053;&#22312;&#24182;&#34892;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#23433;&#25490;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2308.15482</link><description>&lt;p&gt;
&#21442;&#25968;&#26381;&#21153;&#22120;&#22312;&#36845;&#20195;&#25910;&#25947;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38459;&#28382;&#38382;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Straggler Problem in Parameter Server on Iterative Convergent Distributed Machine Learning. (arXiv:2308.15482v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#24403;&#21069;&#38459;&#28382;&#32531;&#35299;&#25216;&#26415;&#22312;&#19981;&#21516;&#36845;&#20195;&#25910;&#25947;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#21442;&#25968;&#26381;&#21153;&#22120;&#31574;&#30053;&#22312;&#24182;&#34892;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#23433;&#25490;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27979;&#35797;&#24403;&#21069;&#38459;&#28382;&#32531;&#35299;&#25216;&#26415;&#22312;&#19981;&#21516;&#37325;&#35201;&#30340;&#36845;&#20195;&#25910;&#25947;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;&#30697;&#38453;&#20998;&#35299;&#65292;&#22810;&#39033;&#36923;&#36753;&#22238;&#24402;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;&#21442;&#25968;&#26381;&#21153;&#22120;&#20307;&#31995;&#32467;&#26500;&#23454;&#29616;&#30340;FlexPS&#31995;&#32479;&#36827;&#34892;&#23454;&#26045;&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;&#25209;&#21516;&#27493;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#65292;&#20197;&#26816;&#26597;&#21442;&#25968;&#26381;&#21153;&#22120;&#22312;&#36845;&#20195;&#25910;&#25947;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38459;&#28382;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#21442;&#25968;&#26381;&#21153;&#22120;&#31574;&#30053;&#22312;&#24182;&#34892;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#23433;&#25490;&#65292;&#36890;&#36807;&#27880;&#20837;&#36890;&#29992;&#38459;&#28382;&#27169;&#24335;&#21644;&#25191;&#34892;&#26368;&#26032;&#30340;&#32531;&#35299;&#25216;&#26415;&#12290;&#30740;&#31350;&#32467;&#26524;&#30340;&#24847;&#20041;&#22312;&#20110;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this study is to test the effectiveness of current straggler mitigation techniques over different important iterative convergent machine learning(ML) algorithm including Matrix Factorization (MF), Multinomial Logistic Regression (MLR), and Latent Dirichlet Allocation (LDA) . The experiment was conducted to implemented using the FlexPS system, which is the latest system implementation that employ parameter server architecture. The experiment employed the Bulk Synchronous Parallel (BSP) computational model to examine the straggler problem in Parameter Server on Iterative Convergent Distributed Machine Learning. Moreover, the current research analyzes the experimental arrangement of the parameter server strategy concerning the parallel learning problems by injecting universal straggler patterns and executing latest mitigation techniques. The findings of the study are significant in that as they will provide the necessary platform for conducting further research into the pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20132;&#26102;&#38388;&#19978;&#36827;&#34892;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#34920;&#31034;&#20316;&#19994;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#31649;&#29702;&#65292;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.15481</link><description>&lt;p&gt;
&#22312;HPC&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Job Failure Prediction in an HPC System. (arXiv:2308.15481v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#25552;&#20132;&#26102;&#38388;&#19978;&#36827;&#34892;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#65292;&#24182;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#34920;&#31034;&#20316;&#19994;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#31649;&#29702;&#65292;&#25552;&#39640;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#31995;&#32479;&#26159;&#22797;&#26434;&#30340;&#26426;&#22120;&#65292;&#23545;&#32463;&#27982;&#21644;&#31038;&#20250;&#37117;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#38500;&#20102;&#35745;&#31639;&#33021;&#21147;&#22806;&#65292;&#33021;&#28304;&#28040;&#32791;&#20063;&#22312;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#24403;&#21069;&#30340;&#29615;&#22659;&#21644;&#33021;&#28304;&#21361;&#26426;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20248;&#21270;HPC&#31995;&#32479;&#31649;&#29702;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#26082;&#21487;&#20197;&#20445;&#35777;&#19968;&#27969;&#30340;&#24615;&#33021;&#65292;&#21448;&#21487;&#20197;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;&#19968;&#31181;&#31574;&#30053;&#26159;&#36890;&#36807;&#22312;&#24037;&#20316;&#36127;&#36733;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#22312;&#31995;&#32479;&#19978;&#25191;&#34892;&#20043;&#21069;&#31361;&#20986;&#26174;&#31034;&#26368;&#26377;&#21487;&#33021;&#22833;&#36133;&#30340;&#20316;&#19994;&#12290;&#20316;&#19994;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#22833;&#36133;&#20250;&#19981;&#24517;&#35201;&#22320;&#21344;&#29992;&#36164;&#28304;&#65292;&#21487;&#33021;&#20250;&#24310;&#36831;&#20854;&#20182;&#20316;&#19994;&#65292;&#23545;&#31995;&#32479;&#24615;&#33021;&#21644;&#33021;&#28304;&#28040;&#32791;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#20102;&#25552;&#20132;&#26102;&#38388;&#19978;&#30340;&#20316;&#19994;&#22833;&#36133;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#65288;i&#65289;&#23558;&#36825;&#20123;&#31639;&#27861;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#34920;&#31034;&#20316;&#19994;&#21644;&#65288;ii&#65289;th
&lt;/p&gt;
&lt;p&gt;
Modern High Performance Computing (HPC) systems are complex machines, with major impacts on economy and society. Along with their computational capability, their energy consumption is also steadily raising, representing a critical issue given the ongoing environmental and energetic crisis. Therefore, developing strategies to optimize HPC system management has paramount importance, both to guarantee top-tier performance and to improve energy efficiency. One strategy is to act at the workload level and highlight the jobs that are most likely to fail, prior to their execution on the system. Jobs failing during their execution unnecessarily occupy resources which could delay other jobs, adversely affecting the system performance and energy consumption. In this paper, we study job failure prediction at submit-time using classical machine learning algorithms. Our novelty lies in (i) the combination of these algorithms with Natural Language Processing (NLP) tools to represent jobs and (ii) th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#24341;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35777;&#26126;&#20102;&#25945;&#24072;&#31574;&#30053;&#22312;&#26080;&#24418;&#29366;&#22870;&#21169;&#19979;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#24182;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15470</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25919;&#31574;&#32452;&#21512;&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy composition in reinforcement learning via multi-objective policy optimization. (arXiv:2308.15470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15470
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#24341;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#35777;&#26126;&#20102;&#25945;&#24072;&#31574;&#30053;&#22312;&#26080;&#24418;&#29366;&#22870;&#21169;&#19979;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#24182;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#25945;&#24072;&#31574;&#30053;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#25104;&#21151;&#30340;&#34892;&#20026;&#31574;&#30053;&#12290;&#25945;&#24072;&#31574;&#30053;&#34987;&#24341;&#20837;&#20316;&#20026;&#30446;&#26631;&#65292;&#38500;&#20102;&#20219;&#21153;&#30446;&#26631;&#20197;&#22806;&#65292;&#22312;&#22810;&#30446;&#26631;&#25919;&#31574;&#20248;&#21270;&#30340;&#35774;&#32622;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#30446;&#26631;&#26368;&#22823;&#21518;&#39564;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#31574;&#30053;&#33021;&#22815;&#21152;&#36895;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#22312;&#32570;&#23569;&#24418;&#29366;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#20855;&#26377;&#36830;&#32493;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#30340;&#20004;&#20010;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#25104;&#21151;&#22320;&#25353;&#39034;&#24207;&#21644;&#24182;&#34892;&#22320;&#32452;&#21512;&#25945;&#24072;&#31574;&#30053;&#65292;&#24182;&#19988;&#36824;&#33021;&#22815;&#36827;&#19968;&#27493;&#25193;&#23637;&#25945;&#24072;&#30340;&#31574;&#30053;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#26681;&#25454;&#20219;&#21153;&#21644;&#25945;&#24072;&#30340;&#25351;&#23450;&#32452;&#21512;&#65292;&#25945;&#24072;&#21487;&#33021;&#33258;&#28982;&#22320;&#38480;&#21046;&#26234;&#33021;&#20307;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#26234;&#33021;&#20307;&#38656;&#35201;&#36981;&#23432;&#25945;&#24072;&#31574;&#30053;&#30340;&#31243;&#24230;&#30001;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#30830;&#23450;&#20102;&#25945;&#24072;&#31574;&#30053;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.  Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14945</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximals&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise-Free Sampling Algorithms via Regularized Wasserstein Proximals. (arXiv:2308.14945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27491;&#21017;&#21270;Wasserstein Proximal&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22122;&#22768;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#28508;&#21183;&#20989;&#25968;&#30830;&#23450;&#24615;&#22320;&#36827;&#34892;&#31890;&#23376;&#28436;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#21644;&#36895;&#24230;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#30001;&#28508;&#21183;&#20989;&#25968;&#25511;&#21046;&#30340;&#20998;&#24067;&#25277;&#26679;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20351;&#24471;&#31890;&#23376;&#30340;&#28436;&#21270;&#21464;&#20026;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28436;&#21270;&#12290;&#35780;&#20998;&#39033;&#30001;&#27491;&#21017;&#21270;&#30340;Wasserstein proximal&#20197;&#38381;&#21512;&#24418;&#24335;&#32473;&#20986;&#65292;&#20351;&#29992;&#37319;&#26679;&#26469;&#36817;&#20284;&#26680;&#21367;&#31215;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#24182;&#19988;&#19982;&#26410;&#35843;&#25972;Langevin&#31639;&#27861;&#21644;Metropolis&#35843;&#25972;Langevin&#31639;&#27861;&#30456;&#27604;&#65292;&#26174;&#31034;&#20102;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#26102;&#38388;&#36793;&#30028;&#30340;&#25913;&#21892;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20102;&#20108;&#27425;&#28508;&#21183;&#20989;&#25968;&#27599;&#27425;&#36845;&#20195;&#30340;&#20998;&#24067;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#34920;&#24449;&#20102;&#26041;&#24046;&#38477;&#20302;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31890;&#23376;&#30340;&#34892;&#20026;&#26159;&#26377;&#32452;&#32455;&#30340;&#65292;&#20301;&#20110;&#28508;&#21183;&#30340;&#31561;&#20540;&#32447;&#19978;&#12290;&#27492;&#22806;&#65292;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from a distribution governed by a potential function. This work proposes an explicit score-based MCMC method that is deterministic, resulting in a deterministic evolution for particles rather than a stochastic differential equation evolution. The score term is given in closed form by a regularized Wasserstein proximal, using a kernel convolution that is approximated by sampling. We demonstrate fast convergence on various problems and show improved dimensional dependence of mixing time bounds for the case of Gaussian distributions compared to the unadjusted Langevin algorithm (ULA) and the Metropolis-adjusted Langevin algorithm (MALA). We additionally derive closed form expressions for the distributions at each iterate for quadratic potential functions, characterizing the variance reduction. Empirical results demonstrate that the particles behave in an organized manner, lying on level set contours of the potential. Moreover, the posterior mean estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;&#65292;&#20197;&#22312;&#22797;&#26434;&#19988;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#25191;&#34892;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Composition of Agent Policies by Markov Decision Process Entity Embeddings and Agent Ensembles. (arXiv:2308.14521v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#20307;&#23884;&#20837;&#21644;&#20195;&#29702;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#19978;&#19979;&#25991;&#24863;&#30693;&#22320;&#32452;&#21512;&#20195;&#29702;&#31574;&#30053;&#65292;&#20197;&#22312;&#22797;&#26434;&#19988;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#25191;&#34892;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#20195;&#29702;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25903;&#25345;&#20154;&#31867;&#65292;&#24182;&#22240;&#27492;&#23384;&#22312;&#24322;&#26500;&#29615;&#22659;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#24182;&#19988;&#21487;&#33021;&#38754;&#20020;&#24040;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#20026;&#20102;&#20197;&#30446;&#26631;&#23548;&#21521;&#30340;&#26041;&#24335;&#25191;&#34892;&#26381;&#21153;&#21644;&#27963;&#21160;&#65292;&#20195;&#29702;&#38656;&#35201;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#24517;&#39035;&#21046;&#23450;&#21644;&#36861;&#27714;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35268;&#23450;&#31574;&#30053;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#38480;&#21046;&#21644;&#19981;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#23427;&#30340;&#21160;&#20316;&#36873;&#25321;&#12290;&#30001;&#20110;&#29615;&#22659;&#21487;&#33021;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#19988;&#22312;&#29366;&#24577;&#21644;&#21487;&#34892;&#21160;&#20316;&#30340;&#25968;&#37327;&#19978;&#22797;&#26434;&#65292;&#22240;&#27492;&#36890;&#24120;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20197;&#31616;&#21270;&#30340;&#26041;&#24335;&#24314;&#27169;&#27963;&#21160;&#65292;&#20197;&#20415;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#24110;&#21161;&#25429;&#25417;&#19978;&#19979;&#25991;&#24182;&#26681;&#25454;&#26368;&#20248;&#26041;&#24335;&#25191;&#34892;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational agents support humans in many areas of life and are therefore found in heterogeneous contexts. This means they operate in rapidly changing environments and can be confronted with huge state and action spaces. In order to perform services and carry out activities in a goal-oriented manner, agents require prior knowledge and therefore have to develop and pursue context-dependent policies. However, prescribing policies in advance is limited and inflexible, especially in dynamically changing environments. Moreover, the context of an agent determines its choice of actions. Since the environments can be stochastic and complex in terms of the number of states and feasible actions, activities are usually modelled in a simplified way by Markov decision processes so that, e.g., agents with reinforcement learning are able to learn policies, that help to capture the context and act accordingly to optimally perform activities. However, training policies for all possible contexts using
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13399</link><description>&lt;p&gt;
EntropyRank: &#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21387;&#32553;&#30340;&#21103;&#20449;&#24687;&#20248;&#21270;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;Shannon&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#21644;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#22312;LM&#19979;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#12290;&#24471;&#21040;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#38598;&#21512;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20449;&#24687;&#35770;&#38382;&#39064;&#65306;&#22914;&#26524;&#20316;&#20026;&#21103;&#20449;&#24687;&#25552;&#20379;&#65292;&#23427;&#20250;&#23548;&#33268;&#20351;&#29992;LM&#21644;&#29109;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#26102;&#30340;&#39044;&#26399;&#26368;&#23567;&#20108;&#36827;&#21046;&#30721;&#38271;&#24230;&#12290;&#21478;&#22806;&#65292;&#24471;&#21040;&#30340;&#38598;&#21512;&#26159;&#36890;&#36807;&#22240;&#26524;LM&#23545;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25991;&#26412;&#29109;&#30340;&#30701;&#35821;&#38598;&#21512;&#30340;&#36817;&#20284;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20851;&#38190;&#35789;&#25552;&#21462;&#22522;&#20934;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#19982;&#26368;&#24120;&#29992;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#22343;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#23884;&#20837;&#22312;&#25512;&#33616;&#20013;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#23884;&#20837;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.12767</link><description>&lt;p&gt;
&#20851;&#20110;&#24179;&#22343;&#23884;&#20837;&#29992;&#20110;&#29289;&#21697;&#25512;&#33616;&#30340;&#19968;&#33268;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Consistency of Average Embeddings for Item Recommendation. (arXiv:2308.12767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#24179;&#22343;&#23884;&#20837;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#23884;&#20837;&#22312;&#25512;&#33616;&#20013;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#23454;&#19990;&#30028;&#23884;&#20837;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#31181;&#27969;&#34892;&#30340;&#20570;&#27861;&#26159;&#23558;&#29289;&#21697;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#20197;&#22312;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#20195;&#34920;&#29992;&#25143;&#25110;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#20570;&#27861;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#31934;&#24230;&#20998;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#24179;&#22343;&#23884;&#20837;&#19982;&#20854;&#26500;&#24314;&#25152;&#20351;&#29992;&#30340;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#38543;&#21518;&#22312;&#20855;&#26377;&#29305;&#23450;&#20551;&#35774;&#30340;&#29702;&#35770;&#29615;&#22659;&#21644;&#26469;&#33258;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20998;&#26512;&#20102;&#35813;&#20998;&#25968;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#21450;&#20854;&#32463;&#39564;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#29616;&#23454;&#19990;&#30028;&#30340;&#24179;&#22343;&#20540;&#22312;&#25512;&#33616;&#20013;&#30340;&#19968;&#33268;&#24615;&#36739;&#20302;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#26356;&#22909;&#22320;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#23884;&#20837;&#19982;&#25105;&#20204;&#29702;&#35770;&#29615;&#22659;&#30340;&#20551;&#35774;&#30456;&#19968;&#33268;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12315</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#27010;&#24565;&#65292;&#23545;&#35813;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#26082;&#20139;&#21463;&#21040;&#20102;&#36825;&#20123;&#25216;&#26415;&#24102;&#26469;&#30340;&#22909;&#22788;&#65292;&#20063;&#38754;&#20020;&#22240;&#36825;&#20123;&#31995;&#32479;&#32780;&#24341;&#21457;&#30340;&#35768;&#22810;&#31038;&#20250;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36275;&#22815;&#22909;&#24182;&#19988;&#21487;&#20449;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#65292;&#32780;&#34920;&#31034;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#22914;&#20309;&#20351;&#34920;&#31034;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#21487;&#20449;&#24230;&#65292;&#20363;&#22914;&#36328;&#39046;&#22495;&#22330;&#26223;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#39046;&#22495;&#37117;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#21644;&#24517;&#35201;&#30340;&#12290;&#22312;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#21487;&#20449;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#36825;&#22235;&#20010;&#27010;&#24565;&#65292;&#23545;&#36825;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11197</link><description>&lt;p&gt;
&#22312;&#35821;&#38899;&#12289;&#35821;&#35328;&#21644;&#21548;&#21147;&#31185;&#23398;&#20013;&#24314;&#31435;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#21151;&#25928;&#20998;&#26512;&#21644;&#26679;&#26412;&#23481;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#23450;&#37327;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#21644;&#27169;&#22411;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32622;&#20449;&#24230;&#12290;&#21516;&#26102;&#65292;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#31532;&#19968;&#20010;&#30446;&#30340;&#26159;&#25552;&#20379;&#23450;&#37327;&#35777;&#25454;&#65292;&#20197;&#28608;&#21169;&#30740;&#31350;&#20154;&#21592;&#25913;&#29992;&#26356;&#20581;&#22766;&#30340;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#30446;&#30340;&#26159;&#22312;&#30740;&#31350;&#35774;&#35745;&#36807;&#31243;&#20013;&#25552;&#20986;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#30340;&#21151;&#25928;&#20998;&#26512;&#26041;&#27861;&#21644;MATLAB&#20195;&#30721;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#37327;&#21270;&#20102;&#25152;&#37319;&#29992;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#12289;&#29305;&#24449;&#30340;&#21028;&#21035;&#21147;&#12289;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#27169;&#22411;&#30340;&#32500;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#32479;&#35745;&#32622;&#20449;&#24230;&#65292;&#27604;&#36739;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65288;&#21333;&#19968;&#30041;&#20986;&#27861;&#12289;10&#25240;&#20132;&#21449;&#39564;&#35777;&#12289;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#27861;&#21644;&#23884;&#22871;10&#25240;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#21033;&#29992;&#38646;&#20551;&#35774;&#21644;&#22791;&#25321;&#20551;&#35774;&#30340;&#20998;&#24067;&#30830;&#23450;&#20102;&#33719;&#24471;&#32479;&#35745;&#26174;&#33879;&#32467;&#26524;&#25152;&#38656;&#30340;&#26368;&#23567;&#26679;&#26412;&#23481;&#37327;&#65288;&#945;=0.05&#65292;1-&#946;=0.8&#65289;&#12290;&#27169;&#22411;&#30340;&#32479;&#35745;&#32622;&#20449;&#24230;&#34987;&#23450;&#20041;&#20026;&#27491;&#30830;&#29305;&#24449;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#28385;&#36275;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#20013;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.09884</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#26694;&#26550;&#65306;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case. (arXiv:2308.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09884
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#28385;&#36275;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#20013;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#20840;&#29699;&#20851;&#27880;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;LLMs&#30340;&#26377;&#25928;&#24615;&#21487;&#24402;&#22240;&#20110;&#20854;&#29992;&#20110;&#35757;&#32451;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#21363;transformers&#12290;Transformer&#27169;&#22411;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#39034;&#24207;&#30340;&#65292;&#21487;&#20197;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39044;&#27979;&#12290;&#39044;&#27979;&#23398;&#39046;&#22495;&#23545;&#31995;&#32479;&#20581;&#24247;&#31649;&#29702;&#21644;&#36866;&#24403;&#30340;&#32500;&#25252;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#26426;&#22120;&#21097;&#20313;&#21487;&#29992;&#23551;&#21629;&#65288;RUL&#65289;&#36827;&#34892;&#21487;&#38752;&#20272;&#35745;&#20855;&#26377;&#33410;&#30465;&#25104;&#26412;&#30340;&#28508;&#21147;&#12290;&#36825;&#21253;&#25324;&#36991;&#20813;&#26426;&#22120;&#31361;&#28982;&#25925;&#38556;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#35774;&#22791;&#65292;&#24182;&#20316;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-Transformer&#26550;&#26500;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#24212;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;</title><link>http://arxiv.org/abs/2308.09175</link><description>&lt;p&gt;
&#25193;&#23637;AI&#65306;&#21521;&#25317;&#26377;&#21019;&#36896;&#24615;&#30340;AlphaZero&#22269;&#38469;&#35937;&#26827;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#31181;&#35745;&#31639;&#20219;&#21153;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;AI&#31995;&#32479;&#20063;&#20250;&#29359;&#38169;&#35823;&#65292;&#26377;&#30450;&#28857;&#65292;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26032;&#24773;&#20917;&#26102;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#21512;&#29702;&#24615;&#25512;&#21040;&#26497;&#38480;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#30340;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#36890;&#36807;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#21487;&#20197;&#32988;&#36807;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#22909;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#20197;&#22269;&#38469;&#35937;&#26827;&#36825;&#20010;&#34987;&#31216;&#20026;AI&#26524;&#34631;&#30340;&#28216;&#25103;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;AlphaZero (AZ)&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#28508;&#21464;&#26465;&#20214;&#26550;&#26500;&#25193;&#23637;&#23427;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20195;&#29702;&#22242;&#38431;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;AZ_db&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#22810;&#26679;&#24615;&#25216;&#26415;&#23545;AZ_db&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#65292;&#24182;&#36890;&#36807;&#27425;&#21152;&#24615;&#35745;&#21010;&#36873;&#25321;&#26368;&#26377;&#24076;&#26395;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AZ_db&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2308.03188</link><description>&lt;p&gt;
&#33258;&#21160;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22810;&#26679;&#21270;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#30340;&#27010;&#36848;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies. (arXiv:2308.03188v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#23545;&#22810;&#26679;&#21270;&#30340;&#33258;&#25105;&#32416;&#27491;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#34892;&#20026;&#12290;&#33258;&#21160;&#21270;&#21453;&#39304;&#25216;&#26415;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#29992;&#21644;&#21487;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#21463;&#21040;&#20102;&#19981;&#21463;&#27426;&#36814;&#21644;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#30340;&#21066;&#24369;&#65292;&#21253;&#25324;&#24187;&#35273;&#12289;&#19981;&#24544;&#23454;&#30340;&#25512;&#29702;&#21644;&#26377;&#23475;&#20869;&#23481;&#12290;&#32416;&#27491;&#36825;&#20123;&#32570;&#38519;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#32416;&#27491;&#65292;&#21363;&#24341;&#23548;&#25110;&#25351;&#23548;LLM&#33258;&#34892;&#20462;&#22797;&#36755;&#20986;&#38382;&#39064;&#12290;&#21033;&#29992;&#33258;&#21160;&#21453;&#39304;&#30340;&#25216;&#26415;--&#26080;&#35770;&#26159;&#30001;LLM&#33258;&#36523;&#20135;&#29983;&#36824;&#26159;&#30001;&#26576;&#20010;&#22806;&#37096;&#31995;&#32479;&#20135;&#29983;--&#23588;&#20854;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20351;&#22522;&#20110;LLM&#30340;&#35299;&#20915;&#26041;&#26696;&#26356;&#23454;&#38469;&#21644;&#21487;&#37096;&#32626;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#65292;&#19988;&#21482;&#38656;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#26032;&#20852;&#25216;&#26415;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#20998;&#31867;&#20102;&#35768;&#22810;&#26368;&#36817;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#12289;&#29983;&#25104;&#26102;&#21644;&#20107;&#21518;&#32416;&#27491;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#36825;&#19968;&#31574;&#30053;&#30340;&#20027;&#35201;&#24212;&#29992;&#65292;&#24182;&#22312;&#26368;&#21518;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item><item><title>CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.01981</link><description>&lt;p&gt;
CartiMorph:&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01981
&lt;/p&gt;
&lt;p&gt;
CartiMorph&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#20998;&#26512;&#65292;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#20102;&#36719;&#39592;&#30340;&#25439;&#22833;&#21644;&#21402;&#24230;&#65292;&#24182;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#34920;&#38754;&#27861;&#32447;&#30340;&#21402;&#24230;&#26144;&#23556;&#26041;&#27861;&#20855;&#26377;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CartiMorph&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#33181;&#20851;&#33410;&#36719;&#39592;&#24418;&#24577;&#23398;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#23427;&#20197;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#36719;&#39592;&#20122;&#21306;&#22495;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#21253;&#25324;&#20840;&#21402;&#24230;&#36719;&#39592;&#20002;&#22833;&#65288;FCL&#65289;&#30340;&#30334;&#20998;&#27604;&#12289;&#24179;&#22343;&#21402;&#24230;&#12289;&#34920;&#38754;&#31215;&#21644;&#20307;&#31215;&#12290;CartiMorph&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#32452;&#32455;&#20998;&#21106;&#12289;&#27169;&#26495;&#26500;&#24314;&#21644;&#27169;&#26495;&#21040;&#22270;&#20687;&#30340;&#27880;&#20876;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;&#34920;&#38754;&#27861;&#32447;&#30340;&#36719;&#39592;&#21402;&#24230;&#26144;&#23556;&#12289;FCL&#20272;&#35745;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#36719;&#39592;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36719;&#39592;&#21402;&#24230;&#22270;&#22312;&#34180;&#21644;&#21608;&#36793;&#21306;&#22495;&#26174;&#31034;&#20986;&#36739;&#23567;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#25163;&#21160;&#20998;&#21106;&#33719;&#24471;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#25152;&#37319;&#29992;&#30340;&#20998;&#21106;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;FCL&#27979;&#37327;&#30340;&#22343;&#26041;&#26681;&#20559;&#24046;&#23567;&#20110;8%&#65292;&#24182;&#19988;&#19982;&#25163;&#21160;&#20998;&#21106;&#30340;&#25351;&#26631;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#26102;&#38388;&#33719;&#21462;&#30340;LiDAR&#28857;&#20113;&#20013;&#21464;&#21270;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#22330;&#36827;&#34892;&#36830;&#32493;&#24418;&#29366;&#37325;&#24314;&#65292;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#21464;&#21270;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#30340;&#31354;&#38388;&#25903;&#25345;&#21644;&#22122;&#22768;&#65292;&#24182;&#22312;&#26816;&#27979;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.15428</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit neural representation for change detection. (arXiv:2307.15428v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#26102;&#38388;&#33719;&#21462;&#30340;LiDAR&#28857;&#20113;&#20013;&#21464;&#21270;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#22330;&#36827;&#34892;&#36830;&#32493;&#24418;&#29366;&#37325;&#24314;&#65292;&#20197;&#21450;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#21464;&#21270;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#30340;&#31354;&#38388;&#25903;&#25345;&#21644;&#22122;&#22768;&#65292;&#24182;&#22312;&#26816;&#27979;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21516;&#22320;&#29702;&#21306;&#22495;&#30340;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#33719;&#21462;&#30340;&#19968;&#23545;3D&#33322;&#31354;LiDAR&#28857;&#20113;&#20013;&#26816;&#27979;&#21464;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31354;&#38388;&#25903;&#25345;&#19981;&#21305;&#37197;&#21644;&#33719;&#21462;&#31995;&#32479;&#22122;&#22768;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#23581;&#35797;&#22522;&#20110;&#30417;&#30563;&#26041;&#27861;&#26469;&#26816;&#27979;&#28857;&#20113;&#19978;&#30340;&#21464;&#21270;&#65292;&#36825;&#38656;&#35201;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#33719;&#24471;&#30340;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;&#36830;&#32493;&#24418;&#29366;&#37325;&#24314;&#30340;&#31070;&#32463;&#22330;&#65288;NF&#65289;&#21644;&#29992;&#20110;&#20998;&#31867;&#21464;&#21270;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#12290;NF&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#19981;&#21305;&#37197;&#30340;&#31354;&#38388;&#25903;&#25345;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#22686;&#21152;&#39640;&#39057;&#32454;&#33410;&#21644;&#20943;&#23569;&#22122;&#22768;&#12290;&#22312;&#20219;&#24847;&#31354;&#38388;&#23610;&#24230;&#19978;&#27604;&#36739;&#27599;&#20010;&#26102;&#38388;&#25139;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#32452;&#27169;&#25311;LiDAR&#28857;&#20113;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Neural Field (NF) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset of simulated LiDAR point clouds for u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.15016</link><description>&lt;p&gt;
Google Bard&#30340;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#22914;&#20309;&#65311;&#24320;&#25918;&#25361;&#25112;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;Google Bard&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;Bard&#22312;&#21508;&#31181;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#36825;&#20984;&#26174;&#20986;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#19982;OpenAI&#30340;ChatGPT&#25104;&#20026;&#20102;&#24378;&#22823;&#30340;&#31454;&#20105;&#23545;&#25163;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Bard&#26368;&#36817;&#24050;&#32463;&#26356;&#26032;&#65292;&#21487;&#20197;&#22312;&#23545;&#35805;&#36807;&#31243;&#20013;&#22788;&#29702;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36755;&#20837;&#12290;&#37492;&#20110;Bard&#22312;&#22788;&#29702;&#25991;&#26412;&#36755;&#20837;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#35299;&#37322;&#30001;&#25991;&#26412;&#38382;&#39064;&#26465;&#20214;&#19979;&#30340;&#35270;&#35273;&#25968;&#25454;&#65288;&#22270;&#20687;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25506;&#32034;&#26377;&#28508;&#21147;&#25581;&#31034;Bard&#21644;&#20854;&#20182;&#21363;&#23558;&#21457;&#24067;&#30340;&#22810;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#20934;&#30830;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#22797;&#26434;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#26102;&#30340;&#26032;&#35265;&#35299;&#21644;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;15&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#21253;&#25324;&#24120;&#35268;&#12289;&#20266;&#35013;&#12289;&#21307;&#23398;&#12289;&#27700;&#19979;&#21644;&#36965;&#24863;&#25968;&#25454;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;Bard&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#34920;&#26126;&#65292;Bard&#22312;&#36825;&#20123;&#35270;&#35273;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#22659;&#65292;&#31361;&#26174;&#20102;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#30340;&#37325;&#35201;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20542;&#21521;&#20110;&#25214;&#21040;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09829</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#22522;&#20110;&#39057;&#29575;&#30340;&#24555;&#25463;&#36335;&#24452;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20542;&#21521;&#20110;&#25214;&#21040;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#29575;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#26426;&#21046;&#38750;&#24120;&#26377;&#29992;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;NNs&#30340;&#23398;&#20064;&#21160;&#24577;&#19978;&#65292;&#32780;&#24456;&#23569;&#26377;&#20851;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25193;&#23637;&#20102;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#30340;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#20102;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#20855;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NNs&#20542;&#21521;&#20110;&#25214;&#21040;&#20998;&#31867;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#22270;&#20687;&#39564;&#35777;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#31867;&#21035;&#39057;&#29575;&#29305;&#24449;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#21487;&#20197;&#22522;&#20110;&#32441;&#29702;&#25110;&#24418;&#29366;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20309;&#31181;&#26041;&#24335;&#33021;&#22815;&#26368;&#22909;&#22320;&#31616;&#21270;&#30446;&#26631;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.07522</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#38381;&#29615;&#20154;&#24037;&#26234;&#33021;&#24341;&#39046;&#30340;&#22522;&#30784;&#31185;&#23398;&#30340;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07522
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20026;&#22522;&#30784;&#31185;&#23398;&#30340;&#21457;&#29616;&#25552;&#20379;&#26426;&#20250;&#65292;&#36890;&#36807;&#20854;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#30340;&#38381;&#29615;&#26041;&#27861;&#65292;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#27491;&#22312;&#39072;&#35206;&#25216;&#26415;&#21019;&#26032;&#12289;&#20135;&#21697;&#24320;&#21457;&#21644;&#25972;&#20010;&#31038;&#20250;&#12290;&#20154;&#24037;&#26234;&#33021;&#23545;&#25216;&#26415;&#30340;&#36129;&#29486;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#36884;&#24452;&#23454;&#29616;&#65292;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26126;&#30830;&#30340;&#24615;&#33021;&#35780;&#20272;&#26631;&#20934;&#65292;&#33539;&#22260;&#20174;&#27169;&#24335;&#35782;&#21035;&#21644;&#20998;&#31867;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#23454;&#36341;&#21644;&#27169;&#22411;&#21457;&#29616;&#38656;&#35201;&#35775;&#38382;&#39640;&#36136;&#37327;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#22522;&#30784;&#31185;&#23398;&#30340;&#36129;&#29486;&#36739;&#23569;&#12290;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#33021;&#20195;&#34920;&#20102;&#36890;&#36807;&#23450;&#37327;&#27169;&#22411;&#22686;&#24378;&#21644;&#21152;&#36895;&#22522;&#30784;&#28145;&#24230;&#31185;&#23398;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26426;&#20250;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#30740;&#31350;&#20102;&#19968;&#31181;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#12289;&#33258;&#21160;&#21270;&#30340;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#26041;&#27861;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#33258;&#20027;&#29983;&#25104;&#20551;&#35774;&#21644;&#24320;&#25918;&#24335;&#33258;&#20027;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#23545;&#20108;&#25112;&#26102;&#26399;&#30340;&#23494;&#30721;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26368;&#29616;&#23454;&#30340;&#24773;&#22659;&#19979;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#23494;&#30721;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37117;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00501</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#20108;&#25112;&#26102;&#26399;&#23494;&#30721;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying World War II Era Ciphers with Machine Learning. (arXiv:2307.00501v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#23545;&#20108;&#25112;&#26102;&#26399;&#30340;&#23494;&#30721;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26368;&#29616;&#23454;&#30340;&#24773;&#22659;&#19979;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#23494;&#30721;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37117;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21482;&#26377;&#23494;&#25991;&#21487;&#29992;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20998;&#31867;&#36873;&#25321;&#30340;&#20108;&#25112;&#26102;&#26399;&#23494;&#30721;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32771;&#34385;&#30340;&#23494;&#30721;&#21253;&#25324;&#24681;&#23612;&#26684;&#29595;&#65288;Enigma&#65289;&#12289;M-209&#12289;Sigaba&#12289;Purple&#21644;Typex&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#22235;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#27599;&#20010;&#27169;&#22411;&#37117;&#22522;&#20110;&#30452;&#26041;&#22270;&#12289;&#20108;&#20803;&#32452;&#21644;&#21407;&#22987;&#23494;&#25991;&#23383;&#27597;&#24207;&#21015;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#24773;&#26223;&#65306;&#22266;&#23450;&#26126;&#25991;&#21644;&#22266;&#23450;&#23494;&#38053;&#12289;&#38543;&#26426;&#26126;&#25991;&#21644;&#22266;&#23450;&#23494;&#38053;&#12289;&#22266;&#23450;&#26126;&#25991;&#21644;&#38543;&#26426;&#23494;&#38053;&#20197;&#21450;&#38543;&#26426;&#26126;&#25991;&#21644;&#38543;&#26426;&#23494;&#38053;&#19979;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#26368;&#29616;&#23454;&#30340;&#24773;&#26223;&#19979;&#65292;&#32473;&#23450;&#27599;&#20010;&#23494;&#30721;1000&#20010;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
We determine the accuracy with which machine learning and deep learning techniques can classify selected World War II era ciphers when only ciphertext is available. The specific ciphers considered are Enigma, M-209, Sigaba, Purple, and Typex. We experiment with three classic machine learning models, namely, Support Vector Machines (SVM), $k$-Nearest Neighbors ($k$-NN), and Random Forest (RF). We also experiment with four deep learning neural network-based models: Multi-Layer Perceptrons (MLP), Long Short-Term Memory (LSTM), Extreme Learning Machines (ELM), and Convolutional Neural Networks (CNN). Each model is trained on features consisting of histograms, digrams, and raw ciphertext letter sequences. Furthermore, the classification problem is considered under four distinct scenarios: Fixed plaintext with fixed keys, random plaintext with fixed keys, fixed plaintext with random keys, and random plaintext with random keys. Under the most realistic scenario, given 1000 characters per ciph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;&#21452;&#37325;&#31639;&#27861;OptAug-CMDP&#65292;&#29992;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#35299;&#20915;&#20102;&#21407;&#20808;&#31639;&#27861;&#20013;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#32570;&#38519;&#65292;&#35777;&#26126;&#20102;&#20854;&#36951;&#25022;&#20540;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.07001</link><description>&lt;p&gt;
&#22522;&#20110;Lagrangian&#26041;&#27861;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#26080;&#38656;&#21462;&#28040;&#24809;&#32602;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Cancellation-Free Regret Bounds for Lagrangian Approaches in Constrained Markov Decision Processes. (arXiv:2306.07001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;&#21452;&#37325;&#31639;&#27861;OptAug-CMDP&#65292;&#29992;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#35299;&#20915;&#20102;&#21407;&#20808;&#31639;&#27861;&#20013;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#32570;&#38519;&#65292;&#35777;&#26126;&#20102;&#20854;&#36951;&#25022;&#20540;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#24314;&#27169;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20854;&#20013;&#23433;&#20840;&#30446;&#26631;&#30001;&#32422;&#26463;&#20989;&#25968;&#24314;&#27169;&#12290;&#22522;&#20110;Lagrangian&#30340;&#21452;&#37325;&#25110;&#21407;&#22987;&#21452;&#37325;&#31639;&#27861;&#20026;CMDPs&#20013;&#30340;&#23398;&#20064;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#26377;&#38480;&#26102;&#38388;&#27573;&#36951;&#25022;&#30028;&#38480;&#20801;&#35768;&#8220;&#21462;&#28040;&#38169;&#35823;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#36890;&#36807;&#21478;&#19968;&#31181;&#22330;&#26223;&#20013;&#30340;&#20005;&#26684;&#32422;&#26463;&#28385;&#36275;&#26469;&#34917;&#20607;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#32422;&#26463;&#36829;&#35268;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#39537;&#21160;&#30340;&#21452;&#37325;&#31639;&#27861;OptAug-CMDP&#65292;&#35813;&#31639;&#27861;&#21463;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#21551;&#21457;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#26469;&#24357;&#34917;&#36825;&#31181;&#32570;&#38519;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;$K$&#20010;&#25506;&#32034;CMDP&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;$\tilde{O}(\sqrt{K})$&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained Markov Decision Processes (CMDPs) are one of the common ways to model safe reinforcement learning problems, where the safety objectives are modeled by constraint functions. Lagrangian-based dual or primal-dual algorithms provide efficient methods for learning in CMDPs. For these algorithms, the currently known regret bounds in the finite-horizon setting allow for a \textit{cancellation of errors}; that is, one can compensate for a constraint violation in one episode with a strict constraint satisfaction in another episode. However, in practical applications, we do not consider such a behavior safe.  In this paper, we overcome this weakness by proposing a novel model-based dual algorithm \textsc{OptAug-CMDP} for tabular finite-horizon CMDPs. Our algorithm is motivated by the augmented Lagrangian method and can be performed efficiently. We show that during $K$ episodes of exploring the CMDP, our algorithm obtains a regret of $\tilde{O}(\sqrt{K})$ for both the objective and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06208</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#24040;&#22823;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;GPU&#21644;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24555;&#36895;&#12289;&#21450;&#26102;&#30340;&#22788;&#29702;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23376;&#20248;&#26144;&#23556;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#26102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#34892;&#20026;&#12290;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#26144;&#23556;&#26159;&#36890;&#36807;&#22810;&#20010;&#36719;&#20214;&#32452;&#20214;&#36827;&#34892;&#30340;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#12289;&#35774;&#22791;&#24211;&#31561;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35745;&#31639;&#29615;&#22659;&#12290;&#38543;&#30528;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#35774;&#22791;&#31561;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27491;&#30830;&#24615;&#30340;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26356;&#25913;&#36719;&#20214;&#32452;&#20214;&#26469;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#39044;&#27979;&#36755;&#20986;&#24046;&#24322;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#22270;&#20687;&#21644;&#25200;&#21160;&#22270;&#20687;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19977;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.06157</link><description>&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#26694;&#26550;&#36716;&#25442;&#30340;&#25925;&#38556;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#23558;&#27169;&#22411;&#20174;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#65292;&#20174;TensorFlow&#21040;PyTorch&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#30446;&#26631;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;DNNs&#65288;MobileNetV2&#12289;ResNet101&#21644;InceptionV3&#65289;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PyTorch&#12289;Keras&#12289;TensorFlow&#65288;TF&#65289;&#21644;TFLite&#65289;&#20043;&#38388;&#36827;&#34892;&#20102;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#39640;&#36798;100&#65285;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20301;&#25925;&#38556;&#21644;&#20462;&#22797;&#26377;&#32570;&#38519;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#65292;&#37325;&#28857;&#25918;&#22312;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#20998;&#26512;&#38454;&#27573;&#65306;1&#65289;&#36716;&#25442;&#24037;&#20855;&#65292;2&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;3&#65289;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;4&#65289;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#24037;&#20855;&#30340;&#25512;&#33616;&#12289;&#35843;&#35797;&#25216;&#24039;&#20197;&#21450;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#20462;&#22797;&#25152;&#26377;&#27979;&#35797;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;MobileNetV2&#65292;ResNet101&#21644;InceptionV3 &#30340;&#26377;&#32570;&#38519;&#30340;&#36716;&#25442;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.18415</link><description>&lt;p&gt;
&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geometric Algebra Transformers. (arXiv:2305.18415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#26550;&#26500;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#34920;&#31034;&#36755;&#20837;&#36755;&#20986;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#21487;&#32553;&#25918;&#24615;&#12289;&#34920;&#36798;&#24615;&#12289;&#22810;&#21151;&#33021;&#24615;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#25968;&#25454;&#38382;&#39064;&#28041;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;&#35768;&#22810;&#24418;&#24335;&#65292;&#20363;&#22914;&#28857;&#12289;&#26041;&#21521;&#21521;&#37327;&#12289;&#24179;&#38754;&#25110;&#21464;&#25442;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22914;&#27492;&#22810;&#31181;&#20960;&#20309;&#31867;&#22411;, &#21516;&#26102;&#23562;&#37325;&#23427;&#20204;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#36890;&#29992;&#26550;&#26500;&#12290;GATr&#20351;&#29992;&#25237;&#24433;&#20960;&#20309;&#20195;&#25968;&#26469;&#34920;&#31034;&#36755;&#20837;&#12289;&#36755;&#20986;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#20854;&#25552;&#20379;&#24120;&#35265;&#20960;&#20309;&#23545;&#35937;&#30340;&#39640;&#25928;16&#32500;&#21521;&#37327;&#31354;&#38388;&#34920;&#31034;&#20197;&#21450;&#20316;&#29992;&#20110;&#23427;&#20204;&#30340;&#36816;&#31639;&#31526;&#12290;GATr&#26159;&#30456;&#23545;&#20110;E(3)&#65288;3D&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#23545;&#31216;&#32676;&#65289;&#31561;&#21464;&#30340;&#12290;&#20316;&#20026;&#21464;&#25442;&#22120;&#65292;GATr&#21487;&#25193;&#23637;&#12289;&#34920;&#36798;&#20016;&#23500;&#19988;&#22810;&#21151;&#33021;&#12290;&#22312;n&#20307;&#24314;&#27169;&#21644;&#26426;&#22120;&#20154;&#35268;&#21010;&#30340;&#23454;&#39564;&#20013;&#65292;GATr&#30456;&#23545;&#20110;&#38750;&#20960;&#20309;&#22522;&#32447;&#22343;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Problems involving geometric data arise in a variety of fields, including computer vision, robotics, chemistry, and physics. Such data can take numerous forms, such as points, direction vectors, planes, or transformations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric algebra, which offers an efficient 16-dimensional vector space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a transformer, GATr is scalable, expressive, and versatile. In experiments with n-body modeling and robotic planning, GATr shows strong improvements over non-geometric baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.14683</link><description>&lt;p&gt;
&#35770;&#36827;&#21270;&#30952;&#38155;&#12289;&#24179;&#22374;&#26497;&#23567;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On progressive sharpening, flat minima and generalisation. (arXiv:2305.14683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#36827;&#21270;&#30952;&#38155;&#21644;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#20013;&#25439;&#22833;&#26354;&#29575;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#28145;&#24230;&#32593;&#32476;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#39057;&#35889;&#32463;&#39564;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#25439;&#22833;&#40657;&#22622;&#30697;&#38453;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#32852;&#31995;&#36215;&#26469;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#36755;&#20837;-&#36755;&#20986;&#38597;&#20811;&#27604;&#30697;&#38453;&#36817;&#20284;&#20854;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#21033;&#26222;&#35199;&#33576;&#33539;&#25968;&#30340;&#31243;&#24230;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32463;&#39564;&#38597;&#20811;&#27604;&#30697;&#38453;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#20851;&#20110;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#36827;&#21270;&#30952;&#38155;&#29616;&#35937;&#20197;&#21450;&#24179;&#22374;&#26497;&#23567;&#30340;&#27867;&#21270;&#24615;&#36136;&#30340;&#26032;&#25551;&#36848;&#12290;&#23454;&#39564;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20027;&#24352;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach to understanding the relationship between loss curvature and generalisation in deep learning. Specifically, we use existing empirical analyses of the spectrum of deep network loss Hessians to ground an ansatz tying together the loss Hessian and the input-output Jacobian of a deep neural network. We then prove a series of theoretical results which quantify the degree to which the input-output Jacobian of a model approximates its Lipschitz norm over a data distribution, and deduce a novel generalisation bound in terms of the empirical Jacobian. We use our ansatz, together with our theoretical results, to give a new account of the recently observed progressive sharpening phenomenon, as well as the generalisation properties of flat minima. Experimental evidence is provided to validate our claims.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#19968;&#20010;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#65292;&#24182;&#22312;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11582</link><description>&lt;p&gt;
&#20320;&#21548;&#21040;&#30340;&#27491;&#26159;&#20320;&#30475;&#21040;&#30340;&#65306;&#20174;&#22270;&#20687;&#36136;&#37327;&#35780;&#20215;&#20013;&#33719;&#24471;&#30340;&#38899;&#39057;&#36136;&#37327;&#35780;&#20215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You Hear Is What You See: Audio Quality Metrics From Image Quality Metrics. (arXiv:2305.11582v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#19968;&#20010;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#65292;&#24182;&#22312;&#38899;&#20048;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#24863;&#30693;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;&#20449;&#21495;&#34920;&#31034;&#25104;&#39057;&#35889;&#22270;&#20197;&#35780;&#20272;&#38899;&#39057;&#20449;&#21495;&#30340;&#21487;&#34892;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#21548;&#35273;&#21644;&#35270;&#35273;&#36890;&#36335;&#20013;&#31070;&#32463;&#26426;&#21046;&#30340;&#30456;&#20284;&#24615;&#65292;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;&#19968;&#20010;&#20855;&#26377;&#24515;&#29702;&#22768;&#23398;&#21512;&#29702;&#32467;&#26500;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22768;&#38899;&#20449;&#21495;&#30340;&#29305;&#27530;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#38899;&#20048;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#20960;&#20010;&#22522;&#20934;&#24230;&#37327;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#21363;&#24230;&#37327;&#26041;&#27861;&#19982;&#20154;&#31867;&#35780;&#20272;&#32773;&#25152;&#35780;&#20272;&#30340;&#38899;&#39057;&#36136;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the feasibility of utilizing state-of-the-art image perceptual metrics for evaluating audio signals by representing them as spectrograms. The encouraging outcome of the proposed approach is based on the similarity between the neural mechanisms in the auditory and visual pathways. Furthermore, we customise one of the metrics which has a psychoacoustically plausible architecture to account for the peculiarities of sound signals. We evaluate the effectiveness of our proposed metric and several baseline metrics using a music dataset, with promising results in terms of the correlation between the metrics and the perceived quality of audio as rated by human evaluators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#65292;&#20197;&#22635;&#34917;&#38750;&#35821;&#35328;&#20132;&#27969;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08854</link><description>&lt;p&gt;
&#31505;&#22768;&#24456;&#37325;&#35201;&#65306;&#24341;&#20837;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#31505;&#33080;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models. (arXiv:2305.08854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#65292;&#20197;&#22635;&#34917;&#38750;&#35821;&#35328;&#20132;&#27969;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#35821;&#38899;&#30340;&#21160;&#30011;&#22312;&#25552;&#39640;&#30495;&#23454;&#24863;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#38750;&#35821;&#35328;&#20132;&#27969;&#26041;&#38754;&#20173;&#28982;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#38745;&#24577;&#32918;&#20687;&#21644;&#21253;&#21547;&#31505;&#22768;&#30340;&#38899;&#39057;&#21098;&#36753;&#29983;&#25104;&#36924;&#30495;&#30340;&#31505;&#33080;&#24207;&#21015;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20256;&#32479;&#38754;&#37096;&#21160;&#30011;&#26041;&#27861;&#30340;&#22833;&#36133;&#26696;&#20363;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#31505;&#33080;&#35270;&#39057;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#21508;&#26679;&#30340;&#31505;&#22768;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#31505;&#22768;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Speech-driven animation has gained significant traction in recent years, with current methods achieving near-photorealistic results. However, the field remains underexplored regarding non-verbal communication despite evidence demonstrating its importance in human interaction. In particular, generating laughter sequences presents a unique challenge due to the intricacy and nuances of this behaviour. This paper aims to bridge this gap by proposing a novel model capable of generating realistic laughter sequences, given a still portrait and an audio clip containing laughter. We highlight the failure cases of traditional facial animation methods and leverage recent advances in diffusion models to produce convincing laughter videos. We train our model on a diverse set of laughter datasets and introduce an evaluation metric specifically designed for laughter. When compared with previous speech-driven approaches, our model achieves state-of-the-art performance across all metrics, even when the
&lt;/p&gt;</description></item><item><title>GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02422</link><description>&lt;p&gt;
GAMIVAL&#65306;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#35270;&#39057;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content. (arXiv:2305.02422v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02422
&lt;/p&gt;
&lt;p&gt;
GAMIVAL&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28216;&#25103;&#19987;&#29992;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22810;&#31181;&#20248;&#28857;&#12290;&#22312;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;NR VQA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31227;&#21160;&#20113;&#28216;&#25103;&#34892;&#19994;&#36805;&#36895;&#22686;&#38271;&#12290;&#24403;&#28216;&#25103;&#35270;&#39057;&#20174;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;&#21040;&#23458;&#25143;&#31471;&#35774;&#22791;&#26102;&#65292;&#38656;&#35201;&#19968;&#31181;&#21487;&#20197;&#30417;&#27979;&#22833;&#30495;&#35270;&#39057;&#36136;&#37327;&#32780;&#26080;&#38656;&#21442;&#32771;&#35270;&#39057;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#30001;&#35745;&#31639;&#26426;&#22270;&#24418;&#24341;&#25806;&#28210;&#26579;&#30340;&#27969;&#24335;&#28216;&#25103;&#35270;&#39057;&#36136;&#37327;&#30340;&#26080;&#21442;&#32771;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;NR VQA&#65289;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#28216;&#25103;&#20869;&#23481;&#36890;&#24120;&#22312;&#32479;&#35745;&#19978;&#19982;&#33258;&#28982;&#35270;&#39057;&#19981;&#21516;&#65292;&#32570;&#20047;&#32454;&#33410;&#65292;&#24182;&#21253;&#21547;&#35768;&#22810;&#24179;&#28369;&#21306;&#22495;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Gaming Video Quality Evaluator&#65288;GAMIVAL&#65289;&#30340;&#26032;&#22411;&#28216;&#25103;&#19987;&#29992;NR VQA&#27169;&#22411;&#65292;&#32467;&#21512;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#28216;&#25103;&#22833;&#30495;&#22330;&#26223;&#32479;&#35745;&#27169;&#22411;&#12289;&#31070;&#32463;&#22122;&#22768;&#27169;&#22411;&#21644;&#23458;&#35266;&#36136;&#37327;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;GAMIVAL&#24050;&#32463;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#31227;&#21160;&#20113;&#28216;&#25103;&#20869;&#23481;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#22312;&#28216;&#25103;&#20869;&#23481;&#30340;NR VQA&#27169;&#22411;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mobile cloud gaming industry has been rapidly growing over the last decade. When streaming gaming videos are transmitted to customers' client devices from cloud servers, algorithms that can monitor distorted video quality without having any reference video available are desirable tools. However, creating No-Reference Video Quality Assessment (NR VQA) models that can accurately predict the quality of streaming gaming videos rendered by computer graphics engines is a challenging problem, since gaming content generally differs statistically from naturalistic videos, often lacks detail, and contains many smooth regions. Until recently, the problem has been further complicated by the lack of adequate subjective quality databases of mobile gaming content. We have created a new gaming-specific NR VQA model called the Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the advantages of spatial and temporal gaming distorted scene statistics models, a neural noise model, 
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.04027</link><description>&lt;p&gt;
NeBLa: &#20351;&#29992;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#37325;&#24314;&#21475;&#33108;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65306;NeBLa&#65292;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#20013;&#36890;&#36807;&#31070;&#32463;&#21860;&#37202;-&#20848;&#20271;&#29305;&#27861;&#37325;&#24314;&#31934;&#30830;&#30340;3D&#21475;&#33108;&#32467;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;X&#32447;&#29255;&#65288;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#65292;PX&#65289;&#26159;&#24120;&#29992;&#20110;&#29273;&#31185;&#26816;&#26597;&#30340;&#25104;&#20687;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;3D&#38181;&#24418;&#26463;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CBCT&#65289;&#30456;&#27604;&#65292;PX&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;PX&#21482;&#25552;&#20379;&#21475;&#33108;&#32467;&#26500;&#30340;&#20108;&#32500;&#25153;&#24179;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#30495;&#23454;&#30340;PX&#22270;&#20687;&#20272;&#35745;3D&#21475;&#33108;&#32467;&#26500;&#12290;&#30001;&#20110;PX&#21644;CBCT&#25968;&#25454;&#30340;&#21305;&#37197;&#19981;&#22810;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#20102;&#20174;CBCT&#27169;&#25311;&#30340;PX&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#20102;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#32447;&#37319;&#26679;&#26041;&#27861;&#65292;&#21463;&#21040;&#20840;&#26223;&#25918;&#23556;&#32447;&#25104;&#20687;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#21860;&#37202;-&#20848;&#20271;&#29305;&#23450;&#24459;&#23548;&#20986;&#28210;&#26579;&#20989;&#25968;&#29983;&#25104;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#36716;&#25442;&#27169;&#22359;&#65292;&#29983;&#25104;&#27169;&#22359;&#21644;&#31934;&#28860;&#27169;&#22359;&#12290;&#36716;&#25442;&#27169;&#22359;&#23558;&#30495;&#23454;&#30340;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#36716;&#25442;&#20026;&#27169;&#25311;&#30340;&#35757;&#32451;&#22270;&#20687;&#39118;&#26684;&#12290;&#29983;&#25104;&#27169;&#22359;&#21033;&#29992;&#23556;&#32447;&#37319;&#26679;&#26041;&#27861;&#24471;&#21040;&#30340;&#27169;&#25311;&#20840;&#26223;&#25918;&#23556;&#32447;&#22270;&#32422;&#26463;&#19979;&#30340;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#32467;&#26500;&#12290;&#31934;&#28860;&#27169;&#22359;&#25913;&#21892;&#20102;3D&#32467;&#26500;&#30340;&#24179;&#28369;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#20840;&#26223;&#25918;&#23556;&#32447;&#29255;&#25552;&#20379;&#30340;&#26377;&#38480;&#20449;&#24687;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#29273;&#31185;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoramic radiography (panoramic X-ray, PX) is a widely used imaging modality for dental examination. However, its applicability is limited as compared to 3D Cone-beam computed tomography (CBCT), because PX only provides 2D flattened images of the oral structure. In this paper, we propose a new framework which estimates 3D oral structure from real-world PX images. Since there are not many matching PX and CBCT data, we used simulated PX from CBCT for training, however, we used real-world panoramic radiographs at the inference time. We propose a new ray-sampling method to make simulated panoramic radiographs inspired by the principle of panoramic radiography along with the rendering function derived from the Beer-Lambert law. Our model consists of three parts: translation module, generation module, and refinement module. The translation module changes the real-world panoramic radiograph to the simulated training image style. The generation module makes the 3D structure from the input ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;VP&#19982;PATE&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#65292;&#22312;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#20013;&#20063;&#26174;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;VP&#22312;DP&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.12247</link><description>&lt;p&gt;
&#25506;&#32034;&#24046;&#20998;&#38544;&#31169;&#20013;&#35270;&#35273;&#25552;&#31034;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Exploring the Benefits of Visual Prompting in Differential Privacy. (arXiv:2303.12247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;VP&#19982;PATE&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#65292;&#22312;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#20013;&#20063;&#26174;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;VP&#22312;DP&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#19988;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20923;&#32467;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;VP&#26500;&#24314;&#24341;&#20154;&#27880;&#30446;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#25506;&#32034;&#24182;&#23558;VP&#25972;&#21512;&#21040;&#32463;&#20856;&#30340;DP&#35757;&#32451;&#26041;&#27861;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#31616;&#21333;&#24615;&#21644;&#25928;&#29575;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VP&#19982;PATE&#65288;&#19968;&#31181;&#21033;&#29992;&#25945;&#24072;&#38598;&#21512;&#30340;&#30693;&#35782;&#36716;&#31227;&#30340;&#26368;&#20808;&#36827;&#30340;DP&#35757;&#32451;&#26041;&#27861;&#65289;&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#23454;&#39564;&#65292;&#20197;&#36827;&#19968;&#27493;&#25581;&#31034;&#22312;DP&#20013;VP&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;VP&#22312;DP&#32771;&#34385;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2303.08112</link><description>&lt;p&gt;
&#29992;&#35843;&#35856;&#36879;&#38236;&#20174;Transformer&#20013;&#33719;&#21462;&#28508;&#22312;&#30340;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Predictions from Transformers with the Tuned Lens. (arXiv:2303.08112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#30340;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#8212;&#8212;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#39564;&#35777;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36845;&#20195;&#25512;&#29702;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;transformers&#27169;&#22411;&#65292;&#26088;&#22312;&#20102;&#35299;&#27169;&#22411;&#39044;&#27979;&#26159;&#22914;&#20309;&#36880;&#23618;&#36827;&#34892;&#31934;&#21270;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#22359;&#35757;&#32451;&#19968;&#20010;&#20223;&#23556;&#25506;&#38024;&#65292;&#20351;&#24471;&#21487;&#20197;&#23558;&#27599;&#20010;&#38544;&#34255;&#29366;&#24577;&#35299;&#30721;&#25104;&#35789;&#27719;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8220;&#35843;&#35856;&#36879;&#38236;&#8221;&#65292;&#26159;&#8220;&#36923;&#36753;&#36879;&#38236;&#8221;&#25216;&#26415;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21069;&#32773;&#32473;&#20986;&#20102;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#20294;&#24120;&#24120;&#26131;&#30862;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20855;&#26377;&#22810;&#36798;20B&#21442;&#25968;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#34920;&#26126;&#20854;&#27604;&#36923;&#36753;&#36879;&#38236;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#26080;&#20559;&#24615;&#12290;&#36890;&#36807;&#22240;&#26524;&#23454;&#39564;&#26174;&#31034;&#65292;&#35843;&#35856;&#36879;&#38236;&#20351;&#29992;&#30340;&#29305;&#24449;&#19982;&#27169;&#22411;&#26412;&#36523;&#31867;&#20284;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#28508;&#22312;&#39044;&#27979;&#30340;&#36712;&#36857;&#21487;&#20197;&#29992;&#20110;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#24694;&#24847;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/AlignmentResearch/tuned-lens &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \emph{tuned lens}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle.  We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04150</link><description>&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Reinforcement Learning: A Survey. (arXiv:2303.04150v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04150
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#26041;&#38754;&#25152;&#21462;&#24471;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#35757;&#32451;&#26234;&#33021;&#20307;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#26827;&#30424;&#28216;&#25103;&#12289;&#34903;&#26426;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#31561;&#21508;&#31181;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#25935;&#24863;&#36229;&#21442;&#25968;&#23548;&#33268;&#30340;&#33030;&#24369;&#25910;&#25947;&#29305;&#24615;&#65292;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26102;&#38388;&#20998;&#37197;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#24615;&#25506;&#32034;&#19981;&#36275;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#29992;&#20998;&#37197;&#22256;&#38590;&#20197;&#21450;&#22870;&#21169;&#20914;&#31361;&#30446;&#26631;&#12290;&#36827;&#21270;&#35745;&#31639;&#32500;&#25252;&#30528;&#19968;&#32676;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24050;&#23637;&#29616;&#20986;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#38598;&#25104;&#36827;&#21270;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.11197</link><description>&lt;p&gt;
&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#19982;&#38543;&#26426;&#25238;&#21160;
&lt;/p&gt;
&lt;p&gt;
Quantized Low-Rank Multivariate Regression with Random Dithering. (arXiv:2302.11197v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65292;&#36890;&#36807;&#37319;&#29992;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#37327;&#21270;&#20165;&#23545;&#20056;&#27861;&#22240;&#23376;&#30053;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#22810;&#20803;&#22238;&#24402;&#65288;LRMR&#65289;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#39640;&#24230;&#30456;&#20851;&#30340;&#20219;&#21153;&#20316;&#20026;&#20855;&#26377;&#20302;&#31209;&#20808;&#39564;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#38382;&#39064;&#36827;&#34892;&#32452;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#21270;&#30340;LRMR&#65292;&#36825;&#26159;&#19968;&#31181;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#21709;&#24212;&#21644;/&#25110;&#21327;&#21464;&#37327;&#34987;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20272;&#35745;&#22522;&#30784;&#31995;&#25968;&#30697;&#38453;&#12290;&#20026;&#20102;&#20351;&#33021;&#22815;&#23454;&#29616;&#20219;&#24847;&#23567;&#35823;&#24046;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22343;&#21248;&#37327;&#21270;&#19982;&#38543;&#26426;&#25238;&#21160;&#65292;&#21363;&#22312;&#37327;&#21270;&#20043;&#21069;&#21521;&#25968;&#25454;&#28155;&#21152;&#36866;&#24403;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21709;&#24212;&#20351;&#29992;&#22343;&#21248;&#25238;&#21160;&#65292;&#21327;&#21464;&#37327;&#20351;&#29992;&#19977;&#35282;&#25238;&#21160;&#12290;&#22522;&#20110;&#37327;&#21270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;Lasso&#21644;&#27491;&#21017;&#21270;Lasso&#20272;&#35745;&#22120;&#65292;&#24182;&#25512;&#23548;&#20102;&#38750;&#28176;&#36817;&#24615;&#35823;&#24046;&#30028;&#12290;&#36890;&#36807;&#25238;&#21160;&#30340;&#24110;&#21161;&#65292;&#20272;&#35745;&#22120;&#23454;&#29616;&#20102;&#26368;&#23567;&#26368;&#20248;&#29575;&#65292;&#32780;&#37327;&#21270;&#20165;&#30053;&#24494;&#24694;&#21270;&#20102;&#20056;&#27861;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank multivariate regression (LRMR) is an important statistical learning model that combines highly correlated tasks as a multiresponse regression problem with low-rank priori on the coefficient matrix. In this paper, we study quantized LRMR, a practical setting where the responses and/or the covariates are discretized to finite precision. We focus on the estimation of the underlying coefficient matrix. To make consistent estimator that could achieve arbitrarily small error possible, we employ uniform quantization with random dithering, i.e., we add appropriate random noise to the data before quantization. Specifically, uniform dither and triangular dither are used for responses and covariates, respectively. Based on the quantized data, we propose the constrained Lasso and regularized Lasso estimators, and derive the non-asymptotic error bounds. With the aid of dithering, the estimators achieve minimax optimal rate, while quantization only slightly worsens the multiplicative factor
&lt;/p&gt;</description></item><item><title>G-Signatures&#26159;&#19968;&#31181;&#20840;&#23616;&#22270;&#20256;&#25773;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#31614;&#21517;&#23454;&#29616;&#12290;&#36890;&#36807;&#23884;&#20837;&#22270;&#32467;&#26500;&#20449;&#24687;&#20026;&#28508;&#22312;&#31354;&#38388;&#36335;&#24452;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#22788;&#29702;&#20840;&#23616;&#22270;&#23646;&#24615;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.08811</link><description>&lt;p&gt;
G-Signatures&#65306;&#20840;&#23616;&#22270;&#20256;&#25773;&#19982;&#38543;&#26426;&#31614;&#21517;
&lt;/p&gt;
&lt;p&gt;
G-Signatures: Global Graph Propagation With Randomized Signatures. (arXiv:2302.08811v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08811
&lt;/p&gt;
&lt;p&gt;
G-Signatures&#26159;&#19968;&#31181;&#20840;&#23616;&#22270;&#20256;&#25773;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#31614;&#21517;&#23454;&#29616;&#12290;&#36890;&#36807;&#23884;&#20837;&#22270;&#32467;&#26500;&#20449;&#24687;&#20026;&#28508;&#22312;&#31354;&#38388;&#36335;&#24452;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#22788;&#29702;&#20840;&#23616;&#22270;&#23646;&#24615;&#12290;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#21457;&#23637;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;GNNs&#21463;&#21040;&#36807;&#24230;&#24179;&#28369;&#33410;&#28857;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#38590;&#20197;&#35299;&#20915;&#19982;&#20840;&#23616;&#22270;&#23646;&#24615;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;G-Signatures&#65292;&#36890;&#36807;&#38543;&#26426;&#31614;&#21517;&#23454;&#29616;&#20840;&#23616;&#22270;&#20256;&#25773;&#12290;G-Signatures&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#36716;&#25442;&#27010;&#24565;&#65292;&#23558;&#22270;&#32467;&#26500;&#21270;&#20449;&#24687;&#23884;&#20837;&#21040;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#28508;&#22312;&#31354;&#38388;&#36335;&#24452;&#30340;&#36335;&#24452;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#28508;&#22312;&#31354;&#38388;&#36335;&#24452;&#26144;&#23556;&#30340;&#27010;&#24565;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36845;&#20195;&#22320;&#36941;&#21382;&#28508;&#22312;&#31354;&#38388;&#36335;&#24452;&#65292;&#24182;&#20840;&#23616;&#22320;&#22788;&#29702;&#20449;&#24687;&#12290;G-Signatures&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#20840;&#23616;&#22270;&#23646;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#22823;&#35268;&#27169;&#22270;&#38382;&#39064;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;G-Signatures&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have evolved into one of the most popular deep learning architectures. However, GNNs suffer from over-smoothing node information and, therefore, struggle to solve tasks where global graph properties are relevant. We introduce G-Signatures, a novel graph learning method that enables global graph propagation via randomized signatures. G-Signatures use a new graph conversion concept to embed graph structured information which can be interpreted as paths in latent space. We further introduce the idea of latent space path mapping. This allows us to iteratively traverse latent space paths, and, thus globally process information. G-Signatures excel at extracting and processing global graph properties, and effectively scale to large graph problems. Empirically, we confirm the advantages of G-Signatures at several classification and regression tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#36824;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11479</link><description>&lt;p&gt;
&#22806;&#26143;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Alien Coding. (arXiv:2301.11479v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#36824;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#26368;&#21021;&#38543;&#26426;&#29983;&#25104;&#31243;&#24207;&#65292;&#28982;&#21518;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#23398;&#20064;&#24207;&#21015;&#21644;&#24050;&#21457;&#29616;&#31243;&#24207;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22120;&#20026;&#27599;&#20010;OEIS&#24207;&#21015;&#25552;&#20986;&#35768;&#22810;&#26032;&#31243;&#24207;&#12290;&#35813;&#31639;&#27861;&#33258;&#24049;&#21457;&#29616;&#20102;&#36229;&#36807;78000&#20010;OEIS&#24207;&#21015;&#30340;&#31243;&#24207;&#65292;&#26377;&#26102;&#24320;&#21457;&#20986;&#38750;&#20256;&#32479;&#30340;&#32534;&#31243;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23454;&#39564;&#20013;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#34892;&#20026;&#21644;&#21457;&#26126;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a self-learning algorithm for synthesizing programs for OEIS sequences. The algorithm starts from scratch initially generating programs at random. Then it runs many iterations of a self-learning loop that interleaves (i) training neural machine translation to learn the correspondence between sequences and the programs discovered so far, and (ii) proposing many new programs for each OEIS sequence by the trained neural machine translator. The algorithm discovers on its own programs for more than 78000 OEIS sequences, sometimes developing unusual programming methods. We analyze its behavior and the invented programs in several experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2301.08491</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement Learning. (arXiv:2301.08491v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#25311;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#36947;&#24503;&#36873;&#25321;&#65292;&#35774;&#35745;&#20102;&#19968;&#22871;&#36947;&#24503;&#22870;&#21169;&#32467;&#26500;&#65292;&#26088;&#22312;&#20998;&#26512;&#21644;&#30740;&#31350;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26234;&#33021;&#20195;&#29702;&#20013;&#32435;&#20837;&#36947;&#24503;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#23637;&#29616;&#12290;&#21516;&#26102;&#20063;&#24378;&#35843;&#65292;&#25353;&#29031;&#20219;&#20309;&#19968;&#31181;&#36947;&#24503;&#35266;&#23450;&#20041;&#39030;&#23618;&#30340;AI&#20262;&#29702;&#32422;&#26463;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#65292;&#24182;&#19988;&#20250;&#24102;&#26469;&#39118;&#38505;&#12290;&#20174;&#24213;&#23618;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25110;&#35768;&#26356;&#36866;&#21512;&#30740;&#31350;&#21644;&#24320;&#21457;AI&#20195;&#29702;&#30340;&#36947;&#24503;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20998;&#26512;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#36947;&#24503;&#22870;&#21169;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#23454;&#34892;&#34892;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#26032;&#20852;&#34892;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#21644;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#36215;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26681;&#25454;&#36947;&#24503;&#29702;&#35770;&#30340;&#22870;&#21169;&#36827;&#34892;&#30340;&#36873;&#25321;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31616;&#21270;&#20294;&#20195;&#34920;&#19968;&#32452;&#20851;&#38190;&#20262;&#29702;&#31995;&#32479;&#30340;&#22870;&#21169;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#21306;&#20998;&#21518;&#26524;&#21644;&#35268;&#33539;&#20262;&#29702;&#30340;&#36947;&#24503;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20197;&#21019;&#24314;&#26032;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22312;&#31038;&#20250;&#22256;&#22659;&#19979;&#36827;&#34892;&#20869;&#22312;&#21160;&#26426;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35780;&#20272;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22797;&#21046;&#24182;&#25193;&#23637;&#26377;&#20851;&#36947;&#24503;&#36873;&#25321;&#30340;&#25991;&#29486;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21457;&#29616;&#65292;&#24182;&#33021;&#22815;&#20986;&#29616;&#20197;&#21069;&#26410;&#26366;&#25253;&#36947;&#30340;&#26032;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical uses of Artificial Intelligence (AI) in the real world have demonstrated the importance of embedding moral choices into intelligent agents. They have also highlighted that defining top-down ethical constraints on AI according to any one type of morality is extremely challenging and can pose risks. A bottom-up learning approach may be more appropriate for studying and developing ethical behavior in AI agents. In particular, we believe that an interesting and insightful starting point is the analysis of emergent behavior of Reinforcement Learning (RL) agents that act according to a predefined set of moral rewards in social dilemmas.  In this work, we present a systematic analysis of the choices made by intrinsically-motivated RL agents whose rewards are based on moral theories. We aim to design reward structures that are simplified yet representative of a set of key ethical systems. Therefore, we first define moral reward functions that distinguish between consequence- and norm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#20102;DeepDFA&#26694;&#26550;&#21644;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#20195;&#30721;&#35821;&#20041;&#30340;&#26356;&#39640;&#25928;&#25429;&#25417;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#24615;&#33021;&#12290;DeepDFA&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#19988;&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;75&#20493;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.08108</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dataflow Analysis-Inspired Deep Learning for Efficient Vulnerability Detection. (arXiv:2212.08108v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#28431;&#27934;&#26816;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#20102;DeepDFA&#26694;&#26550;&#21644;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#20195;&#30721;&#35821;&#20041;&#30340;&#26356;&#39640;&#25928;&#25429;&#25417;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#24615;&#33021;&#12290;DeepDFA&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#19988;&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;75&#20493;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#30740;&#31350;&#20013;&#36229;&#36807;&#20102;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26368;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;token&#30340;transformer&#27169;&#22411;&#65292;&#36825;&#19981;&#26159;&#25429;&#25417;&#28431;&#27934;&#26816;&#27979;&#25152;&#38656;&#30340;&#20195;&#30721;&#35821;&#20041;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#31243;&#24207;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#27969;&#20998;&#26512;&#65292;&#21487;&#20197;&#26681;&#25454;&#20854;&#26681;&#26412;&#21407;&#22240;&#26816;&#27979;&#20986;&#35768;&#22810;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27492;&#31867;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#28431;&#27934;&#26816;&#27979;&#31639;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeepDFA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#21644;&#19968;&#31181;&#23884;&#20837;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#22270;&#23398;&#20064;&#27169;&#25311;&#25968;&#25454;&#27969;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepDFA&#26082;&#20855;&#26377;&#24615;&#33021;&#21448;&#20855;&#26377;&#25928;&#29575;&#12290;DeepDFA&#36229;&#36807;&#20102;&#25152;&#26377;&#38750;transformer&#22522;&#32447;&#27169;&#22411;&#12290;&#23427;&#30340;&#35757;&#32451;&#26102;&#38388;&#21482;&#38656;9&#20998;&#38047;&#65292;&#27604;&#20855;&#26377;&#26368;&#39640;&#24615;&#33021;&#30340;&#22522;&#32447;&#27169;&#22411;&#24555;75&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based vulnerability detection has shown great performance and, in some studies, outperformed static analysis tools. However, the highest-performing approaches use token-based transformer models, which are not the most efficient to capture code semantics required for vulnerability detection. Classical program analysis techniques such as dataflow analysis can detect many types of bugs based on their root causes. In this paper, we propose to combine such causal-based vulnerability detection algorithms with deep learning, aiming to achieve more efficient and effective vulnerability detection. Specifically, we designed DeepDFA, a dataflow analysis-inspired graph learning framework and an embedding technique that enables graph learning to simulate dataflow computation. We show that DeepDFA is both performant and efficient. DeepDFA outperformed all non-transformer baselines. It was trained in 9 minutes, 75x faster than the highest-performing baseline model. When using only 50+ v
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#37319;&#29992;&#20102;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#31665;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.06074</link><description>&lt;p&gt;
&#24102;&#26377;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Regression with Label Differential Privacy. (arXiv:2212.06074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#21270;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#37319;&#29992;&#20102;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#30340;&#31665;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20445;&#35777;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#26631;&#31614;&#20540;&#30340;&#20840;&#23616;&#20808;&#39564;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#21487;&#20197;&#31169;&#23494;&#22320;&#33719;&#21462;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22312;&#32473;&#23450;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#19979;&#26368;&#20248;&#30340;&#26631;&#31614;DP&#38543;&#26426;&#21270;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#26426;&#21046;&#37319;&#29992;&#8220;&#23545;&#31665;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#8221;&#30340;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23547;&#25214;&#26368;&#20248;&#31665;&#20540;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a "randomized response on bins", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#21453;&#21521;&#20256;&#25773;&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21457;&#29616;&#24403;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2212.04614</link><description>&lt;p&gt;
&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#22909;&#65311;&#29983;&#29289;&#23398;&#20064;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop. (arXiv:2212.04614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#21453;&#21521;&#20256;&#25773;&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21457;&#29616;&#24403;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#19981;&#34987;&#35748;&#20026;&#26159;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#20197;&#26469;&#65292;&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#23427;&#20204;&#37117;&#27604;BP&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20811;&#26381;BP&#30340;&#29983;&#29289;&#23398;&#19981;&#21512;&#29702;&#24615;&#65292;&#20351;&#29992;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24378;&#28872;&#21160;&#26426;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;BP&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20840;&#38754;&#27604;&#36739;&#65292;&#20197;&#22238;&#31572;&#29983;&#29289;&#23398;&#20064;&#26159;&#21542;&#27604;BP&#25552;&#20379;&#39069;&#22806;&#30340;&#22909;&#22788;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#27979;&#35797;&#29983;&#29289;&#31639;&#27861;&#65292;&#22914;&#20165;&#20351;&#29992;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#26102;&#36164;&#28304;&#32422;&#26463;&#12289;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31232;&#30095;&#21270;&#20197;&#21450;&#21521;&#36755;&#20837;&#26679;&#26412;&#28155;&#21152;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#29983;&#29289;&#31639;&#27861;&#36229;&#36807;BP&#30340;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#22312;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;BP&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bio-inspired learning has been gaining popularity recently given that Backpropagation (BP) is not considered biologically plausible. Many algorithms have been proposed in the literature which are all more biologically plausible than BP. However, apart from overcoming the biological implausibility of BP, a strong motivation for using Bio-inspired algorithms remains lacking. In this study, we undertake a holistic comparison of BP vs. multiple Bio-inspired algorithms to answer the question of whether Bio-learning offers additional benefits over BP. We test Bio-algorithms under different design choices such as access to only partial training data, resource constraints in terms of the number of training epochs, sparsification of the neural network parameters and addition of noise to input samples. Through these experiments, we notably find two key advantages of Bio-algorithms over BP. Firstly, Bio-algorithms perform much better than BP when the entire training dataset is not supplied. Four 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.00736</link><description>&lt;p&gt;
&#19968;&#20010;&#25351;&#25968;&#22686;&#38271;&#30340;&#36890;&#29992;&#37327;&#23376;&#30005;&#36335;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
An exponentially-growing family of universal quantum circuits. (arXiv:2212.00736v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#20294;&#23427;&#23384;&#22312;&#19968;&#23450;&#30340;&#29702;&#35770;&#21644;&#30828;&#20214;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#28040;&#22833;&#26799;&#24230;&#38382;&#39064;&#25110;&#31216;&#20026;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#20351;&#24471;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#30005;&#36335;&#65292;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#33021;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#21487;&#20197;&#20351;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#12290;&#21478;&#22806;&#65292;&#29420;&#31435;&#30340;&#35282;&#24230;&#23884;&#20837;&#30417;&#30563;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#19982;&#32534;&#30721;&#28145;&#24230;&#21644;&#32534;&#30721;&#24212;&#29992;&#20110;&#30340;&#24182;&#34892;&#27604;&#29305;&#25968;&#30452;&#25509;&#30456;&#20851;&#30340;&#25130;&#26029;&#20613;&#37324;&#21494;&#32423;&#25968;&#12290;&#20613;&#37324;&#21494;&#32423;&#25968;&#30340;&#27425;&#25968;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20854;&#20613;&#37324;&#21494;&#32423;&#25968;&#30340;&#27425;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#65306;&#39034;&#24207;&#21644;&#24182;&#34892;&#30340;&#25351;&#25968;&#22686;&#38271;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#26102;&#39640;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#22686;&#21152;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning has become an area of growing interest but has certain theoretical and hardware-specific limitations. Notably, the problem of vanishing gradients, or barren plateaus, renders the training impossible for circuits with high qubit counts, imposing a limit on the number of qubits that data scientists can use for solving problems. Independently, angle-embedded supervised quantum neural networks were shown to produce truncated Fourier series with a degree directly dependent on two factors: the depth of the encoding and the number of parallel qubits the encoding applied to. The degree of the Fourier series limits the model expressivity. This work introduces two new architectures whose Fourier degrees grow exponentially: the sequential and parallel exponential quantum machine learning architectures. This is done by efficiently using the available Hilbert space when encoding, increasing the expressivity of the quantum encoding. Therefore, the exponential growth allows s
&lt;/p&gt;</description></item><item><title>RecXplainer&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.14935</link><description>&lt;p&gt;
RecXplainer: &#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RecXplainer: Amortized Attribute-based Personalized Explanations for Recommender Systems. (arXiv:2211.14935v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14935
&lt;/p&gt;
&lt;p&gt;
RecXplainer&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#20998;&#25674;&#23646;&#24615;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#20197;&#35299;&#20915;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#20043;&#38388;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#35768;&#22810;&#20132;&#20114;&#65292;&#24433;&#21709;&#30528;&#25105;&#20204;&#36141;&#29289;&#12289;&#27983;&#35272;YouTube&#25110;TikTok&#26102;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;&#37202;&#24215;&#24179;&#21488;&#26102;&#23637;&#31034;&#32473;&#25105;&#20204;&#30340;&#39184;&#39302;&#21644;&#37202;&#24215;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#26159;&#22522;&#20110;&#19987;&#26377;&#21644;&#24320;&#28304;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#24222;&#22823;&#19988;&#19981;&#36879;&#26126;&#30340;&#27169;&#22411;&#12290;&#33258;&#28982;&#32780;&#28982;&#22320;&#65292;&#22312;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#26041;&#38754;&#24341;&#21457;&#20102;&#20449;&#20219;&#38382;&#39064;&#65306;&#31995;&#32479;&#26159;&#21542;&#27491;&#24120;&#24037;&#20316;&#65292;&#20026;&#20160;&#20040;&#29992;&#25143;&#25910;&#21040;&#65288;&#25110;&#26410;&#25910;&#21040;&#65289;&#29305;&#23450;&#30340;&#25512;&#33616;&#65311;&#22312;&#25512;&#33616;&#26049;&#36793;&#25552;&#20379;&#35299;&#37322;&#21487;&#20197;&#20943;&#36731;&#19968;&#20123;&#36825;&#20123;&#20851;&#27880;&#12290;&#30446;&#21069;&#36741;&#21161;&#25512;&#33616;&#31995;&#32479;&#21453;&#39304;&#30340;&#29616;&#29366;&#35201;&#20040;&#26159;&#29992;&#25143;&#29305;&#23450;&#30340;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#8220;&#36141;&#20080;&#21830;&#21697;B&#30340;&#29992;&#25143;&#20063;&#36141;&#20080;&#20102;&#21830;&#21697;A&#8221;&#65289;&#65292;&#35201;&#20040;&#26159;&#29289;&#21697;&#29305;&#23450;&#30340;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#20204;&#25512;&#33616;&#21830;&#21697;A&#26159;&#22240;&#20026;&#24744;&#35266;&#30475;/&#36141;&#20080;&#20102;&#21830;&#21697;B&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23558;&#20010;&#24615;&#21270;&#30340;&#32972;&#26223;&#20449;&#24687;&#24102;&#20837;&#20182;&#20204;&#30340;&#25628;&#32034;&#20307;&#39564;&#20013;&#65292;&#23558;&#19968;&#20010;&#29289;&#21697;&#30340;&#20215;&#20540;&#35270;&#20026;&#35813;&#29289;&#21697;&#30340;&#20989;&#25968;.
&lt;/p&gt;
&lt;p&gt;
Recommender systems influence many of our interactions in the digital world -- impacting how we shop for clothes, sorting what we see when browsing YouTube or TikTok, and determining which restaurants and hotels we are shown when using hospitality platforms. Modern recommender systems are large, opaque models trained on a mixture of proprietary and open-source datasets. Naturally, issues of trust arise on both the developer and user side: is the system working correctly, and why did a user receive (or not receive) a particular recommendation? Providing an explanation alongside a recommendation alleviates some of these concerns. The status quo for auxiliary recommender system feedback is either user-specific explanations (e.g., "users who bought item B also bought item A") or item-specific explanations (e.g., "we are recommending item A because you watched/bought item B"). However, users bring personalized context into their search experience, valuing an item as a function of that item'
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;&#37319;&#29992;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#65292;&#21487;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2211.09929</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Contrastive Credibility Propagation for Reliable Semi-Supervised Learning. (arXiv:2211.09929v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09929
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;&#37319;&#29992;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#65292;&#21487;&#22312;&#21508;&#31181;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26631;&#31614;&#23481;&#26131;&#20986;&#38169;&#65292;&#36825;&#20351;&#24471;&#21322;&#30417;&#30563;&#23398;&#20064;(Semi-Supervised Learning, SSL)&#21464;&#24471;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;&#31639;&#27861;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#26080;&#27861;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#20116;&#31181;&#24120;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#21322;&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#22330;&#26223;&#65306;&#23569;&#26631;&#31614;&#26679;&#26412;&#12289;&#24320;&#25918;&#22495;&#26679;&#26412;&#12289;&#22024;&#26434;&#26631;&#31614;&#12289;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#38598;&#21512;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#19981;&#22343;&#34913;/&#38169;&#20301;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#21487;&#20449;&#24230;&#20256;&#25773;(Contrastive Credibility Propagation, CCP)&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#30340;&#20256;&#23548;&#24335;&#20266;&#26631;&#31614;&#32454;&#21270;&#23454;&#29616;&#28145;&#24230;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;CCP&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#32479;&#19968;&#22312;&#19968;&#36215;&#65292;&#30446;&#30340;&#26159;&#22312;&#20219;&#20309;&#25968;&#25454;&#22330;&#26223;&#20013;&#21487;&#38752;&#22320;&#36229;&#36807;&#26377;&#30417;&#30563;&#22522;&#20934;&#12290;&#19982;&#19987;&#27880;&#20110;&#23376;&#38598;&#22330;&#26223;&#30340;&#20043;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;CCP&#22312;&#25152;&#26377;&#22330;&#26223;&#20013;&#29420;&#29305;&#22320;&#36229;&#36807;&#20102;&#26377;&#30417;&#30563;&#22522;&#20934;&#65292;&#25903;&#25345;&#22312;&#26631;&#27880;&#25968;&#25454;&#25110;&#26080;&#26631;&#31614;&#25968;&#25454;&#36136;&#37327;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#30340;&#23454;&#36341;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing labels for unlabeled data is error-prone, making semi-supervised learning (SSL) troublesome. Often, little is known about when and why an algorithm fails to outperform a supervised baseline. Using benchmark datasets, we craft five common real-world SSL data scenarios: few-label, open-set, noisy-label, and class distribution imbalance/misalignment in the labeled and unlabeled sets. We propose a novel algorithm called Contrastive Credibility Propagation (CCP) for deep SSL via iterative transductive pseudo-label refinement. CCP unifies semi-supervised learning and noisy label learning for the goal of reliably outperforming a supervised baseline in any data scenario. Compared to prior methods which focus on a subset of scenarios, CCP uniquely outperforms the supervised baseline in all scenarios, supporting practitioners when the qualities of labeled or unlabeled data are unknown.
&lt;/p&gt;</description></item><item><title>Diffiner&#26159;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#32463;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#21518;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17287</link><description>&lt;p&gt;
Diffiner: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#29983;&#25104;&#32454;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement. (arXiv:2210.17287v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17287
&lt;/p&gt;
&lt;p&gt;
Diffiner&#26159;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#32463;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#21518;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;DNN&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38477;&#20302;&#25152;&#29983;&#25104;&#36755;&#20986;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;Diffiner&#65292;&#26088;&#22312;&#25913;&#21892;&#36890;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#36807;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#20165;&#21253;&#21547;&#28165;&#26224;&#35821;&#38899;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#26377;&#25928;&#22320;&#23558;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#26032;&#29983;&#25104;&#30340;&#28165;&#26224;&#37096;&#20998;&#19982;&#20043;&#21069;&#30340;SE&#26041;&#27861;&#36896;&#25104;&#30340;&#36864;&#21270;&#21644;&#22833;&#30495;&#37096;&#20998;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#20135;&#29983;&#32454;&#21270;&#30340;&#35821;&#38899;&#12290;&#19968;&#26086;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#35757;&#32451;&#20026;&#19968;&#32452;&#28165;&#26224;&#30340;&#35821;&#38899;&#65292;&#23427;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;SE&#27169;&#22359;&#19987;&#38376;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#21487;&#20197;&#26159;&#30456;&#23545;&#20110;SE&#26041;&#27861;&#30340;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#21518;&#22788;&#29702;&#27169;&#22359;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural network (DNN)-based speech enhancement (SE) methods outperform the previous non-DNN-based ones, they often degrade the perceptual quality of generated outputs. To tackle this problem, we introduce a DNN-based generative refiner, Diffiner, aiming to improve perceptual speech quality pre-processed by an SE method. We train a diffusion-based generative model by utilizing a dataset consisting of clean speech only. Then, our refiner effectively mixes clean parts newly generated via denoising diffusion restoration into the degraded and distorted parts caused by a preceding SE method, resulting in refined speech. Once our refiner is trained on a set of clean speech, it can be applied to various SE methods without additional training specialized for each SE module. Therefore, our refiner can be a versatile post-processing module w.r.t. SE methods and has high potential in terms of modularity. Experimental results show that our method improved perceptual speech quality rega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.10264</link><description>&lt;p&gt;
SignReLU&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#36924;&#36817;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#30340;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#20102;DNN&#20013;&#31070;&#32463;&#20803;&#22914;&#20309;&#22788;&#29702;&#36755;&#20837;&#20449;&#21495;&#12290;&#23427;&#20204;&#23545;&#20110;&#23398;&#20064;&#38750;&#32447;&#24615;&#21464;&#25442;&#21644;&#22312;&#36830;&#32493;&#31070;&#32463;&#20803;&#23618;&#20043;&#38388;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#32773;&#20204;&#36890;&#36807;&#30740;&#31350;DNN&#30340;&#36924;&#36817;&#33021;&#21147;&#26469;&#35299;&#37322;&#20854;&#24378;&#22823;&#21644;&#25104;&#21151;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;SignReLU&#30340;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#65292;&#25506;&#32034;&#20102;DNN&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;SignReLU&#32593;&#32476;&#22312;&#36924;&#36817;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26377;&#29702;&#25968;&#21644;ReLU&#32593;&#32476;&#12290;&#25968;&#20540;&#23454;&#39564;&#27604;&#36739;&#20102;SignReLU&#19982;&#29616;&#26377;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;Leaky ReLU&#21644;ELU&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;SignReLU&#30340;&#31454;&#20105;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#20013;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#36827;&#34892;&#27604;&#36739;&#21457;&#29616;&#65292;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2209.14624</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65311;&#19968;&#20010;&#20851;&#20110;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#20013;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#36827;&#34892;&#27604;&#36739;&#21457;&#29616;&#65292;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#22240;&#20026;&#20154;&#20204;&#21457;&#29616;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23433;&#20840;&#22320;&#21024;&#38500;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22823;&#37327;&#26435;&#37325;&#12290;&#33258;&#37027;&#26102;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20462;&#21098;&#26041;&#27861;&#65292;&#27599;&#19968;&#31181;&#37117;&#22768;&#31216;&#27604;&#21069;&#19968;&#31181;&#26356;&#22909;&#12290;&#20170;&#22825;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#37117;&#20381;&#36182;&#20110;&#20351;&#29992;&#37325;&#35201;&#24615;&#20998;&#25968;&#12289;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#33719;&#21462;&#21453;&#39304;&#25110;&#22522;&#20110;&#21551;&#21457;&#24335;&#20462;&#21098;&#35268;&#21017;&#31561;&#22797;&#26434;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#31181;&#24341;&#20837;&#22797;&#26434;&#24615;&#26159;&#21542;&#30495;&#30340;&#26377;&#24517;&#35201;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#20462;&#21098;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;(Global MP)&#12290;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26681;&#25454;&#26435;&#37325;&#30340;&#22823;&#23567;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#24182;&#20462;&#21098;&#26368;&#23567;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#22312;&#20854;&#21407;&#22987;&#24418;&#24335;&#20013;&#65292;&#23427;&#26159;&#26368;&#31616;&#21333;&#30340;&#20462;&#21098;&#25216;&#26415;&#20043;&#19968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#24182;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#31181;&#26550;&#26500;&#27169;&#25311;&#20102;&#20174;&#36817;&#20284;&#21464;&#20998;&#25110;&#24369;&#24418;&#24335;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#25968;&#20540;&#35299;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12871</link><description>&lt;p&gt;
&#21464;&#20998;&#25311;&#24577;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Variationally Mimetic Operator Networks. (arXiv:2209.12871v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#31181;&#26550;&#26500;&#27169;&#25311;&#20102;&#20174;&#36817;&#20284;&#21464;&#20998;&#25110;&#24369;&#24418;&#24335;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#25968;&#20540;&#35299;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31639;&#23376;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#36817;&#20284;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26377;&#24076;&#26395;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#20123;&#32593;&#32476;&#23558;&#25551;&#36848;&#26448;&#26009;&#23646;&#24615;&#12289;&#24378;&#36843;&#20989;&#25968;&#21644;&#36793;&#30028;&#25968;&#25454;&#30340;&#36755;&#20837;&#20989;&#25968;&#26144;&#23556;&#21040;PDE&#30340;&#35299;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#27169;&#25311;&#20102;&#20174;&#36817;&#20284;&#21464;&#20998;&#25110;&#24369;&#24418;&#24335;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#25968;&#20540;&#35299;&#30340;&#24418;&#24335;&#12290;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#36890;&#29992;&#26925;&#22278;PDE&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#21464;&#20998;&#25311;&#24577;&#31639;&#23376;&#32593;&#32476;&#65288;VarMiON&#65289;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONet&#65289;&#65292;VarMiON&#20063;&#30001;&#19968;&#20010;&#23376;&#32593;&#32476;&#21644;&#21478;&#19968;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65292;&#29992;&#20110;&#26500;&#36896;&#36755;&#20986;&#30340;&#22522;&#20989;&#25968;&#21644;&#36825;&#20123;&#22522;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;&#28982;&#32780;&#65292;&#19982;DeepONet&#19981;&#21516;&#30340;&#26159;&#65292;VarMiON&#20013;&#36825;&#20123;&#23376;&#32593;&#32476;&#30340;&#26550;&#26500;&#26159;&#31934;&#30830;&#23450;&#20041;&#30340;&#12290;&#23545;VarMiON&#35299;&#30340;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#21253;&#21547;&#19968;&#20123;&#19982;&#20256;&#32479;&#31639;&#23376;&#32593;&#32476;&#19981;&#21516;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years operator networks have emerged as promising deep learning tools for approximating the solution to partial differential equations (PDEs). These networks map input functions that describe material properties, forcing functions and boundary data to the solution of a PDE. This work describes a new architecture for operator networks that mimics the form of the numerical solution obtained from an approximate variational or weak formulation of the problem. The application of these ideas to a generic elliptic PDE leads to a variationally mimetic operator network (VarMiON). Like the conventional Deep Operator Network (DeepONet) the VarMiON is also composed of a sub-network that constructs the basis functions for the output and another that constructs the coefficients for these basis functions. However, in contrast to the DeepONet, the architecture of these sub-networks in the VarMiON is precisely determined. An analysis of the error in the VarMiON solution reveals that it contai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#20998;&#21516;&#32986;&#32676;&#19978;&#36827;&#34892;&#24418;&#29366;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#36896;&#36817;&#20284;&#30340;&#20445;&#25345;&#26041;&#21521;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#26222;&#36941;&#36924;&#36817;&#24615;&#36136;&#21644;Lipschitz&#24120;&#25968;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.11141</link><description>&lt;p&gt;
&#23545;&#20110;&#20248;&#21270;&#24418;&#29366;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#22522;&#20110;&#24494;&#20998;&#21516;&#32986;&#32676;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks on diffeomorphism groups for optimal shape reparameterization. (arXiv:2207.11141v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#20998;&#21516;&#32986;&#32676;&#19978;&#36827;&#34892;&#24418;&#29366;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#36896;&#36817;&#20284;&#30340;&#20445;&#25345;&#26041;&#21521;&#30340;&#24494;&#20998;&#21516;&#32986;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#26222;&#36941;&#36924;&#36817;&#24615;&#36136;&#21644;Lipschitz&#24120;&#25968;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22312;&#35745;&#31639;&#24418;&#29366;&#20043;&#38388;&#30340;&#27979;&#22320;&#36317;&#31163;&#20043;&#21069;&#65292;&#23558;&#26354;&#32447;&#25110;&#34920;&#38754;&#23545;&#40784;&#12290;&#25214;&#21040;&#23454;&#29616;&#27492;&#23545;&#40784;&#30340;&#26368;&#20339;&#37325;&#26032;&#21442;&#25968;&#21270;&#26159;&#19968;&#20010;&#35745;&#31639;&#37327;&#24222;&#22823;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#36890;&#36807;&#22312;&#24494;&#20998;&#21516;&#32986;&#32676;&#19978;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#26469;&#23436;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#26412;&#24494;&#20998;&#21516;&#32986;&#30340;&#32452;&#21512;&#26500;&#36896;&#20445;&#25345;&#26041;&#21521;&#30340;&#24494;&#20998;&#21516;&#32986;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;PyTorch&#23454;&#29616;&#65292;&#24182;&#36866;&#29992;&#20110;&#38750;&#21442;&#25968;&#26354;&#32447;&#21644;&#34920;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#36896;&#30340;&#26550;&#26500;&#20855;&#26377;&#26222;&#36941;&#36924;&#36817;&#24615;&#36136;&#65292;&#24182;&#33719;&#24471;&#20102;&#25152;&#24471;&#24494;&#20998;&#21516;&#32986;&#30340;Lipschitz&#24120;&#25968;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental problems in shape analysis is to align curves or surfaces before computing geodesic distances between their shapes. Finding the optimal reparametrization realizing this alignment is a computationally demanding task, typically done by solving an optimization problem on the diffeomorphism group. In this paper, we propose an algorithm for constructing approximations of orientation-preserving diffeomorphisms by composition of elementary diffeomorphisms. The algorithm is implemented using PyTorch, and is applicable for both unparametrized curves and surfaces. Moreover, we show universal approximation properties for the constructed architectures, and obtain bounds for the Lipschitz constants of the resulting diffeomorphisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.01255</link><description>&lt;p&gt;
&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#39640;&#32500;&#25193;&#25955;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23450;&#20041;&#22312;&#39640;&#32500;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#22495;&#19978;&#30340;&#25193;&#25955;&#26041;&#31243;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#19978;&#36817;&#20284;&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25968;&#23398;&#24314;&#27169;&#24037;&#20855;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#37329;&#34701;&#21040;&#35745;&#31639;&#21270;&#23398;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#26631;&#20934;&#25968;&#20540;&#25216;&#26415;&#36890;&#24120;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#23450;&#20041;&#22312;&#39640;&#32500;&#22495;&#19978;&#20855;&#26377;&#21608;&#26399;&#36793;&#30028;&#26465;&#20214;&#30340;&#23450;&#24120;&#25193;&#25955;&#26041;&#31243;&#12290;&#21463;&#39640;&#32500;&#31232;&#30095;&#20989;&#25968;&#36924;&#36817;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21387;&#32553;&#20613;&#37324;&#21494;&#33394;&#25955;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21387;&#32553;&#24863;&#30693;&#21644;&#35889;&#33394;&#25955;&#30340;&#24605;&#24819;&#65292;&#29992;&#33945;&#29305;&#21345;&#32599;&#25277;&#26679;&#20195;&#26367;&#20102;&#32467;&#26500;&#21270;&#33394;&#25955;&#32593;&#26684;&#30340;&#20351;&#29992;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#24674;&#22797;&#25216;&#26415;&#65288;&#22914;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#21644;&#8467;^1&#26368;&#23567;&#21270;&#65289;&#26469;&#36817;&#20284;PDE&#35299;&#30340;&#20613;&#37324;&#21494;&#31995;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Partial Differential Equations (PDEs) are a popular mathematical modelling tool, with applications ranging from finance to computational chemistry. However, standard numerical techniques for solving these PDEs are typically affected by the curse of dimensionality. In this work, we tackle this challenge while focusing on stationary diffusion equations defined over a high-dimensional domain with periodic boundary conditions. Inspired by recent progress in sparse function approximation in high dimensions, we propose a new method called compressive Fourier collocation. Combining ideas from compressive sensing and spectral collocation, our method replaces the use of structured collocation grids with Monte Carlo sampling and employs sparse recovery techniques, such as orthogonal matching pursuit and $\ell^1$ minimization, to approximate the Fourier coefficients of the PDE solution. We conduct a rigorous theoretical analysis showing that the approximation error of the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25913;&#36827;&#26041;&#26696;&#65292;&#21363;&#26696;&#20363;&#24863;&#30693;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#38450;&#24481;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.09398</link><description>&lt;p&gt;
&#26696;&#20363;&#24863;&#30693;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Case-Aware Adversarial Training. (arXiv:2204.09398v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25913;&#36827;&#26041;&#26696;&#65292;&#21363;&#26696;&#20363;&#24863;&#30693;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#38450;&#24481;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25104;&#20026;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#24212;&#29992;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;NN&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#65288;AEs&#65289;&#26497;&#20854;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20026;&#20102;&#38450;&#24481;AEs&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#34987;&#35748;&#20026;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#37327;&#22823;&#65292;AT&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;AT&#25913;&#36827;&#26041;&#26696;&#65292;&#21363;&#26696;&#20363;&#24863;&#30693;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#23569;&#37096;&#20998;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#23545;&#22823;&#22810;&#25968;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#12290;&#22914;&#26524;&#21482;&#20351;&#29992;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;AEs&#36827;&#34892;AT&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;AT&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#20445;&#25345;&#38450;&#24481;&#25928;&#26524;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;CAT&#23454;&#29616;&#20102;&#20004;&#20010;&#31361;&#30772;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;AE&#20449;&#24687;&#24230;&#30340;&#26041;&#27861;&#29992;&#20110;AE&#36807;&#28388;&#12290;&#20854;&#27425;&#65292;&#20026;&#36827;&#19968;&#27493;&#20016;&#23500;&#29992;&#20110;AT&#30340;&#20449;&#24687;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#26679;&#26412;&#36873;&#25321;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural network (NN) becomes one of the most heated type of models in various signal processing applications. However, NNs are extremely vulnerable to adversarial examples (AEs). To defend AEs, adversarial training (AT) is believed to be the most effective method while due to the intensive computation, AT is limited to be applied in most applications. In this paper, to resolve the problem, we design a generic and efficient AT improvement scheme, namely case-aware adversarial training (CAT). Specifically, the intuition stems from the fact that a very limited part of informative samples can contribute to most of model performance. Alternatively, if only the most informative AEs are used in AT, we can lower the computation complexity of AT significantly as maintaining the defense effect. To achieve this, CAT achieves two breakthroughs. First, a method to estimate the information degree of adversarial examples is proposed for AE filtering. Second, to further enrich the information that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29420;&#31435;&#20110;&#20855;&#20307;&#25299;&#25169;&#21644;&#20379;&#24212;&#20219;&#21153;&#30340;&#36890;&#29992;&#35299;&#12290;&#25105;&#20204;&#22312;&#20013;&#21387;&#22522;&#20934;&#32593;&#26684;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.07000</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving AC Power Flow with Graph Neural Networks under Realistic Constraints. (arXiv:2204.07000v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29420;&#31435;&#20110;&#20855;&#20307;&#25299;&#25169;&#21644;&#20379;&#24212;&#20219;&#21153;&#30340;&#36890;&#29992;&#35299;&#12290;&#25105;&#20204;&#22312;&#20013;&#21387;&#22522;&#20934;&#32593;&#26684;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35299;&#20915;&#32771;&#34385;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#20445;&#37197;&#30005;&#32593;&#30340;&#23433;&#20840;&#21644;&#24377;&#24615;&#36816;&#34892;&#65292;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#35745;&#31639;&#26159;&#30830;&#23450;&#32593;&#26684;&#36816;&#34892;&#38480;&#21046;&#25110;&#20998;&#26512;&#32593;&#26684;&#36164;&#20135;&#21033;&#29992;&#24773;&#20917;&#30340;&#36873;&#25321;&#25163;&#27573;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30005;&#21147;&#27969;&#29289;&#29702;&#32422;&#26463;&#30340;&#26694;&#26550;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#29420;&#31435;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#20855;&#20307;&#25299;&#25169;&#21644;&#20379;&#24212;&#20219;&#21153;&#30340;&#20132;&#27969;&#30005;&#21151;&#29575;&#27969;&#20844;&#24335;&#30340;&#36890;&#29992;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20013;&#21387;&#22522;&#20934;&#32593;&#26684;&#19978;&#23637;&#31034;&#12289;&#39564;&#35777;&#21644;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#37197;&#30005;&#32593;&#30340;&#29289;&#29702;&#21644;&#25299;&#25169;&#29305;&#24615;&#65292;&#20197;&#25552;&#20379;&#36866;&#29992;&#20110;&#23454;&#38469;&#32593;&#26684;&#25299;&#25169;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a graph neural network architecture to solve the AC power flow problem under realistic constraints. To ensure a safe and resilient operation of distribution grids, AC power flow calculations are the means of choice to determine grid operating limits or analyze grid asset utilization in planning procedures. In our approach, we demonstrate the development of a framework that uses graph neural networks to learn the physical constraints of the power flow. We present our model architecture on which we perform unsupervised training to learn a general solution of the AC power flow formulation independent of the specific topologies and supply tasks used for training. Finally, we demonstrate, validate and discuss our results on medium voltage benchmark grids. In our approach, we focus on the physical and topological properties of distribution grids to provide scalable solutions for real grid topologies. Therefore, we take a data-driven approach, using large and diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2112.14417</link><description>&lt;p&gt;
Temporal Difference&#23398;&#20064;&#31639;&#27861;&#30340;&#25511;&#21046;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;Temporal Difference (TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;TD&#23398;&#20064;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#19982;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32473;&#23450;&#31574;&#30053;&#30456;&#20851;&#30340;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#23384;&#22312;&#22810;&#31687;&#20851;&#20110;TD&#23398;&#20064;&#29702;&#35770;&#29702;&#35299;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;&#20960;&#24180;&#65292;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#23545;&#20854;&#32479;&#35745;&#25928;&#29575;&#25552;&#20379;&#20855;&#20307;&#20445;&#35777;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#25511;&#21046;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;TD&#23398;&#20064;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#39046;&#22495;&#30340;&#24050;&#26377;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#35770;&#23548;&#20986;&#30340;&#31616;&#21333;&#20998;&#26512;&#24037;&#20855;&#65292;&#20026;TD&#23398;&#20064;&#30340;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#24191;&#38420;&#39046;&#22495;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24352;&#37327;&#21015;&#34917;&#20840;&#38382;&#39064;&#65292;&#25512;&#23548;&#20102;&#27491;&#20132;&#25237;&#24433;&#21644;&#26680;&#30456;&#24178;&#24615;&#30340;&#30028;&#38480;&#65292;&#24182;&#20026;&#20855;&#26377;&#36741;&#21161;&#23376;&#31354;&#38388;&#20449;&#24687;&#30340;&#34917;&#20840;&#38382;&#39064;&#25552;&#20379;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2110.03975</link><description>&lt;p&gt;
&#24352;&#37327;&#21015;&#34917;&#20840;&#65306;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#23616;&#37096;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Tensor train completion: local recovery guarantees via Riemannian optimization. (arXiv:2110.03975v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Riemannian&#20248;&#21270;&#30340;&#24352;&#37327;&#21015;&#34917;&#20840;&#38382;&#39064;&#65292;&#25512;&#23548;&#20102;&#27491;&#20132;&#25237;&#24433;&#21644;&#26680;&#30456;&#24178;&#24615;&#30340;&#30028;&#38480;&#65292;&#24182;&#20026;&#20855;&#26377;&#36741;&#21161;&#23376;&#31354;&#38388;&#20449;&#24687;&#30340;&#34917;&#20840;&#38382;&#39064;&#25552;&#20379;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#38543;&#26426;&#36873;&#25321;&#30340;&#20803;&#32032;&#25968;&#37327;&#65292;&#35813;&#25968;&#37327;&#20445;&#35777;&#20102;Riemannian&#26799;&#24230;&#19979;&#38477;&#22312;&#24352;&#37327;&#21015;&#34917;&#20840;&#20013;&#20855;&#26377;&#23616;&#37096;&#25910;&#25947;&#24615;&#30340;&#39640;&#27010;&#29575;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#20132;&#25237;&#24433;&#19978;&#20999;&#31354;&#38388;&#30340;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#22522;&#20110;&#23637;&#24320;&#30340;&#22855;&#24322;&#20540;&#30340;&#35843;&#21644;&#24179;&#22343;&#65292;&#24182;&#24341;&#20837;&#20102;&#24352;&#37327;&#21015;&#30340;&#26680;&#30456;&#24178;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#20855;&#26377;&#36741;&#21161;&#23376;&#31354;&#38388;&#20449;&#24687;&#30340;&#24352;&#37327;&#21015;&#34917;&#20840;&#65292;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#23616;&#37096;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we estimate the number of randomly selected elements of a tensor that with high probability guarantees local convergence of Riemannian gradient descent for tensor train completion. We derive a new bound for the orthogonal projections onto the tangent spaces based on the harmonic mean of the unfoldings' singular values and introduce a notion of core coherence for tensor trains. We also extend the results to tensor train completion with auxiliary subspace information and obtain the corresponding local convergence guarantees.
&lt;/p&gt;</description></item><item><title>NeXtQSM&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#25972;&#21512;&#22788;&#29702;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#19968;&#33268;&#24615;&#23450;&#37327;&#30913;&#25935;&#24863;&#26144;&#23556;&#35745;&#31639;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;</title><link>http://arxiv.org/abs/2107.07752</link><description>&lt;p&gt;
NeXtQSM -- &#19968;&#31181;&#23436;&#25972;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#19982;&#28151;&#21512;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#23450;&#37327;&#30913;&#25935;&#24863;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
NeXtQSM -- A complete deep learning pipeline for data-consistent quantitative susceptibility mapping trained with hybrid data. (arXiv:2107.07752v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.07752
&lt;/p&gt;
&lt;p&gt;
NeXtQSM&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#25972;&#21512;&#22788;&#29702;&#27493;&#39588;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#19968;&#33268;&#24615;&#23450;&#37327;&#30913;&#25935;&#24863;&#26144;&#23556;&#35745;&#31639;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23450;&#37327;&#30913;&#25935;&#24863;&#26144;&#23556;&#65288;QSM&#65289;&#36817;&#24180;&#26469;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#38750;&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#26159;&#25968;&#25454;&#19968;&#33268;&#30340;&#65292;&#38656;&#35201;&#20307;&#20869;&#35757;&#32451;&#25968;&#25454;&#25110;&#32773;&#23558;QSM&#38382;&#39064;&#20998;&#20026;&#36830;&#32493;&#30340;&#27493;&#39588;&#26469;&#35299;&#20915;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#30340;&#20256;&#25773;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#32852;&#21512;&#35299;&#20915;QSM&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#32593;&#32476;&#23558;QSM&#27169;&#22411;&#39033;&#21644;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#39033;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25968;&#25454;&#19968;&#33268;&#30340;&#26041;&#24335;&#36827;&#34892;&#32972;&#26223;&#22330;&#26657;&#27491;&#21644;&#20598;&#26497;&#32763;&#36716;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NeXtQSM&#20811;&#26381;&#20102;&#20808;&#21069;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;NeXtQSM&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35745;&#31639;&#23450;&#37327;&#30913;&#25935;&#24863;&#26144;&#23556;&#65292;&#23558;&#27599;&#20010;&#22788;&#29702;&#27493;&#39588;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;&#65292;&#24182;&#25552;&#20379;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great potential in recent years, obtaining similar results to established non-learning approaches. Many current deep learning approaches are not data consistent, require in vivo training data or solve the QSM problem in consecutive steps resulting in the propagation of errors. Here we aim to overcome these limitations and developed a framework to solve the QSM processing steps jointly. We developed a new hybrid training data generation method that enables the end-to-end training for solving background field correction and dipole inversion in a data-consistent fashion using a variational network that combines the QSM model term and a learned regularizer. We demonstrate that NeXtQSM overcomes the limitations of previous deep learning methods. NeXtQSM offers a new deep learning based pipeline for computing quantitative susceptibility maps that integrates each processing step into the training and provides results that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2104.10751</link><description>&lt;p&gt;
&#20998;&#31867;&#35268;&#21017;&#29983;&#25104;&#65306;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rule Generation for Classification: Scalability, Interpretability, and Fairness. (arXiv:2104.10751v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10751
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#20855;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25152;&#24471;&#23450;&#20215;&#23376;&#38382;&#39064;&#34987;&#35777;&#26126;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#20195;&#29702;&#23450;&#20215;&#23376;&#38382;&#39064;&#20197;&#21152;&#36895;&#12290;&#35813;&#26041;&#27861;&#36820;&#22238;&#19968;&#32452;&#35268;&#21017;&#20197;&#21450;&#23427;&#20204;&#30340;&#26368;&#20248;&#26435;&#37325;&#65292;&#25351;&#31034;&#27599;&#20010;&#35268;&#21017;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#35268;&#21017;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#24615;&#65292;&#24182;&#23558;&#20844;&#24179;&#24615;&#30340;&#19968;&#33324;&#20998;&#31163;&#20934;&#21017;&#25512;&#24191;&#21040;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35814;&#32454;&#38416;&#36848;&#20854;&#19981;&#21516;&#26041;&#38754;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new rule-based optimization method for classification with constraints. The proposed method leverages column generation for linear programming, and hence, is scalable to large datasets. The resulting pricing subproblem is shown to be NP-Hard. We recourse to a decision tree-based heuristic and solve a proxy pricing subproblem for acceleration. The method returns a set of rules along with their optimal weights indicating the importance of each rule for learning. We address interpretability and fairness by assigning cost coefficients to the rules and introducing additional constraints. In particular, we focus on local interpretability and generalize separation criterion in fairness to multiple sensitive attributes and classes. We test the performance of the proposed methodology on a collection of datasets and present a case study to elaborate on its different aspects. The proposed rule-based learning method exhibits a good compromise between local interpretability and fairn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29983;&#25104;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2011.01710</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Ballistocardiogram artifact removal in simultaneous EEG-fMRI using generative adversarial network. (arXiv:2011.01710v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.01710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#21516;&#26102;&#36827;&#34892;&#30340;EEG-fMRI&#20013;&#21435;&#38500;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#25913;&#21892;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29983;&#25104;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21516;&#26102;&#36827;&#34892;&#30340;&#33041;&#30005;&#22270;-&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;EEG-fMRI&#65289;&#30340;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#20248;&#21183;&#65292;&#35813;&#25216;&#26415;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33041;&#31185;&#23398;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#33041;&#37096;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#36807;&#31243;&#20013;&#65292;&#24515;&#21160;&#25581;&#31034;&#29616;&#35937;&#65288;BCG&#65289;&#20250;&#20005;&#37325;&#27745;&#26579;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#12290;&#20316;&#20026;&#19968;&#20010;&#38750;&#37197;&#23545;&#38382;&#39064;&#65292;BCG&#25581;&#31034;&#29616;&#35937;&#30340;&#21435;&#38500;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#30456;&#24212;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#20248;&#21270;&#27599;&#20010;&#27169;&#22359;&#30340;&#21442;&#25968;&#26469;&#25913;&#36827;&#32593;&#32476;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24076;&#26395;&#25552;&#39640;&#32593;&#32476;&#27169;&#22411;&#30340;&#23616;&#37096;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#21487;&#38752;&#30340;&#29992;&#20110;&#21435;&#38500;BCG&#25581;&#31034;&#29616;&#35937;&#30340;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#21442;&#32771;&#20449;&#21495;&#25110;&#22797;&#26434;&#30340;&#30828;&#20214;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its advantages of high temporal and spatial resolution, the technology of simultaneous electroencephalogram-functional magnetic resonance imaging (EEG-fMRI) acquisition and analysis has attracted much attention, and has been widely used in various research fields of brain science. However, during the fMRI of the brain, ballistocardiogram (BCG) artifacts can seriously contaminate the EEG. As an unpaired problem, BCG artifact removal now remains a considerable challenge. Aiming to provide a solution, this paper proposed a novel modular generative adversarial network (GAN) and corresponding training strategy to improve the network performance by optimizing the parameters of each module. In this manner, we hope to improve the local representation ability of the network model, thereby improving its overall performance and obtaining a reliable generator for BCG artifact removal. Moreover, the proposed method does not rely on additional reference signal or complex hardware equipment. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#25506;&#32034;&#38477;&#20302;&#26041;&#21521;&#23545;&#20110;&#32422;&#26463;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20339;&#30340;&#26412;&#22320;&#38477;&#20302;&#26041;&#21521;&#26159;&#36127;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#21521;&#23548;&#25968;&#65292;&#20197;&#21450;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#30456;&#24403;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#12290;&#21478;&#22806;&#65292;Frank-Wolfe&#39030;&#28857;&#23545;&#24212;&#20110;&#20351;&#29992;&#36127;&#26799;&#24230;&#26041;&#21521;&#30340;&#8220;&#26080;&#38480;&#8221;&#27493;&#39588;&#25237;&#24433;&#21040;&#22810;&#32990;&#20307;&#19978;&#12290;</title><link>http://arxiv.org/abs/2006.08426</link><description>&lt;p&gt;
&#22312;&#38459;&#23612;&#26368;&#23567;&#21270;&#20013;&#38477;&#20302;&#26041;&#21521;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Walking in the Shadow: A New Perspective on Descent Directions for Constrained Minimization. (arXiv:2006.08426v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#25506;&#32034;&#38477;&#20302;&#26041;&#21521;&#23545;&#20110;&#32422;&#26463;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#26368;&#20339;&#30340;&#26412;&#22320;&#38477;&#20302;&#26041;&#21521;&#26159;&#36127;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#21521;&#23548;&#25968;&#65292;&#20197;&#21450;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#30456;&#24403;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#21160;&#21147;&#23398;&#12290;&#21478;&#22806;&#65292;Frank-Wolfe&#39030;&#28857;&#23545;&#24212;&#20110;&#20351;&#29992;&#36127;&#26799;&#24230;&#26041;&#21521;&#30340;&#8220;&#26080;&#38480;&#8221;&#27493;&#39588;&#25237;&#24433;&#21040;&#22810;&#32990;&#20307;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#20302;&#26041;&#21521;&#65292;&#22914;&#26397;&#21521;&#12289;Frank-Wolfe&#39030;&#28857;&#12289;&#31163;&#24320;&#27493;&#39588;&#12289;&#26397;&#21521;&#20869;&#37096;&#30340;&#31163;&#24320;&#27493;&#39588;&#21644;&#37197;&#23545;&#26041;&#21521;&#65292;&#26159;&#26377;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#65288;CGD&#65289;&#21464;&#20307;&#20013;&#30340;&#37325;&#35201;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#12290;&#25105;&#20204;&#35797;&#22270;&#25581;&#31034;&#36825;&#20123;&#26041;&#21521;&#23545;&#20110;&#23454;&#29616;&#32422;&#26463;&#26368;&#23567;&#32773;&#30340;&#24433;&#21709;&#12290;&#26368;&#20339;&#30340;&#26412;&#22320;&#38477;&#20302;&#26041;&#21521;&#26159;&#36127;&#26799;&#24230;&#30340;&#25237;&#24433;&#30340;&#26041;&#21521;&#23548;&#25968;&#65288;&#21363;&#38452;&#24433;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26041;&#21521;&#26159;&#21487;&#33021;&#30340;&#26368;&#20339;&#31163;&#24320;&#27493;&#39588;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#26102;&#38388;&#20869;&#65292;&#27839;&#30528;&#38452;&#24433;&#31227;&#21160;&#30340;&#21160;&#21147;&#23398;&#31561;&#21516;&#20110;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#30340;&#21160;&#21147;&#23398;&#65292;&#23613;&#31649;&#31163;&#25955;&#21270;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Frank-Wolfe&#65288;FW&#65289;&#39030;&#28857;&#23545;&#24212;&#20110;&#20351;&#29992;&#36127;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#8220;&#26080;&#38480;&#8221;&#27493;&#39588;&#25237;&#24433;&#21040;&#22810;&#32990;&#20307;&#19978;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#32467;&#21512;&#21040;&#19968;&#20010;&#26032;&#30340;Shadow-CG&#26041;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Descent directions such as movement towards Descent directions, including movement towards Frank-Wolfe vertices, away-steps, in-face away-steps and pairwise directions, have been an important design consideration in conditional gradient descent (CGD) variants. In this work, we attempt to demystify the impact of the movement in these directions towards attaining constrained minimizers. The optimal local direction of descent is the directional derivative (i.e., shadow) of the projection of the negative gradient. We show that this direction is the best away-step possible, and the continuous-time dynamics of moving in the shadow is equivalent to the dynamics of projected gradient descent (PGD), although it's non-trivial to discretize. We also show that Frank-Wolfe (FW) vertices correspond to projecting onto the polytope using an "infinite" step in the direction of the negative gradient, thus providing a new perspective on these steps. We combine these insights into a novel Shadow-CG method
&lt;/p&gt;</description></item><item><title>Coagent Networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#26159;&#25351;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#12290;&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#25191;&#34892;&#36335;&#24452;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#36825;&#19968;&#24605;&#24819;&#23454;&#29616;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#31616;&#27905;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2001.10474</link><description>&lt;p&gt;
Coagent Networks&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Coagent Networks Revisited. (arXiv:2001.10474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.10474
&lt;/p&gt;
&lt;p&gt;
Coagent Networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#26159;&#25351;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#12290;&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#25191;&#34892;&#36335;&#24452;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#36825;&#19968;&#24605;&#24819;&#23454;&#29616;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#31616;&#27905;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Coagent networks&#65288;&#20849;&#26234;&#32593;&#32476;&#65289;&#24418;&#24335;&#21270;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21327;&#20316;&#20197;&#37319;&#21462;&#34892;&#21160;&#30340;&#38543;&#26426;&#20195;&#29702;&#32593;&#32476;&#30340;&#27010;&#24565;&#12290;&#20849;&#26234;&#32593;&#32476;&#30340;&#26174;&#33879;&#24212;&#29992;&#21253;&#25324;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#20351;&#29992;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;HRL&#20195;&#29702;&#20013;&#20018;&#32852;&#22810;&#20010;&#38543;&#26426;&#32593;&#32476;&#24341;&#20837;&#19981;&#21516;&#23618;&#27425;&#30340;&#25277;&#35937;&#21160;&#20316;&#65292;&#26469;&#35299;&#20915;&#25506;&#32034;&#21033;&#29992;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#20849;&#26234;&#32593;&#32476;&#20013;&#24418;&#24335;&#21270;&#25191;&#34892;&#35268;&#21017;&#12289;&#36890;&#36807;&#20849;&#26234;&#32593;&#32476;&#20013;&#25191;&#34892;&#36335;&#24452;&#30340;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25551;&#36848;&#35768;&#22810;&#19981;&#21516;&#30340;&#20363;&#23376;&#12290;&#22312;&#23618;&#27425;&#36873;&#39033;&#35780;&#35770;&#32773;&#26550;&#26500;&#20013;&#21463;&#21040;&#21442;&#25968;&#20849;&#20139;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20849;&#26234;&#32593;&#32476;&#29702;&#35770;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#25191;&#34892;&#36335;&#24452;&#24605;&#24819;&#24471;&#21040;&#20102;&#23545;&#31574;&#26799;&#24230;&#23450;&#29702;&#30340;&#26356;&#31616;&#27905;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#21442;&#25968;&#20849;&#20139;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coagent networks formalize the concept of arbitrary networks of stochastic agents that collaborate to take actions in a reinforcement learning environment. Prominent examples of coagent networks in action include approaches to hierarchical reinforcement learning (HRL), such as those using options, which attempt to address the exploration exploitation trade-off by introducing abstract actions at different levels by sequencing multiple stochastic networks within the HRL agents. We first provide a unifying perspective on the many diverse examples that fall under coagent networks. We do so by formalizing the rules of execution in a coagent network, enabled by the novel and intuitive idea of execution paths in a coagent network. Motivated by parameter sharing in the hierarchical option-critic architecture, we revisit the coagent network theory and achieve a much shorter proof of the policy gradient theorem using our idea of execution paths, without any assumption on how parameters are share
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#37319;&#26679;&#20998;&#24067;&#19979;&#30340;&#20302;&#31209;&#36857;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#23792;&#20540;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#35777;&#26126;&#36857;&#22238;&#24402;&#37319;&#26679;&#31639;&#23376;&#24378;&#20984;&#24615;&#21644;&#33719;&#24471;&#38750;&#28176;&#36827;&#12289;&#36817;&#20046;&#26368;&#20248;&#30028;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#35823;&#24046;&#30028;&#25193;&#23637;&#21040;&#20197;&#20132;&#21449;&#39564;&#35777;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/1904.08576</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#19968;&#33324;&#37319;&#26679;&#20998;&#24067;&#19979;&#30340;&#20302;&#31209;&#36857;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Low-rank Trace Regression under General Sampling Distribution. (arXiv:1904.08576v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.08576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#33324;&#37319;&#26679;&#20998;&#24067;&#19979;&#30340;&#20302;&#31209;&#36857;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#23792;&#20540;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#35777;&#26126;&#36857;&#22238;&#24402;&#37319;&#26679;&#31639;&#23376;&#24378;&#20984;&#24615;&#21644;&#33719;&#24471;&#38750;&#28176;&#36827;&#12289;&#36817;&#20046;&#26368;&#20248;&#30028;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#23558;&#35823;&#24046;&#30028;&#25193;&#23637;&#21040;&#20197;&#20132;&#21449;&#39564;&#35777;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31209;&#27491;&#21017;&#21270;&#22238;&#24402;&#30340;&#20984;&#26494;&#24347;&#25110;&#27491;&#21017;&#21270;&#38750;&#20984;&#20248;&#21270;&#26469;&#20272;&#35745;&#21442;&#25968;&#30697;&#38453;B*&#30340;&#36857;&#22238;&#24402;&#38382;&#39064;&#12290;&#24050;&#30693;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#23545;B*&#30340;&#31209;&#12289;&#19968;&#33268;&#24615;&#21644;&#23792;&#20540;&#24615;&#20551;&#35774;&#19979;&#28385;&#36275;&#36817;&#20046;&#26368;&#20248;&#30340;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#23545;B*&#30340;&#19968;&#31181;&#36890;&#29992;&#23792;&#20540;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#25552;&#20379;&#20102;&#35777;&#26126;&#36857;&#22238;&#24402;&#37319;&#26679;&#31639;&#23376;&#30340;&#21463;&#38480;&#24378;&#20984;&#24615;&#20197;&#21450;&#33719;&#24471;&#20272;&#35745;&#35823;&#24046;&#30340;&#38750;&#28176;&#36827;&#12289;&#36817;&#20046;&#26368;&#20248;&#30028;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#31867;&#20284;&#65292;&#36825;&#20123;&#32467;&#26524;&#35201;&#27714;&#27491;&#21017;&#21270;&#21442;&#25968;&#39640;&#20110;&#26576;&#20010;&#29702;&#35770;&#19978;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#21462;&#20915;&#20110;&#23454;&#36341;&#20013;&#21487;&#33021;&#26410;&#30693;&#30340;&#35266;&#27979;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#35823;&#24046;&#30028;&#25193;&#23637;&#21040;&#20197;&#20132;&#21449;&#39564;&#35777;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#29616;&#26377;&#20851;&#20110;&#20132;&#21449;&#39564;&#35777;&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#32467;&#26524;(Kale&#31561;)&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the trace regression when a matrix of parameters B* is estimated via the convex relaxation of a rank-regularized regression or via regularized non-convex optimization. It is known that these estimators satisfy near-optimal error bounds under assumptions on the rank, coherence, and spikiness of B*. We start by introducing a general notion of spikiness for B* that provides a generic recipe to prove the restricted strong convexity of the sampling operator of the trace regression and obtain near-optimal and non-asymptotic error bounds for the estimation error. Similar to the existing literature, these results require the regularization parameter to be above a certain theory-inspired threshold that depends on observation noise that may be unknown in practice. Next, we extend the error bounds to cases where the regularization parameter is chosen via cross-validation. This result is significant in that existing theoretical results on cross-validated estimators (Kale et
&lt;/p&gt;</description></item></channel></rss>