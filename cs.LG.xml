<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00775</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20934;&#30830;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#36879;&#26126;&#24230;&#29289;&#31181;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Harnessing machine learning for accurate treatment of overlapping opacity species in GCMs. (arXiv:2311.00775v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#22806;&#34892;&#26143;&#21644;&#26837;&#30702;&#26143;&#30340;&#39640;&#31934;&#24230;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#38656;&#35201;&#35814;&#32454;&#21644;&#22797;&#26434;&#30340;&#28085;&#30422;&#20102;&#27969;&#20307;&#21147;&#23398;&#12289;&#21270;&#23398;&#21644;&#36752;&#23556;&#30340;&#36890;&#29992;&#29615;&#27969;&#27169;&#22411;&#65288;GCMs&#65289;&#12290;&#26412;&#30740;&#31350;&#20855;&#20307;&#32771;&#23519;&#20102;GCMs&#20013;&#21270;&#23398;&#21644;&#36752;&#23556;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#65292;&#24182;&#27604;&#36739;&#20102;&#22312;&#30456;&#20851;-k&#20551;&#35774;&#20013;&#28151;&#21512;&#19981;&#21516;&#21270;&#23398;&#29289;&#31181;&#36879;&#26126;&#24230;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#22312;&#19981;&#33021;&#20551;&#35774;&#24179;&#34913;&#21270;&#23398;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DeepSets&#65288;DS&#65289;&#30340;&#24555;&#36895;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#12290;&#25105;&#20204;&#23558;DS&#26041;&#27861;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#22914;&#33258;&#36866;&#24212;&#31561;&#20215;&#28040;&#20809;&#65288;AEE&#65289;&#21644;&#24102;&#26377;&#37325;&#26032;&#20998;&#32452;&#21644;&#25490;&#24207;&#30340;&#38543;&#26426;&#37325;&#21472;&#65288;RORR&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#28151;&#21512;&#26041;&#27861;&#25972;&#21512;&#21040;&#20102;&#25105;&#20204;&#30340;GCM (expeRT/MITgcm)&#20013;&#65292;&#24182;&#23545;&#28909;&#26408;&#26143;HD~209458 b&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DS&#26041;&#27861;&#22312;GCM&#20351;&#29992;&#26102;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#65292;&#32780;RORR&#26041;&#27861;&#21017;&#19981;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand high precision observations of exoplanets and brown dwarfs, we need detailed and complex general circulation models (GCMs) that incorporate hydrodynamics, chemistry, and radiation. In this study, we specifically examine the coupling between chemistry and radiation in GCMs and compare different methods for mixing opacities of different chemical species in the correlated-k assumption, when equilibrium chemistry cannot be assumed. We propose a fast machine learning method based on DeepSets (DS), which effectively combines individual correlated-k opacities (k-tables). We evaluate the DS method alongside other published methods like adaptive equivalent extinction (AEE) and random overlap with rebinning and resorting (RORR). We integrate these mixing methods into our GCM (expeRT/MITgcm) and assess their accuracy and performance for the example of the hot Jupiter HD~209458 b. Our findings indicate that the DS method is both accurate and efficient for GCM usage, whereas RORR is t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15890</link><description>&lt;p&gt;
&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2310.15890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36328;&#29305;&#24449;&#23545;&#27604;&#25439;&#22833;&#23454;&#29616;&#25968;&#25454;&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#22823;&#22810;&#25968;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#22312;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#20855;&#26377;&#26174;&#33879;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#21449;&#29305;&#24449;&#19978;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#21644;&#23545;&#27604;&#25439;&#22833;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23545;&#20110;&#19968;&#23545;&#30456;&#37051;&#20195;&#29702;&#65292;&#36328;&#29305;&#24449;&#26159;&#20174;&#19968;&#20010;&#20195;&#29702;&#30340;&#25968;&#25454;&#33719;&#21462;&#30340;&#29305;&#24449;&#65288;&#21363;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#28608;&#27963;&#65289;&#20851;&#20110;&#21478;&#19968;&#20010;&#20195;&#29702;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#35814;&#23613;&#30340;&#23454;&#39564;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;CIFAR-10&#12289;CIFAR-100&#12289;Fashion MNIST &#21644; ImageNet&#65289;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32593;&#32476;&#25299;&#25169;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.11891</link><description>&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#19982;&#20043;&#30456;&#20851;&#30340;&#20445;&#35777;&#12290;&#23427;&#20204;&#30340;&#21487;&#35775;&#38382;&#24615;&#20063;&#25171;&#24320;&#20102;&#22522;&#20110;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#20808;&#31579;&#36873;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#20960;&#20309;&#24046;&#24322;&#65292;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20004;&#31181;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#24230;&#37327;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#26680;&#21644;&#32463;&#20856;&#26680;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#12290;&#35813;&#24230;&#37327;&#25351;&#31034;&#20102;&#37327;&#23376;&#21644;&#32463;&#20856;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#23427;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#19982;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#20851;&#31995;&#65292;&#20960;&#20309;&#24046;&#24322;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#38500;&#20102;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#20043;&#22806;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical ma
&lt;/p&gt;</description></item><item><title>Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11230</link><description>&lt;p&gt;
Zipformer&#65306;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11230
&lt;/p&gt;
&lt;p&gt;
Zipformer&#26159;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;U-Net-like&#32534;&#30721;&#22120;&#32467;&#26500;&#12289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#12289;&#25913;&#36827;&#30340;LayerNorm&#12289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#26032;&#30340;&#20248;&#21270;&#22120;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#31561;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Conformer&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#26368;&#27969;&#34892;&#30340;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#23427;&#22312;&#21464;&#25442;&#22120;&#20013;&#21152;&#20837;&#20102;&#21367;&#31215;&#27169;&#22359;&#20197;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#26356;&#33410;&#30465;&#20869;&#23384;&#12289;&#24615;&#33021;&#26356;&#22909;&#30340;&#21464;&#25442;&#22120;&#8212;&#8212;Zipformer&#12290;&#24314;&#27169;&#25913;&#21464;&#21253;&#25324;&#65306;1&#65289;&#31867;&#20284;U-Net&#30340;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20013;&#38388;&#22534;&#26632;&#22312;&#36739;&#20302;&#30340;&#24103;&#29575;&#19979;&#36816;&#34892;&#65307;2&#65289;&#37325;&#26032;&#32452;&#32455;&#30340;&#22359;&#32467;&#26500;&#65292;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#27169;&#22359;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#22797;&#20351;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#20197;&#25552;&#39640;&#25928;&#29575;&#65307;3&#65289;&#19968;&#31181;&#25913;&#36827;&#30340;LayerNorm&#24418;&#24335;&#65292;&#31216;&#20026;BiasNorm&#65292;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19968;&#20123;&#38271;&#24230;&#20449;&#24687;&#65307;4&#65289;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;SwooshR&#21644;SwooshL&#30340;&#24615;&#33021;&#20248;&#20110;Swish&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;ScaledAdam&#65292;&#23427;&#36890;&#36807;&#24403;&#21069;&#24352;&#37327;&#30340;&#35268;&#27169;&#26469;&#32553;&#25918;&#26356;&#26032;&#65292;&#20197;&#20445;&#25345;&#30456;&#23545;&#21464;&#21270;&#22823;&#33268;&#30456;&#21516;&#65292;&#24182;&#26126;&#30830;&#23398;&#20064;&#21442;&#25968;&#35268;&#27169;&#12290;&#19982;Adam&#30456;&#27604;&#65292;&#23427;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;LibriSpeech&#12289;Aishell-1&#21644;Wenet&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.08577</link><description>&lt;p&gt;
&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#29702;&#35299;&#24182;&#38750;&#28304;&#33258;&#25193;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models. (arXiv:2310.08577v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;&#34429;&#28982;&#22312;&#26576;&#20123;&#26679;&#24335;&#21270;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#35270;&#35273;&#35821;&#20041;&#20869;&#23481;&#35782;&#21035;&#25928;&#26524;&#65292;&#21253;&#25324;&#20986;&#33394;&#30340;&#22797;&#21512;&#22270;&#20687;&#29702;&#35299;&#23454;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#35782;&#21035;&#65292;&#36825;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#24863;&#30693;&#25216;&#33021;&#65292;&#23545;&#25968;&#25454;&#25972;&#29702;&#65288;&#20363;&#22914;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#22122;&#22768;&#25968;&#25454;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#26816;&#32034;&#65289;&#21644;&#33258;&#20027;&#35270;&#35273;&#65288;&#20363;&#22914;&#21306;&#20998;&#19981;&#21516;&#30340;&#22825;&#27668;&#21464;&#21270;&#21644;&#30456;&#26426;&#38236;&#22836;&#27745;&#26579;&#65289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32463;&#36807;27&#31181;&#35270;&#35273;&#25968;&#25454;&#31867;&#22411;&#30340;&#21160;&#29289;&#22270;&#20687;&#30340;&#20462;&#25913;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#23545;39&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;100M&#21040;80B&#30340;VLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#24615;&#33021;&#26223;&#35266;&#12290;&#34429;&#28982;VLMs&#22312;&#35782;&#21035;&#26576;&#20123;&#26679;&#24335;&#21270;&#30340;&#25968;&#25454;&#31867;&#22411;&#65288;&#20363;&#22914;&#21345;&#36890;&#21644;&#33609;&#22270;&#65289;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#22522;&#26412;&#25805;&#20316;&#65288;&#20363;&#22914;&#22270;&#20687;&#26059;&#36716;&#25110;&#28155;&#21152;&#22122;&#22768;&#65289;&#24341;&#36215;&#30340;&#31616;&#21333;&#25968;&#25454;&#31867;&#22411;&#19978;&#34920;&#29616;&#20986;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the development of vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content, including impressive instances of compositional image understanding. Here, we introduce the novel task of Visual Data-Type Identification, a basic perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. While VLMs are reasonably good at identifying certain stylistic \textit{data-types}, such as cartoons and sketches, they struggle with simpler data-types arising from basic manipulations like image rotations or additive noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.03696</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#22823;&#31867;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;/&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21464;&#20998;&#20248;&#21270;&#24615;&#65288;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#31639;&#23376;&#21644;k-&#24179;&#38754;&#21464;&#25442;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#34920;&#31034;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#35828;&#26126;&#22312;&#36825;&#20123;Banach&#31354;&#38388;&#19978;&#25552;&#20986;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#30340;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#24687;&#24687;&#30456;&#20851;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#31070;&#32463;&#32593;&#32476;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#21253;&#25324;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#12289;&#33539;&#25968;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;&#22312;&#34180;&#26495;/&#22810;&#27425;&#35856;&#27874;&#26679;&#26465;&#29702;&#35770;&#20013;&#25214;&#21040;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#22312;&#20869;&#30340;&#22810;&#31181;&#32463;&#20856;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10954</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20855;&#26377;&#35768;&#22810;&#26631;&#31614;&#30340;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#25552;&#31034;&#20013;&#38590;&#20197;&#36866;&#24212;&#36275;&#22815;&#25968;&#37327;&#30340;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32469;&#36807;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#27599;&#27425;&#25512;&#29702;&#35843;&#29992;&#21482;&#32473;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#23436;&#25972;&#26631;&#31614;&#31354;&#38388;&#30340;&#37096;&#20998;&#35270;&#22270;&#12290;&#22312;&#26368;&#36817;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;(OPT, LLaMA)&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#36229;&#36234;&#20102;&#24494;&#35843;&#24615;&#33021;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#35268;&#27169;&#19979;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#32780;&#19968;&#33268;&#22320;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;&#36890;&#36807;&#36816;&#34892;&#20960;&#20010;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#20197;&#19979;&#20869;&#23481;&#30340;&#20351;&#29992;&#65306;a)&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#24403;&#21069;&#36755;&#20837;&#30340;&#30456;&#20284;&#24230;, b) &#21363;&#26102;&#26597;&#35810;&#35821;&#21477;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07992</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#20869;&#30740;&#31350;&#27969;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#23792;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#23792;&#20540;&#27169;&#24335;&#12290;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#21160;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#65292;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#26631;&#35760;&#25968;&#25454;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#27700;&#25991;&#23398;&#23478;&#26816;&#27979;&#32654;&#22269;&#19996;&#21271;&#22320;&#21306;&#20020;&#30028;&#22320;&#24102;&#30740;&#31350;&#27969;&#22495;&#20256;&#24863;&#22120;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#23792;&#20540;&#27169;&#24335;&#24322;&#24120;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#25110;&#33258;&#28982;&#29616;&#35937;&#24341;&#36215;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#23384;&#22312;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#22522;&#20934;&#21644;&#36873;&#25321;&#26368;&#36866;&#21512;&#32473;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23558;&#21512;&#25104;&#30340;&#23792;&#20540;&#27169;&#24335;&#27880;&#20837;&#21040;&#21512;&#25104;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#24102;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#32467;&#21512;&#33258;&#21160;&#21270;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#26426;&#21046;&#12290;&#35813;&#26426;&#21046;&#20174;&#20116;&#31181;&#36873;&#25321;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#26550;&#26500;&#21644;&#35757;&#32451;&#21442;&#25968;&#30340;&#20248;&#21270;&#27169;&#22411;&#23454;&#20363;&#65292;&#21363;&#26102;&#24207;&#21367;&#31215;&#32593;&#32476;&#65288;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated machine learning framework designed to assist hydrologists in detecting anomalies in time series data generated by sensors in a research watershed in the northeastern United States critical zone. The framework specifically focuses on identifying peak-pattern anomalies, which may arise from sensor malfunctions or natural phenomena. However, the use of classification methods for anomaly detection poses challenges, such as the requirement for labeled data as ground truth and the selection of the most suitable deep learning model for the given task and dataset. To address these challenges, our framework generates labeled datasets by injecting synthetic peak patterns into synthetically generated time series data and incorporates an automated hyperparameter optimization mechanism. This mechanism generates an optimized model instance with the best architectural and training parameters from a pool of five selected models, namely Temporal Convolutional Network (
&lt;/p&gt;</description></item><item><title>TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13490</link><description>&lt;p&gt;
TpuGraphs:&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13490
&lt;/p&gt;
&lt;p&gt;
TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#30828;&#20214;&#24615;&#33021;&#27169;&#22411;&#22312;&#20195;&#30721;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#32534;&#35793;&#22120;&#20570;&#20986;&#21551;&#21457;&#24615;&#20915;&#31574;&#65292;&#25110;&#32773;&#24110;&#21161;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#25214;&#21040;&#32473;&#23450;&#31243;&#24207;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TpuGraphs&#65292;&#19968;&#31181;&#22312;Tensor Processing Units&#65288;TPUs&#65289;&#19978;&#36816;&#34892;&#30340;&#20840;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#31243;&#24207;&#20197;&#35745;&#31639;&#22270;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#20027;&#35201;&#35745;&#31639;&#65292;&#20363;&#22914;&#35757;&#32451;&#21608;&#26399;&#25110;&#25512;&#26029;&#27493;&#39588;&#12290;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#19968;&#20010;&#35745;&#31639;&#22270;&#12289;&#19968;&#20010;&#32534;&#35793;&#37197;&#32622;&#65292;&#20197;&#21450;&#20351;&#29992;&#35813;&#37197;&#32622;&#32534;&#35793;&#26102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the datas
&lt;/p&gt;</description></item><item><title>zkDL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#36890;&#36807;zkReLU&#21327;&#35758;&#21644;&#26032;&#39062;&#30340;&#31639;&#26415;&#30005;&#36335;&#26500;&#24314;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.16273</link><description>&lt;p&gt;
zkDL: &#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training. (arXiv:2307.16273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16273
&lt;/p&gt;
&lt;p&gt;
zkDL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#36890;&#36807;zkReLU&#21327;&#35758;&#21644;&#26032;&#39062;&#30340;&#31639;&#26415;&#30005;&#36335;&#26500;&#24314;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#32473;&#20154;&#20204;&#30340;&#29983;&#27963;&#24102;&#26469;&#20102;&#37325;&#22823;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#21512;&#27861;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25252;&#19981;&#21463;&#20449;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#36890;&#24120;&#31105;&#27490;&#39564;&#35777;&#32773;&#36890;&#36807;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#26469;&#30452;&#25509;&#26816;&#26597;&#35757;&#32451;&#36807;&#31243;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;zkDL&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#38646;&#30693;&#35782;&#35777;&#26126;&#12290;zkDL&#30340;&#26680;&#24515;&#26159;zkReLU&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#38646;&#30693;&#35782;&#35777;&#26126;&#21327;&#35758;&#65292;&#20855;&#26377;&#20248;&#21270;&#30340;&#35777;&#26126;&#26102;&#38388;&#21644;&#35777;&#26126;&#22823;&#23567;&#65292;&#36825;&#26159;&#21487;&#39564;&#35777;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#30340;&#38750;&#31639;&#26415;&#24615;&#36136;&#12290;&#20026;&#20102;&#23558;zkReLU&#25972;&#21512;&#21040;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#30340;&#35777;&#26126;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#26500;&#24314;&#31639;&#26415;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;&#24182;&#34892;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#26500;&#24314;&#26041;&#26696;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#35777;&#26126;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep networks. However, to protect the intellectual properties of untrusted AI developers, directly examining the training process by accessing the model parameters and training data by verifiers is often prohibited.  In response to this challenge, we present zkDL, an efficient zero-knowledge proof of deep learning training. At the core of zkDL is zkReLU, a specialized zero-knowledge proof protocol with optimized proving time and proof size for the ReLU activation function, a major obstacle in verifiable training due to its non-arithmetic nature. To integrate zkReLU into the proof system for the entire training process, we devise a novel construction of an arithmetic circuit from neural networks. By leveraging the abundant parallel computation resources, this constru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.09477</link><description>&lt;p&gt;
&#36827;&#23637;&#20013;&#30340;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#34913;&#37327;&#65288;&#32463;&#39564;&#65289;&#25968;&#25454;&#20013;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#20351;&#29992;&#23545;&#35937;&#30340;&#25968;&#23383;&#23646;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21457;&#23637;&#20986;&#30340;&#24207;&#21015;&#26041;&#27861;&#25968;&#37327;&#30456;&#23545;&#36739;&#23569;&#12290;&#36896;&#25104;&#36825;&#19968;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#19978;&#20010;&#19990;&#32426;&#65292;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#26080;&#27861;&#28385;&#36275;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#23545;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#36807;&#20110;&#25968;&#23398;&#20005;&#35880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#35752;&#35770;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#8220;&#35745;&#31639;&#8221;&#24207;&#21015;&#32467;&#26500;&#65288;&#19968;&#31867;&#29305;&#23450;&#30340;&#26377;&#21521;&#22270;&#65289;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20174;&#20854;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#24314;&#31435;&#20026;&#19968;&#39033;&#20840;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;&#38500;&#20102;&#19982;&#20854;&#20182;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#30340;&#20132;&#21449;&#20114;&#34917;&#22806;&#65292;&#24191;&#27867;&#30340;&#23398;&#31185;&#39046;&#22495;&#20063;&#23558;&#21463;&#30410;&#20110;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#36890;&#36807;&#20998;&#35299;&#22797;&#26434;&#30340;&#25552;&#31034;&#24182;&#20351;&#29992;VQA&#27169;&#22411;&#36827;&#34892;&#27979;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#23545;&#40784;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.04749</link><description>&lt;p&gt;
&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#65306;&#36890;&#36807;&#36845;&#20195;VQA&#21453;&#39304;&#35780;&#20272;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback. (arXiv:2307.04749v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21010;&#20998;&#12289;&#35780;&#20272;&#21644;&#32454;&#21270;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#36890;&#36807;&#20998;&#35299;&#22797;&#26434;&#30340;&#25552;&#31034;&#24182;&#20351;&#29992;VQA&#27169;&#22411;&#36827;&#34892;&#27979;&#37327;&#65292;&#26368;&#32456;&#24471;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#23545;&#40784;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#20986;&#29616;&#65292;&#20197;&#25991;&#26412;&#20026;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#33879;&#24615;&#65292;&#20294;&#26159;&#38543;&#30528;&#25991;&#26412;&#36755;&#20837;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20173;&#21487;&#33021;&#26080;&#27861;&#29983;&#25104;&#20934;&#30830;&#20256;&#36798;&#32473;&#23450;&#25552;&#31034;&#35821;&#20041;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;&#36825;&#31181;&#19981;&#23545;&#40784;&#24448;&#24448;&#34987;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26410;&#33021;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#20998;&#35299;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#35299;&#23545;&#40784;&#20998;&#25968;&#65292;&#23427;&#23558;&#22797;&#26434;&#25552;&#31034;&#20998;&#35299;&#20026;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#26029;&#35328;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;VQA&#27169;&#22411;&#26469;&#27979;&#37327;&#27599;&#20010;&#26029;&#35328;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#23545;&#40784;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#23558;&#19981;&#21516;&#26029;&#35328;&#30340;&#23545;&#40784;&#20998;&#25968;&#21512;&#24182;&#21518;&#65292;&#24471;&#21040;&#26368;&#32456;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a Decompositional-Alignment-Score which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimenta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.03761</link><description>&lt;p&gt;
&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Attention for Anomaly Detection in Heterogeneous Sensor Networks. (arXiv:2307.03761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DyGATAD&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35782;&#21035;&#38598;&#20307;&#24322;&#24120;&#34892;&#20026;&#65292;&#20854;&#20013;&#24322;&#24120;&#34892;&#20026;&#21487;&#33021;&#30001;&#20110;&#31995;&#32479;&#20869;&#37096;&#30456;&#20114;&#20851;&#31995;&#30340;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#30417;&#25511;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#30001;&#24037;&#19994;&#29289;&#32852;&#32593; (IIoT) &#30417;&#25511;&#30340;&#31995;&#32479;&#36890;&#36807;&#24322;&#26500;&#20256;&#24863;&#22120;&#32593;&#32476;&#29983;&#25104;&#22823;&#37327;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015; (MTS) &#25968;&#25454;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#21161;&#20110;&#26465;&#20214;&#30417;&#25511;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#20294;&#26159;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#20851;&#31995;&#20063;&#32473;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#24322;&#24120;&#21644;&#32972;&#26223;&#24322;&#24120;&#65292;&#23545;&#38598;&#20307;&#24322;&#24120;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#38598;&#20307;&#24322;&#24120;&#30340;&#19968;&#31181;&#24120;&#35265;&#21464;&#31181;&#26159;&#24322;&#24120;&#38598;&#20307;&#34892;&#20026;&#30001;&#31995;&#32479;&#20869;&#37096;&#30340;&#30456;&#20114;&#20851;&#31995;&#21464;&#21270;&#24341;&#36215;&#12290;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#24322;&#24120;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#36807;&#28909;&#65289;&#12289;&#30001;&#20110;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#30340;&#19981;&#27491;&#30830;&#25805;&#20316;&#35774;&#32622;&#25110;&#31995;&#32479;&#32423;&#25925;&#38556;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DyGATAD&#65288;&#19968;&#31181;&#21160;&#24577;&#22270;&#27880;&#24847;&#21147;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65289;&#65292;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of digital transformation, systems monitored by the Industrial Internet of Things (IIoTs) generate large amounts of Multivariate Time Series (MTS) data through heterogeneous sensor networks. While this data facilitates condition monitoring and anomaly detection, the increasing complexity and interdependencies within the sensor network pose significant challenges for anomaly detection. Despite progress in this field, much of the focus has been on point anomalies and contextual anomalies, with lesser attention paid to collective anomalies. A less addressed but common variant of collective anomalies is when the abnormal collective behavior is caused by shifts in interrelationships within the system. This can be due to abnormal environmental conditions like overheating, improper operational settings resulting from cyber-physical attacks, or system-level faults. To address these challenges, this paper proposes DyGATAD (Dynamic Graph Attention for Anomaly Detection), a graph-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16111</link><description>&lt;p&gt;
&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Time Regularization in Optimal Time Variable Learning. (arXiv:2306.16111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#27491;&#21017;&#21270;&#22312;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#29992;&#20110;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;arXiv:2204.08528&#20013;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#30340;&#26368;&#20248;&#26102;&#38388;&#21464;&#37327;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#33539;&#22260;&#30452;&#25509;&#30456;&#20851;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25193;&#23637;&#20102;&#36825;&#20010;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;Residual Neural Networks (ResNets)&#65292;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#33879;&#21517;&#30340;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;PyTorch&#20195;&#30721;&#21487;&#22312;https://github.com/frederikkoehne/time_variable_learning&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, optimal time variable learning in deep neural networks (DNNs) was introduced in arXiv:2204.08528. In this manuscript we extend the concept by introducing a regularization term that directly relates to the time horizon in discrete dynamical systems. Furthermore, we propose an adaptive pruning approach for Residual Neural Networks (ResNets), which reduces network complexity without compromising expressiveness, while simultaneously decreasing training time. The results are illustrated by applying the proposed concepts to classification tasks on the well known MNIST and Fashion MNIST data sets. Our PyTorch code is available on https://github.com/frederikkoehne/time_variable_learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14468</link><description>&lt;p&gt;
&#36890;&#29992;&#26694;&#26550;&#19979;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30740;&#31350;&#36890;&#29992;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19979;&#23545;&#20004;&#20010;&#36866;&#24212;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#39318;&#27425;&#25506;&#32034;&#65306;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#21644;&#25209;&#27425;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31216;&#20026;Eluder Condition&#31867;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22312;EC&#31867;&#21035;&#19978;&#23454;&#29616;&#20102;&#22823;&#32422;$ \widetilde{\mathcal{O}}(\log K)$&#30340;&#20999;&#25442;&#20195;&#20215;&#21644;$\widetilde{\mathcal{O}}(\sqrt{K})$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#23545;&#20110;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;$B$&#20010;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#22823;&#32422;$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#31687;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#27169;&#22411;&#65292;&#22914;&#34920;&#26684;MDP (Bai et al. 2019; Zhang et al. 2020)&#12289;&#32447;&#24615;MDP (Wang et al. 2021; Gao et al. 2021)&#12289;&#20302;Eluder&#32500;&#24230;MDP (Kong et al. 2021; Gao et al. 2021)&#12289;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#31867;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38544;&#31169;&#32452;&#21512;&#30340;&#26694;&#26550;&#65292;&#23558;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#19982;&#20107;&#21069;&#38544;&#31169;&#26426;&#21046;&#32467;&#21512;&#20351;&#29992;&#20197;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.13824</link><description>&lt;p&gt;
&#38754;&#21521;&#31934;&#24230;&#20248;&#20808;&#26426;&#21046;&#30340;&#33258;&#36866;&#24212;&#38544;&#31169;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Adaptive Privacy Composition for Accuracy-first Mechanisms. (arXiv:2306.13824v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38544;&#31169;&#32452;&#21512;&#30340;&#26694;&#26550;&#65292;&#23558;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#19982;&#20107;&#21069;&#38544;&#31169;&#26426;&#21046;&#32467;&#21512;&#20351;&#29992;&#20197;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20174;&#19994;&#32773;&#35797;&#22270;&#22312;&#30446;&#26631;&#31934;&#24230;&#27700;&#24179;&#19979;&#25552;&#20379;&#26368;&#20339;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#36890;&#36807;&#21033;&#29992;&#8220;&#22122;&#22768;&#20943;&#23569;&#8221;&#24605;&#24819;&#23558;&#30456;&#20851;&#22122;&#22768;&#21152;&#21040;&#31169;&#26377;&#35745;&#31639;&#30340;&#20805;&#20998;&#32479;&#35745;&#20013;&#24182;&#29983;&#25104;&#19968;&#31995;&#21015;&#36234;&#26469;&#36234;&#20934;&#30830;&#31572;&#26696;&#30340;&#8220;&#31934;&#24230;&#20248;&#20808;&#8221;&#26426;&#21046;&#24050;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#28857;&#26159;&#20998;&#26512;&#20154;&#21592;&#20165;&#38656;&#25903;&#20184;&#21457;&#24067;&#30340;&#26368;&#19981;&#22024;&#26434;&#25110;&#26368;&#20934;&#30830;&#31572;&#26696;&#30340;&#38544;&#31169;&#25104;&#26412;&#12290;&#34429;&#28982;&#36825;&#31181;&#21333;&#29420;&#30340;&#21560;&#24341;&#21147;&#23646;&#24615;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#23558;&#23427;&#20204;&#19982;&#20854;&#20182;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#32467;&#21512;&#20351;&#29992;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#26159;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#30340;&#38544;&#31169;&#20445;&#35777;&#34987;&#65288;&#24517;&#28982;&#22320;&#65289;&#21046;&#23450;&#20026;&#8220;&#20107;&#21518;&#38544;&#31169;&#8221;&#65292;&#20854;&#20197;&#21457;&#24067;&#30340;&#32467;&#26524;&#20026;&#20989;&#25968;&#36793;&#30028;&#38544;&#31169;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#33267;&#20170;&#20173;&#32570;&#20047;&#19968;&#31181;&#23558;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#19982;&#8220;&#20107;&#21069;&#38544;&#31169;&#8221;&#26426;&#21046;&#65288;&#20363;&#22914;&#38543;&#26426;&#21709;&#24212;&#25110;&#25289;&#26222;&#25289;&#26031;&#26426;&#21046;&#65289;&#36827;&#34892;&#32452;&#21512;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#36825;&#20123;&#26426;&#21046;&#30452;&#25509;&#20316;&#20026;&#36755;&#20837;&#22823;&#23567;&#25110;&#26597;&#35810;&#24037;&#20316;&#37327;&#30340;&#38544;&#31169;&#25439;&#22833;&#36793;&#30028;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#33258;&#36866;&#24212;&#38544;&#31169;&#32452;&#21512;&#30340;&#22122;&#22768;&#20943;&#23569;&#26426;&#21046;&#21644;&#20107;&#21069;&#38544;&#31169;&#26426;&#21046;&#65292;&#20197;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21644;&#26597;&#35810;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical applications of differential privacy, practitioners seek to provide the best privacy guarantees subject to a target level of accuracy. A recent line of work by \cite{LigettNeRoWaWu17, WhitehouseWuRaRo22} has developed such accuracy-first mechanisms by leveraging the idea of \emph{noise reduction} that adds correlated noise to the sufficient statistic in a private computation and produces a sequence of increasingly accurate answers. A major advantage of noise reduction mechanisms is that the analysts only pay the privacy cost of the least noisy or most accurate answer released. Despite this appealing property in isolation, there has not been a systematic study on how to use them in conjunction with other differentially private mechanisms. A fundamental challenge is that the privacy guarantee for noise reduction mechanisms is (necessarily) formulated as \emph{ex-post privacy} that bounds the privacy loss as a function of the released outcome. Furthermore, there has yet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.13255</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#19979;&#22810;&#31867;&#20998;&#31867;&#30340;&#28176;&#36827;&#27867;&#21270;&#31934;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#21327;&#21464;&#37327;&#21452;&#23618;&#27169;&#22411;&#19979;&#65292;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#28176;&#36827;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#25968;&#12289;&#29305;&#24449;&#21644;&#31867;&#21035;&#25968;&#37117;&#21516;&#26102;&#22686;&#38271;&#12290;&#25105;&#20204;&#23436;&#20840;&#35299;&#20915;&#20102;Subramanian&#31561;&#20154;&#22312;'22&#24180;&#25152;&#25552;&#20986;&#30340;&#29468;&#24819;&#65292;&#19982;&#39044;&#27979;&#30340;&#27867;&#21270;&#21306;&#38388;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#30340;&#19979;&#30028;&#31867;&#20284;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#65306;&#23427;&#20204;&#33021;&#22815;&#30830;&#31435;&#35823;&#20998;&#31867;&#29575;&#36880;&#28176;&#36235;&#36817;&#20110;0&#25110;1.&#25105;&#20204;&#32039;&#23494;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#26159;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22238;&#24402;&#22120;&#26368;&#20248;&#30340;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22312;&#28176;&#36827;&#19978;&#27425;&#20248;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31181;&#26032;&#30340;Hanson-Wright&#19981;&#31561;&#24335;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#22810;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#31867;&#22411;&#20998;&#26512;&#22312;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#20197;&#21450;&#36793;&#26041;&#21521;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08274</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#22312;&#26377;&#21521;&#25110;&#26080;&#21521;&#22270;&#20013;&#65292;&#20026;&#20160;&#20040;&#35201;&#36873;&#25321;&#21738;&#20010;&#65311;&#23454;&#35777;&#30740;&#31350;&#21644;&#31616;&#21333;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Using Either Aggregated Features or Adjacency Lists in Directed or Undirected Graph? Empirical Study and Simple Classification Method. (arXiv:2306.08274v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#30740;&#31350;&#20102;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32858;&#21512;&#29305;&#24449;&#25110;&#37051;&#25509;&#34920;&#20197;&#21450;&#36793;&#26041;&#21521;&#23545;&#20998;&#31867;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#20998;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#26368;&#28909;&#38376;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#20851;&#27880;&#33410;&#28857;&#34920;&#31034;&#65288;&#32858;&#21512;&#29305;&#24449; vs. &#37051;&#25509;&#34920;&#65289;&#21644;&#36755;&#20837;&#22270;&#30340;&#36793;&#26041;&#21521;&#65288;&#26377;&#21521; vs. &#26080;&#21521;&#65289;&#36873;&#25321;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#19981;&#21516;&#33410;&#28857;&#34920;&#31034;&#21644;&#36793;&#26041;&#21521;&#30340;&#21508;&#31181;GNNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21333;&#19968;&#30340;&#32452;&#21512;&#31283;&#23450;&#22320;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#38656;&#35201;&#26681;&#25454;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#20840;&#38754;&#30340;&#20998;&#31867;&#26041;&#27861;A2DUG&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#20013;&#33410;&#28857;&#34920;&#31034;&#21464;&#37327;&#30340;&#25152;&#26377;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;A2DUG&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#31283;&#23450;&#34920;&#29616;&#33391;&#22909;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification is one of the hottest tasks in graph analysis. In this paper, we focus on the choices of node representations (aggregated features vs. adjacency lists) and the edge direction of an input graph (directed vs. undirected), which have a large influence on classification results. We address the first empirical study to benchmark the performance of various GNNs that use either combination of node representations and edge directions. Our experiments demonstrate that no single combination stably achieves state-of-the-art results across datasets, which indicates that we need to select appropriate combinations depending on the characteristics of datasets. In response, we propose a simple yet holistic classification method A2DUG which leverages all combinations of node representation variants in directed and undirected graphs. We demonstrate that A2DUG stably performs well on various datasets. Surprisingly, it largely outperforms the current state-of-the-art methods in several
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20083;&#33146;H&amp;E&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#26469;&#28304;&#20986;&#21457;&#65292;&#31995;&#32479;&#24615;&#22320;&#32508;&#36848;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24773;&#20917;&#65292;&#21253;&#21547;&#20102;12&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#29305;&#24449;&#20449;&#24687;&#12290;&#24635;&#32467;&#20986;&#25968;&#25454;&#38598;&#30340;&#32570;&#22833;&#20197;&#21450;&#26356;&#20840;&#38754;&#21644;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#24314;&#31435;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01546</link><description>&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#20083;&#33146;&#32452;&#32455;&#30149;&#29702;&#23398;H&amp;E&#20840;&#20999;&#29255;&#22270;&#20687;&#25968;&#25454;&#38598;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available datasets of breast histopathology H&amp;E whole-slide images: A systematic review. (arXiv:2306.01546v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20083;&#33146;H&amp;E&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#26469;&#28304;&#20986;&#21457;&#65292;&#31995;&#32479;&#24615;&#22320;&#32508;&#36848;&#20102;&#24403;&#21069;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24773;&#20917;&#65292;&#21253;&#21547;&#20102;12&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#21644;&#30456;&#20851;&#29305;&#24449;&#20449;&#24687;&#12290;&#24635;&#32467;&#20986;&#25968;&#25454;&#38598;&#30340;&#32570;&#22833;&#20197;&#21450;&#26356;&#20840;&#38754;&#21644;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#24314;&#31435;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#36827;&#27493;&#23545;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290; &#28982;&#32780;&#65292;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#20083;&#33146;&#30284;&#32452;&#32455;&#23398;&#22270;&#20687;&#34920;&#31034;&#30528;&#19968;&#20010;&#37325;&#22823;&#38590;&#39064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20934;&#30830;&#21644;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290; &#22312;&#36825;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#29992;&#20110;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#20083;&#33146;H&#65286;E&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#31995;&#32479;&#22320;&#25628;&#32034;&#20102;9&#20010;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;9&#20010;&#30740;&#31350;&#25968;&#25454;&#23384;&#20648;&#24211;&#12290; &#25105;&#20204;&#25214;&#21040;&#20102;12&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;5153&#20010;&#20083;&#33146;&#30284;H&amp;E WSI&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#25253;&#21578;&#20102;&#22270;&#20687;&#20803;&#25968;&#25454;&#21644;&#29305;&#24449;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#36866;&#21512;&#20083;&#33146;&#30284;&#35745;&#31639;&#30149;&#29702;&#23398;&#20855;&#20307;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290; &#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#32534;&#21046;&#20102;&#29992;&#20110;&#21253;&#25324;&#25991;&#31456;&#30340;&#20462;&#34917;&#31243;&#24207;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#21015;&#34920;&#12290; &#25105;&#20204;&#30340;&#32508;&#36848;&#24378;&#35843;&#20102;&#38656;&#35201;&#26356;&#20840;&#38754;&#21644;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#21644;&#24378;&#22823;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in digital pathology and computing resources have made a significant impact in the field of computational pathology for breast cancer diagnosis and treatment. However, access to high-quality labeled histopathological images of breast cancer is a big challenge that limits the development of accurate and robust deep learning models. In this systematic review, we identified the publicly available datasets of breast H&amp;E stained whole-slide images (WSI) that can be used to develop deep learning algorithms. We systematically searched nine scientific literature databases and nine research data repositories. We found twelve publicly available datasets, containing 5153 H&amp;E WSIs of breast cancer. Moreover, we reported image metadata and characteristics for each dataset to assist researchers in selecting proper datasets for specific tasks in breast cancer computational pathology. In addition, we compiled a list of patch and private datasets that were used in the included articles as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#27809;&#26377;&#24433;&#21709;&#65292;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#26469;&#35828;&#36825;&#19968;&#19968;&#33268;&#24615;&#36143;&#31359;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#19988;&#22823;&#23485;&#24230;&#19979;&#30340;&#32467;&#26500;&#29305;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#34920;&#26126;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.18411</link><description>&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#32593;&#32476;&#22312;&#23454;&#38469;&#35268;&#27169;&#19979;&#20855;&#26377;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Feature-Learning Networks Are Consistent Across Widths At Realistic Scales. (arXiv:2305.18411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23485;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#27809;&#26377;&#24433;&#21709;&#65292;&#32593;&#32476;&#22312;&#26089;&#26399;&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#24615;&#65292;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#26469;&#35828;&#36825;&#19968;&#19968;&#33268;&#24615;&#36143;&#31359;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#19988;&#22823;&#23485;&#24230;&#19979;&#30340;&#32467;&#26500;&#29305;&#24615;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#34920;&#26126;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#32593;&#32476;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24449;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#23545;&#32593;&#32476;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#35757;&#32451;&#26089;&#26399;&#65292;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#23485;&#32593;&#32476;&#19981;&#20165;&#20855;&#26377;&#30456;&#21516;&#30340;&#25439;&#22833;&#26354;&#32447;&#65292;&#32780;&#19988;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27979;&#35797;&#39044;&#27979;&#20063;&#26159;&#19968;&#33268;&#30340;&#12290;&#23545;&#20110;&#20687;CIFAR-5m&#36825;&#26679;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#36825;&#36866;&#29992;&#20110;&#20855;&#26377;&#23454;&#38469;&#23485;&#24230;&#30340;&#32593;&#32476;&#30340;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#21253;&#25324;&#20869;&#37096;&#34920;&#31034;&#12289;&#39044;&#28608;&#27963;&#20998;&#24067;&#12289;&#31283;&#23450;&#24615;&#36793;&#32536;&#29616;&#35937;&#21644;&#22823;&#23398;&#20064;&#29575;&#25928;&#24212;&#65292;&#22312;&#22823;&#23485;&#24230;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;&#36825;&#21551;&#21457;&#20102;&#19968;&#20010;&#20551;&#35774;&#65306;&#22312;&#29305;&#24449;&#23398;&#20064;&#26497;&#38480;&#19979;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#29616;&#23454;&#27169;&#22411;&#20013;&#20986;&#29616;&#30340;&#29616;&#35937;&#12290;&#23545;&#20110;&#26356;&#38590;&#30340;&#20219;&#21153;&#65288;&#22914;ImageNet&#21644;&#35821;&#35328;&#24314;&#27169;&#65289;&#21644;&#26356;&#26202;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#26377;&#38480;&#23485;&#24230;&#30340;&#20559;&#24046;&#20250;&#31995;&#32479;&#22320;&#22686;&#38271;&#12290;&#36825;&#20123;&#20559;&#24046;&#26159;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#25928;&#24212;&#24341;&#36215;&#30340;&#12290;&#39318;&#20808;&#65292;&#32593;&#32476;&#36755;&#20986;&#20855;&#26377;&#21021;&#22987;&#21270;&#30456;&#20851;&#30340;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
We study the effect of width on the dynamics of feature-learning neural networks across a variety of architectures and datasets. Early in training, wide neural networks trained on online data have not only identical loss curves but also agree in their point-wise test predictions throughout training. For simple tasks such as CIFAR-5m this holds throughout training for networks of realistic widths. We also show that structural properties of the models, including internal representations, preactivation distributions, edge of stability phenomena, and large learning rate effects are consistent across large widths. This motivates the hypothesis that phenomena seen in realistic models can be captured by infinite-width, feature-learning limits. For harder tasks (such as ImageNet and language modeling), and later training times, finite-width deviations grow systematically. Two distinct effects cause these deviations across widths. First, the network output has initialization-dependent variance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27010;&#29575;&#27979;&#24230;&#25554;&#20540;&#21644;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#31354;&#38388;&#19978;&#29983;&#25104;&#27979;&#24230;&#30340;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#26080;&#38656;&#20284;&#28982;&#25110;&#27169;&#25311;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17209</link><description>&lt;p&gt;
&#21151;&#33021;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Functional Flow Matching. (arXiv:2305.17209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#27010;&#29575;&#27979;&#24230;&#25554;&#20540;&#21644;&#23398;&#20064;&#24213;&#23618;&#20989;&#25968;&#31354;&#38388;&#19978;&#29983;&#25104;&#27979;&#24230;&#30340;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#26080;&#38656;&#20284;&#28982;&#25110;&#27169;&#25311;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#27969;&#21305;&#37197;&#65288;Functional Flow Matching, FFM&#65289;&#30340;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#27969;&#21305;&#37197;&#65288;Flow Matching&#65289;&#30452;&#25509;&#25512;&#24191;&#21040;&#26080;&#38480;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23450;&#20041;&#20102;&#19968;&#32452;&#27010;&#29575;&#27979;&#24230;&#36335;&#24452;&#65292;&#22312;&#22266;&#23450;&#30340;&#39640;&#26031;&#27979;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#28982;&#21518;&#23398;&#20064;&#20989;&#25968;&#30340;&#24213;&#23618;&#31354;&#38388;&#19978;&#29983;&#25104;&#27492;&#27979;&#24230;&#36335;&#24452;&#30340;&#21521;&#37327;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20284;&#28982;&#25110;&#27169;&#25311;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#20989;&#25968;&#31354;&#38388;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#19981;&#20165;&#25552;&#20379;&#26500;&#24314;&#36825;&#31181;&#27169;&#22411;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36824;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FFM&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#31181;&#20989;&#25968;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15613</link><description>&lt;p&gt;
&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;
&lt;/p&gt;
&lt;p&gt;
Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;nD&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#20854;&#22312;&#28857;&#20113;&#20998;&#26512;&#20013;&#31561;&#21464;&#20110;&#27491;&#20132;&#36716;&#25442;&#65292;&#21033;&#29992;&#20102;&#36229;&#29699;&#20307;&#21644;&#24120;&#35268;n&#21333;&#24418;&#20307;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#29702;&#35770;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#31561;&#21464;&#21644;&#20960;&#20309;&#21464;&#25442;&#19979;&#19981;&#21464;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36817;&#26399;&#21457;&#23637;&#30340;&#21487;&#25805;&#32437;3D&#29699;&#24418;&#31070;&#32463;&#20803;&#29702;&#35770;--&#22522;&#20110;&#29699;&#24418;&#20915;&#31574;&#38754;&#30340;SO&#65288;3&#65289;-&#31561;&#21464;&#28388;&#27874;&#22120;&#32452;&#65292;&#23558;&#35813;&#31070;&#32463;&#20803;&#25193;&#23637;&#21040;&#20102;nD&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28145;&#24230;&#31561;&#21464;&#36229;&#29699;&#20307;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#22534;&#21472;&#22312;&#22810;&#23618;&#20013;&#12290;&#21033;&#29992;ModelNet40&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31561;&#21464;&#36229;&#29699;&#20307;&#30340;&#28508;&#22312;&#23454;&#29992;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#26657;&#27491;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14164</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#20462;&#27491;&#25552;&#39640;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improved Convergence of Score-Based Diffusion Models via Prediction-Correction. (arXiv:2305.14164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#26657;&#27491;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#26159;&#20174;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#26159;&#65288;i&#65289;&#36890;&#36807;&#21521;&#25968;&#25454;&#28155;&#21152;&#22122;&#22768;&#36816;&#34892;&#26102;&#38388;&#20026;$T_1$&#30340;&#27491;&#21521;&#36807;&#31243;&#65292;&#65288;ii&#65289;&#20272;&#35745;&#20854;&#24471;&#20998;&#20989;&#25968;&#65292;&#24182;&#65288;iii&#65289;&#20351;&#29992;&#27492;&#20272;&#35745;&#20540;&#36816;&#34892;&#21453;&#21521;&#36807;&#31243;&#12290;&#30001;&#20110;&#21453;&#21521;&#36807;&#31243;&#20197;&#27491;&#21521;&#36807;&#31243;&#30340;&#24179;&#31283;&#20998;&#24067;&#20316;&#20026;&#21021;&#22987;&#20540;&#65292;&#22240;&#27492;&#29616;&#26377;&#30340;&#20998;&#26512;&#33539;&#24335;&#35201;&#27714;$T_1\to\infty$&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#20998;&#25968;&#36924;&#36817;&#31934;&#24230;&#65292;&#24403;$T_1$&#21457;&#25955;&#26102;&#65292;&#25910;&#25947;&#20445;&#35777;&#23558;&#22833;&#36133;&#65307;&#20174;&#23454;&#38469;&#35282;&#24230;&#26469;&#30475;&#65292;$T_1$&#36234;&#22823;&#65292;&#35745;&#31639;&#25104;&#26412;&#23601;&#36234;&#39640;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#35823;&#24046;&#20256;&#25773;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#27969;&#34892;&#30340;&#39044;&#27979;&#22120;&#26657;&#27491;&#26041;&#26696;&#30340;&#19968;&#20010;&#29256;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#22312;&#36816;&#34892;&#27491;&#21521;&#36807;&#31243;&#20043;&#21518;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340; Langevin &#21160;&#21147;&#23398;&#20272;&#35745;&#26368;&#32456;&#20998;&#24067;&#65292;&#28982;&#21518;&#24674;&#22797;&#35813;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) are powerful tools to sample from complex data distributions. Their underlying idea is to (i) run a forward process for time $T_1$ by adding noise to the data, (ii) estimate its score function, and (iii) use such estimate to run a reverse process. As the reverse process is initialized with the stationary distribution of the forward one, the existing analysis paradigm requires $T_1\to\infty$. This is however problematic: from a theoretical viewpoint, for a given precision of the score approximation, the convergence guarantee fails as $T_1$ diverges; from a practical viewpoint, a large $T_1$ increases computational costs and leads to error propagation. This paper addresses the issue by considering a version of the popular predictor-corrector scheme: after running the forward process, we first estimate the final distribution via an inexact Langevin dynamics and then revert the process. Our key technical contribution is to provide convergence guarantees
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#30456;&#20851;&#24341;&#29702;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#19968;&#20123;&#20854;&#20182;&#25216;&#26415;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#19978;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#24674;&#22797;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#22914;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.11042</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#36890;&#29992;&#27867;&#21270;&#30028;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#30456;&#20851;&#24341;&#29702;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#19968;&#20123;&#20854;&#20182;&#25216;&#26415;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#19978;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#24674;&#22797;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#22914;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23548;&#20986;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#12290;&#20027;&#35201;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#22522;&#20110;&#25913;&#21464;&#27979;&#24230;&#21644;&#26494;&#24347;Young&#19981;&#31561;&#24335;&#22312;$L_{\psi_p}$Orlicz&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#21435;&#30456;&#20851;&#24615;&#24341;&#29702;&#12290;&#37319;&#29992;&#21435;&#30456;&#20851;&#24615;&#24341;&#29702;&#19982;&#20854;&#20182;&#25216;&#26415;&#65292;&#22914;&#23545;&#31216;&#21270;&#12289;&#32806;&#21512;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;chaining&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#21253;&#25324;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#65292;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#24674;&#22797;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#21253;&#25324;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;&#27492;&#22806;&#65292;Fernique-Talagrand&#19978;&#30028;&#20063;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#21576;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01090</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#20013;&#30340;&#27969;&#24418;&#32500;&#24230;&#21644;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#21644;&#24037;&#31243;&#23398;&#20013;&#30340;&#35768;&#22810;&#29616;&#35937;&#22312;&#24418;&#24335;&#19978;&#26159;&#39640;&#32500;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#38271;&#26102;&#38388;&#21160;&#24577;&#24448;&#24448;&#29983;&#27963;&#22312;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#23558;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;$L_2$&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#12289;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#27969;&#24418;&#32500;&#24230;&#30340;&#33021;&#21147;&#65292;&#38024;&#23545;&#22810;&#31181;&#22797;&#26434;&#24230;&#30340;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#20998;&#26512;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#20102;&#35299;&#20302;&#31209;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#27599;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#23618;&#20849;&#21516;&#26500;&#25104;&#20102;&#20302;&#31209;&#34920;&#31034;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#33258;&#25105;&#32416;&#27491;&#12290;&#23398;&#20064;&#30340;&#22352;&#26631;&#31995;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#29289;&#29702;&#27934;&#35265;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many phenomena in physics and engineering are formally high-dimensional, their long-time dynamics often live on a lower-dimensional manifold. The present work introduces an autoencoder framework that combines implicit regularization with internal linear layers and $L_2$ regularization (weight decay) to automatically estimate the underlying dimensionality of a data set, produce an orthogonal manifold coordinate system, and provide the mapping functions between the ambient space and manifold space, allowing for out-of-sample projections. We validate our framework's ability to estimate the manifold dimension for a series of datasets from dynamical systems of varying complexities and compare to other state-of-the-art estimators. We analyze the training dynamics of the network to glean insight into the mechanism of low-rank learning and find that collectively each of the implicit regularizing layers compound the low-rank representation and even self-correct during training. Analysis o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;</title><link>http://arxiv.org/abs/2304.14176</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#21619;&#36947;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#21619;&#36947;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#20855;&#20307;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22522;&#26412;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340; $U(1)$ &#33655;&#36827;&#34892;&#23398;&#20064;&#65292;&#20195;&#29702;&#26041;&#26696;&#25214;&#21040;&#20102;21&#20010;&#19982;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#23454;&#39564;&#27979;&#37327;&#36136;&#37327;&#21644;&#28151;&#21512;&#35282;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#27491;&#24207;&#30340;&#22266;&#26377;&#20540;&#24448;&#24448;&#22823;&#20110;&#21453;&#24207;&#65292;&#27491;&#24207;&#19982;&#30446;&#21069;&#30340;&#23454;&#39564;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#31526;&#21512;&#12290;&#20195;&#29702;&#30340;&#33258;&#20027;&#34892;&#20026;&#26681;&#25454;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21494;&#23376;&#22330;&#30340;&#35282;&#25104;&#20998;&#24341;&#36215;&#30340;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09128</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#20195;&#30721;&#20998;&#26512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Exploring Distributional Shifts in Large Language Models for Code Analysis. (arXiv:2303.09128v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09128
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#26512;&#20013;&#22788;&#29702;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32452;&#32455;&#12289;&#39033;&#30446;&#21644;&#27169;&#22359;&#30340;&#33258;&#28982;&#36793;&#30028;&#20998;&#21106;&#26041;&#27861;&#65292;&#21457;&#29616;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#20135;&#29983;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#19982;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20004;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; CodeT5 &#21644; Codex &#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#25512;&#24191;&#21040;&#39046;&#22495;&#22806;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#22522;&#26412;&#24212;&#29992;&#65306;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#25353;&#29031;&#20854;&#33258;&#28982;&#36793;&#30028;&#65288;&#25353;&#32452;&#32455;&#12289;&#25353;&#39033;&#30446;&#21644;&#25353;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#27169;&#22359;&#65289;&#23558;&#25968;&#25454;&#20998;&#20026;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#36825;&#26679;&#65292;&#22312;&#37096;&#32626;&#26102;&#65292;&#35782;&#21035;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#21464;&#24471;&#31616;&#21333;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#27599;&#20010;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#37117;&#20250;&#32473;&#36825;&#20004;&#20010;&#27169;&#22411;&#24102;&#26469;&#20998;&#24067;&#20559;&#31227;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#22810;&#20219;&#21153;&#23398;&#20064;&#26412;&#36523;&#26159;&#19968;&#20010;&#21512;&#29702;&#30340;&#22522;&#32447;&#65292;&#20294;&#23558;&#20854;&#19982;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#32034;&#30340;&#31034;&#20363;&#30340;&#23569;&#37327;&#24494;&#35843;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#38750;&#24120;&#24378;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20248;&#20110;&#30452;&#25509;&#35843;&#25972;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically study the capacity of two large language models for code CodeT5 and Codex - to generalize to out-of-domain data. In this study, we consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. This makes recognition of in-domain vs out-of-domain data at the time of deployment trivial. We establish that samples from each new domain present both models with a significant challenge of distribution shift. We study how well different established methods can adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. In fact, according to our experiments, this solution can outperform direct finetuning for very low-data scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.02560</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22240;&#26524;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;: &#22312;&#32654;&#22269;&#35780;&#20272;&#26356;&#20005;&#26684;&#30340;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#30340;&#20581;&#24247;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US. (arXiv:2302.02560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#31574;&#30740;&#31350;&#20013;&#65292;&#20272;&#35745;&#36830;&#32493;&#24615;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#26159;&#26368;&#20851;&#38190;&#30340;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20559;&#31227;-&#21709;&#24212;&#20989;&#25968;&#65288;SRF&#65289;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#28041;&#21450;&#24378;&#20581;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#23454;&#29616;&#65292;&#29992;&#20110;SRF&#20272;&#35745;&#12290;&#21463;&#20844;&#20849;&#21355;&#29983;&#20013;&#30340;&#20851;&#38190;&#25919;&#31574;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#24378;&#20581;&#24615;&#21644;&#25928;&#29575;&#20445;&#35777;&#30340;SRF&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#20197;&#20272;&#35745;&#23558;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#26368;&#36817;&#25552;&#35758;&#20174;12 &#956;g/m&#179;&#25913;&#20026;9 &#956;g/m&#179;&#30340;PM2.5&#30340;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#30340;&#20462;&#35746;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39318;&#27425;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem shift-response function (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00845</link><description>&lt;p&gt;
CD-GraB&#65306;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#35777;&#26126;&#21152;&#36895;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training. (arXiv:2302.00845v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CD-GraB&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#20998;&#24067;&#24335;&#31034;&#20363;&#39034;&#24207;&#20197;&#21152;&#36895;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;CD-GraB&#23637;&#29616;&#20986;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#24182;&#19988;&#22312;&#22522;&#20934;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#22312;&#32447;&#26799;&#24230;&#24179;&#34913;&#65288;GraB&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#22522;&#20110;&#32622;&#25442;&#30340;&#31034;&#20363;&#25490;&#24207;&#21487;&#20197;&#20445;&#35777;&#20248;&#20110;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#12290;&#32780;RR&#20250;&#20219;&#24847;&#25490;&#21015;&#35757;&#32451;&#31034;&#20363;&#65292;GraB&#21033;&#29992;&#20808;&#21069;&#26102;&#26399;&#30340;&#38472;&#26087;&#26799;&#24230;&#23545;&#31034;&#20363;&#36827;&#34892;&#25490;&#24207;--&#23454;&#29616;&#27604;RR&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#20294;&#26159;&#65292;GraB&#22312;&#35774;&#35745;&#19978;&#23384;&#22312;&#38480;&#21046;&#65306;&#34429;&#28982;&#23427;&#23637;&#31034;&#20102;&#22312;&#38598;&#20013;&#25968;&#25454;&#19978;&#25193;&#23637;&#35757;&#32451;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#24182;&#19981;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#29616;&#20195;&#20998;&#24067;&#24335;ML&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#35843;&#20998;&#24067;&#24335;GraB&#65288;CD-GraB&#65289;&#65292;&#23427;&#21033;&#29992;&#20808;&#21069;&#20851;&#20110;&#20869;&#26680;&#31232;&#30095;&#21270;&#24037;&#20316;&#30340;&#27934;&#23519;&#21147;&#65292;&#23558;&#32622;&#25442;&#25490;&#24207;&#30340;&#21487;&#35777;&#26126;&#26356;&#24555;&#30340;&#20248;&#21183;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#35774;&#32622;&#12290;CD-GraB&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#65292;&#22312;&#20013;&#22830;&#38598;&#26435;GraB&#19978;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#20248;&#20110;&#20998;&#24067;&#24335;RR&#31561;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages stale gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: While it demonstrates an impressive ability to scale-up training on centralized data, it does not naturally extend to modern distributed ML workloads. We therefore propose Coordinated Distributed GraB (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms baselines empirically, including distributed RR, on a variety of benchmark tasks.
&lt;/p&gt;</description></item><item><title>&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#19981;&#31934;&#30830;&#12289;&#20247;&#21253;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty. (arXiv:2210.16380v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16380
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#31934;&#30830;&#30340;&#12289;&#20247;&#21253;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#23545;&#20110;&#26368;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#37117;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20004;&#20010;&#38382;&#39064;&#20351;&#24471;&#36825;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#65292;&#21363;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;AL-ALL&#21644;AL-PUB&#25968;&#25454;&#38598;--&#21253;&#21547;&#26469;&#33258;&#21476;&#24076;&#33098;&#32440;&#33609;&#30340;&#22270;&#20687;&#30340;&#32039;&#23494;&#35009;&#21098;&#30340;&#21333;&#20010;&#23383;&#31526;--&#21463;&#21040;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#23558;&#38598;&#21512;&#24314;&#27169;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#30001;&#20960;&#20046;&#30456;&#21516;&#30340;ResNets&#32452;&#25104;&#30340;&#22534;&#21472;&#27867;&#21270;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65306;&#19968;&#20010;&#21033;&#29992;&#31232;&#30095;&#20132;&#21449;&#29109;&#65288;CXE&#65289;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;Kullback-Liebler&#25955;&#24230;&#65288;KLD&#65289;&#12290;&#20004;&#20010;&#32593;&#32476;&#37117;&#20351;&#29992;&#20174;&#20247;&#21253;&#19968;&#33268;&#24615;&#20013;&#24471;&#20986;&#30340;&#26631;&#31614;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#32593;&#32476;&#65292;KLD&#26159;&#30456;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#65288;NDA&#65289;&#35745;&#31639;&#30340;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#38598;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#24212;&#29992;k-&#36817;&#37051;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped, individual characters from images of ancient Greek papyri -- are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from the crowdsourced consensus. For the second network, the KLD is calculated with respect to the proposed Normalized Distribution of Annotations (NDA). For our ensemble model, we apply a k-near
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13179</link><description>&lt;p&gt;
Occam&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Occam learning. (arXiv:2210.13179v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#23450;&#38544;&#34255;&#23618;&#20998;&#24067;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36873;&#25321;&#31616;&#21333;&#12289;&#26131;&#35299;&#37322;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#21516;&#26102;&#35757;&#32451;&#26377;&#25928;&#12290;&#27169;&#22411;&#30340;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#20316;&#32773;&#35748;&#20026;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#24212;&#35813;&#36981;&#24490;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#27169;&#22411;&#20013;&#65292;&#38544;&#34255;&#23618;&#30340;&#20998;&#24067;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#37319;&#29992;&#36825;&#31181;&#20307;&#31995;&#26550;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#35768;&#22810;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#36136;&#12290;&#20363;&#22914;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36873;&#25321;&#20026;&#31616;&#21333;&#19988;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#32780;&#19988;&#22312;&#28909;&#21147;&#23398;&#24847;&#20041;&#19979;&#65292;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#24403;&#38544;&#34255;&#21333;&#20803;&#20026;&#20108;&#20803;&#21464;&#37327;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20197;&#29305;&#24449;&#20026;&#22522;&#30784;&#30340;&#33258;&#28982;&#35299;&#37322;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32570;&#20047;&#29305;&#24449;&#30340;&#29366;&#24577;&#23545;&#24212;&#20110;&#22312;&#29305;&#24449;&#26041;&#38754;&#26368;&#22823;&#31243;&#24230;&#30340;&#26080;&#30693;&#29366;&#24577;&#65292;&#24182;&#19988;&#65292;&#23398;&#20064;&#31532;&#19968;&#20010;&#29305;&#24449;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#38750;&#39640;&#26031;&#32479;&#35745;&#23646;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#24212;&#35813;&#26681;&#25454;&#26368;&#22823;&#20851;&#32852;&#24230;&#21407;&#21017;&#36873;&#25321;&#38544;&#34255;&#21464;&#37327;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#23618;&#29305;&#24449;&#27169;&#22411;&#65288;HFM&#65289;&#20316;&#20026;&#28385;&#36275;&#36825;&#19968;&#21407;&#21017;&#24182;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;&#20013;&#24615;&#20808;&#39564;&#32452;&#32455;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss probabilistic neural network models for unsupervised learning where the distribution of the hidden layer is fixed. We argue that learning machines with this architecture enjoy a number of desirable properties. For example, the model can be chosen as a simple and interpretable one, it does not need to be over-parametrised and training is argued to be efficient in a thermodynamic sense. When hidden units are binary variables, these models have a natural interpretation in terms of features. We show that the featureless state corresponds to a state of maximal ignorance about the features and that learning the first feature depends on non-Gaussian statistical properties of the data. We suggest that the distribution of hidden variables should be chosen according to the principle of maximal relevance. We introduce the Hierarchical Feature Model (HFM) as an example of a model that satisfies this principle, and that encodes a neutral a priori organisation of the feature space. We pre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;</title><link>http://arxiv.org/abs/2204.08620</link><description>&lt;p&gt;
&#23621;&#27665;&#20247;&#21253;&#20013;&#31354;&#38388;&#27424;&#25253;&#21578;&#24046;&#24322;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22478;&#24066;&#27835;&#29702;&#20013;&#23621;&#27665;&#20247;&#21253;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#20351;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#19981;&#20877;&#25104;&#20026;&#22478;&#24066;&#27835;&#29702;&#19979;&#28216;&#35299;&#20915;&#20107;&#20214;&#36895;&#24230;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#26681;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22478;&#24066;&#27835;&#29702;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20247;&#21253;(&#8220;&#21327;&#21516;&#29983;&#20135;&#8221;)&#65292;&#20197;&#35782;&#21035; downed trees &#21644; power lines &#31561;&#38382;&#39064;&#12290;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#23621;&#27665;&#19981;&#20197;&#30456;&#21516;&#30340;&#36895;&#29575;&#25253;&#21578;&#38382;&#39064;&#65292;&#19981;&#21516;&#30340;&#25253;&#21578;&#24322;&#36136;&#24615;&#30452;&#25509;&#36716;&#21270;&#20026;&#19979;&#28216;&#22312;&#35299;&#20915;&#20107;&#20214;&#30340;&#36895;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#27979;&#37327;&#36825;&#26679;&#30340;&#27424;&#25253;&#21578;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#32479;&#35745;&#20219;&#21153;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#25105;&#20204;&#19981;&#33021;&#35266;&#23519;&#21040;&#27809;&#26377;&#34987;&#25253;&#21578;&#30340;&#20107;&#20214;&#25110;&#25253;&#21578;&#20107;&#20214;&#31532;&#19968;&#27425;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#19981;&#33021;&#21333;&#32431;&#22320;&#21306;&#20998;&#20302;&#25253;&#36947;&#29575;&#21644;&#20302;&#22522;&#20934;&#30495;&#23454;&#20107;&#20214;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;(&#24322;&#36136;&#30340;)&#25253;&#36947;&#29575;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#22312;&#30456;&#21516;&#20107;&#20214;&#30340; $\textit{duplicate}$ &#25253;&#21578;&#20013;&#30340;&#27604;&#29575;&#21487;&#20197;&#21033;&#29992;&#26469;&#28040;&#38500;&#25253;&#21578;&#29575;&#38543;&#20107;&#20214;&#21457;&#29983;&#32780;&#21457;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20511;&#21161;&#36825;&#20010;&#24605;&#24819;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#26631;&#20934;&#30340;&#27850;&#26494;&#29575;&#20272;&#35745;&#20219;&#21153;&#65292;&#23613;&#31649;&#26631;&#39064;&#26377;&#24456;&#22810;&#25216;&#26415;&#26415;&#35821;&#65292;&#20294;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#20247;&#21253;&#26469;&#24110;&#21161;&#22478;&#24066;&#27835;&#29702;&#65292;&#19981;&#21516;&#30340;&#25253;&#36947;&#29575;&#22914;&#20309;&#23548;&#33268;&#38382;&#39064;&#35299;&#20915;&#30340;&#24046;&#24322;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#27979;&#37327;&#25253;&#36947;&#29575;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#22806;&#37096;&#25968;&#25454;&#12290;&#20182;&#20204;&#21033;&#29992;&#22810;&#27425;&#25253;&#21578;&#30340;&#20107;&#20214;&#21487;&#20197;&#24110;&#21161;&#21306;&#20998;&#20107;&#20214;&#26159;&#21542;&#21457;&#29983;&#20197;&#21450;&#20854;&#25253;&#36947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern city governance relies heavily on crowdsourcing ("co-production") to identify problems such as downed trees and power lines. A major concern is that residents do not report problems at the same rates, with reporting heterogeneity directly translating to downstream disparities in how quickly incidents can be addressed. Measuring such under-reporting is a difficult statistical task, as, by definition, we do not observe incidents that are not reported or when reported incidents first occurred. Thus, low reporting rates and low ground-truth incident rates cannot be naively distinguished. We develop a method to identify (heterogeneous) reporting rates, without using external ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged to disambiguate whether an incident has occurred with its reporting rate once it has occurred. Using this idea, we reduce the question to a standard Poisson rate estimation task -- even though the
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.10629</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10629
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#26041;&#38754;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29978;&#33267;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#20197;&#20415;&#26377;&#25928;&#22320;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#32454;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25968;&#25454;&#26377;&#38480;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#24320;&#21457;&#25104;&#26412;&#21463;&#38480;&#65307;&#65288;iii&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#20415;&#26377;&#25928;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#27010;&#24565;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#19968;&#20010;&#31934;&#24515;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#32454;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#21487;&#20197;&#24046;&#24322;&#24040;&#22823;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#23454;&#29616;&#20102;&#32858;&#31867;&#19968;&#33268;&#24615;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.12909</link><description>&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#26368;&#20248;&#21464;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Optimal Variable Clustering for High-Dimensional Matrix Valued Data. (arXiv:2112.12909v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#39640;&#32500;&#30697;&#38453;&#25968;&#25454;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#65292;&#29702;&#35770;&#19978;&#23454;&#29616;&#20102;&#32858;&#31867;&#19968;&#33268;&#24615;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#20540;&#25968;&#25454;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26085;&#30410;&#26222;&#21450;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36825;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#32858;&#31867;&#26041;&#27861;&#26159;&#38024;&#23545;&#24179;&#22343;&#27169;&#22411;&#35774;&#35745;&#30340;&#65292;&#19981;&#32771;&#34385;&#29305;&#24449;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#32780;&#35813;&#32467;&#26500;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#20174;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#21462;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#29305;&#24449;&#25490;&#21015;&#25104;&#30697;&#38453;&#24418;&#24335;&#65292;&#24182;&#20351;&#29992;&#19968;&#20123;&#26410;&#30693;&#30340;&#25104;&#21592;&#30697;&#38453;&#34920;&#31034;&#34892;&#21644;&#21015;&#30340;&#32858;&#31867;&#12290;&#22312;&#27492;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31867;&#20351;&#29992;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#24046;&#24322;&#20316;&#20026;&#19981;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23454;&#29616;&#32858;&#31867;&#19968;&#33268;&#24615;&#12290;&#34429;&#28982;&#36825;&#31181;&#19968;&#33268;&#24615;&#32467;&#26524;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#24191;&#27867;&#30340;&#21152;&#26435;&#21327;&#26041;&#24046;&#30697;&#38453;&#31867;&#21035;&#65292;&#20294;&#36825;&#20010;&#32467;&#26524;&#30340;&#26465;&#20214;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#20989;&#25968;&#21644;&#21152;&#26435;&#26426;&#21046;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#29305;&#24449;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix valued data has become increasingly prevalent in many applications. Most of the existing clustering methods for this type of data are tailored to the mean model and do not account for the dependence structure of the features, which can be very informative, especially in high-dimensional settings. To extract the information from the dependence structure for clustering, we propose a new latent variable model for the features arranged in matrix form, with some unknown membership matrices representing the clusters for the rows and columns. Under this model, we further propose a class of hierarchical clustering algorithms using the difference of a weighted covariance matrix as the dissimilarity measure. Theoretically, we show that under mild conditions, our algorithm attains clustering consistency in the high-dimensional setting. While this consistency result holds for our algorithm with a broad class of weighted covariance matrices, the conditions for this result depend on the choic
&lt;/p&gt;</description></item></channel></rss>