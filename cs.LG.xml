<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#20132;&#38598;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#39640;&#26031;&#35757;&#32451;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#19978;&#30028;&#65292;&#21253;&#25324;&#19968;&#31181;TDS&#23398;&#20064;$k$&#20010;&#40784;&#27425;&#21322;&#31354;&#38388;&#20132;&#38598;&#36798;&#21040;&#31934;&#24230;$\epsilon$&#30340;$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$&#26102;&#38388;&#31639;&#27861;&#65288;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65289;&#12290;</title><link>https://arxiv.org/abs/2404.02364</link><description>&lt;p&gt;
&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#21322;&#31354;&#38388;&#20132;&#38598;&#65306;&#25913;&#36827;&#31639;&#27861;&#21644;SQ&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Learning Intersections of Halfspaces with Distribution Shift: Improved Algorithms and SQ Lower Bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02364
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23398;&#20064;&#19982;&#20998;&#24067;&#20559;&#31227;&#30340;&#20132;&#38598;&#38382;&#39064;&#65292;&#22312;&#22522;&#20110;&#39640;&#26031;&#35757;&#32451;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#19978;&#30028;&#65292;&#21253;&#25324;&#19968;&#31181;TDS&#23398;&#20064;$k$&#20010;&#40784;&#27425;&#21322;&#31354;&#38388;&#20132;&#38598;&#36798;&#21040;&#31934;&#24230;$\epsilon$&#30340;$2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$&#26102;&#38388;&#31639;&#27861;&#65288;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Klivans&#12289;Stavropoulos&#21644;Vasilyan&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#21457;&#20102;&#23545;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#21487;&#27979;&#35797;&#23398;&#20064;&#65288;TDS&#23398;&#20064;&#65289;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#20174;&#35757;&#32451;&#20998;&#24067;$\mathcal{D}$&#33719;&#24471;&#26631;&#35760;&#26679;&#26412;&#65292;&#20174;&#27979;&#35797;&#20998;&#24067;$\mathcal{D}'$&#33719;&#24471;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#30446;&#26631;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#36890;&#36807;&#30456;&#24212;&#30340;&#27979;&#35797;&#26102;&#36755;&#20986;&#22312;$\mathcal{D}'$&#19978;&#20855;&#26377;&#20302;&#35823;&#24046;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#27169;&#22411;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#25152;&#26377;&#24037;&#20316;&#65292;&#22240;&#20026;$\mathcal{D}'$&#19978;&#27809;&#26377;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#24403;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#36793;&#38469;&#30456;&#31561;&#26102;&#65292;&#27979;&#35797;&#24517;&#39035;&#25509;&#21463;&#65288;&#20197;&#39640;&#27010;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02364v1 Announce Type: cross  Abstract: Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of testable learning with distribution shift (TDS learning), where a learner is given labeled samples from training distribution $\mathcal{D}$, unlabeled samples from test distribution $\mathcal{D}'$, and the goal is to output a classifier with low error on $\mathcal{D}'$ whenever the training samples pass a corresponding test. Their model deviates from all prior work in that no assumptions are made on $\mathcal{D}'$. Instead, the test must accept (with high probability) when the marginals of the training and test distributions are equal.   Here we focus on the fundamental case of intersections of halfspaces with respect to Gaussian training distributions and prove a variety of new upper bounds including a $2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$-time algorithm for TDS learning intersections of $k$ homogeneous halfspaces to accuracy $\epsilon$ (prior work achieved
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2403.18370</link><description>&lt;p&gt;
&#35270;&#37326;&#20013;&#30340;&#33337;&#21482;&#65306;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Ship in Sight: Diffusion Models for Ship-Image Super Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#21463;&#21040;&#19981;&#26029;&#22686;&#38271;&#30340;&#23545;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#23376;&#20219;&#21153;&#65288;&#22914;&#20462;&#34917;&#12289;&#21435;&#22122;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#39640;&#36136;&#37327;&#32467;&#26524;&#30340;&#38656;&#27714;&#39537;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28145;&#20837;&#25506;&#35752;&#20102;&#33337;&#21482;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#36825;&#23545;&#27839;&#28023;&#21644;&#28207;&#21475;&#30417;&#35270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24212;&#29992;&#26426;&#20250;&#65292;&#21033;&#29992;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#23398;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#25991;&#26412;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24182;&#22312;&#31867;&#21035;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#20445;&#23384;&#33337;&#21482;&#30340;&#20851;&#38190;&#32454;&#33410;&#22312;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18370v1 Announce Type: cross  Abstract: In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resolut
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13502</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#39033;&#32508;&#21512;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Automated Control Systems: A Comprehensive Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#30340;&#26032;&#39062;&#20445;&#25252;&#26041;&#27861;&#65292;&#24182;&#20026;&#30830;&#20445;&#24037;&#19994;&#20013;&#30340;&#31283;&#20581;&#25925;&#38556;&#35786;&#26029;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26426;&#22120;&#23398;&#20064;&#25972;&#21512;&#21040;&#33258;&#21160;&#25511;&#21046;&#31995;&#32479;&#65288;ACS&#65289;&#20013;&#22686;&#24378;&#20102;&#24037;&#19994;&#36807;&#31243;&#31649;&#29702;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#39033;&#38480;&#21046;&#24037;&#19994;&#26222;&#36941;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#26159;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Tennessee Eastman&#36807;&#31243;&#25968;&#25454;&#38598;&#22312;ACS&#20013;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25925;&#38556;&#35786;&#26029;&#26102;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#20102;&#20845;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#25506;&#35752;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#24378;&#22823;&#33030;&#24369;&#24615;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#30340;&#19981;&#21516;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#38450;&#24481;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#30830;&#20445;&#24037;&#19994;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#24615;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#30830;&#20445;&#20102;&#24037;&#19994;&#20013;&#31283;&#20581;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13502v1 Announce Type: new  Abstract: Integrating machine learning into Automated Control Systems (ACS) enhances decision-making in industrial process management. One of the limitations to the widespread adoption of these technologies in industry is the vulnerability of neural networks to adversarial attacks. This study explores the threats in deploying deep learning models for fault diagnosis in ACS using the Tennessee Eastman Process dataset. By evaluating three neural networks with different architectures, we subject them to six types of adversarial attacks and explore five different defense methods. Our results highlight the strong vulnerability of models to adversarial samples and the varying effectiveness of defense strategies. We also propose a novel protection approach by combining multiple defense methods and demonstrate it's efficacy. This research contributes several insights into securing machine learning within ACS, ensuring robust fault diagnosis in industrial 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08258</link><description>&lt;p&gt;
Skipformer&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35821;&#38899;&#35782;&#21035;&#30340;&#36339;&#36807;&#21644;&#24674;&#22797;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08258
&lt;/p&gt;
&lt;p&gt;
Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#26438;&#27169;&#22411;&#12290;&#36890;&#24120;&#24341;&#20837;&#19968;&#20010;&#31354;&#30333;&#31526;&#21495;&#26469;&#23545;&#40784;CTC&#25110;RNN-T&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38271;&#36755;&#20837;&#38271;&#24230;&#20250;&#23545;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#36896;&#25104;&#20108;&#27425;&#36127;&#33655;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Skipformer&#30340;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;Conformer&#26550;&#26500;&#65292;&#20197;&#21160;&#24577;&#21644;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#12290;Skipformer&#20351;&#29992;&#20013;&#38388;CTC&#36755;&#20986;&#20316;&#20026;&#26631;&#20934;&#23558;&#24103;&#20998;&#20026;&#19977;&#32452;&#65306;&#20851;&#38190;&#12289;&#36339;&#36807;&#21644;&#24573;&#30053;&#12290;&#20851;&#38190;&#32452;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;Conformer&#22359;&#65292;&#20854;&#36755;&#20986;&#19982;&#36339;&#36807;&#32452;&#36890;&#36807;&#21407;&#22987;&#26102;&#38388;&#39034;&#24207;&#32852;&#25509;&#20316;&#20026;&#26368;&#32456;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Aishell-1&#19978;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#20943;&#23569;&#20102;31&#20493;&#65292;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#20943;&#23569;&#20102;22&#20493;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08258v1 Announce Type: new  Abstract: Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a "Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#21644;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#21040;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#31561;&#25928;&#30340;&#34920;&#36798;&#33021;&#21147;&#65307;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#22810;&#39033;&#24335;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07954</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#65306;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#21644;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#21040;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#31561;&#25928;&#30340;&#34920;&#36798;&#33021;&#21147;&#65307;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#22810;&#39033;&#24335;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20063;&#31216;&#20026;&#35889;&#22270;&#28388;&#27874;&#22120;&#65292;&#22312;&#32593;&#32476;&#22270;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#32469;&#36807;&#29305;&#24449;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#20934;&#36827;&#34892;&#28388;&#27874;&#22120;&#35757;&#32451;&#30340;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#65292;&#20197;&#36817;&#20284;&#22270;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#30740;&#31350;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#25506;&#35752;&#22810;&#26679;&#21270;&#30340;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#20197;&#21450;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#32479;&#19968;&#25104;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#30456;&#21516;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#30340;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#22810;&#39033;&#24335;&#30340;&#28176;&#36817;&#25910;&#25947;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#24322;&#36136;&#31243;&#24230;&#30340;&#22270;&#20013;&#30340;&#26377;&#38480;&#36866;&#24212;&#24615;&#12290;&#21463;&#21040;&#36825;&#20123;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#22810;&#39033;&#24335;&#22522;&#20934;&#65292;&#24182;&#21487;&#35777;&#26126;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07954v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization.   In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.05110</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#36890;&#36807;&#32452;&#21512;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Collection for Robotic Manipulation via Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05110
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#22797;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#36991;&#20813;&#25910;&#38598;&#22788;&#29702;&#22797;&#21512;&#24773;&#20917;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25910;&#38598;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#22914;&#20309;&#26377;&#25928;&#22320;&#25910;&#38598;&#25968;&#25454;&#20197;&#20419;&#36827;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#32570;&#20047;&#24456;&#22810;&#29702;&#35299;&#12290;&#26368;&#36817;&#20851;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#25910;&#38598;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#21464;&#21270;&#20102;&#35768;&#22810;&#29615;&#22659;&#22240;&#32032;&#65292;&#22914;&#29289;&#20307;&#31867;&#22411;&#21644;&#26700;&#38754;&#32441;&#29702;&#12290;&#34429;&#28982;&#36825;&#20123;&#30740;&#31350;&#35797;&#22270;&#28085;&#30422;&#21508;&#31181;&#21508;&#26679;&#30340;&#22330;&#26223;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#32771;&#34385;&#21040;&#22522;&#20110;&#25968;&#25454;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#33021;&#20855;&#26377;&#30340;&#22797;&#21512;&#33021;&#21147;&#12290;&#22914;&#26524;&#26426;&#22120;&#20154;&#31574;&#30053;&#33021;&#22815;&#20174;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#32452;&#21512;&#19981;&#21516;&#30340;&#29615;&#22659;&#21464;&#37327;&#65288;&#20363;&#22914;&#29289;&#20307;&#31867;&#22411;&#12289;&#26700;&#38754;&#39640;&#24230;&#65289;&#20197;&#22312;&#36935;&#21040;&#30475;&#19981;&#35265;&#30340;&#22240;&#32032;&#32452;&#21512;&#26102;&#25104;&#21151;&#65292;&#37027;&#20040;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#28857;&#26469;&#36991;&#20813;&#20026;&#22797;&#21512;&#22788;&#29702;&#30340;&#24773;&#20917;&#25910;&#38598;&#25968;&#25454;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05110v1 Announce Type: cross  Abstract: Data collection has become an increasingly important problem in robotic manipulation, yet there still lacks much understanding of how to effectively collect data to facilitate broad generalization. Recent works on large-scale robotic data collection typically vary a wide range of environmental factors during data collection, such as object types and table textures. While these works attempt to cover a diverse variety of scenarios, they do not explicitly account for the possible compositional abilities of policies trained on the data. If robot policies are able to compose different environmental factors of variation (e.g., object types, table heights) from their training data to succeed when encountering unseen factor combinations, then we can exploit this to avoid collecting data for situations that composition would address. To investigate this possibility, we conduct thorough empirical studies both in simulation and on a real robot t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.04493</link><description>&lt;p&gt;
&#20351;&#22270;&#20687;&#30495;&#23454;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What makes an image realistic?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04493
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#30475;&#36215;&#26469;&#30495;&#23454;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#26080;&#35770;&#26159;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#36824;&#26159;&#35270;&#39057;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20043;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#21363;&#37327;&#21270;&#29616;&#23454;&#20027;&#20041;&#65292;&#21363;&#35774;&#35745;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#21644;&#19981;&#30495;&#23454;&#25968;&#25454;&#30340;&#20989;&#25968;&#12290;&#20174;&#31639;&#27861;&#20449;&#24687;&#29702;&#35770;&#30340;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#21333;&#29420;&#19981;&#33021;&#35299;&#20915;&#23427;&#65292;&#20197;&#21450;&#19968;&#20010;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#26159;&#20160;&#20040;&#26679;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#35780;&#35770;&#32773;&#30340;&#27010;&#24565;&#65292;&#19981;&#20687;&#23545;&#25239;&#24615;&#35780;&#35770;&#32773;&#37027;&#26679;&#38656;&#35201;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#23613;&#31649;&#36890;&#29992;&#35780;&#35770;&#32773;&#24182;&#19981;&#31435;&#21363;&#23454;&#29992;&#65292;&#20294;&#23427;&#20204;&#26082;&#21487;&#20197;&#20316;&#20026;&#24341;&#23548;&#23454;&#38469;&#23454;&#29616;&#30340;&#21271;&#26497;&#26143;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04493v1 Announce Type: new  Abstract: The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool 
&lt;/p&gt;</description></item><item><title>&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#24314;&#31435;&#20102;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#20197;&#36817;&#20284;&#20174;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02867</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#26694;&#26550;&#29992;&#20110;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Continuous-time Diffusion Framework for Network Inference and Influence Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02867
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#24314;&#31435;&#20102;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#20197;&#36817;&#20284;&#20174;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#36830;&#32493;&#26102;&#38388;&#20449;&#24687;&#20256;&#25773;&#30340;&#30740;&#31350;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#24403;&#21482;&#33021;&#35775;&#38382;&#20256;&#25773;&#36319;&#36394;&#65288;&#32423;&#32852;&#65289;&#26102;&#65292;&#22522;&#20110;&#32423;&#32852;&#30340;&#32593;&#32476;&#25512;&#26029;&#21644;&#24433;&#21709;&#20272;&#35745;&#26159;&#20004;&#20010;&#24517;&#39035;&#25506;&#35752;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25512;&#26029;&#21644;&#22788;&#29702;&#36229;&#36807;&#20960;&#21315;&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#25193;&#25955;&#36807;&#31243;&#35270;&#20026;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#65292;&#22522;&#20110;&#27492;&#24314;&#31435;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65288;FIM&#65289;&#65292;&#20197;&#36817;&#20284;&#20174;&#21487;&#29992;&#32423;&#32852;&#20013;&#25512;&#26029;&#20986;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#30340;&#25193;&#25955;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;FIM&#22312;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#23454;&#29616;&#24433;&#21709;&#20272;&#35745;&#30340;&#25152;&#38656;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02867v1 Announce Type: cross  Abstract: The study of continuous-time information diffusion has been an important area of research for many applications in recent years. When only the diffusion traces (cascades) are accessible, cascade-based network inference and influence estimation are two essential problems to explore. Alas, existing methods exhibit limited capability to infer and process networks with more than a few thousand nodes, suffering from scalability issues. In this paper, we view the diffusion process as a continuous-time dynamical system, based on which we establish a continuous-time diffusion model. Subsequently, we instantiate the model to a scalable and effective framework (FIM) to approximate the diffusion propagation from available cascades, thereby inferring the underlying network structure. Furthermore, we undertake an analysis of the approximation error of FIM for network inference. To achieve the desired scalability for influence estimation, we devise 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01221</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25104;&#26412;&#25928;&#29575;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#32467;&#26524;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25512;&#33616;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25805;&#20316;&#30340;&#36755;&#20837;&#26356;&#25913;&#65292;&#23558;&#19981;&#33391;&#31995;&#32479;&#36755;&#20986;&#36716;&#21464;&#20026;&#26399;&#26395;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;&#35299;&#37322;&#21333;&#20010;&#23454;&#20363;&#65292;&#20294;&#19968;&#20123;&#30495;&#23454;&#30340;&#29992;&#20363;&#65288;&#22914;&#23458;&#25143;&#28385;&#24847;&#24230;&#65289;&#38656;&#35201;&#35782;&#21035;&#33021;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;&#23454;&#20363;&#65288;&#20363;&#22914;&#23458;&#25143;&#65289;&#30340;&#21333;&#19968;&#21453;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25214;&#21040;&#36825;&#26679;&#30340;&#23454;&#20363;&#32452;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01221v1 Announce Type: cross  Abstract: Counterfactual explanations constitute among the most popular methods for analyzing the predictions of black-box systems since they can recommend cost-efficient and actionable changes to the input to turn an undesired system's output into a desired output. While most of the existing counterfactual methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single counterfactual that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance counterfactual explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.01218</link><description>&lt;p&gt;
&#31895;&#31961;&#21453;&#23398;&#20064;&#38656;&#35201;&#26356;&#21152;&#35880;&#24910;&#30340;&#35780;&#20272;&#20197;&#36991;&#20813;&#34394;&#20551;&#38544;&#31169;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01218
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#21453;&#23398;&#20064;&#29615;&#22659;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23545;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#30340;&#39640;&#25104;&#26412;&#20351;&#24471;&#24320;&#21457;&#21453;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#21024;&#38500;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20174;&#30452;&#35273;&#19978;&#35762;&#65292;&#19968;&#26086;&#27169;&#22411;&#23436;&#25104;&#21453;&#23398;&#20064;&#65292;&#19982;&#35813;&#27169;&#22411;&#20132;&#20114;&#30340;&#23545;&#25163;&#23601;&#19981;&#24212;&#20877;&#33021;&#22815;&#21028;&#26029;&#21453;&#23398;&#20064;&#30340;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#12290;&#22312;&#38544;&#31169;&#39046;&#22495;&#65292;&#36825;&#34987;&#31216;&#20026;&#25104;&#21592;&#25512;&#26029;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#23545;&#21453;&#23398;&#20064;&#35774;&#32622;&#30340;&#35843;&#25972;&#65288;&#23548;&#33268;&#23427;&#20204;&#30340;&#8220;U-MIA&#8221;&#23545;&#24212;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29616;&#26377;U-MIA&#30340;&#20998;&#31867;&#65292;&#23558;&#20854;&#20998;&#20026;&#8220;&#20154;&#21475;U-MIA&#8221;&#65292;&#20854;&#20013;&#21516;&#19968;&#25915;&#20987;&#32773;&#36866;&#29992;&#20110;&#25152;&#26377;&#31034;&#20363;&#65292;&#21644;&#8220;&#27599;&#20010;&#31034;&#20363;U-MIA&#8221;&#65292;&#20854;&#20013;&#20026;&#27599;&#20010;&#31034;&#20363;&#23454;&#20363;&#21270;&#20102;&#19987;&#29992;&#25915;&#20987;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21518;&#19968;&#31867;&#21035;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#20026;&#27599;&#20010;&#23454;&#20363;&#23450;&#21046;&#20854;&#25104;&#21592;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#65292;&#20197;&#35299;&#20915;&#26657;&#20934;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17655</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence-Aware Multi-Field Model Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#65292;&#20197;&#35299;&#20915;&#26657;&#20934;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29992;&#25143;&#21453;&#39304;&#27010;&#29575;&#65288;&#22914;&#28857;&#20987;&#21644;&#36716;&#25442;&#65289;&#23545;&#20110;&#24191;&#21578;&#25490;&#21517;&#21644;&#31454;&#20215;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#31227;&#21644;&#22266;&#26377;&#27169;&#22411;&#20559;&#24046;&#65292;&#39044;&#27979;&#27010;&#29575;&#19982;&#30495;&#23454;&#21487;&#33021;&#24615;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#19981;&#24076;&#26395;&#30340;&#19981;&#19968;&#33268;&#12290;&#26657;&#20934;&#26088;&#22312;&#36890;&#36807;&#21518;&#22788;&#29702;&#27169;&#22411;&#39044;&#27979;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#32780;&#22522;&#20110;&#23383;&#27573;&#30340;&#26657;&#20934;&#21487;&#20197;&#35843;&#25972;&#19981;&#21516;&#29305;&#24449;&#23383;&#27573;&#20540;&#19978;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#32454;&#31890;&#24230;&#30340;&#24191;&#21578;&#38656;&#27714;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#24212;&#20110;&#26576;&#20123;&#23383;&#27573;&#20540;&#30340;&#35266;&#23519;&#26679;&#26412;&#21487;&#33021;&#22826;&#26377;&#38480;&#65292;&#26080;&#27861;&#36827;&#34892;&#26377;&#20449;&#24515;&#30340;&#26657;&#20934;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#25918;&#22823;&#21644;&#22312;&#32447;&#24178;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#22810;&#23383;&#27573;&#26657;&#20934;&#26041;&#27861;&#65292;&#26681;&#25454;&#26679;&#26412;&#32479;&#35745;&#25512;&#23548;&#30340;&#32622;&#20449;&#27700;&#24179;&#33258;&#36866;&#24212;&#35843;&#25972;&#26657;&#20934;&#24378;&#24230;&#12290;&#23427;&#36824;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#23383;&#27573;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17655v1 Announce Type: new  Abstract: Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16918</link><description>&lt;p&gt;
m2mKD&#65306;&#27169;&#22359;&#38388;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#27169;&#22359;&#21270;Transformer
&lt;/p&gt;
&lt;p&gt;
m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32467;&#26500;&#22240;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26032;&#39046;&#22495;&#30340;&#39640;&#25928;&#36866;&#24212;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#31232;&#30095;&#36830;&#25509;&#23548;&#33268;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#25972;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#31561;&#25216;&#26415;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22312;&#22810;&#20010;&#26469;&#28304;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24182;&#19981;&#38024;&#23545;&#27169;&#22359;&#21270;&#27169;&#22411;&#35774;&#35745;&#65292;&#30452;&#25509;&#24212;&#29992;&#26102;&#21487;&#33021;&#22833;&#36133;&#65292;&#36825;&#26159;&#30001;&#20110;&#29420;&#29305;&#30340;&#26550;&#26500;&#21644;&#22823;&#37327;&#28041;&#21450;&#30340;&#21442;&#25968;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16918v1 Announce Type: new  Abstract: Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approac
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35777;&#20070;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#12289;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#30340;&#35774;&#32622;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20248;&#21270;&#32422;&#26463;&#21160;&#24577;&#35843;&#25972;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13937</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Verifying message-passing neural networks via topology-based bounds tightening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35777;&#20070;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#12289;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#30340;&#35774;&#32622;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20248;&#21270;&#32422;&#26463;&#21160;&#24577;&#35843;&#25972;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32463;&#24120;&#23481;&#26131;&#36973;&#21463;&#25915;&#20987;&#65292;&#25105;&#20204;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#25552;&#20379;&#24378;&#22823;&#30340;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#20043;&#19978;&#65292;&#32534;&#30721;&#20102;&#22810;&#31181;&#23376;&#38382;&#39064;&#65292;&#20363;&#22914;&#20801;&#35768;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#65292;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#26469;&#25910;&#32039;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20351;&#29992;&#31215;&#26497;&#30340;&#30028;&#38480;&#32039;&#32553;&#26469;&#21160;&#24577;&#25913;&#21464;&#20248;&#21270;&#32422;&#26463;&#65292;&#21363;&#36890;&#36807;&#25910;&#32039;&#21464;&#37327;&#30028;&#38480;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24320;&#28304;&#30340;&#20998;&#25903;&#23450;&#30028;&#27714;&#35299;&#22120;SCIP&#12290;&#25105;&#20204;&#22312;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13937v1 Announce Type: cross  Abstract: Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classifi
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;</title><link>https://arxiv.org/abs/2402.09802</link><description>&lt;p&gt;
&#20934;&#21017;&#23849;&#28291;&#21644;&#25439;&#22833;&#20998;&#24067;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Criterion collapse and loss distribution control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#21508;&#31181;&#23398;&#20064;&#20934;&#21017;&#19979;&#23849;&#28291;&#25104;&#35823;&#24046;&#27010;&#29575;&#26368;&#23567;&#21270;&#22120;&#30340;&#26465;&#20214;&#65292;&#20174;DRO&#21644;OCE&#39118;&#38505;&#65288;CVaR&#12289;&#20542;&#26012;ERM&#65289;&#21040;&#25991;&#29486;&#20013;&#25506;&#32034;&#30340;&#26368;&#26032;&#19978;&#21319;-&#19979;&#38477;&#31639;&#27861;&#30340;&#38750;&#21333;&#35843;&#20934;&#21017;&#65288;&#27946;&#27700;&#12289;SoftAD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20271;&#21162;&#21033;&#20998;&#24067;&#25439;&#22833;&#30340;&#32972;&#26223;&#19979;&#65292;CVaR&#21644;DRO&#30340;&#29616;&#26377;&#32467;&#26524;&#36828;&#36828;&#36229;&#36234;&#20102;&#23849;&#28291;&#30340;&#33539;&#22260;&#65292;&#28982;&#21518;&#25193;&#22823;&#20102;&#25105;&#20204;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20195;&#29702;&#25439;&#22833;&#65292;&#23637;&#31034;&#20102;&#20687;&#20542;&#26012;ERM&#36825;&#26679;&#30340;&#21333;&#35843;&#20934;&#21017;&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#30340;&#26465;&#20214;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09802v1 Announce Type: cross  Abstract: In this work, we consider the notion of "criterion collapse," in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08845</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#36890;&#36807;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#36827;&#34892;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#38454;&#27573;&#25200;&#21160;&#27979;&#35797;&#26469;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;FAMs&#65289;&#36890;&#36807;&#25200;&#21160;&#27979;&#35797;&#26469;&#27979;&#37327;&#27599;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#25200;&#21160;&#19979;&#30340;&#39044;&#27979;&#24046;&#24322;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#30340;&#39044;&#27979;&#21464;&#21270;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#25200;&#21160;&#27979;&#35797;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#22686;&#24378;FAMs&#22312;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#19981;&#21516;&#29305;&#24449;&#36129;&#29486;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25200;&#21160;&#19968;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#21464;&#21270;&#36215;&#21040;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#20316;&#29992;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#20316;&#20026;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#36827;&#34892;&#29305;&#24449;&#24402;&#22240;&#65288;FANS&#65289;&#65292;&#36890;&#36807;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#65288;&#20107;&#23454;&#24615;&#21644;&#24178;&#39044;&#24615;&#65289;&#30340;&#25200;&#21160;&#27979;&#35797;&#35745;&#31639;PNS&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20026;&#20102;&#29983;&#25104;&#21453;&#20107;&#23454;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08845v1 Announce Type: new Abstract: We investigate the problem of explainability in machine learning.To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations.However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation.In order to enhance the ability of FAMs to distinguish different features' contributions in this challenging setting, we propose to utilize the probability (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance.Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional).In practice, to generate counterfactual samples, we use a re
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.08526</link><description>&lt;p&gt;
Concept-1K&#65306;&#19968;&#31181;&#29992;&#20110;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Concept-1K: A Novel Benchmark for Instance Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Concept-1K&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24433;&#21709;&#22240;&#32032;&#21253;&#25324;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#12290;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;LoRA&#25216;&#26415;&#26080;&#27861;&#28385;&#36275;&#24615;&#33021;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#32531;&#35299;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#65288;IL&#65289;&#23545;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20154;&#31867;&#32423;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IL&#22330;&#26223;&#21644;&#25968;&#25454;&#38598;&#26080;&#27861;&#35780;&#20272;PLM&#20013;&#30340;&#36951;&#24536;&#65292;&#20351;&#20154;&#35823;&#20197;&#20026;PLM&#19981;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;IL&#22330;&#26223;&#65292;&#31216;&#20026;&#23454;&#20363;&#22686;&#37327;&#23398;&#20064;&#65288;IIL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25903;&#25345;&#25968;&#37327;&#32423;&#26356;&#22823;&#30340;IL&#27493;&#39588;&#30340;&#26032;&#25968;&#25454;&#38598;Concept-1K&#12290;&#22522;&#20110;&#23545;Concept-1K&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21313;&#20159;&#21442;&#25968;&#30340;PLM&#20173;&#28982;&#36973;&#21463;&#30528;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#36951;&#24536;&#21463;&#27169;&#22411;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#21644;&#32531;&#20914;&#21306;&#22823;&#23567;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;IL&#26041;&#27861;&#21644;&#19968;&#31181;&#27969;&#34892;&#30340;&#24494;&#35843;&#25216;&#26415;LoRA&#37117;&#26410;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;PLM&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#40723;&#21169;&#35774;&#35745;&#26356;&#24378;&#22823;&#30340;&#25216;&#26415;&#20197;&#20943;&#36731;PLM&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental learning (IL) is essential to realize the human-level intelligence in the neural network. However, existing IL scenarios and datasets are unqualified for assessing forgetting in PLMs, giving an illusion that PLMs do not suffer from catastrophic forgetting. To this end, we propose a challenging IL scenario called instance-incremental learning (IIL) and a novel dataset called Concept-1K, which supports an order of magnitude larger IL steps. Based on the experiments on Concept-1K, we reveal that billion-parameter PLMs still suffer from catastrophic forgetting, and the forgetting is affected by both model scale, pretraining, and buffer size. Furthermore, existing IL methods and a popular finetuning technique, LoRA, fail to achieve satisfactory performance. Our study provides a novel scenario for future studies to explore the catastrophic forgetting of PLMs and encourage more powerful techniques to be designed for alleviating the forgetting in PLMs. The data, code and scripts ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08086</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Text-centric Alignment for Multi-Modality Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65288;TAMML&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#21363;&#25512;&#29702;&#38454;&#27573;&#21487;&#29992;&#30340;&#27169;&#24577;&#19982;&#35757;&#32451;&#38454;&#27573;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25991;&#26412;&#20013;&#24515;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;TAMML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#20511;&#21161;&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#31995;&#32479;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#25991;&#26412;&#20316;&#20026;&#32479;&#19968;&#30340;&#35821;&#20041;&#31354;&#38388;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;TAMML&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#27169;&#24577;&#32452;&#21512;&#26041;&#38754;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;TAMML&#19981;&#20165;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#36824;&#33021;&#20445;&#25345;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#20811;&#26381;&#20256;&#32479;&#30340;&#22266;&#23450;&#27169;&#24577;&#26694;&#26550;&#20013;&#30340;&#34920;&#31034;&#23884;&#20837;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#20250;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper addresses the challenge of modality mismatch in multimodal learning, where the modalities available during inference differ from those available at training. We propose the Text-centric Alignment for Multi-Modality Learning (TAMML) approach, an innovative method that utilizes Large Language Models (LLMs) with in-context learning and foundation models to enhance the generalizability of multimodal systems under these conditions. By leveraging the unique properties of text as a unified semantic space, TAMML demonstrates significant improvements in handling unseen, diverse, and unpredictable modality combinations. TAMML not only adapts to varying modalities but also maintains robust performance, showcasing the potential of foundation models in overcoming the limitations of traditional fixed-modality frameworks in embedding representations. This study contributes to the field by offering a flexible, effective solution for real-world applications where modality availabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07692</link><description>&lt;p&gt;
Bayesian&#20248;&#21270;&#20013;&#30340;&#36793;&#30028;&#25506;&#32034;&#19982;&#26410;&#30693;&#29289;&#29702;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20248;&#21270;&#35780;&#20272;&#27425;&#25968;&#20005;&#37325;&#38480;&#21046;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#19968;&#20123;&#29289;&#29702;&#25110;&#31995;&#32479;&#38480;&#21046;&#65292;&#24456;&#38590;&#25110;&#32773;&#19981;&#21487;&#33021;&#20107;&#20808;&#30693;&#36947;&#21738;&#20123;&#35774;&#35745;&#26159;&#21487;&#34892;&#30340;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#35299;&#36890;&#24120;&#20301;&#20110;&#35774;&#35745;&#31354;&#38388;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#27604;&#20855;&#26377;&#20869;&#37096;&#26368;&#20248;&#35299;&#30340;&#24773;&#20917;&#26356;&#21152;&#22256;&#38590;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#20026;&#20102;&#30830;&#23450;&#36793;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#32422;&#26463;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#22797;&#26434;&#36793;&#30028;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06293</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting of Irregular Time Series via Conditional Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#22825;&#25991;&#23398;&#21644;&#27668;&#20505;&#23398;&#12290;&#30446;&#21069;&#35813;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20165;&#20272;&#35745;&#21333;&#20010;&#36890;&#36947;&#21644;&#21333;&#20010;&#26102;&#38388;&#28857;&#19978;&#35266;&#27979;&#20540;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#24418;&#29366;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;ProFITi&#65292;&#29992;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#22312;&#36807;&#21435;&#35266;&#27979;&#21644;&#26597;&#35810;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#19978;&#26465;&#20214;&#19979;&#26102;&#38388;&#24207;&#21015;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#20316;&#20026;&#27169;&#22411;&#32452;&#20214;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#19968;&#20010;&#21487;&#36870;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#25972;&#20010;&#23454;&#25968;&#32447;&#19978;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. State-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. In this work, we propose a novel model, ProFITi, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. The model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. As model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. We conduct extensive experiments on four datasets and demonstrate that the proposed model pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21028;&#21035;&#36229;&#24179;&#38754;&#26469;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#36824;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.05279</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21028;&#21035;&#36229;&#24179;&#38754;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Safety Filters for Black-Box Dynamical Systems by Learning Discriminating Hyperplanes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21028;&#21035;&#36229;&#24179;&#38754;&#26469;&#23454;&#29616;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#36824;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#27491;&#22312;&#25104;&#20026;&#40657;&#30418;&#21160;&#24577;&#31995;&#32479;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26377;&#25928;&#36884;&#24452;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#35777;&#26126;&#20989;&#25968;&#65288;&#22914;&#25511;&#21046;&#30028;&#38754;&#20989;&#25968;&#65288;CBFs&#65289;&#21644;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20540;&#20989;&#25968;&#65289;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#35748;&#35782;&#21040;&#26368;&#32456;&#23558;&#23433;&#20840;&#32422;&#26463;&#20316;&#20026;&#27599;&#20010;&#29366;&#24577;&#30340;&#25511;&#21046;&#36755;&#20837;&#32422;&#26463;&#26469;&#24378;&#21046;&#25191;&#34892;&#25165;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#36825;&#20010;&#32422;&#26463;&#65292;&#25105;&#20204;&#21487;&#20197;&#28040;&#38500;&#23545;&#20219;&#20309;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#35774;&#35745;&#30340;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21028;&#21035;&#36229;&#24179;&#38754;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#24418;&#25104;&#25511;&#21046;&#36755;&#20837;&#30340;&#21322;&#31354;&#38388;&#32422;&#26463;&#65292;&#20316;&#20026;&#23433;&#20840;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20010;&#27010;&#24565;&#19981;&#20165;&#24191;&#27867;&#36866;&#29992;&#20256;&#32479;&#30340;&#23433;&#20840;&#26041;&#27861;&#65292;&#32780;&#19988;&#36890;&#36807;&#28040;&#38500;&#23545;&#29305;&#23450;&#35777;&#26126;&#20989;&#25968;&#30340;&#20381;&#36182;&#65292;&#31616;&#21270;&#20102;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#21028;&#21035;&#36229;&#24179;&#38754;&#30340;&#31574;&#30053;&#65306;&#65288;a&#65289;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#20808;&#39564;&#35777;&#30340;&#35777;&#26126;&#20989;&#25968;&#20449;&#24687;&#26469;&#35757;&#32451;&#27169;&#22411;&#65307;&#65288;b&#65289;&#21152;&#36895;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#21028;&#21035;&#36229;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based approaches are emerging as an effective approach for safety filters for black-box dynamical systems. Existing methods have relied on certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi (HJ) reachability value functions. The primary motivation for our work is the recognition that ultimately, enforcing the safety constraint as a control input constraint at each state is what matters. By focusing on this constraint, we can eliminate dependence on any specific certificate function-based design. To achieve this, we define a discriminating hyperplane that shapes the half-space constraint on control input at each state, serving as a sufficient condition for safety. This concept not only generalizes over traditional safety methods but also simplifies safety filter design by eliminating dependence on specific certificate functions. We present two strategies to learn the discriminating hyperplane: (a) a supervised learning approach, using pre-verified c
&lt;/p&gt;</description></item><item><title>AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: &#19968;&#31181;&#29992;&#20110;&#22768;&#20809;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#24863;&#30693;&#21644;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#28041;&#21450;&#24314;&#31569;&#12289;&#23433;&#20840;&#12289;&#28023;&#27915;&#32771;&#21476;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#12290;&#24694;&#21155;&#30340;&#25805;&#20316;&#26465;&#20214;&#12289;&#33030;&#24369;&#30340;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#23548;&#33322;&#25511;&#21046;&#36890;&#24120;&#23548;&#33268;&#27700;&#19979;&#33322;&#34892;&#22120;&#38480;&#21046;&#20854;&#36816;&#21160;&#33539;&#22260;&#21644;&#27979;&#37327;&#22522;&#32447;&#12290;&#22312;&#19977;&#32500;&#22330;&#26223;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30693;&#36947;&#36739;&#23567;&#30340;&#22522;&#32447;&#20250;&#22686;&#21152;&#37325;&#24314;&#38590;&#24230;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65288;AONeuS&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#19982;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#36827;&#34892;&#34701;&#21512;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#22312;&#21463;&#38480;&#22522;&#32447;&#19978;&#25429;&#33719;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;&#20986;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate tha
&lt;/p&gt;</description></item><item><title>LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.00024</link><description>&lt;p&gt;
LLaMA&#21644;ChatGPT&#23884;&#20837;&#22312;&#20998;&#23376;&#23884;&#20837;&#20013;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00024
&lt;/p&gt;
&lt;p&gt;
LLaMA&#21644;ChatGPT&#27604;&#36739;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;SMILES&#23383;&#31526;&#20018;&#23884;&#20837;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#20013;&#65292;LLaMA&#30456;&#23545;&#20110;ChatGPT&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#37322;Simplified Molecular Input Line Entry System (SMILES)&#26041;&#38754;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;SMILES&#23383;&#31526;&#20018;&#35299;&#30721;&#20026;&#21521;&#37327;&#34920;&#31034;&#65292;&#20026;&#29702;&#35299;&#21270;&#23398;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#22312;&#23884;&#20837;SMILES&#23383;&#31526;&#20018;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38598;&#20013;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#65306;&#20998;&#23376;&#24615;&#36136;&#65288;MP&#65289;&#39044;&#27979;&#21644;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#65292;&#36825;&#22312;&#33647;&#29289;&#24320;&#21457;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;LLaMA&#29983;&#25104;&#30340;SMILES&#23884;&#20837;&#22312;MP&#21644;DDI&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#20110;LLaMA&#30340;SMILES&#23884;&#20837;&#22312;&#36825;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#22312;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#24212;&#29992;LLMs&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;SMILES&#36827;&#34892;&#23884;&#20837;&#26041;&#38754;&#65292;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly recognized for their potential in the field of cheminformatics, particularly in interpreting Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs can decode SMILES strings into vector representations, providing a novel approach to understanding chemical graphs.   Methods: We investigate the performance of ChatGPT and LLaMA in embedding SMILES strings. Our evaluation focuses on two key applications: molecular property (MP) prediction and drug-drug interaction (DDI) prediction, both essential in drug development and healthcare.   Results: We find that SMILES embeddings generated using LLaMA outperform those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to existing methods in both prediction tasks.   Conclusion: The application of LLMs in cheminformatics, particularly in utilizi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2401.17583</link><description>&lt;p&gt;
&#25935;&#25463;&#20294;&#23433;&#20840;&#65306;&#23398;&#20064;&#26080;&#30896;&#25758;&#39640;&#36895;&#22235;&#36275;&#26426;&#22120;&#20154;&#34892;&#36208;
&lt;/p&gt;
&lt;p&gt;
Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#34892;&#36208;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#26469;&#23454;&#29616;&#31574;&#30053;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#21327;&#20316;&#36816;&#34892;&#30340;&#25935;&#25463;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#36895;&#19988;&#23433;&#20840;&#22320;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#34892;&#36208;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#24517;&#39035;&#26082;&#25935;&#25463;&#20197;&#25552;&#39640;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#65292;&#21448;&#35201;&#30830;&#20445;&#23433;&#20840;&#65292;&#36991;&#20813;&#19982;&#38556;&#30861;&#29289;&#25110;&#20154;&#30896;&#25758;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35201;&#20040;&#24320;&#21457;&#20445;&#23432;&#30340;&#25511;&#21046;&#22120;&#65288;&#36895;&#24230;&#23567;&#20110;1.0 m/s&#65289;&#20197;&#30830;&#20445;&#23433;&#20840;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#25935;&#25463;&#24615;&#32780;&#26410;&#32771;&#34385;&#28508;&#22312;&#33268;&#21629;&#30340;&#30896;&#25758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#20294;&#23433;&#20840;&#65288;ABS&#65289;&#30340;&#23398;&#20064;&#25511;&#21046;&#26694;&#26550;&#65292;&#20026;&#22235;&#36275;&#26426;&#22120;&#20154;&#23454;&#29616;&#20102;&#25935;&#25463;&#19988;&#26080;&#30896;&#25758;&#30340;&#34892;&#36208;&#12290;ABS&#21253;&#25324;&#19968;&#20010;&#25935;&#25463;&#31574;&#30053;&#26469;&#22312;&#38556;&#30861;&#29289;&#20013;&#25191;&#34892;&#28789;&#27963;&#30340;&#21160;&#20316;&#25216;&#33021;&#65292;&#24182;&#19988;&#26377;&#19968;&#20010;&#24674;&#22797;&#31574;&#30053;&#26469;&#36991;&#20813;&#22833;&#36133;&#65292;&#20849;&#21516;&#23454;&#29616;&#39640;&#36895;&#19988;&#26080;&#30896;&#25758;&#30340;&#23548;&#33322;&#12290;ABS&#20013;&#30340;&#31574;&#30053;&#20999;&#25442;&#30001;&#19968;&#20010;&#23398;&#20064;&#24471;&#21040;&#30340;&#25511;&#21046;&#35770;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#25511;&#21046;&#65292;&#35813;&#32593;&#32476;&#20063;&#25351;&#23548;&#24674;&#22797;&#31574;&#30053;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#38381;&#29615;&#20013;&#20445;&#25252;&#26426;&#22120;&#20154;&#12290;&#35757;&#32451;&#36807;&#31243;&#28041;&#21450;&#25935;&#25463;&#31574;&#30053;&#12289;&#21040;&#36798;-&#36991;&#20813;&#20540;&#32593;&#32476;&#12289;&#24674;&#22797;&#31574;&#30053;&#21644;&#22806;&#24863;&#30693;&#34920;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (&lt; 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#26032;&#22411;&#30340;&#36830;&#32493;&#21464;&#37327;&#23884;&#20837;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20449;&#24687;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2311.11343</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#36830;&#32493;&#21464;&#37327;&#30340;&#26032;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#26032;&#22411;&#30340;&#36830;&#32493;&#21464;&#37327;&#23884;&#20837;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20449;&#24687;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#24555;&#36895;&#21407;&#22411;&#21046;&#20316;&#20855;&#26377;&#25152;&#38656;&#24615;&#33021;&#30340;&#26448;&#26009;&#30340;&#25361;&#25112;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#24494;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#32473;&#23450;&#24615;&#33021;&#23547;&#25214;&#24494;&#32467;&#26500;&#36890;&#24120;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#38382;&#39064;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#23558;&#36830;&#32493;&#23646;&#24615;&#21464;&#37327;&#20316;&#20026;&#27169;&#22411;&#30340;&#26465;&#20214;&#36755;&#20837;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#23558;&#20854;&#19982;&#19968;&#31181;&#22522;&#20110;&#28014;&#28857;&#25968;&#30340;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#23884;&#20837;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20102;&#20449;&#24687;&#65292;&#24182;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26465;&#20214;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11343v2 Announce Type: replace  Abstract: In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#36890;&#36807;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14732</link><description>&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#30721;&#20070;&#30340;&#27531;&#20313;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Quantization with Implicit Neural Codebooks. (arXiv:2401.14732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#36890;&#36807;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30690;&#37327;&#37327;&#21270;&#26159;&#25968;&#25454;&#21387;&#32553;&#21644;&#30690;&#37327;&#25628;&#32034;&#30340;&#22522;&#26412;&#25805;&#20316;&#12290;&#20026;&#20102;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#22810;&#30721;&#20070;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#30721;&#20070;&#20013;&#30340;&#30721;&#23383;&#26469;&#34920;&#31034;&#27599;&#20010;&#30690;&#37327;&#26469;&#22686;&#21152;&#36895;&#29575;&#12290;&#27531;&#20313;&#37327;&#21270;&#65288;RQ&#65289;&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37327;&#21270;&#19978;&#19968;&#27493;&#30340;&#35823;&#24046;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35823;&#24046;&#20998;&#24067;&#20381;&#36182;&#20110;&#20808;&#21069;&#36873;&#25321;&#30340;&#30721;&#23383;&#65292;&#22312;&#20256;&#32479;RQ&#20013;&#26410;&#23545;&#27492;&#36827;&#34892;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#22312;&#27599;&#20010;&#37327;&#21270;&#27493;&#39588;&#20013;&#20351;&#29992;&#36890;&#29992;&#30721;&#20070;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QINCo&#65292;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27531;&#20313;&#37327;&#21270;&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#27599;&#20010;&#30690;&#37327;&#30340;&#19987;&#38376;&#30721;&#20070;&#65292;&#26465;&#20214;&#26159;&#20808;&#21069;&#27493;&#39588;&#30340;&#21521;&#37327;&#36817;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#30721;&#20070;&#22823;&#23567;&#19978;&#65292;QINCo&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#24456;&#22810;&#12290;&#20363;&#22914;&#65292;QINCo&#20351;&#29992;12&#23383;&#33410;&#30340;&#30721;&#23383;&#22312;BigANN&#19978;&#27604;&#20351;&#29992;16&#23383;&#33410;&#30340;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods increase the rate by representing each vector using codewords across multiple codebooks. Residual quantization (RQ) is one such method, which increases accuracy by iteratively quantizing the error of the previous step. The error distribution is dependent on previously selected codewords. This dependency is, however, not accounted for in conventional RQ as it uses a generic codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant which predicts specialized codebooks per vector using a neural network that is conditioned on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12 bytes codes than other methods using 16 bytes on the BigANN a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28779;&#26143;&#36827;&#20837;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#25552;&#39640;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14411</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22823;&#27668;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#28779;&#26143;&#31934;&#20934;&#36827;&#20837;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks. (arXiv:2401.14411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14411
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#28779;&#26143;&#36827;&#20837;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23454;&#26102;&#21442;&#25968;&#35843;&#25972;&#65292;&#20197;&#25552;&#39640;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#28779;&#26143;&#22823;&#27668;&#23494;&#24230;&#19982;&#26426;&#36733;&#23494;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#20250;&#20005;&#37325;&#24433;&#21709;&#33322;&#22825;&#22120;&#36827;&#20837;&#23548;&#33322;&#28388;&#27874;&#22120;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28779;&#26143;&#36827;&#20837;&#22312;&#32447;&#28388;&#27874;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22823;&#27668;&#23494;&#24230;&#65292;&#24182;&#21033;&#29992;&#19968;&#31181;&#32771;&#34385;&#20998;&#26512;&#26469;&#32771;&#34385;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#32593;&#32476;&#20197;&#25351;&#25968;&#22823;&#27668;&#23494;&#24230;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#23427;&#30340;&#21442;&#25968;&#65292;&#20197;&#36866;&#24212;&#30495;&#23454;&#23494;&#24230;&#19982;&#20272;&#35745;&#23494;&#24230;&#20043;&#38388;&#30340;&#20219;&#20309;&#19981;&#21305;&#37197;&#12290;&#32593;&#32476;&#30340;&#35843;&#25972;&#34987;&#24418;&#24335;&#21270;&#20026;&#26368;&#22823;&#20284;&#28982;&#38382;&#39064;&#65292;&#21033;&#29992;&#28388;&#27874;&#22120;&#30340;&#27979;&#37327;&#21019;&#26032;&#26469;&#35782;&#21035;&#26368;&#20339;&#32593;&#32476;&#21442;&#25968;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#20351;&#24471;&#21487;&#20197;&#22312;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#30340;&#32972;&#26223;&#19979;&#20351;&#29992;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry by using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatches between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem, leveraging the measurement innovations of the filter to identify optimal network parameters. The incorporation of a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain within the context of the maximum likelihood approach. Performance comparisons against previous approaches are co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#27169;&#22411;&#30340;&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12254</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#30340;&#32435;&#31859;&#20809;&#23376;&#23398;&#36870;&#21521;&#24314;&#27169;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Transfer learning-assisted inverse modeling in nanophotonics based on mixture density networks. (arXiv:2401.12254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#22686;&#24378;&#30340;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#27169;&#22411;&#30340;&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#20809;&#23376;&#32467;&#26500;&#30340;&#27169;&#25311;&#20381;&#36182;&#20110;&#30005;&#30913;&#27714;&#35299;&#22120;&#65292;&#22312;&#29702;&#35299;&#20854;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27714;&#35299;&#22120;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20248;&#21270;&#31561;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#31934;&#30830;&#21644;&#39640;&#25928;&#22320;&#24314;&#27169;&#21644;&#35774;&#35745;&#20809;&#23376;&#22120;&#20214;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#29305;&#21035;&#21463;&#21040;&#20851;&#27880;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#21019;&#24314;&#21069;&#21521;&#27169;&#22411;&#21644;&#36870;&#21521;&#27169;&#22411;&#12290;&#36870;&#21521;&#24314;&#27169;&#26041;&#27861;&#36991;&#20813;&#20102;&#23558;&#21069;&#21521;&#27169;&#22411;&#19982;&#20248;&#21270;&#22120;&#32806;&#21512;&#30340;&#38656;&#27714;&#65292;&#24182;&#30452;&#25509;&#25191;&#34892;&#26368;&#20339;&#35774;&#35745;&#21442;&#25968;&#20540;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The simulation of nanophotonic structures relies on electromagnetic solvers, which play a crucial role in understanding their behavior. However, these solvers often come with a significant computational cost, making their application in design tasks, such as optimization, impractical. To address this challenge, machine learning techniques have been explored for accurate and efficient modeling and design of photonic devices. Deep neural networks, in particular, have gained considerable attention in this field. They can be used to create both forward and inverse models. An inverse modeling approach avoids the need for coupling a forward model with an optimizer and directly performs the prediction of the optimal design parameters values.  In this paper, we propose an inverse modeling method for nanophotonic structures, based on a mixture density network model enhanced by transfer learning. Mixture density networks can predict multiple possible solutions at a time including their respectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#65292;&#20174;&#32780;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10313</link><description>&lt;p&gt;
&#40657;&#23458;&#25915;&#20987;&#39044;&#27979;&#22120;&#24847;&#21619;&#30528;&#40657;&#23458;&#25915;&#20987;&#27773;&#36710;&#65306;&#21033;&#29992;&#25935;&#24863;&#24615;&#20998;&#26512;&#35782;&#21035;&#33258;&#21160;&#39550;&#39542;&#23433;&#20840;&#20013;&#30340;&#36712;&#36857;&#39044;&#27979;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to Identify Trajectory Prediction Vulnerabilities for Autonomous Driving Security. (arXiv:2401.10313v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#65292;&#20174;&#32780;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22522;&#20110;&#23398;&#20064;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;&#30340;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20294;&#26159;&#20851;&#20110;&#38500;&#20102;&#29366;&#24577;&#21382;&#21490;&#20197;&#22806;&#30340;&#36712;&#36857;&#39044;&#27979;&#22120;&#36755;&#20837;&#30340;&#25200;&#21160;&#25928;&#26524;&#20197;&#21450;&#36825;&#20123;&#25915;&#20987;&#23545;&#19979;&#28216;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#24433;&#21709;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;Trajectron++&#21644;AgentFormer&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#25152;&#26377;&#30340;&#36755;&#20837;&#20013;&#65292;Trajectron++&#30340;&#20960;&#20046;&#25152;&#26377;&#25200;&#21160;&#25935;&#24863;&#24615;&#20165;&#38480;&#20110;&#26368;&#36817;&#30340;&#29366;&#24577;&#21382;&#21490;&#26102;&#38388;&#28857;&#65292;&#32780;AgentFormer&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#21017;&#20998;&#24067;&#22312;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29366;&#24577;&#21382;&#21490;&#20013;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#23613;&#31649;&#23545;&#29366;&#24577;&#21382;&#21490;&#30340;&#25200;&#21160;&#20855;&#26377;&#20027;&#23548;&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#20351;&#29992;&#24555;&#36895;&#26799;&#24230;&#31526;&#21495;&#27861;&#21046;&#20316;&#30340;&#19981;&#21487;&#26816;&#27979;&#30340;&#22270;&#20687;&#22320;&#22270;&#25200;&#21160;&#21487;&#20197;&#23548;&#33268;&#36825;&#20004;&#20010;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22270;&#20687;&#22320;&#22270;&#23545;&#20110;&#36825;&#20004;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#21487;&#33021;&#21482;&#26377;&#36731;&#24494;&#30340;&#36129;&#29486;&#65292;&#20294;&#36825;&#20010;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#25200;&#21160;&#22270;&#20687;&#22320;&#22270;&#26469;&#30772;&#22351;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on learning-based trajectory predictors have already been demonstrated. However, there are still open questions about the effects of perturbations on trajectory predictor inputs other than state histories, and how these attacks impact downstream planning and control. In this paper, we conduct a sensitivity analysis on two trajectory prediction models, Trajectron++ and AgentFormer. We observe that between all inputs, almost all of the perturbation sensitivities for Trajectron++ lie only within the most recent state history time point, while perturbation sensitivities for AgentFormer are spread across state histories over time. We additionally demonstrate that, despite dominant sensitivity on state history perturbations, an undetectable image map perturbation made with the Fast Gradient Sign Method can induce large prediction error increases in both models. Even though image maps may contribute slightly to the prediction output of both models, this result reveals that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09507</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#24418;&#29366;&#26657;&#20934;&#65306;&#22312;&#32447;&#24191;&#21578;&#20013;&#30340;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Ensemble Shape Calibration: Multi-Field Post-hoc Calibration in Online Advertising. (arXiv:2401.09507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09507
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#22810;&#22330;&#21518;&#22788;&#29702;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#22330;&#26223;&#20013;&#65292;&#20272;&#35745;CTR&#21644;CVR&#30340;&#30495;&#23454;&#27010;&#29575;&#65288;&#31216;&#20026;&#26657;&#20934;&#20272;&#35745;&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#30452;&#25509;&#24433;&#21709;&#20080;&#26041;&#12289;&#21334;&#26041;&#21644;&#24179;&#21488;&#30340;&#21033;&#30410;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#39564;&#35777;&#38598;&#35757;&#32451;&#26657;&#20934;&#22120;&#65292;&#24182;&#38543;&#21518;&#22312;&#22312;&#32447;&#25512;&#26029;&#36807;&#31243;&#20013;&#24212;&#29992;&#36825;&#20123;&#26657;&#20934;&#22120;&#26469;&#20462;&#27491;&#21407;&#22987;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#22330;&#26223;&#30340;&#25361;&#25112;&#22312;&#20110;&#22810;&#22330;&#26657;&#20934;&#12290;&#22810;&#22330;&#26657;&#20934;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#38382;&#39064;&#65306;&#25968;&#20540;&#26657;&#20934;&#21644;&#24418;&#29366;&#26657;&#20934;&#12290;&#25968;&#20540;&#26657;&#20934;&#34987;&#23450;&#20041;&#20026;&#27599;&#20010;&#20851;&#27880;&#39046;&#22495;&#19979;&#27599;&#20010;&#25968;&#20540;&#30340;&#19981;&#36807;&#24230;&#25110;&#19981;&#20302;&#20272;&#12290;&#24418;&#29366;&#26657;&#20934;&#34987;&#23450;&#20041;&#20026;&#22312;&#20851;&#27880;&#39046;&#22495;&#26465;&#20214;&#19979;&#29305;&#23450;&#33539;&#22260;&#20869;&#27599;&#20010;pCTR&#23376;&#38598;&#30340;&#19981;&#36807;&#24230;&#25110;&#19981;&#20302;&#20272;&#12290;&#20026;&#20102;&#23454;&#29616;&#24418;&#29366;&#26657;&#20934;&#21644;&#25968;&#20540;&#26657;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#24418;&#29366;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the e-commerce advertising scenario, estimating the true probabilities (known as a calibrated estimate) on CTR and CVR is critical and can directly affect the benefits of the buyer, seller and platform. Previous research has introduced numerous solutions for addressing the calibration problem. These methods typically involve the training of calibrators using a validation set and subsequently applying these calibrators to correct the original estimated values during online inference. However, what sets e-commerce advertising scenarios is the challenge of multi-field calibration. Multi-field calibration can be subdivided into two distinct sub-problems: value calibration and shape calibration. Value calibration is defined as no over- or under-estimation for each value under concerned fields. Shape calibration is defined as no over- or under-estimation for each subset of the pCTR within the specified range under condition of concerned fields. In order to achieve shape calibration and va
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.07656</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24615;&#33021;&#26356;&#22909;&#30340;POMDP&#31574;&#30053;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#35760;&#24518;&#12290;&#19968;&#31181;&#34920;&#31034;&#36825;&#31181;&#35760;&#24518;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;L*&#31639;&#27861;&#23398;&#20064;&#31574;&#30053;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#31574;&#30053;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#20307;&#31215;&#26174;&#33879;&#26356;&#23567;&#65292;&#22240;&#27492;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#19982;&#30452;&#25509;&#20174;POMDP&#21512;&#25104;&#33258;&#21160;&#26426;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#21487;&#27604;&#25311;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.02041</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26368;&#36817;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#25152;&#25215;&#35834;&#30340;&#20248;&#21183;&#20381;&#36182;&#20110;&#23558;&#37327;&#23376;&#25805;&#20316;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#29289;&#29702;&#23454;&#29616;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#20197;&#20419;&#36827;&#36825;&#31181;&#36716;&#21270;&#12290;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DMs&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#65292;&#36825;&#26159;&#20808;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#12290;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30005;&#36335;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25903;&#25345;&#20856;&#22411;&#30340;DM&#25193;&#23637;&#65292;&#20363;&#22914;&#25513;&#30721;&#21644;&#32534;&#36753;&#65292;&#20197;&#20351;&#30005;&#36335;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#37327;&#23376;&#35774;&#22791;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#37327;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
&lt;/p&gt;</description></item><item><title>MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09833</link><description>&lt;p&gt;
MIR2:&#38754;&#21521;&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#21487;&#35777;&#26126;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization. (arXiv:2310.09833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09833
&lt;/p&gt;
&lt;p&gt;
MIR2&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#24182;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#25552;&#21319;&#40065;&#26834;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#23545;&#20110;&#26410;&#30693;&#30431;&#21451;&#30340;&#19981;&#30830;&#23450;&#25110;&#26368;&#22351;&#24773;&#20917;&#34892;&#21160;&#38656;&#35201;&#20855;&#22791;&#24377;&#24615;&#12290;&#29616;&#26377;&#30340;&#40065;&#26834;MARL&#20013;&#30340;&#26368;&#22823;&#26368;&#23567;&#20248;&#21270;&#25216;&#26415;&#36890;&#36807;&#35757;&#32451;&#26234;&#33021;&#20307;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#30340;&#23545;&#25163;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#38590;&#20197;&#25805;&#20316;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#35797;&#22270;&#31616;&#21270;&#36825;&#31181;&#22797;&#26434;&#24615;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#20110;&#24754;&#35266;&#30340;&#31574;&#30053;&#12289;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#40065;&#26834;&#24615;&#19981;&#36275;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#20154;&#31867;&#22312;&#23398;&#20064;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#34892;&#20026;&#26102;&#33258;&#28982;&#32780;&#28982;&#22320;&#19981;&#38656;&#35201;&#20934;&#22791;&#27599;&#31181;&#21487;&#33021;&#30340;&#26368;&#22351;&#24773;&#20917;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIR2&#65292;&#23427;&#22312;&#24120;&#35268;&#24773;&#20917;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#20316;&#20026;&#40065;&#26834;&#27491;&#21017;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#35270;&#20026;&#19968;&#20010;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#21382;&#21490;&#21644;&#34892;&#21160;&#20043;&#38388;&#26368;&#23567;&#21270;&#20114;&#20449;&#24687;&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#40065;&#26834;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robust
&lt;/p&gt;</description></item><item><title>TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03589</link><description>&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG]) - &#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TimeGPT-1. (arXiv:2310.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03589
&lt;/p&gt;
&lt;p&gt;
TimeGPT&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#23427;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20511;&#37492;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26377;&#26395;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TimeGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#29983;&#25104;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#24050;&#24314;&#31435;&#30340;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;TimeGPT&#30340;&#38646;-shot&#25512;&#29702;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#31616;&#27905;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#30340;&#20854;&#20182;&#39046;&#22495;&#30340;&#35265;&#35299;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20026;&#20154;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#27665;&#20027;&#21270;&#35775;&#38382;&#31934;&#30830;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EnCodecMAE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#31163;&#25955;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#22810;&#20010;&#38899;&#39057;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;EnCodecMAE&#36798;&#21040;&#20102;&#19982;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07391</link><description>&lt;p&gt;
EnCodecMAE: &#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#36827;&#34892;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EnCodecMAE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#31163;&#25955;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#22810;&#20010;&#38899;&#39057;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;EnCodecMAE&#36798;&#21040;&#20102;&#19982;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#21487;&#20197;&#29992;&#20110;&#28041;&#21450;&#35821;&#38899;&#12289;&#38899;&#20048;&#25110;&#29615;&#22659;&#22768;&#38899;&#30340;&#21508;&#31181;&#21518;&#32493;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#21463;&#33258;&#30417;&#30563;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38899;&#39057;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#22240;&#27492;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#38899;&#39057;&#38656;&#35201;&#25913;&#21464;&#23398;&#20064;&#30446;&#26631;&#25110;&#23558;&#38899;&#39057;&#20449;&#21495;&#26144;&#23556;&#21040;&#19968;&#32452;&#31163;&#25955;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#30340;&#31163;&#25955;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;EnCodecMAE&#65292;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#24191;&#27867;&#38899;&#39057;&#20219;&#21153;&#19978;&#65292;&#20854;&#24615;&#33021;&#30456;&#24403;&#25110;&#20248;&#20110;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music or environmental sounds. To approach this problem, methods inspired by self-supervised models from NLP, like BERT, are often used and adapted to audio. These models rely on the discrete nature of text, hence adopting this type of approach for audio processing requires either a change in the learning objective or mapping the audio signal to a set of discrete classes. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for learning an universal audio model based on a masked autoencoder (MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of audio tasks spanning speech, music and environmental sounds, achieving performances comparable or better than leading audio representation models.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.03318</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03318
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;&#36951;&#20256;&#31639;&#27861;&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36951;&#20256;&#31639;&#27861;&#20013;&#36827;&#34892;&#36866;&#24212;&#24230;&#36817;&#20284;&#65292;&#37325;&#28857;&#26159;&#22312;Gymnasium&#65288;&#28216;&#25103;&#65289;&#27169;&#25311;&#22120;&#20013;&#30340;&#36827;&#21270;&#20195;&#29702;&#19978; - &#22312;&#36825;&#37324;&#36866;&#24212;&#24230;&#35745;&#31639;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#32500;&#25252;&#19968;&#20010;&#37319;&#26679;&#20010;&#20307;&#21450;&#20854;&#23454;&#38469;&#36866;&#24212;&#24230;&#24471;&#20998;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#25972;&#20010;&#36827;&#21270;&#36807;&#31243;&#20013;&#19981;&#26029;&#26356;&#26032;&#19968;&#20010;&#36866;&#24212;&#24230;&#36817;&#20284;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;1&#65289;&#22312;&#23454;&#38469;&#36866;&#24212;&#24230;&#21644;&#36817;&#20284;&#36866;&#24212;&#24230;&#20043;&#38388;&#20999;&#25442;&#65292;2&#65289;&#23545;&#31181;&#32676;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#21450;3&#65289;&#21152;&#26435;&#37319;&#26679;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36866;&#24212;&#24230;&#35745;&#31639;&#30340;&#36817;&#20284;&#27604;&#20363;&#21462;&#20915;&#20110;&#23436;&#20840;&#36816;&#34892;GA&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36827;&#21270;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#36866;&#24212;&#24230;&#24471;&#20998;&#35201;&#20040;&#19982;&#23436;&#20840;&#36816;&#34892;&#30340;GA&#30456;&#21516;&#65292;&#35201;&#20040;&#31245;&#24494;&#20302;&#19968;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to performing fitness approximation in genetic algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary agents in Gymnasium (game) simulators -- where fitness computation is costly. Maintaining a dataset of sampled individuals along with their actual fitness scores, we continually update throughout an evolutionary run a fitness-approximation ML model. We compare different methods for: 1) switching between actual and approximate fitness, 2) sampling the population, and 3) weighting the samples. Experimental findings demonstrate significant improvement in evolutionary runtimes, with fitness scores that are either identical or slightly lower than that of the fully run GA -- depending on the ratio of approximate-to-actual-fitness computation. Our approach is generic and can be easily applied to many different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.00203</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#32447;&#24615;&#35268;&#21010;&#38477;&#32500;&#26041;&#27861;&#65306;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;LP&#65289;&#12290;&#32473;&#23450;&#36807;&#21435;&#30340;$n$&#32500;LP&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;$n\times k$&#30340;&#8220;&#25237;&#24433;&#30697;&#38453;&#8221;&#65288;$n &gt; k$&#65289;&#65292;&#23558;&#32500;&#25968;&#20174;$n$&#38477;&#20302;&#21040;$k$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;$k$&#32500;LP&#38382;&#39064;&#24182;&#36890;&#36807;&#20056;&#20197;&#25237;&#24433;&#30697;&#38453;&#26469;&#24674;&#22797;$n$&#32500;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#26410;&#26469;&#30340;LP&#23454;&#20363;&#12290;&#36825;&#20010;&#24605;&#24819;&#19982;&#20219;&#20309;&#29992;&#25143;&#39318;&#36873;&#30340;LP&#27714;&#35299;&#22120;&#20860;&#23481;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;LP&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#25165;&#33021;&#30830;&#20445;&#24674;&#22797;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#30340;&#24605;&#24819;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#23558;&#36275;&#22815;&#36827;&#34892;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#8220;&#20266;&#32500;&#24230;&#8221;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20266;&#32500;&#24230;&#30340;$\tilde{\mathrm{O}}(nk^2)$&#19978;&#30028;&#65288;$\tilde{\mathrm{O}}$&#21387;&#32553;&#20102;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;$\Omega(nk)$&#19979;&#30028;&#26469;&#34917;&#20805;&#23427;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n &gt; k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence 
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.03813</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#30340;&#20302;&#20998;&#36776;&#29575;&#28857;&#20113;&#23436;&#25104;&#21464;&#25442;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers. (arXiv:2308.03813v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39045;&#32570;&#25439;&#37325;&#24314;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#37117;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#20154;&#36973;&#21463;&#21508;&#31181;&#31867;&#22411;&#30340;&#39045;&#39592;&#20260;&#23475;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#26893;&#20837;&#29289;&#65292;&#25163;&#24037;&#35774;&#35745;&#26114;&#36149;&#19988;&#36153;&#26102;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19987;&#29992;&#31995;&#32479;&#26469;&#22686;&#21152;&#20010;&#24615;&#21270;&#39045;&#39592;&#37325;&#24314;&#30340;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#24517;&#35201;&#12290;&#33258;&#21160;&#39045;&#39592;&#32570;&#25439;&#37325;&#24314;&#30340;&#38382;&#39064;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#24418;&#29366;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#19987;&#29992;&#28145;&#24230;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30446;&#21069;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20307;&#31215;&#34920;&#31034;&#27861;&#24182;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#20998;&#36776;&#29575;&#20307;&#31215;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#28857;&#20113;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#20998;&#36776;&#29575;&#19979;&#37325;&#24314;&#39045;&#32570;&#25439;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#19988;&#36164;&#28304;&#39640;&#25928;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.02058</link><description>&lt;p&gt;
&#25972;&#21512;&#40065;&#33725;&#34892;&#20026;&#21040;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40065;&#33725;&#34892;&#20026;&#24341;&#20837;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#25512;&#33616;&#31995;&#32479;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#39118;&#38505;&#27700;&#24179;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#21487;&#38752;&#24615;&#27979;&#37327;&#30340;&#25512;&#33616;&#31995;&#32479;&#24448;&#24448;&#22312;&#39044;&#27979;&#20013;&#26356;&#21152;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#36825;&#23548;&#33268;&#20102;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#26032;&#39062;&#24615;&#30340;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#30697;&#38453;&#20998;&#35299;&#22411;&#25512;&#33616;&#31995;&#32479;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#39033;&#26032;&#30340;&#39033;&#65292;&#31216;&#20026;&#40065;&#33725;&#34892;&#20026;&#65292;&#23427;&#21487;&#20197;&#25511;&#21046;&#22312;&#20570;&#20986;&#20851;&#20110;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20915;&#31574;&#26102;&#25152;&#24076;&#26395;&#30340;&#39118;&#38505;&#27700;&#24179;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#40065;&#33725;&#34892;&#20026;&#19981;&#20165;&#20801;&#35768;&#36827;&#34892;&#39118;&#38505;&#35843;&#25511;&#65292;&#36824;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#30340;&#39044;&#27979;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.06328</link><description>&lt;p&gt;
Offline RL&#30340;&#39044;&#31639;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06328
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#38480;&#21046;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#28508;&#22312;&#21160;&#20316;&#39046;&#22495;&#20869;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#22256;&#22659;&#25152;&#24341;&#36215;&#65306;&#22914;&#26524;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#30340;&#34892;&#21160;&#20250;&#24590;&#20040;&#26679;&#65311;&#36825;&#20123;&#24773;&#20917;&#36890;&#24120;&#20250;&#23548;&#33268;&#25351;&#25968;&#32423;&#32047;&#31215;&#30340;&#22806;&#25512;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#35748;&#35782;&#21040;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#20915;&#31574;&#27493;&#39588;&#23545;&#26368;&#32456;&#32467;&#26524;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#24182;&#22312;&#25919;&#31574;&#21046;&#23450;&#20013;&#39044;&#31639;&#21453;&#20107;&#23454;&#20915;&#31574;&#30340;&#25968;&#37327;&#20197;&#25511;&#21046;&#22806;&#25512;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25919;&#31574;&#25110;&#20540;&#20989;&#25968;&#19978;&#20351;&#29992;&#35268;&#21017;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26126;&#30830;&#38480;&#21046;&#35757;&#32451;&#26399;&#38388;&#30340;&#36229;&#20986;&#20998;&#24067;&#21160;&#20316;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#26469;&#20915;&#23450;&#22312;&#21738;&#37324;&#36827;&#34892;&#22806;&#25512;&#21644;&#22312;&#21738;&#37324;&#19981;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#19988;&#23545;&#20915;&#31574;&#30340;&#19978;&#38480;&#19981;&#21516;&#20110;&#34892;&#20026;&#31574;&#30053;&#12290;&#23427;&#22312;&#28508;&#22312;&#25913;&#36827;&#30340;&#28508;&#21147;&#21644;&#22806;&#25512;&#25511;&#21046;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge of offline reinforcement learning, where data is limited, arises from a sequence of counterfactual reasoning dilemmas within the realm of potential actions: What if we were to choose a different course of action? These circumstances frequently give rise to extrapolation errors, which tend to accumulate exponentially with the problem horizon. Hence, it becomes crucial to acknowledge that not all decision steps are equally important to the final outcome, and to budget the number of counterfactual decisions a policy make in order to control the extrapolation. Contrary to existing approaches that use regularization on either the policy or value function, we propose an approach to explicitly bound the amount of out-of-distribution actions during training. Specifically, our method utilizes dynamic programming to decide where to extrapolate and where not to, with an upper bound on the decisions different from behavior policy. It balances between the potential for improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02752</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2307.02752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;CQL&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#26469;&#25552;&#21462;&#31574;&#30053;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#20013;&#23545;&#22522;&#20934;&#30340;&#26222;&#36941;&#20351;&#29992;&#23548;&#33268;&#20102;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24573;&#35270;&#12290;&#30001;&#20110;&#25506;&#32034;&#25110;&#23433;&#20840;&#32771;&#34385;&#30340;&#25361;&#25112;&#65292;&#23454;&#38469;&#31163;&#32447;RL&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#31354;&#38388;&#19978;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20855;&#20307;&#35828;&#26126;&#20102;&#31163;&#32447;RL&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#29366;&#24577;&#35206;&#30422;&#29575;&#36981;&#24490;&#19968;&#20010;&#30001;&#20559;&#24577;&#31574;&#30053;&#25152;&#29305;&#24449;&#21270;&#30340;&#24130;&#24459;&#20998;&#24067;&#12290;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#20998;&#24067;&#32422;&#26463;&#30340;&#20856;&#22411;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;&#20445;&#23432;Q&#23398;&#20064;&#65288;CQL&#65289;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#25552;&#21462;&#31574;&#30053;&#26159;&#26080;&#25928;&#30340;&#12290;&#21463;&#33258;&#28982;&#26234;&#33021;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;CQL&#30340;&#22686;&#24378;&#19982;&#22238;&#28335;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#22238;&#24518;&#20197;&#24448;&#30456;&#20851;&#32463;&#39564;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent use of benchmarks in current offline reinforcement learning (RL) research has led to a neglect of the imbalance of real-world dataset distributions in the development of models. The real-world offline RL dataset is often imbalanced over the state space due to the challenge of exploration or safety considerations. In this paper, we specify properties of imbalanced datasets in offline RL, where the state coverage follows a power law distribution characterized by skewed policies. Theoretically and empirically, we show that typically offline RL methods based on distributional constraints, such as conservative Q-learning (CQL), are ineffective in extracting policies under the imbalanced dataset. Inspired by natural intelligence, we propose a novel offline RL method that utilizes the augmentation of CQL with a retrieval process to recall past related experiences, effectively alleviating the challenges posed by imbalanced datasets. We evaluate our method on several tasks in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#28151;&#21512;&#24577;&#37325;&#26500;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#19979;&#19981;&#21516;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26356;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.01840</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#29366;&#24577;&#37325;&#26500;&#30340;&#32463;&#39564;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Empirical Sample Complexity of Neural Network Mixed State Reconstruction. (arXiv:2307.01840v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#28151;&#21512;&#24577;&#37325;&#26500;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#19979;&#19981;&#21516;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#30340;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#35774;&#35745;&#26356;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#37327;&#23376;&#24577;&#36827;&#34892;&#37327;&#23376;&#29366;&#24577;&#37325;&#26500;&#34987;&#25552;&#20986;&#20316;&#20026;&#20943;&#23569;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37327;&#23376;&#20987;&#31359;&#22797;&#26434;&#24615;&#30340;&#21487;&#34892;&#24037;&#20855;&#65292;&#24182;&#19988;&#22312;&#20027;&#35201;&#20851;&#27880;&#26080;&#22122;&#22768;&#24773;&#20917;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28151;&#21512;&#24577;&#65306;&#26377;&#38480;&#28201;&#24230;&#20234;&#36763;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#25968;&#23383;&#19978;&#30340;&#24615;&#33021;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31995;&#32479;&#22320;&#20943;&#23569;&#31639;&#27861;&#30340;&#37327;&#23376;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#31070;&#32463;&#37327;&#23376;&#24577;&#32534;&#30721;&#65292;&#21363;&#31070;&#32463;&#23494;&#24230;&#31639;&#31526;&#21644;&#27491;&#31639;&#31526;&#20540;&#27979;&#37327;&#34920;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#24577;&#28151;&#21512;&#31243;&#24230;&#21464;&#21270;&#26102;&#30340;&#19981;&#21516;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19981;&#21516;&#28151;&#21512;&#24230;&#33539;&#22260;&#20869;&#65292;&#26576;&#20123;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#25351;&#20986;&#35774;&#35745;&#26356;&#21152;&#39640;&#25928;&#32534;&#30721;&#30340;&#38656;&#27714;&#65292;&#26080;&#35770;&#26159;&#20174;&#35745;&#31639;&#22797;&#26434;&#24615;&#36824;&#26159;&#20174;&#24615;&#33021;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum state reconstruction using Neural Quantum States has been proposed as a viable tool to reduce quantum shot complexity in practical applications, and its advantage over competing techniques has been shown in numerical experiments focusing mainly on the noiseless case. In this work, we numerically investigate the performance of different quantum state reconstruction techniques for mixed states: the finite-temperature Ising model. We show how to systematically reduce the quantum resource requirement of the algorithms by applying variance reduction techniques. Then, we compare the two leading neural quantum state encodings of the state, namely, the Neural Density Operator and the positive operator-valued measurement representation, and illustrate their different performance as the mixedness of the target state varies. We find that certain encodings are more efficient in different regimes of mixedness and point out the need for designing more efficient encodings in terms of both cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#22810;&#32452;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-omics Prediction from High-content Cellular Imaging with Deep Learning. (arXiv:2306.09391v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#20013;&#30452;&#25509;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#23454;&#29616;&#26174;&#33879;&#25104;&#26524;&#65292;&#20026;&#32454;&#32990;&#32452;&#23398;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20869;&#23481;&#32454;&#32990;&#25104;&#20687;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34507;&#30333;&#36136;&#32452;&#23398;&#25968;&#25454;&#20026;&#24433;&#21709;&#32454;&#32990;&#29366;&#24577;&#21644;&#21151;&#33021;&#30340;&#29983;&#29289;&#20998;&#23376;&#23618;&#25552;&#20379;&#20102;&#20016;&#23500;&#21644;&#20114;&#34917;&#30340;&#35270;&#35282;&#12290;&#20294;&#26159;&#65292;&#23578;&#26410;&#31995;&#32479;&#22320;&#25506;&#35752;&#22810;&#32452;&#23398;&#27979;&#37327;&#20540;&#24433;&#21709;&#32454;&#32990;&#24418;&#24577;&#30340;&#29983;&#29289;&#23398;&#20915;&#23450;&#22240;&#32032;&#65292;&#22240;&#27492;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#32454;&#32990;&#25104;&#20687;&#26159;&#21542;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22810;&#32452;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;Image2Omics&#8212;&#8212;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#30452;&#25509;&#20174;&#29992;&#22810;&#37325;&#33639;&#20809;&#26579;&#26009;&#26579;&#33394;&#30340;&#39640;&#20869;&#23481;&#22270;&#20687;&#20013;&#39044;&#27979;&#32454;&#32990;&#32676;&#20307;&#30340;&#22810;&#32452;&#23398;&#26159;&#21542;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#21050;&#28608;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#35825;&#23548;&#22810;&#33021;&#24178;&#32454;&#32990;&#65288;hiPSC&#65289;&#34893;&#29983;&#30340;&#22522;&#22240;&#32534;&#36753;&#24040;&#22124;&#32454;&#32990;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;Image2Omics&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-content cellular imaging, transcriptomics, and proteomics data provide rich and complementary views on the molecular layers of biology that influence cellular states and function. However, the biological determinants through which changes in multi-omics measurements influence cellular morphology have not yet been systematically explored, and the degree to which cell imaging could potentially enable the prediction of multi-omics directly from cell imaging data is therefore currently unclear. Here, we address the question of whether it is possible to predict bulk multi-omics measurements directly from cell images using Image2Omics -- a deep learning approach that predicts multi-omics in a cell population directly from high-content images stained with multiplexed fluorescent dyes. We perform an experimental evaluation in gene-edited macrophages derived from human induced pluripotent stem cell (hiPSC) under multiple stimulation conditions and demonstrate that Image2Omics achieves sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.13314</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Simplifying Full Waveform Inversion via Domain-Independent Self-Supervised Learning. (arXiv:2305.13314v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#20013;&#24212;&#29992;&#24471;&#21040;&#25104;&#21151;&#65292;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#29616;&#35937;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21508;&#33258;&#30340;&#39046;&#22495;&#20013;&#20998;&#21035;&#35757;&#32451;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#36328;&#39046;&#22495;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SimFWI&#65292;&#19968;&#20010;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#30340;&#26032;&#33539;&#24335;&#65306;(a)&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#20998;&#21035;&#23398;&#20064;&#22320;&#38663;&#32534;&#30721;&#22120;&#21644;&#36895;&#24230;&#35299;&#30721;&#22120;&#65307;(b)&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Geophysics has witnessed success in applying deep learning to one of its core problems: full waveform inversion (FWI) to predict subsurface velocity maps from seismic data. It is treated as an image-to-image translation problem, jointly training an encoder for seismic data and a decoder for the velocity map from seismic-velocity pairs. In this paper, we report a surprising phenomenon: when training an encoder and decoder separately in their own domains via self-supervised learning, a linear relationship is observed across domains in the latent spaces. Moreover, this phenomenon connects multiple FWI datasets in an elegant manner: these datasets can share the self-learned encoder and decoder with different linear mappings.  Based on these findings, we develop SimFWI, a new paradigm that includes two steps: (a) learning a seismic encoder and a velocity decoder separately by masked image modeling over multiple datasets; (b) learning a linear mapping per dataset. Experimental results show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11512</link><description>&lt;p&gt;
&#20016;&#23500;&#35299;&#32544;&#32467;&#65306;&#20174;&#23450;&#20041;&#21040;&#24230;&#37327;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enriching Disentanglement: Definitions to Metrics. (arXiv:2305.11512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#20998;&#31163;&#22810;&#20010;&#21464;&#21270;&#22240;&#32032;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#23398;&#20064;&#21644;&#35780;&#20272;&#35299;&#32544;&#32467;&#34920;&#31034;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#30495;&#27491;&#37327;&#21270;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27604;&#36739;&#23427;&#20204;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19968;&#38454;&#26041;&#31243;&#35859;&#35789;&#23450;&#20041;&#30340;&#35299;&#32544;&#32467;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#20860;&#23481;&#30340;&#23450;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#24230;&#37327;&#25110;&#31163;&#25955;&#24230;&#26367;&#25442;(i) &#31561;&#24335;&#65292;&#29992;&#25490;&#24207;&#25805;&#20316;&#26367;&#25442; (ii) &#36923;&#36753;&#32852;&#32467;&#35789;&#65292;&#29992;&#32858;&#21512;&#26367;&#25442; (iii) &#36890;&#29992;&#37327;&#35789;&#65292;&#29992;&#26368;&#20339;&#36924;&#36817;&#26367;&#25442; (iv) &#23384;&#22312;&#37327;&#35789;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#34920;&#31034;&#25552;&#21462;&#22120;&#25152;&#38656;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#35299;&#32544;&#32467;&#34920;&#31034;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#65292;&#24182;&#34913;&#37327;&#19968;&#31181;&#26041;&#27861;&#20013;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled representation learning is a challenging task that involves separating multiple factors of variation in complex data. Although various metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what these metrics truly quantify and how to compare them. In this work, we study the definitions of disentanglement given by first-order equational predicates and introduce a systematic approach for transforming an equational definition into a compatible quantitative metric based on enriched category theory. Specifically, we show how to replace (i) equality with metric or divergence, (ii) logical connectives with order operations, (iii) universal quantifier with aggregation, and (iv) existential quantifier with the best approximation. Using this approach, we derive metrics for measuring the desired properties of a disentangled representation extractor and demonstrate their effectiveness on synthetic data. Our proposed approach provides p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11265</link><description>&lt;p&gt;
&#25163;&#33109;&#21160;&#20316;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20855;&#26377;&#39057;&#32321;&#21464;&#21270;&#30340;&#36816;&#21160;&#30151;&#29366;&#65292;&#25345;&#32493;&#30340;&#30151;&#29366;&#30417;&#27979;&#21487;&#20197;&#23454;&#29616;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#27835;&#30103;&#12290;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;PD&#30151;&#29366;&#30417;&#27979;&#26102;&#24615;&#33021;&#26377;&#38480;&#65292;&#22240;&#20026;PD&#36816;&#21160;&#27169;&#24335;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#20294;&#25968;&#25454;&#38598;&#24456;&#23567;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;InceptionTime&#21644;RandOm&#21367;&#31215;&#26680;&#21464;&#25442;&#65288;ROCKET&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;TSC&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;&#20110;PD&#30151;&#29366;&#30417;&#27979;&#38750;&#24120;&#26377;&#21069;&#26223;&#65306;InceptionTime&#30340;&#39640;&#23398;&#20064;&#33021;&#21147;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;ROCKET&#36866;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25628;&#32034;&#25214;&#21040;&#20102;&#26368;&#39640;&#24471;&#20998;&#30340;InceptionTime&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;ROCKET&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;PD&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;TSC&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurodegenerative disease with frequently changing motor symptoms where continuous symptom monitoring enables more targeted treatment. Classical time series classification (TSC) and deep learning techniques have limited performance for PD symptom monitoring using wearable accelerometer data because PD movement patterns are complex, but datasets are small. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) because they are state-of-the-art for TSC and promising for PD symptom monitoring: InceptionTime's high learning capacity is suited to modeling complex movement patterns while ROCKET is suited to small datasets. We used a random search to find the highest-scoring InceptionTime architecture and compared it to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all approaches are suitable for estimating tremor severity and bradykinesia presence but struggle with detecti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2304.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;
&lt;/p&gt;
&lt;p&gt;
Data as voters: instance selection using approval-based multi-winner voting. (arXiv:2304.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#25110;&#25968;&#25454;&#25366;&#25496;&#65289;&#20013;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#36873;&#20030;&#20013;&#20195;&#34920;&#24615;&#34920;&#24449;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23454;&#20363;&#25198;&#28436;&#36873;&#27665;&#21644;&#20505;&#36873;&#20154;&#30340;&#21452;&#37325;&#35282;&#33394;&#12290;&#27599;&#20010;&#35757;&#32451;&#38598;&#20013;&#30340;&#23454;&#20363;&#65288;&#20316;&#20026;&#36873;&#27665;&#65289;&#36190;&#25104;&#20854;&#26412;&#22320;&#38598;&#21512;&#20013;&#30340;&#23454;&#20363;&#65288;&#25198;&#28436;&#20505;&#36873;&#20154;&#30340;&#35282;&#33394;&#65289;&#65288;&#38500;&#33258;&#36523;&#20197;&#22806;&#30340;&#23454;&#20363;&#65289;&#65292;&#36825;&#20010;&#27010;&#24565;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#36873;&#20030;&#33719;&#32988;&#32773;&#65292;&#24182;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#20013;&#30340;&#25968;&#25454;&#23454;&#20363;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to the instance selection problem in machine learning (or data mining). Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. Each instance in the training set (acting as a voter) approves of the instances (playing the role of candidates) belonging to its local set (except itself), a concept already existing in the literature. We then select the election winners using a representative voting rule, and such winners are the data instances kept in the reduced training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02384</link><description>&lt;p&gt;
&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Training of Deep Neural Networks Using Early Exiting. (arXiv:2303.02384v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#30340;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#20026;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36828;&#31163;&#33719;&#21462;&#25968;&#25454;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20113;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#22686;&#21152;&#20102;&#36890;&#20449;&#25104;&#26412;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#23618;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#22312;&#36793;&#32536;&#21644;&#20113;&#24037;&#20316;&#32773;&#20043;&#38388;&#20998;&#21106;&#26550;&#26500;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12289;&#35757;&#32451;&#36816;&#34892;&#26102;&#38388;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks provide state-of-the-art accuracy for vision tasks but they require significant resources for training. Thus, they are trained on cloud servers far from the edge devices that acquire the data. This issue increases communication cost, runtime and privacy concerns. In this study, a novel hierarchical training method for deep neural networks is proposed that uses early exits in a divided architecture between edge and cloud workers to reduce the communication cost, training runtime and privacy concerns. The method proposes a brand-new use case for early exits to separate the backward pass of neural networks between the edge and the cloud during the training phase. We address the issues of most available methods that due to the sequential nature of the training phase, cannot train the levels of hierarchy simultaneously or they do it with the cost of compromising privacy. In contrast, our method can use both edge and cloud workers simultaneously, does not share the raw i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.01829</link><description>&lt;p&gt;
t-SMILES&#65306;&#29992;&#20110;&#20840;&#26032;&#20998;&#23376;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#12289;&#22522;&#20110;&#30862;&#29255;&#30340;&#22810;&#23610;&#24230;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65288;&#22522;&#20110;&#26641;&#30340;SMILES&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#31639;&#27861;&#65306;TSSA&#65288;&#24102;&#26377;&#20849;&#20139;&#21407;&#23376;&#30340;t-SMILES&#65289;&#12289;TSDY&#65288;&#24102;&#26377;&#34394;&#25311;&#21407;&#23376;&#30340;t-SMILES&#65289;&#21644;TSID&#65288;&#24102;&#26377;ID&#30340;t-SMILES&#65289;&#12290;&#23427;&#20351;&#29992;&#20174;&#20998;&#23376;&#22270;&#30340;&#30862;&#29255;&#24418;&#25104;&#30340;&#20840;&#20108;&#21449;&#26641;&#19978;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#24471;&#21040;&#30340;SMILES&#31867;&#22411;&#23383;&#31526;&#20018;&#26469;&#25551;&#36848;&#20998;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;JTVAE&#12289;BRICS&#12289;MMPA&#21644;Scaffold&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#26500;&#24314;&#22810;&#20195;&#30721;&#20998;&#23376;&#25551;&#36848;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#21508;&#31181;&#25551;&#36848;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#35770;&#27169;&#22411;&#26159;&#21407;&#22987;&#30340;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#36824;&#26159;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#12290;&#23427;&#22312;goa&#31561;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;SMILES&#12289;DeepSMILES&#12289;SELFIES&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective representation of molecules is a crucial factor affecting the performance of artificial intelligence models. This study introduces a flexible, fragment-based, multiscale molecular representation framework called t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It describes molecules using SMILES-type strings obtained by performing a breadth-first search on a full binary tree formed from a fragmented molecular graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the feasibility to construct a multi-code molecular description system, where various descriptions complement each other, enhancing the overall performance. Additionally, it exhibits impressive performance on low-resource datasets, whether the model is original, data augmented, or pre-training fine-tuned. It significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline models in goa
&lt;/p&gt;</description></item></channel></rss>